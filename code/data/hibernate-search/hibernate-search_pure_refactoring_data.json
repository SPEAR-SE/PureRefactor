[
    {
        "type": "Move Method",
        "description": "Move Method\tpublic assertThrown() : AbstractThrowableAssert<?,Throwable> from class org.hibernate.search.util.impl.test.SubTest to public assertThrown() : AbstractThrowableAssert<?,Throwable> from class org.hibernate.search.util.impl.test.SubTest.ExceptionThrowingSubTest",
        "diffLocations": [
            {
                "filePath": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/SubTest.java",
                "startLine": 67,
                "endLine": 70,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/SubTest.java",
                "startLine": 86,
                "endLine": 89,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public AbstractThrowableAssert<?, Throwable> assertThrown() {\n\t\treturn new ThrowableAssert( thrown )\n\t\t\t\t.as( description );\n\t}",
        "filePathBefore": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/SubTest.java",
        "isPureRefactoring": true,
        "commitId": "f50cbfc772b75e7d7a983359b9baca383bc6ae9f",
        "packageNameBefore": "org.hibernate.search.util.impl.test",
        "classNameBefore": "org.hibernate.search.util.impl.test.SubTest",
        "methodNameBefore": "org.hibernate.search.util.impl.test.SubTest#assertThrown",
        "classSignatureBefore": "public class SubTest ",
        "methodNameBeforeSet": [
            "org.hibernate.search.util.impl.test.SubTest#assertThrown"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.util.impl.test.SubTest"
        ],
        "classSignatureBeforeSet": [
            "public class SubTest "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.test;\n\nimport static org.junit.Assert.fail;\n\nimport java.util.concurrent.Callable;\n\nimport org.junit.Test;\n\nimport org.assertj.core.api.AbstractThrowableAssert;\nimport org.assertj.core.api.ThrowableAssert;\n\n/**\n * A util allowing to run blocks of code (\"sub-tests\"), expecting them to throw an exception.\n * <p>\n * Useful in particular when expecting an exception for each execution of a loop,\n * in which case {@link org.junit.rules.ExpectedException} or {@link Test#expected()} cannot be used.\n * <p>\n * By default any thrown exception will be accepted; if you want to run additional checks on the thrown exception,\n * use {@link #assertThrown()}.\n */\npublic class SubTest {\n\n\tpublic static SubTest expectException(Runnable runnable) {\n\t\treturn expectException( runnable.toString(), runnable );\n\t}\n\n\tpublic static SubTest expectException(String description, Runnable runnable) {\n\t\treturn expectException(\n\t\t\t\tdescription,\n\t\t\t\t() -> {\n\t\t\t\t\trunnable.run();\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t);\n\t}\n\n\tpublic static SubTest expectException(Callable<?> callable) {\n\t\treturn expectException( callable.toString(), callable );\n\t}\n\n\tpublic static SubTest expectException(String description, Callable<?> callable) {\n\t\ttry {\n\t\t\tcallable.call();\n\t\t\tfail( \"'\" + description + \"' should have thrown an exception\" );\n\t\t\tthrow new IllegalStateException( \"This should never happen\" );\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\treturn new SubTest( \"Exception thrown by '\" + description + \"'\", e );\n\t\t}\n\t}\n\n\tprivate final String description;\n\n\tprivate final Throwable thrown;\n\n\tprivate SubTest(String description, Throwable thrown) {\n\t\tthis.description = description;\n\t\tthis.thrown = thrown;\n\t}\n\n\tpublic AbstractThrowableAssert<?, Throwable> assertThrown() {\n\t\treturn new ThrowableAssert( thrown )\n\t\t\t\t.as( description );\n\t}\n\n}\n",
        "filePathAfter": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/SubTest.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.test;\n\nimport static org.junit.Assert.fail;\n\nimport java.util.concurrent.Callable;\n\nimport org.junit.Test;\n\nimport org.assertj.core.api.AbstractThrowableAssert;\nimport org.assertj.core.api.ThrowableAssert;\n\n/**\n * A util allowing to run blocks of code as \"sub-tests\".\n * <p>\n * This class is useful when looping over several executions of the same set of assertions:\n * <ul>\n *     <li>\n *         When executing code that both produces and consumes instances of a different generic type T\n *         for each execution of a loop,\n *         you usually cannot write type-safe code easily, because of limitations and how generics work,\n *         but you can with {@link #expectSuccess(Object, ParameterizedSubTest)}.\n *     </li>\n *     <li>\n *         When expecting an exception for each execution of a loop,\n *         you cannot use {@link org.junit.rules.ExpectedException} or {@link Test#expected()},\n *         but you can use {@link #expectException(String, Runnable)}.\n *         By default any thrown exception will be accepted; if you want to run additional checks on the thrown exception,\n *         use {@link ExceptionThrowingSubTest#assertThrown()}.\n *     </li>\n * </ul>\n */\npublic class SubTest {\n\n\tpublic static <T> void expectSuccess(T parameter, ParameterizedSubTest<T> subTest) {\n\t\tsubTest.test( parameter );\n\t}\n\n\tpublic static ExceptionThrowingSubTest expectException(Runnable runnable) {\n\t\treturn expectException( runnable.toString(), runnable );\n\t}\n\n\tpublic static ExceptionThrowingSubTest expectException(String description, Runnable runnable) {\n\t\treturn expectException(\n\t\t\t\tdescription,\n\t\t\t\t() -> {\n\t\t\t\t\trunnable.run();\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t);\n\t}\n\n\tpublic static ExceptionThrowingSubTest expectException(Callable<?> callable) {\n\t\treturn expectException( callable.toString(), callable );\n\t}\n\n\tpublic static ExceptionThrowingSubTest expectException(String description, Callable<?> callable) {\n\t\ttry {\n\t\t\tcallable.call();\n\t\t\tfail( \"'\" + description + \"' should have thrown an exception\" );\n\t\t\tthrow new IllegalStateException( \"This should never happen\" );\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\treturn new ExceptionThrowingSubTest( \"Exception thrown by '\" + description + \"'\", e );\n\t\t}\n\t}\n\n\tprivate SubTest() {\n\t}\n\n\tpublic static final class ExceptionThrowingSubTest {\n\t\tprivate final String description;\n\n\t\tprivate final Throwable thrown;\n\n\t\tprivate ExceptionThrowingSubTest(String description, Throwable thrown) {\n\t\t\tthis.description = description;\n\t\t\tthis.thrown = thrown;\n\t\t}\n\n\t\tpublic AbstractThrowableAssert<?, Throwable> assertThrown() {\n\t\t\treturn new ThrowableAssert( thrown )\n\t\t\t\t\t.as( description );\n\t\t}\n\t}\n\n\t@FunctionalInterface\n\tpublic interface ParameterizedSubTest<T> {\n\t\tvoid test(T param);\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "public AbstractThrowableAssert<?, Throwable> assertThrown() {\n\t\t\treturn new ThrowableAssert( thrown )\n\t\t\t\t\t.as( description );\n\t\t}",
        "diffSourceCode": "-   67: \tpublic AbstractThrowableAssert<?, Throwable> assertThrown() {\n-   68: \t\treturn new ThrowableAssert( thrown )\n-   69: \t\t\t\t.as( description );\n-   70: \t}\n+   67: \t\t}\n+   68: \t\tcatch (Exception e) {\n+   69: \t\t\treturn new ExceptionThrowingSubTest( \"Exception thrown by '\" + description + \"'\", e );\n+   70: \t\t}\n+   86: \t\tpublic AbstractThrowableAssert<?, Throwable> assertThrown() {\n+   87: \t\t\treturn new ThrowableAssert( thrown )\n+   88: \t\t\t\t\t.as( description );\n+   89: \t\t}\n",
        "uniqueId": "f50cbfc772b75e7d7a983359b9baca383bc6ae9f_67_70__86_89",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Move And Inline Method",
        "description": "Move And Inline Method\tpublic end() : SearchSort moved from class org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext to class org.hibernate.search.integrationtest.backend.lucene.ExtensionIT & inlined to public sort_fromLuceneSortField_separateSort() : void",
        "diffLocations": [
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/ExtensionIT.java",
                "startLine": 184,
                "endLine": 230,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/ExtensionIT.java",
                "startLine": 184,
                "endLine": 230,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/ExtensionIT.java",
                "startLine": 17,
                "endLine": 24,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "import org.apache.lucene.document.LatLonPoint;\nimport org.apache.lucene.document.NumericDocValuesField;\nimport org.apache.lucene.document.StringField;\nimport org.apache.lucene.index.IndexableField;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.search.SortField.Type;",
        "filePathBefore": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/ExtensionIT.java",
        "isPureRefactoring": true,
        "commitId": "b67c74bdf7a5a1a8102fb0b0179bdc0774366c29",
        "packageNameBefore": "org.hibernate.search.engine.search.dsl.sort",
        "classNameBefore": "org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext",
        "methodNameBefore": "org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext#end",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext#toSort\n methodBody: SearchSort toSort();",
        "classSignatureBefore": "public interface SearchSortTerminalContext ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext#end"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext"
        ],
        "classSignatureBeforeSet": [
            "public interface SearchSortTerminalContext "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.lucene;\n\nimport static org.hibernate.search.util.impl.integrationtest.common.assertion.DocumentReferencesSearchResultAssert.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMapperUtils.referenceProvider;\n\nimport java.util.List;\nimport java.util.function.Consumer;\n\nimport org.apache.lucene.document.Field.Store;\nimport org.apache.lucene.document.IntPoint;\nimport org.apache.lucene.document.LatLonPoint;\nimport org.apache.lucene.document.NumericDocValuesField;\nimport org.apache.lucene.document.StringField;\nimport org.apache.lucene.index.IndexableField;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.search.SortField.Type;\nimport org.apache.lucene.search.TermQuery;\nimport org.assertj.core.api.Assertions;\n\nimport org.hibernate.search.backend.lucene.LuceneBackend;\nimport org.hibernate.search.backend.lucene.index.LuceneIndexManager;\nimport org.hibernate.search.engine.backend.Backend;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldAccessor;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.document.model.dsl.Sortable;\nimport org.hibernate.search.engine.backend.index.IndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\nimport org.hibernate.search.engine.common.spi.SearchIntegration;\nimport org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexSearchTarget;\nimport org.hibernate.search.backend.lucene.LuceneExtension;\nimport org.hibernate.search.engine.common.spi.SessionContext;\nimport org.hibernate.search.integrationtest.backend.tck.util.rule.SearchSetupHelper;\nimport org.hibernate.search.engine.logging.spi.EventContexts;\nimport org.hibernate.search.engine.search.DocumentReference;\nimport org.hibernate.search.engine.search.SearchPredicate;\nimport org.hibernate.search.engine.search.SearchQuery;\nimport org.hibernate.search.engine.search.SearchSort;\nimport org.hibernate.search.engine.spatial.GeoPoint;\nimport org.hibernate.search.util.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.FailureReportUtils;\nimport org.hibernate.search.util.impl.integrationtest.common.assertion.ProjectionsSearchResultAssert;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.StubSessionContext;\nimport org.hibernate.search.util.impl.test.SubTest;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\n\npublic class ExtensionIT {\n\n\tprivate static final String BACKEND_NAME = \"myLuceneBackend\";\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\n\tprivate static final String FIRST_ID = \"1\";\n\tprivate static final String SECOND_ID = \"2\";\n\tprivate static final String THIRD_ID = \"3\";\n\tprivate static final String FOURTH_ID = \"4\";\n\tprivate static final String FIFTH_ID = \"5\";\n\n\t@Rule\n\tpublic SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic ExpectedException thrown = ExpectedException.none();\n\n\tprivate SearchIntegration integration;\n\tprivate IndexAccessors indexAccessors;\n\tprivate MappedIndexManager<?> indexManager;\n\tprivate SessionContext sessionContext = new StubSessionContext();\n\n\t@Before\n\tpublic void setup() {\n\t\tthis.integration = setupHelper.withDefaultConfiguration( BACKEND_NAME )\n\t\t\t\t.withIndex(\n\t\t\t\t\t\t\"MappedType\", INDEX_NAME,\n\t\t\t\t\t\tctx -> this.indexAccessors = new IndexAccessors( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.indexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.setup();\n\n\t\tinitData();\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.bool( b -> {\n\t\t\t\t\tb.should( c -> c.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) )\n\t\t\t\t\t);\n\t\t\t\t} ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery_separatePredicate() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchPredicate predicate1 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) ).end();\n\t\tSearchPredicate predicate2 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) ).end();\n\t\tSearchPredicate predicate3 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) ).end();\n\t\tSearchPredicate booleanPredicate = searchTarget.predicate().bool( b -> {\n\t\t\tb.should( predicate1 );\n\t\t\tb.should( predicate2 );\n\t\t\tb.should( predicate3 );\n\t\t} ).end();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( booleanPredicate )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension().ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSort( new Sort(\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tTHIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField_separateSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchSort sort1 = searchTarget.sort().extension()\n\t\t\t\t\t\t.ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.orElseFail()\n\t\t\t\t.end();\n\t\tSearchSort sort2 = searchTarget.sort().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t.end();\n\t\tSearchSort sort3 = searchTarget.sort().extension()\n\t\t\t\t.ifSupported(\n\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.orElseFail()\n\t\t\t\t.end();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1 ).then().by( sort2 ).then().by( sort3 ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID );\n\n\t\tSearchSort sort = searchTarget.sort()\n\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneSort( new Sort(\n\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.end();\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, THIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_throwsException() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"match() predicate on unsupported native field\",\n\t\t\t\t() -> searchTarget.query( sessionContext )\n\t\t\t\t\t\t.asReferences()\n\t\t\t\t\t\t.predicate( root -> root\n\t\t\t\t\t\t\t\t.match().onField( \"nativeField\" ).matching( \"37\" )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining predicates with the DSL: use the Lucene extension and a native query.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void sort_nativeField_throwsException() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"sort on unsupported native field\",\n\t\t\t\t() -> searchTarget.query( sessionContext )\n\t\t\t\t\t\t.asReferences()\n\t\t\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t\t\t.sort( c -> c.byField( \"nativeField\" ) )\n\t\t\t\t\t\t.build()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining sorts with the DSL: use the Lucene extension and a native sort.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_nativeQuery() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root\n\t\t\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneQuery( new TermQuery( new Term( \"nativeField\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<List<?>> query = searchTarget.query( sessionContext )\n\t\t\t\t.asProjections( searchTarget.projection().field( \"nativeField\", Integer.class ).toProjection() )\n\t\t\t\t.predicate( root -> root.match().onField( \"string\" ).matching( \"text 1\" ) )\n\t\t\t\t.build();\n\n\t\tProjectionsSearchResultAssert.assertThat( query ).hasProjectionsHitsAnyOrder( c -> {\n\t\t\tc.projection( 37 );\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField_unsupportedProjection() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\t// let's check that it's possible to query the field beforehand\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root\n\t\t\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneQuery( new TermQuery( new Term( \"nativeField_unsupportedProjection\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\n\t\t// now, let's check that projecting on the field throws an exception\n\t\tSubTest.expectException(\n\t\t\t\t\"projection on native field not supporting projections\",\n\t\t\t\t() -> {\n\t\t\t\t\t\tSearchQuery<List<?>> projectionQuery = searchTarget.query( sessionContext )\n\t\t\t\t\t\t\t\t.asProjections( searchTarget.projection().field( \"nativeField_unsupportedProjection\", Integer.class ).toProjection() )\n\t\t\t\t\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t\t\t\t\t.build();\n\t\t\t\t\t\tprojectionQuery.execute();\n\t\t\t\t} )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Projections are not enabled for field\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField_unsupportedProjection\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_nativeSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.extension( LuceneExtension.get() ).fromLuceneSortField( new SortField( \"nativeField\", Type.LONG ) ) )\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, THIRD_ID, FIRST_ID, FIFTH_ID, SECOND_ID, FOURTH_ID );\n\t}\n\n\t@Test\n\tpublic void nativeField_invalidFieldPath() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan( sessionContext );\n\n\t\tSubTest.expectException(\n\t\t\t\t\"native field contributing field with invalid field path\",\n\t\t\t\t() -> workPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\t\t\tindexAccessors.nativeField_invalidFieldPath.write( document, 45 );\n\t\t\t\t} ) )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Invalid field path; expected path 'nativeField_invalidFieldPath', got 'not the expected path'.\" );\n\t}\n\n\t@Test\n\tpublic void backend_unwrap() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tAssertions.assertThat( backend.unwrap( LuceneBackend.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void backend_unwrap_error_unknownType() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene backend to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this backend can only be unwrapped to '\" + LuceneBackend.class.getName() + \"'\" );\n\n\t\tbackend.unwrap( String.class );\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\t\tAssertions.assertThat( indexManager.unwrap( LuceneIndexManager.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap_error_unknownType() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene index manager to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this index manager can only be unwrapped to '\" + LuceneIndexManager.class.getName() + \"'\" );\n\n\t\tindexManager.unwrap( String.class );\n\t}\n\n\tprivate void initData() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan( sessionContext );\n\t\tworkPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tindexAccessors.string.write( document, \"text 1\" );\n\n\t\t\tindexAccessors.nativeField.write( document, 37 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 37 );\n\n\t\t\tindexAccessors.sort1.write( document, \"a\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tindexAccessors.integer.write( document, 2 );\n\n\t\t\tindexAccessors.nativeField.write( document, 78 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 78 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"a\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tindexAccessors.geoPoint.write( document, GeoPoint.of( 40.12, -71.34 ) );\n\n\t\t\tindexAccessors.nativeField.write( document, 13 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 13 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tindexAccessors.nativeField.write( document, 89 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 89 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tindexAccessors.string.write( document, \"text 2\" );\n\t\t\tindexAccessors.integer.write( document, 1 );\n\t\t\tindexAccessors.geoPoint.write( document, GeoPoint.of( 45.12, -75.34 ) );\n\n\t\t\tindexAccessors.nativeField.write( document, 53 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 53 );\n\n\t\t\tindexAccessors.sort1.write( document, \"zz\" );\n\t\t\tindexAccessors.sort2.write( document, \"zz\" );\n\t\t\tindexAccessors.sort3.write( document, \"zz\" );\n\t\t} );\n\n\t\tworkPlan.execute().join();\n\n\t\t// Check that all documents are searchable\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsAnyOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}\n\n\tprivate static class IndexAccessors {\n\t\tfinal IndexFieldAccessor<Integer> integer;\n\t\tfinal IndexFieldAccessor<String> string;\n\t\tfinal IndexFieldAccessor<GeoPoint> geoPoint;\n\t\tfinal IndexFieldAccessor<Integer> nativeField;\n\t\tfinal IndexFieldAccessor<Integer> nativeField_unsupportedProjection;\n\t\tfinal IndexFieldAccessor<Integer> nativeField_invalidFieldPath;\n\n\t\tfinal IndexFieldAccessor<String> sort1;\n\t\tfinal IndexFieldAccessor<String> sort2;\n\t\tfinal IndexFieldAccessor<String> sort3;\n\n\t\tIndexAccessors(IndexSchemaElement root) {\n\t\t\tinteger = root.field( \"integer\" )\n\t\t\t\t\t.asInteger()\n\t\t\t\t\t.createAccessor();\n\t\t\tstring = root.field( \"string\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.createAccessor();\n\t\t\tgeoPoint = root.field( \"geoPoint\" )\n\t\t\t\t\t.asGeoPoint()\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField = root.field( \"nativeField\" )\n\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t.asLuceneField( Integer.class, ExtensionIT::contributeNativeField, ExtensionIT::fromNativeField )\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField_unsupportedProjection = root.field( \"nativeField_unsupportedProjection\" )\n\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t.asLuceneField( Integer.class, ExtensionIT::contributeNativeField )\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField_invalidFieldPath = root.field( \"nativeField_invalidFieldPath\" )\n\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t.asLuceneField( Integer.class, ExtensionIT::contributeNativeFieldInvalidFieldPath )\n\t\t\t\t\t.createAccessor();\n\n\t\t\tsort1 = root.field( \"sort1\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.sortable( Sortable.YES )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort2 = root.field( \"sort2\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.sortable( Sortable.YES )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort3 = root.field( \"sort3\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.sortable( Sortable.YES )\n\t\t\t\t\t.createAccessor();\n\t\t}\n\t}\n\n\tprivate static void contributeNativeField(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( absoluteFieldPath, value.toString(), Store.YES ) );\n\t\tcollector.accept( new NumericDocValuesField( absoluteFieldPath, value.longValue() ) );\n\t}\n\n\tprivate static Integer fromNativeField(IndexableField field) {\n\t\treturn Integer.parseInt( field.stringValue() );\n\t}\n\n\tprivate static void contributeNativeFieldInvalidFieldPath(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( \"not the expected path\", value.toString(), Store.YES ) );\n\t}\n}\n",
        "filePathAfter": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/ExtensionIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.lucene;\n\nimport static org.hibernate.search.util.impl.integrationtest.common.assertion.DocumentReferencesSearchResultAssert.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMapperUtils.referenceProvider;\n\nimport java.util.List;\nimport java.util.function.Consumer;\n\nimport org.apache.lucene.document.Field.Store;\nimport org.apache.lucene.document.IntPoint;\nimport org.apache.lucene.document.LatLonPoint;\nimport org.apache.lucene.document.NumericDocValuesField;\nimport org.apache.lucene.document.StringField;\nimport org.apache.lucene.index.IndexableField;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.search.SortField.Type;\nimport org.apache.lucene.search.TermQuery;\nimport org.assertj.core.api.Assertions;\n\nimport org.hibernate.search.backend.lucene.LuceneBackend;\nimport org.hibernate.search.backend.lucene.index.LuceneIndexManager;\nimport org.hibernate.search.engine.backend.Backend;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldAccessor;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.document.model.dsl.Sortable;\nimport org.hibernate.search.engine.backend.index.IndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\nimport org.hibernate.search.engine.common.spi.SearchIntegration;\nimport org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexSearchTarget;\nimport org.hibernate.search.backend.lucene.LuceneExtension;\nimport org.hibernate.search.engine.common.spi.SessionContext;\nimport org.hibernate.search.integrationtest.backend.tck.util.rule.SearchSetupHelper;\nimport org.hibernate.search.engine.logging.spi.EventContexts;\nimport org.hibernate.search.engine.search.DocumentReference;\nimport org.hibernate.search.engine.search.SearchPredicate;\nimport org.hibernate.search.engine.search.SearchQuery;\nimport org.hibernate.search.engine.search.SearchSort;\nimport org.hibernate.search.engine.spatial.GeoPoint;\nimport org.hibernate.search.util.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.FailureReportUtils;\nimport org.hibernate.search.util.impl.integrationtest.common.assertion.ProjectionsSearchResultAssert;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.StubSessionContext;\nimport org.hibernate.search.util.impl.test.SubTest;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\n\npublic class ExtensionIT {\n\n\tprivate static final String BACKEND_NAME = \"myLuceneBackend\";\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\n\tprivate static final String FIRST_ID = \"1\";\n\tprivate static final String SECOND_ID = \"2\";\n\tprivate static final String THIRD_ID = \"3\";\n\tprivate static final String FOURTH_ID = \"4\";\n\tprivate static final String FIFTH_ID = \"5\";\n\n\t@Rule\n\tpublic SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic ExpectedException thrown = ExpectedException.none();\n\n\tprivate SearchIntegration integration;\n\tprivate IndexAccessors indexAccessors;\n\tprivate MappedIndexManager<?> indexManager;\n\tprivate SessionContext sessionContext = new StubSessionContext();\n\n\t@Before\n\tpublic void setup() {\n\t\tthis.integration = setupHelper.withDefaultConfiguration( BACKEND_NAME )\n\t\t\t\t.withIndex(\n\t\t\t\t\t\t\"MappedType\", INDEX_NAME,\n\t\t\t\t\t\tctx -> this.indexAccessors = new IndexAccessors( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.indexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.setup();\n\n\t\tinitData();\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.bool( b -> {\n\t\t\t\t\tb.should( c -> c.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) )\n\t\t\t\t\t);\n\t\t\t\t} ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery_separatePredicate() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchPredicate predicate1 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) ).toPredicate();\n\t\tSearchPredicate predicate2 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) ).toPredicate();\n\t\tSearchPredicate predicate3 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) ).toPredicate();\n\t\tSearchPredicate booleanPredicate = searchTarget.predicate().bool( b -> {\n\t\t\tb.should( predicate1 );\n\t\t\tb.should( predicate2 );\n\t\t\tb.should( predicate3 );\n\t\t} ).toPredicate();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( booleanPredicate )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension().ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSort( new Sort(\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tTHIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField_separateSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchSort sort1 = searchTarget.sort().extension()\n\t\t\t\t\t\t.ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.orElseFail()\n\t\t\t\t.toSort();\n\t\tSearchSort sort2 = searchTarget.sort().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t.toSort();\n\t\tSearchSort sort3 = searchTarget.sort().extension()\n\t\t\t\t.ifSupported(\n\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.orElseFail()\n\t\t\t\t.toSort();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1 ).then().by( sort2 ).then().by( sort3 ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID );\n\n\t\tSearchSort sort = searchTarget.sort()\n\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneSort( new Sort(\n\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.toSort();\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, THIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_throwsException() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"match() predicate on unsupported native field\",\n\t\t\t\t() -> searchTarget.query( sessionContext )\n\t\t\t\t\t\t.asReferences()\n\t\t\t\t\t\t.predicate( root -> root\n\t\t\t\t\t\t\t\t.match().onField( \"nativeField\" ).matching( \"37\" )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining predicates with the DSL: use the Lucene extension and a native query.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void sort_nativeField_throwsException() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"sort on unsupported native field\",\n\t\t\t\t() -> searchTarget.query( sessionContext )\n\t\t\t\t\t\t.asReferences()\n\t\t\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t\t\t.sort( c -> c.byField( \"nativeField\" ) )\n\t\t\t\t\t\t.build()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining sorts with the DSL: use the Lucene extension and a native sort.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_nativeQuery() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root\n\t\t\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneQuery( new TermQuery( new Term( \"nativeField\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<List<?>> query = searchTarget.query( sessionContext )\n\t\t\t\t.asProjections( searchTarget.projection().field( \"nativeField\", Integer.class ).toProjection() )\n\t\t\t\t.predicate( root -> root.match().onField( \"string\" ).matching( \"text 1\" ) )\n\t\t\t\t.build();\n\n\t\tProjectionsSearchResultAssert.assertThat( query ).hasProjectionsHitsAnyOrder( c -> {\n\t\t\tc.projection( 37 );\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField_unsupportedProjection() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\t// let's check that it's possible to query the field beforehand\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root\n\t\t\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneQuery( new TermQuery( new Term( \"nativeField_unsupportedProjection\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\n\t\t// now, let's check that projecting on the field throws an exception\n\t\tSubTest.expectException(\n\t\t\t\t\"projection on native field not supporting projections\",\n\t\t\t\t() -> {\n\t\t\t\t\t\tSearchQuery<List<?>> projectionQuery = searchTarget.query( sessionContext )\n\t\t\t\t\t\t\t\t.asProjections( searchTarget.projection().field( \"nativeField_unsupportedProjection\", Integer.class ).toProjection() )\n\t\t\t\t\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t\t\t\t\t.build();\n\t\t\t\t\t\tprojectionQuery.execute();\n\t\t\t\t} )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Projections are not enabled for field\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField_unsupportedProjection\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_nativeSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.extension( LuceneExtension.get() ).fromLuceneSortField( new SortField( \"nativeField\", Type.LONG ) ) )\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, THIRD_ID, FIRST_ID, FIFTH_ID, SECOND_ID, FOURTH_ID );\n\t}\n\n\t@Test\n\tpublic void nativeField_invalidFieldPath() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan( sessionContext );\n\n\t\tSubTest.expectException(\n\t\t\t\t\"native field contributing field with invalid field path\",\n\t\t\t\t() -> workPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\t\t\tindexAccessors.nativeField_invalidFieldPath.write( document, 45 );\n\t\t\t\t} ) )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Invalid field path; expected path 'nativeField_invalidFieldPath', got 'not the expected path'.\" );\n\t}\n\n\t@Test\n\tpublic void backend_unwrap() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tAssertions.assertThat( backend.unwrap( LuceneBackend.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void backend_unwrap_error_unknownType() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene backend to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this backend can only be unwrapped to '\" + LuceneBackend.class.getName() + \"'\" );\n\n\t\tbackend.unwrap( String.class );\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\t\tAssertions.assertThat( indexManager.unwrap( LuceneIndexManager.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap_error_unknownType() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene index manager to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this index manager can only be unwrapped to '\" + LuceneIndexManager.class.getName() + \"'\" );\n\n\t\tindexManager.unwrap( String.class );\n\t}\n\n\tprivate void initData() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan( sessionContext );\n\t\tworkPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tindexAccessors.string.write( document, \"text 1\" );\n\n\t\t\tindexAccessors.nativeField.write( document, 37 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 37 );\n\n\t\t\tindexAccessors.sort1.write( document, \"a\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tindexAccessors.integer.write( document, 2 );\n\n\t\t\tindexAccessors.nativeField.write( document, 78 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 78 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"a\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tindexAccessors.geoPoint.write( document, GeoPoint.of( 40.12, -71.34 ) );\n\n\t\t\tindexAccessors.nativeField.write( document, 13 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 13 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tindexAccessors.nativeField.write( document, 89 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 89 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tindexAccessors.string.write( document, \"text 2\" );\n\t\t\tindexAccessors.integer.write( document, 1 );\n\t\t\tindexAccessors.geoPoint.write( document, GeoPoint.of( 45.12, -75.34 ) );\n\n\t\t\tindexAccessors.nativeField.write( document, 53 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 53 );\n\n\t\t\tindexAccessors.sort1.write( document, \"zz\" );\n\t\t\tindexAccessors.sort2.write( document, \"zz\" );\n\t\t\tindexAccessors.sort3.write( document, \"zz\" );\n\t\t} );\n\n\t\tworkPlan.execute().join();\n\n\t\t// Check that all documents are searchable\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsAnyOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}\n\n\tprivate static class IndexAccessors {\n\t\tfinal IndexFieldAccessor<Integer> integer;\n\t\tfinal IndexFieldAccessor<String> string;\n\t\tfinal IndexFieldAccessor<GeoPoint> geoPoint;\n\t\tfinal IndexFieldAccessor<Integer> nativeField;\n\t\tfinal IndexFieldAccessor<Integer> nativeField_unsupportedProjection;\n\t\tfinal IndexFieldAccessor<Integer> nativeField_invalidFieldPath;\n\n\t\tfinal IndexFieldAccessor<String> sort1;\n\t\tfinal IndexFieldAccessor<String> sort2;\n\t\tfinal IndexFieldAccessor<String> sort3;\n\n\t\tIndexAccessors(IndexSchemaElement root) {\n\t\t\tinteger = root.field( \"integer\" )\n\t\t\t\t\t.asInteger()\n\t\t\t\t\t.createAccessor();\n\t\t\tstring = root.field( \"string\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.createAccessor();\n\t\t\tgeoPoint = root.field( \"geoPoint\" )\n\t\t\t\t\t.asGeoPoint()\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField = root.field( \"nativeField\" )\n\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t.asLuceneField( Integer.class, ExtensionIT::contributeNativeField, ExtensionIT::fromNativeField )\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField_unsupportedProjection = root.field( \"nativeField_unsupportedProjection\" )\n\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t.asLuceneField( Integer.class, ExtensionIT::contributeNativeField )\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField_invalidFieldPath = root.field( \"nativeField_invalidFieldPath\" )\n\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t.asLuceneField( Integer.class, ExtensionIT::contributeNativeFieldInvalidFieldPath )\n\t\t\t\t\t.createAccessor();\n\n\t\t\tsort1 = root.field( \"sort1\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.sortable( Sortable.YES )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort2 = root.field( \"sort2\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.sortable( Sortable.YES )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort3 = root.field( \"sort3\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.sortable( Sortable.YES )\n\t\t\t\t\t.createAccessor();\n\t\t}\n\t}\n\n\tprivate static void contributeNativeField(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( absoluteFieldPath, value.toString(), Store.YES ) );\n\t\tcollector.accept( new NumericDocValuesField( absoluteFieldPath, value.longValue() ) );\n\t}\n\n\tprivate static Integer fromNativeField(IndexableField field) {\n\t\treturn Integer.parseInt( field.stringValue() );\n\t}\n\n\tprivate static void contributeNativeFieldInvalidFieldPath(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( \"not the expected path\", value.toString(), Store.YES ) );\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext#toSort\n methodBody: SearchSort toSort();"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void sort_fromLuceneSortField_separateSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchSort sort1 = searchTarget.sort().extension()\n\t\t\t\t\t\t.ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.orElseFail()\n\t\t\t\t.toSort();\n\t\tSearchSort sort2 = searchTarget.sort().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t.toSort();\n\t\tSearchSort sort3 = searchTarget.sort().extension()\n\t\t\t\t.ifSupported(\n\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.orElseFail()\n\t\t\t\t.toSort();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1 ).then().by( sort2 ).then().by( sort3 ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID );\n\n\t\tSearchSort sort = searchTarget.sort()\n\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneSort( new Sort(\n\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.toSort();\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, THIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID );\n\t}",
        "diffSourceCode": "    17: import org.apache.lucene.document.LatLonPoint;\n    18: import org.apache.lucene.document.NumericDocValuesField;\n    19: import org.apache.lucene.document.StringField;\n    20: import org.apache.lucene.index.IndexableField;\n    21: import org.apache.lucene.index.Term;\n    22: import org.apache.lucene.search.Sort;\n    23: import org.apache.lucene.search.SortField;\n    24: import org.apache.lucene.search.SortField.Type;\n   184: \t@Test\n   185: \tpublic void sort_fromLuceneSortField_separateSort() {\n   186: \t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n   187: \n   188: \t\tSearchSort sort1 = searchTarget.sort().extension()\n   189: \t\t\t\t\t\t.ifSupported(\n   190: \t\t\t\t\t\t\t\tLuceneExtension.get(),\n   191: \t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n   192: \t\t\t\t\t\t)\n   193: \t\t\t\t\t\t.orElseFail()\n-  194: \t\t\t\t.end();\n+  194: \t\t\t\t.toSort();\n   195: \t\tSearchSort sort2 = searchTarget.sort().extension( LuceneExtension.get() )\n   196: \t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n-  197: \t\t\t\t.end();\n+  197: \t\t\t\t.toSort();\n   198: \t\tSearchSort sort3 = searchTarget.sort().extension()\n   199: \t\t\t\t.ifSupported(\n   200: \t\t\t\t\t\tLuceneExtension.get(),\n   201: \t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n   202: \t\t\t\t)\n   203: \t\t\t\t.orElseFail()\n-  204: \t\t\t\t.end();\n+  204: \t\t\t\t.toSort();\n   205: \n   206: \t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n   207: \t\t\t\t.asReferences()\n   208: \t\t\t\t.predicate( root -> root.matchAll() )\n   209: \t\t\t\t.sort( c -> c.by( sort1 ).then().by( sort2 ).then().by( sort3 ) )\n   210: \t\t\t\t.build();\n   211: \t\tassertThat( query )\n   212: \t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID );\n   213: \n   214: \t\tSearchSort sort = searchTarget.sort()\n   215: \t\t\t\t.extension( LuceneExtension.get() ).fromLuceneSort( new Sort(\n   216: \t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n   217: \t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n   218: \t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n   219: \t\t\t\t\t)\n   220: \t\t\t\t)\n-  221: \t\t\t\t.end();\n+  221: \t\t\t\t.toSort();\n   222: \n   223: \t\tquery = searchTarget.query( sessionContext )\n   224: \t\t\t\t.asReferences()\n   225: \t\t\t\t.predicate( root -> root.matchAll() )\n   226: \t\t\t\t.sort( c -> c.by( sort ) )\n   227: \t\t\t\t.build();\n   228: \t\tassertThat( query )\n   229: \t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, THIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID );\n   230: \t}\n",
        "uniqueId": "b67c74bdf7a5a1a8102fb0b0179bdc0774366c29_184_230__184_230_17_24",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Move And Inline Method",
        "description": "Move And Inline Method\tpublic end() : SearchSort moved from class org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext to class org.hibernate.search.integrationtest.backend.elasticsearch.ExtensionIT & inlined to public sort_fromJsonString_separateSort() : void",
        "diffLocations": [
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/ExtensionIT.java",
                "startLine": 198,
                "endLine": 246,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/ExtensionIT.java",
                "startLine": 198,
                "endLine": 246,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/ExtensionIT.java",
                "startLine": 17,
                "endLine": 24,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "import org.hibernate.search.engine.backend.document.IndexFieldAccessor;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.index.IndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexSearchTarget;\nimport org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\nimport org.hibernate.search.engine.common.spi.SearchIntegration;\nimport org.hibernate.search.engine.common.spi.SessionContext;\nimport org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;",
        "filePathBefore": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/ExtensionIT.java",
        "isPureRefactoring": true,
        "commitId": "b67c74bdf7a5a1a8102fb0b0179bdc0774366c29",
        "packageNameBefore": "org.hibernate.search.engine.search.dsl.sort",
        "classNameBefore": "org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext",
        "methodNameBefore": "org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext#end",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext#toSort\n methodBody: SearchSort toSort();",
        "classSignatureBefore": "public interface SearchSortTerminalContext ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext#end"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext"
        ],
        "classSignatureBeforeSet": [
            "public interface SearchSortTerminalContext "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.elasticsearch;\n\nimport static org.hibernate.search.util.impl.integrationtest.common.assertion.DocumentReferencesSearchResultAssert.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMapperUtils.referenceProvider;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchBackend;\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchExtension;\nimport org.hibernate.search.backend.elasticsearch.index.ElasticsearchIndexManager;\nimport org.hibernate.search.engine.backend.Backend;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldAccessor;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.index.IndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexSearchTarget;\nimport org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\nimport org.hibernate.search.engine.common.spi.SearchIntegration;\nimport org.hibernate.search.engine.common.spi.SessionContext;\nimport org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;\nimport org.hibernate.search.engine.search.DocumentReference;\nimport org.hibernate.search.engine.search.SearchPredicate;\nimport org.hibernate.search.engine.search.SearchQuery;\nimport org.hibernate.search.engine.search.SearchSort;\nimport org.hibernate.search.integrationtest.backend.tck.util.rule.SearchSetupHelper;\nimport org.hibernate.search.util.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.StubSessionContext;\n\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\n\nimport org.apache.http.nio.client.HttpAsyncClient;\nimport org.assertj.core.api.Assertions;\nimport org.elasticsearch.client.Response;\nimport org.elasticsearch.client.RestClient;\n\npublic class ExtensionIT {\n\n\tprivate static final String BACKEND_NAME = \"myElasticsearchBackend\";\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\n\tprivate static final String FIRST_ID = \"1\";\n\tprivate static final String SECOND_ID = \"2\";\n\tprivate static final String THIRD_ID = \"3\";\n\tprivate static final String FOURTH_ID = \"4\";\n\tprivate static final String FIFTH_ID = \"5\";\n\tprivate static final String EMPTY_ID = \"empty\";\n\n\t@Rule\n\tpublic SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic ExpectedException thrown = ExpectedException.none();\n\n\tprivate SearchIntegration integration;\n\tprivate IndexAccessors indexAccessors;\n\tprivate MappedIndexManager<?> indexManager;\n\tprivate SessionContext sessionContext = new StubSessionContext();\n\n\t@Before\n\tpublic void setup() {\n\t\tthis.integration = setupHelper.withDefaultConfiguration( BACKEND_NAME )\n\t\t\t\t.withIndex(\n\t\t\t\t\t\t\"MappedType\", INDEX_NAME,\n\t\t\t\t\t\tctx -> this.indexAccessors = new IndexAccessors( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.indexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.setup();\n\n\t\tinitData();\n\t}\n\n\t@Test\n\tpublic void predicate_fromJsonString() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.bool( b -> {\n\t\t\t\t\tb.should( c -> c.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t.fromJsonString( \"{'match': {'string': 'text 1'}}\" )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t.fromJsonString( \"{'match': {'integer': 2}}\" )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t.fromJsonString(\n\t\t\t\t\t\t\t\t\t\"{\"\n\t\t\t\t\t\t\t\t\t\t+ \"'geo_distance': {\"\n\t\t\t\t\t\t\t\t\t\t\t+ \"'distance': '200km',\"\n\t\t\t\t\t\t\t\t\t\t\t+ \"'geoPoint': {\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'lat': 40,\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'lon': -70\"\n\t\t\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t)\n\t\t\t\t\t);\n\t\t\t\t\t// Also test using the standard DSL on a field defined with the extension\n\t\t\t\t\tb.should( c -> c.match().onField( \"yearDays\" ).matching( \"'2018:12'\" ) );\n\t\t\t\t} ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID )\n\t\t\t\t.hasHitCount( 4 );\n\t}\n\n\t@Test\n\tpublic void predicate_fromJsonString_separatePredicate() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchPredicate predicate1 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'match': {'string': 'text 1'}}\" ).end();\n\t\tSearchPredicate predicate2 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'match': {'integer': 2}}\" ).end();\n\t\tSearchPredicate predicate3 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString(\n\t\t\t\t\t\t\"{\"\n\t\t\t\t\t\t\t+ \"'geo_distance': {\"\n\t\t\t\t\t\t\t\t+ \"'distance': '200km',\"\n\t\t\t\t\t\t\t\t+ \"'geoPoint': {\"\n\t\t\t\t\t\t\t\t\t+ \"'lat': 40,\"\n\t\t\t\t\t\t\t\t\t+ \"'lon': -70\"\n\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t\t\t.end();\n\t\t// Also test using the standard DSL on a field defined with the extension\n\t\tSearchPredicate predicate4 = searchTarget.predicate().match().onField( \"yearDays\" )\n\t\t\t\t.matching( \"'2018:12'\" ).end();\n\t\tSearchPredicate booleanPredicate = searchTarget.predicate().bool( b -> {\n\t\t\tb.should( predicate1 );\n\t\t\tb.should( predicate2 );\n\t\t\tb.should( predicate3 );\n\t\t\tb.should( predicate4 );\n\t\t} ).end();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( booleanPredicate )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID )\n\t\t\t\t.hasHitCount( 4 );\n\t}\n\n\t@Test\n\tpublic void sort_fromJsonString() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort1': 'asc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort2': 'asc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort3': 'asc'}\" )\n\t\t\t\t\t\t// Also test using the standard DSL on a field defined with the extension\n\t\t\t\t\t\t.then().byField( \"sort4\" ).asc().onMissingValue().sortLast()\n\t\t\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, EMPTY_ID, FIFTH_ID\n\t\t);\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort1': 'desc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort2': 'desc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort3': 'desc'}\" )\n\t\t\t\t\t\t.then().byField( \"sort4\" ).desc().onMissingValue().sortLast()\n\t\t\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFOURTH_ID, THIRD_ID, SECOND_ID, FIRST_ID, EMPTY_ID, FIFTH_ID\n\t\t);\n\t}\n\n\t@Test\n\tpublic void sort_fromJsonString_separateSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchSort sort1Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort1': 'asc'}\" )\n\t\t\t\t.end();\n\t\tSearchSort sort2Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort2': 'asc'}\" )\n\t\t\t\t.end();\n\t\tSearchSort sort3Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort3': 'asc'}\" )\n\t\t\t\t.end();\n\t\t// Also test using the standard DSL on a field defined with the extension\n\t\tSearchSort sort4Asc = searchTarget.sort()\n\t\t\t\t.byField( \"sort4\" ).asc().onMissingValue().sortLast()\n\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t.end();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1Asc ).then().by( sort2Asc ).then().by( sort3Asc ).then().by( sort4Asc ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, EMPTY_ID, FIFTH_ID );\n\n\t\tSearchSort sort1Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort1': 'desc'}\" )\n\t\t\t\t.end();\n\t\tSearchSort sort2Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort2': 'desc'}\" )\n\t\t\t\t.end();\n\t\tSearchSort sort3Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort3': 'desc'}\" )\n\t\t\t\t.end();\n\t\tSearchSort sort4Desc = searchTarget.sort()\n\t\t\t\t.byField( \"sort4\" ).desc().onMissingValue().sortLast()\n\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t.end();\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1Desc ).then().by( sort2Desc ).then().by( sort3Desc ).then().by( sort4Desc ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FOURTH_ID, THIRD_ID, SECOND_ID, FIRST_ID, EMPTY_ID, FIFTH_ID );\n\t}\n\n\t@Test\n\tpublic void backend_unwrap() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tAssertions.assertThat( backend.unwrap( ElasticsearchBackend.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void backend_unwrap_error_unknownType() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap an Elasticsearch backend to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this backend can only be unwrapped to '\" + ElasticsearchBackend.class.getName() + \"'\" );\n\n\t\tbackend.unwrap( String.class );\n\t}\n\n\t@Test\n\tpublic void backend_getClient() throws Exception {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tElasticsearchBackend elasticsearchBackend = backend.unwrap( ElasticsearchBackend.class );\n\t\tRestClient restClient = elasticsearchBackend.getClient( RestClient.class );\n\n\t\t// Test that the client actually works\n\t\tResponse response = restClient.performRequest( \"GET\", \"/\" );\n\t\tAssertions.assertThat( response.getStatusLine().getStatusCode() ).isEqualTo( 200 );\n\t}\n\n\t@Test\n\tpublic void backend_getClient_error_invalidClass() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tElasticsearchBackend elasticsearchBackend = backend.unwrap( ElasticsearchBackend.class );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( HttpAsyncClient.class.getName() );\n\t\tthrown.expectMessage( \"the client can only be unwrapped to\" );\n\t\tthrown.expectMessage( RestClient.class.getName() );\n\n\t\telasticsearchBackend.getClient( HttpAsyncClient.class );\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\t\tAssertions.assertThat( indexManager.unwrap( ElasticsearchIndexManager.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap_error_unknownType() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap an Elasticsearch index manager to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this index manager can only be unwrapped to '\" + ElasticsearchIndexManager.class.getName() + \"'\" );\n\n\t\tindexManager.unwrap( String.class );\n\t}\n\n\tprivate void initData() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan( sessionContext );\n\t\tworkPlan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tindexAccessors.integer.write( document, \"2\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"a\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t\tindexAccessors.sort4.write( document, \"z\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tindexAccessors.string.write( document, \"'text 1'\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"a\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t\tindexAccessors.sort4.write( document, \"z\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tindexAccessors.geoPoint.write( document, \"{'lat': 40.12, 'lon': -71.34}\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"a\" );\n\t\t\tindexAccessors.sort4.write( document, \"z\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tindexAccessors.yearDays.write( document, \"'2018:012'\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t\tindexAccessors.sort4.write( document, \"a\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tindexAccessors.string.write( document, \"'text 2'\" );\n\t\t\tindexAccessors.integer.write( document, \"1\" );\n\t\t\tindexAccessors.geoPoint.write( document, \"{'lat': 45.12, 'lon': -75.34}\" );\n\t\t\tindexAccessors.yearDays.write( document, \"'2018:025'\" );\n\n\t\t\tindexAccessors.sort5.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( EMPTY_ID ), document -> { } );\n\n\t\tworkPlan.execute().join();\n\n\t\t// Check that all documents are searchable\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsAnyOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID, EMPTY_ID\n\t\t);\n\t}\n\n\tprivate static class IndexAccessors {\n\t\tfinal IndexFieldAccessor<String> integer;\n\t\tfinal IndexFieldAccessor<String> string;\n\t\tfinal IndexFieldAccessor<String> geoPoint;\n\t\tfinal IndexFieldAccessor<String> yearDays;\n\n\t\tfinal IndexFieldAccessor<String> sort1;\n\t\tfinal IndexFieldAccessor<String> sort2;\n\t\tfinal IndexFieldAccessor<String> sort3;\n\t\tfinal IndexFieldAccessor<String> sort4;\n\t\tfinal IndexFieldAccessor<String> sort5;\n\n\t\tIndexAccessors(IndexSchemaElement root) {\n\t\t\tinteger = root.field( \"integer\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'integer'}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tstring = root.field( \"string\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword'}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tgeoPoint = root.field( \"geoPoint\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'geo_point'}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tyearDays = root.field( \"yearDays\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'date', 'format': 'yyyy:DDD'}\" )\n\t\t\t\t\t.createAccessor();\n\n\t\t\tsort1 = root.field( \"sort1\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort2 = root.field( \"sort2\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort3 = root.field( \"sort3\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort4 = root.field( \"sort4\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort5 = root.field( \"sort5\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t}\n\t}\n\n}",
        "filePathAfter": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/ExtensionIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.elasticsearch;\n\nimport static org.hibernate.search.util.impl.integrationtest.common.assertion.DocumentReferencesSearchResultAssert.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMapperUtils.referenceProvider;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchBackend;\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchExtension;\nimport org.hibernate.search.backend.elasticsearch.index.ElasticsearchIndexManager;\nimport org.hibernate.search.engine.backend.Backend;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldAccessor;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.index.IndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexSearchTarget;\nimport org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\nimport org.hibernate.search.engine.common.spi.SearchIntegration;\nimport org.hibernate.search.engine.common.spi.SessionContext;\nimport org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;\nimport org.hibernate.search.engine.search.DocumentReference;\nimport org.hibernate.search.engine.search.SearchPredicate;\nimport org.hibernate.search.engine.search.SearchQuery;\nimport org.hibernate.search.engine.search.SearchSort;\nimport org.hibernate.search.integrationtest.backend.tck.util.rule.SearchSetupHelper;\nimport org.hibernate.search.util.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.StubSessionContext;\n\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\n\nimport org.apache.http.nio.client.HttpAsyncClient;\nimport org.assertj.core.api.Assertions;\nimport org.elasticsearch.client.Response;\nimport org.elasticsearch.client.RestClient;\n\npublic class ExtensionIT {\n\n\tprivate static final String BACKEND_NAME = \"myElasticsearchBackend\";\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\n\tprivate static final String FIRST_ID = \"1\";\n\tprivate static final String SECOND_ID = \"2\";\n\tprivate static final String THIRD_ID = \"3\";\n\tprivate static final String FOURTH_ID = \"4\";\n\tprivate static final String FIFTH_ID = \"5\";\n\tprivate static final String EMPTY_ID = \"empty\";\n\n\t@Rule\n\tpublic SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic ExpectedException thrown = ExpectedException.none();\n\n\tprivate SearchIntegration integration;\n\tprivate IndexAccessors indexAccessors;\n\tprivate MappedIndexManager<?> indexManager;\n\tprivate SessionContext sessionContext = new StubSessionContext();\n\n\t@Before\n\tpublic void setup() {\n\t\tthis.integration = setupHelper.withDefaultConfiguration( BACKEND_NAME )\n\t\t\t\t.withIndex(\n\t\t\t\t\t\t\"MappedType\", INDEX_NAME,\n\t\t\t\t\t\tctx -> this.indexAccessors = new IndexAccessors( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.indexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.setup();\n\n\t\tinitData();\n\t}\n\n\t@Test\n\tpublic void predicate_fromJsonString() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.bool( b -> {\n\t\t\t\t\tb.should( c -> c.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t.fromJsonString( \"{'match': {'string': 'text 1'}}\" )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t.fromJsonString( \"{'match': {'integer': 2}}\" )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t.fromJsonString(\n\t\t\t\t\t\t\t\t\t\"{\"\n\t\t\t\t\t\t\t\t\t\t+ \"'geo_distance': {\"\n\t\t\t\t\t\t\t\t\t\t\t+ \"'distance': '200km',\"\n\t\t\t\t\t\t\t\t\t\t\t+ \"'geoPoint': {\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'lat': 40,\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'lon': -70\"\n\t\t\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t)\n\t\t\t\t\t);\n\t\t\t\t\t// Also test using the standard DSL on a field defined with the extension\n\t\t\t\t\tb.should( c -> c.match().onField( \"yearDays\" ).matching( \"'2018:12'\" ) );\n\t\t\t\t} ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID )\n\t\t\t\t.hasHitCount( 4 );\n\t}\n\n\t@Test\n\tpublic void predicate_fromJsonString_separatePredicate() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchPredicate predicate1 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'match': {'string': 'text 1'}}\" ).toPredicate();\n\t\tSearchPredicate predicate2 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'match': {'integer': 2}}\" ).toPredicate();\n\t\tSearchPredicate predicate3 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString(\n\t\t\t\t\t\t\"{\"\n\t\t\t\t\t\t\t+ \"'geo_distance': {\"\n\t\t\t\t\t\t\t\t+ \"'distance': '200km',\"\n\t\t\t\t\t\t\t\t+ \"'geoPoint': {\"\n\t\t\t\t\t\t\t\t\t+ \"'lat': 40,\"\n\t\t\t\t\t\t\t\t\t+ \"'lon': -70\"\n\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t\t\t.toPredicate();\n\t\t// Also test using the standard DSL on a field defined with the extension\n\t\tSearchPredicate predicate4 = searchTarget.predicate().match().onField( \"yearDays\" )\n\t\t\t\t.matching( \"'2018:12'\" ).toPredicate();\n\t\tSearchPredicate booleanPredicate = searchTarget.predicate().bool( b -> {\n\t\t\tb.should( predicate1 );\n\t\t\tb.should( predicate2 );\n\t\t\tb.should( predicate3 );\n\t\t\tb.should( predicate4 );\n\t\t} ).toPredicate();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( booleanPredicate )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID )\n\t\t\t\t.hasHitCount( 4 );\n\t}\n\n\t@Test\n\tpublic void sort_fromJsonString() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort1': 'asc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort2': 'asc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort3': 'asc'}\" )\n\t\t\t\t\t\t// Also test using the standard DSL on a field defined with the extension\n\t\t\t\t\t\t.then().byField( \"sort4\" ).asc().onMissingValue().sortLast()\n\t\t\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, EMPTY_ID, FIFTH_ID\n\t\t);\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort1': 'desc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort2': 'desc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort3': 'desc'}\" )\n\t\t\t\t\t\t.then().byField( \"sort4\" ).desc().onMissingValue().sortLast()\n\t\t\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFOURTH_ID, THIRD_ID, SECOND_ID, FIRST_ID, EMPTY_ID, FIFTH_ID\n\t\t);\n\t}\n\n\t@Test\n\tpublic void sort_fromJsonString_separateSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchSort sort1Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort1': 'asc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort2Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort2': 'asc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort3Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort3': 'asc'}\" )\n\t\t\t\t.toSort();\n\t\t// Also test using the standard DSL on a field defined with the extension\n\t\tSearchSort sort4Asc = searchTarget.sort()\n\t\t\t\t.byField( \"sort4\" ).asc().onMissingValue().sortLast()\n\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t.toSort();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1Asc ).then().by( sort2Asc ).then().by( sort3Asc ).then().by( sort4Asc ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, EMPTY_ID, FIFTH_ID );\n\n\t\tSearchSort sort1Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort1': 'desc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort2Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort2': 'desc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort3Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort3': 'desc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort4Desc = searchTarget.sort()\n\t\t\t\t.byField( \"sort4\" ).desc().onMissingValue().sortLast()\n\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t.toSort();\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1Desc ).then().by( sort2Desc ).then().by( sort3Desc ).then().by( sort4Desc ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FOURTH_ID, THIRD_ID, SECOND_ID, FIRST_ID, EMPTY_ID, FIFTH_ID );\n\t}\n\n\t@Test\n\tpublic void backend_unwrap() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tAssertions.assertThat( backend.unwrap( ElasticsearchBackend.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void backend_unwrap_error_unknownType() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap an Elasticsearch backend to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this backend can only be unwrapped to '\" + ElasticsearchBackend.class.getName() + \"'\" );\n\n\t\tbackend.unwrap( String.class );\n\t}\n\n\t@Test\n\tpublic void backend_getClient() throws Exception {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tElasticsearchBackend elasticsearchBackend = backend.unwrap( ElasticsearchBackend.class );\n\t\tRestClient restClient = elasticsearchBackend.getClient( RestClient.class );\n\n\t\t// Test that the client actually works\n\t\tResponse response = restClient.performRequest( \"GET\", \"/\" );\n\t\tAssertions.assertThat( response.getStatusLine().getStatusCode() ).isEqualTo( 200 );\n\t}\n\n\t@Test\n\tpublic void backend_getClient_error_invalidClass() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tElasticsearchBackend elasticsearchBackend = backend.unwrap( ElasticsearchBackend.class );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( HttpAsyncClient.class.getName() );\n\t\tthrown.expectMessage( \"the client can only be unwrapped to\" );\n\t\tthrown.expectMessage( RestClient.class.getName() );\n\n\t\telasticsearchBackend.getClient( HttpAsyncClient.class );\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\t\tAssertions.assertThat( indexManager.unwrap( ElasticsearchIndexManager.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap_error_unknownType() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap an Elasticsearch index manager to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this index manager can only be unwrapped to '\" + ElasticsearchIndexManager.class.getName() + \"'\" );\n\n\t\tindexManager.unwrap( String.class );\n\t}\n\n\tprivate void initData() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan( sessionContext );\n\t\tworkPlan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tindexAccessors.integer.write( document, \"2\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"a\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t\tindexAccessors.sort4.write( document, \"z\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tindexAccessors.string.write( document, \"'text 1'\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"a\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t\tindexAccessors.sort4.write( document, \"z\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tindexAccessors.geoPoint.write( document, \"{'lat': 40.12, 'lon': -71.34}\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"a\" );\n\t\t\tindexAccessors.sort4.write( document, \"z\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tindexAccessors.yearDays.write( document, \"'2018:012'\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t\tindexAccessors.sort4.write( document, \"a\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tindexAccessors.string.write( document, \"'text 2'\" );\n\t\t\tindexAccessors.integer.write( document, \"1\" );\n\t\t\tindexAccessors.geoPoint.write( document, \"{'lat': 45.12, 'lon': -75.34}\" );\n\t\t\tindexAccessors.yearDays.write( document, \"'2018:025'\" );\n\n\t\t\tindexAccessors.sort5.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( EMPTY_ID ), document -> { } );\n\n\t\tworkPlan.execute().join();\n\n\t\t// Check that all documents are searchable\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsAnyOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID, EMPTY_ID\n\t\t);\n\t}\n\n\tprivate static class IndexAccessors {\n\t\tfinal IndexFieldAccessor<String> integer;\n\t\tfinal IndexFieldAccessor<String> string;\n\t\tfinal IndexFieldAccessor<String> geoPoint;\n\t\tfinal IndexFieldAccessor<String> yearDays;\n\n\t\tfinal IndexFieldAccessor<String> sort1;\n\t\tfinal IndexFieldAccessor<String> sort2;\n\t\tfinal IndexFieldAccessor<String> sort3;\n\t\tfinal IndexFieldAccessor<String> sort4;\n\t\tfinal IndexFieldAccessor<String> sort5;\n\n\t\tIndexAccessors(IndexSchemaElement root) {\n\t\t\tinteger = root.field( \"integer\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'integer'}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tstring = root.field( \"string\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword'}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tgeoPoint = root.field( \"geoPoint\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'geo_point'}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tyearDays = root.field( \"yearDays\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'date', 'format': 'yyyy:DDD'}\" )\n\t\t\t\t\t.createAccessor();\n\n\t\t\tsort1 = root.field( \"sort1\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort2 = root.field( \"sort2\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort3 = root.field( \"sort3\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort4 = root.field( \"sort4\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort5 = root.field( \"sort5\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t}\n\t}\n\n}",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.search.dsl.sort.SearchSortTerminalContext#toSort\n methodBody: SearchSort toSort();"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void sort_fromJsonString_separateSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchSort sort1Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort1': 'asc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort2Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort2': 'asc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort3Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort3': 'asc'}\" )\n\t\t\t\t.toSort();\n\t\t// Also test using the standard DSL on a field defined with the extension\n\t\tSearchSort sort4Asc = searchTarget.sort()\n\t\t\t\t.byField( \"sort4\" ).asc().onMissingValue().sortLast()\n\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t.toSort();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1Asc ).then().by( sort2Asc ).then().by( sort3Asc ).then().by( sort4Asc ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, EMPTY_ID, FIFTH_ID );\n\n\t\tSearchSort sort1Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort1': 'desc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort2Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort2': 'desc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort3Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort3': 'desc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort4Desc = searchTarget.sort()\n\t\t\t\t.byField( \"sort4\" ).desc().onMissingValue().sortLast()\n\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t.toSort();\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1Desc ).then().by( sort2Desc ).then().by( sort3Desc ).then().by( sort4Desc ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FOURTH_ID, THIRD_ID, SECOND_ID, FIRST_ID, EMPTY_ID, FIFTH_ID );\n\t}",
        "diffSourceCode": "    17: import org.hibernate.search.engine.backend.document.IndexFieldAccessor;\n    18: import org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\n    19: import org.hibernate.search.engine.backend.index.IndexManager;\n    20: import org.hibernate.search.engine.backend.index.spi.IndexSearchTarget;\n    21: import org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\n    22: import org.hibernate.search.engine.common.spi.SearchIntegration;\n    23: import org.hibernate.search.engine.common.spi.SessionContext;\n    24: import org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;\n   198: \t@Test\n   199: \tpublic void sort_fromJsonString_separateSort() {\n   200: \t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n   201: \n   202: \t\tSearchSort sort1Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n   203: \t\t\t\t.fromJsonString( \"{'sort1': 'asc'}\" )\n-  204: \t\t\t\t.end();\n+  204: \t\t\t\t.toSort();\n   205: \t\tSearchSort sort2Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n   206: \t\t\t\t.fromJsonString( \"{'sort2': 'asc'}\" )\n-  207: \t\t\t\t.end();\n+  207: \t\t\t\t.toSort();\n   208: \t\tSearchSort sort3Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n   209: \t\t\t\t.fromJsonString( \"{'sort3': 'asc'}\" )\n-  210: \t\t\t\t.end();\n+  210: \t\t\t\t.toSort();\n   211: \t\t// Also test using the standard DSL on a field defined with the extension\n   212: \t\tSearchSort sort4Asc = searchTarget.sort()\n   213: \t\t\t\t.byField( \"sort4\" ).asc().onMissingValue().sortLast()\n   214: \t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n-  215: \t\t\t\t.end();\n+  215: \t\t\t\t.toSort();\n   216: \n   217: \t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n   218: \t\t\t\t.asReferences()\n   219: \t\t\t\t.predicate( root -> root.matchAll() )\n   220: \t\t\t\t.sort( c -> c.by( sort1Asc ).then().by( sort2Asc ).then().by( sort3Asc ).then().by( sort4Asc ) )\n   221: \t\t\t\t.build();\n   222: \t\tassertThat( query )\n   223: \t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, EMPTY_ID, FIFTH_ID );\n   224: \n   225: \t\tSearchSort sort1Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n   226: \t\t\t\t.fromJsonString( \"{'sort1': 'desc'}\" )\n-  227: \t\t\t\t.end();\n+  227: \t\t\t\t.toSort();\n   228: \t\tSearchSort sort2Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n   229: \t\t\t\t.fromJsonString( \"{'sort2': 'desc'}\" )\n-  230: \t\t\t\t.end();\n+  230: \t\t\t\t.toSort();\n   231: \t\tSearchSort sort3Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n   232: \t\t\t\t.fromJsonString( \"{'sort3': 'desc'}\" )\n-  233: \t\t\t\t.end();\n+  233: \t\t\t\t.toSort();\n   234: \t\tSearchSort sort4Desc = searchTarget.sort()\n   235: \t\t\t\t.byField( \"sort4\" ).desc().onMissingValue().sortLast()\n   236: \t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n-  237: \t\t\t\t.end();\n+  237: \t\t\t\t.toSort();\n   238: \n   239: \t\tquery = searchTarget.query( sessionContext )\n   240: \t\t\t\t.asReferences()\n   241: \t\t\t\t.predicate( root -> root.matchAll() )\n   242: \t\t\t\t.sort( c -> c.by( sort1Desc ).then().by( sort2Desc ).then().by( sort3Desc ).then().by( sort4Desc ) )\n   243: \t\t\t\t.build();\n   244: \t\tassertThat( query )\n   245: \t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FOURTH_ID, THIRD_ID, SECOND_ID, FIRST_ID, EMPTY_ID, FIFTH_ID );\n   246: \t}\n",
        "uniqueId": "b67c74bdf7a5a1a8102fb0b0179bdc0774366c29_198_246__198_246_17_24",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Move And Inline Method",
        "description": "Move And Inline Method\tpublic end() : SearchPredicate moved from class org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext to class org.hibernate.search.integrationtest.backend.lucene.ExtensionIT & inlined to public predicate_fromLuceneQuery_separatePredicate() : void",
        "diffLocations": [
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/ExtensionIT.java",
                "startLine": 117,
                "endLine": 140,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/ExtensionIT.java",
                "startLine": 117,
                "endLine": 140,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/ExtensionIT.java",
                "startLine": 17,
                "endLine": 24,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "import org.apache.lucene.document.LatLonPoint;\nimport org.apache.lucene.document.NumericDocValuesField;\nimport org.apache.lucene.document.StringField;\nimport org.apache.lucene.index.IndexableField;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.search.SortField.Type;",
        "filePathBefore": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/ExtensionIT.java",
        "isPureRefactoring": true,
        "commitId": "b67c74bdf7a5a1a8102fb0b0179bdc0774366c29",
        "packageNameBefore": "org.hibernate.search.engine.search.dsl.predicate",
        "classNameBefore": "org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext",
        "methodNameBefore": "org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext#end",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext#toPredicate\n methodBody: SearchPredicate toPredicate();",
        "classSignatureBefore": "public interface SearchPredicateTerminalContext ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext#end"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext"
        ],
        "classSignatureBeforeSet": [
            "public interface SearchPredicateTerminalContext "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.lucene;\n\nimport static org.hibernate.search.util.impl.integrationtest.common.assertion.DocumentReferencesSearchResultAssert.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMapperUtils.referenceProvider;\n\nimport java.util.List;\nimport java.util.function.Consumer;\n\nimport org.apache.lucene.document.Field.Store;\nimport org.apache.lucene.document.IntPoint;\nimport org.apache.lucene.document.LatLonPoint;\nimport org.apache.lucene.document.NumericDocValuesField;\nimport org.apache.lucene.document.StringField;\nimport org.apache.lucene.index.IndexableField;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.search.SortField.Type;\nimport org.apache.lucene.search.TermQuery;\nimport org.assertj.core.api.Assertions;\n\nimport org.hibernate.search.backend.lucene.LuceneBackend;\nimport org.hibernate.search.backend.lucene.index.LuceneIndexManager;\nimport org.hibernate.search.engine.backend.Backend;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldAccessor;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.document.model.dsl.Sortable;\nimport org.hibernate.search.engine.backend.index.IndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\nimport org.hibernate.search.engine.common.spi.SearchIntegration;\nimport org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexSearchTarget;\nimport org.hibernate.search.backend.lucene.LuceneExtension;\nimport org.hibernate.search.engine.common.spi.SessionContext;\nimport org.hibernate.search.integrationtest.backend.tck.util.rule.SearchSetupHelper;\nimport org.hibernate.search.engine.logging.spi.EventContexts;\nimport org.hibernate.search.engine.search.DocumentReference;\nimport org.hibernate.search.engine.search.SearchPredicate;\nimport org.hibernate.search.engine.search.SearchQuery;\nimport org.hibernate.search.engine.search.SearchSort;\nimport org.hibernate.search.engine.spatial.GeoPoint;\nimport org.hibernate.search.util.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.FailureReportUtils;\nimport org.hibernate.search.util.impl.integrationtest.common.assertion.ProjectionsSearchResultAssert;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.StubSessionContext;\nimport org.hibernate.search.util.impl.test.SubTest;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\n\npublic class ExtensionIT {\n\n\tprivate static final String BACKEND_NAME = \"myLuceneBackend\";\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\n\tprivate static final String FIRST_ID = \"1\";\n\tprivate static final String SECOND_ID = \"2\";\n\tprivate static final String THIRD_ID = \"3\";\n\tprivate static final String FOURTH_ID = \"4\";\n\tprivate static final String FIFTH_ID = \"5\";\n\n\t@Rule\n\tpublic SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic ExpectedException thrown = ExpectedException.none();\n\n\tprivate SearchIntegration integration;\n\tprivate IndexAccessors indexAccessors;\n\tprivate MappedIndexManager<?> indexManager;\n\tprivate SessionContext sessionContext = new StubSessionContext();\n\n\t@Before\n\tpublic void setup() {\n\t\tthis.integration = setupHelper.withDefaultConfiguration( BACKEND_NAME )\n\t\t\t\t.withIndex(\n\t\t\t\t\t\t\"MappedType\", INDEX_NAME,\n\t\t\t\t\t\tctx -> this.indexAccessors = new IndexAccessors( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.indexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.setup();\n\n\t\tinitData();\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.bool( b -> {\n\t\t\t\t\tb.should( c -> c.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) )\n\t\t\t\t\t);\n\t\t\t\t} ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery_separatePredicate() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchPredicate predicate1 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) ).end();\n\t\tSearchPredicate predicate2 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) ).end();\n\t\tSearchPredicate predicate3 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) ).end();\n\t\tSearchPredicate booleanPredicate = searchTarget.predicate().bool( b -> {\n\t\t\tb.should( predicate1 );\n\t\t\tb.should( predicate2 );\n\t\t\tb.should( predicate3 );\n\t\t} ).end();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( booleanPredicate )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension().ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSort( new Sort(\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tTHIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField_separateSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchSort sort1 = searchTarget.sort().extension()\n\t\t\t\t\t\t.ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.orElseFail()\n\t\t\t\t.end();\n\t\tSearchSort sort2 = searchTarget.sort().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t.end();\n\t\tSearchSort sort3 = searchTarget.sort().extension()\n\t\t\t\t.ifSupported(\n\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.orElseFail()\n\t\t\t\t.end();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1 ).then().by( sort2 ).then().by( sort3 ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID );\n\n\t\tSearchSort sort = searchTarget.sort()\n\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneSort( new Sort(\n\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.end();\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, THIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_throwsException() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"match() predicate on unsupported native field\",\n\t\t\t\t() -> searchTarget.query( sessionContext )\n\t\t\t\t\t\t.asReferences()\n\t\t\t\t\t\t.predicate( root -> root\n\t\t\t\t\t\t\t\t.match().onField( \"nativeField\" ).matching( \"37\" )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining predicates with the DSL: use the Lucene extension and a native query.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void sort_nativeField_throwsException() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"sort on unsupported native field\",\n\t\t\t\t() -> searchTarget.query( sessionContext )\n\t\t\t\t\t\t.asReferences()\n\t\t\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t\t\t.sort( c -> c.byField( \"nativeField\" ) )\n\t\t\t\t\t\t.build()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining sorts with the DSL: use the Lucene extension and a native sort.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_nativeQuery() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root\n\t\t\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneQuery( new TermQuery( new Term( \"nativeField\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<List<?>> query = searchTarget.query( sessionContext )\n\t\t\t\t.asProjections( searchTarget.projection().field( \"nativeField\", Integer.class ).toProjection() )\n\t\t\t\t.predicate( root -> root.match().onField( \"string\" ).matching( \"text 1\" ) )\n\t\t\t\t.build();\n\n\t\tProjectionsSearchResultAssert.assertThat( query ).hasProjectionsHitsAnyOrder( c -> {\n\t\t\tc.projection( 37 );\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField_unsupportedProjection() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\t// let's check that it's possible to query the field beforehand\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root\n\t\t\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneQuery( new TermQuery( new Term( \"nativeField_unsupportedProjection\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\n\t\t// now, let's check that projecting on the field throws an exception\n\t\tSubTest.expectException(\n\t\t\t\t\"projection on native field not supporting projections\",\n\t\t\t\t() -> {\n\t\t\t\t\t\tSearchQuery<List<?>> projectionQuery = searchTarget.query( sessionContext )\n\t\t\t\t\t\t\t\t.asProjections( searchTarget.projection().field( \"nativeField_unsupportedProjection\", Integer.class ).toProjection() )\n\t\t\t\t\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t\t\t\t\t.build();\n\t\t\t\t\t\tprojectionQuery.execute();\n\t\t\t\t} )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Projections are not enabled for field\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField_unsupportedProjection\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_nativeSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.extension( LuceneExtension.get() ).fromLuceneSortField( new SortField( \"nativeField\", Type.LONG ) ) )\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, THIRD_ID, FIRST_ID, FIFTH_ID, SECOND_ID, FOURTH_ID );\n\t}\n\n\t@Test\n\tpublic void nativeField_invalidFieldPath() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan( sessionContext );\n\n\t\tSubTest.expectException(\n\t\t\t\t\"native field contributing field with invalid field path\",\n\t\t\t\t() -> workPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\t\t\tindexAccessors.nativeField_invalidFieldPath.write( document, 45 );\n\t\t\t\t} ) )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Invalid field path; expected path 'nativeField_invalidFieldPath', got 'not the expected path'.\" );\n\t}\n\n\t@Test\n\tpublic void backend_unwrap() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tAssertions.assertThat( backend.unwrap( LuceneBackend.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void backend_unwrap_error_unknownType() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene backend to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this backend can only be unwrapped to '\" + LuceneBackend.class.getName() + \"'\" );\n\n\t\tbackend.unwrap( String.class );\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\t\tAssertions.assertThat( indexManager.unwrap( LuceneIndexManager.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap_error_unknownType() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene index manager to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this index manager can only be unwrapped to '\" + LuceneIndexManager.class.getName() + \"'\" );\n\n\t\tindexManager.unwrap( String.class );\n\t}\n\n\tprivate void initData() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan( sessionContext );\n\t\tworkPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tindexAccessors.string.write( document, \"text 1\" );\n\n\t\t\tindexAccessors.nativeField.write( document, 37 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 37 );\n\n\t\t\tindexAccessors.sort1.write( document, \"a\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tindexAccessors.integer.write( document, 2 );\n\n\t\t\tindexAccessors.nativeField.write( document, 78 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 78 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"a\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tindexAccessors.geoPoint.write( document, GeoPoint.of( 40.12, -71.34 ) );\n\n\t\t\tindexAccessors.nativeField.write( document, 13 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 13 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tindexAccessors.nativeField.write( document, 89 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 89 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tindexAccessors.string.write( document, \"text 2\" );\n\t\t\tindexAccessors.integer.write( document, 1 );\n\t\t\tindexAccessors.geoPoint.write( document, GeoPoint.of( 45.12, -75.34 ) );\n\n\t\t\tindexAccessors.nativeField.write( document, 53 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 53 );\n\n\t\t\tindexAccessors.sort1.write( document, \"zz\" );\n\t\t\tindexAccessors.sort2.write( document, \"zz\" );\n\t\t\tindexAccessors.sort3.write( document, \"zz\" );\n\t\t} );\n\n\t\tworkPlan.execute().join();\n\n\t\t// Check that all documents are searchable\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsAnyOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}\n\n\tprivate static class IndexAccessors {\n\t\tfinal IndexFieldAccessor<Integer> integer;\n\t\tfinal IndexFieldAccessor<String> string;\n\t\tfinal IndexFieldAccessor<GeoPoint> geoPoint;\n\t\tfinal IndexFieldAccessor<Integer> nativeField;\n\t\tfinal IndexFieldAccessor<Integer> nativeField_unsupportedProjection;\n\t\tfinal IndexFieldAccessor<Integer> nativeField_invalidFieldPath;\n\n\t\tfinal IndexFieldAccessor<String> sort1;\n\t\tfinal IndexFieldAccessor<String> sort2;\n\t\tfinal IndexFieldAccessor<String> sort3;\n\n\t\tIndexAccessors(IndexSchemaElement root) {\n\t\t\tinteger = root.field( \"integer\" )\n\t\t\t\t\t.asInteger()\n\t\t\t\t\t.createAccessor();\n\t\t\tstring = root.field( \"string\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.createAccessor();\n\t\t\tgeoPoint = root.field( \"geoPoint\" )\n\t\t\t\t\t.asGeoPoint()\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField = root.field( \"nativeField\" )\n\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t.asLuceneField( Integer.class, ExtensionIT::contributeNativeField, ExtensionIT::fromNativeField )\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField_unsupportedProjection = root.field( \"nativeField_unsupportedProjection\" )\n\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t.asLuceneField( Integer.class, ExtensionIT::contributeNativeField )\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField_invalidFieldPath = root.field( \"nativeField_invalidFieldPath\" )\n\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t.asLuceneField( Integer.class, ExtensionIT::contributeNativeFieldInvalidFieldPath )\n\t\t\t\t\t.createAccessor();\n\n\t\t\tsort1 = root.field( \"sort1\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.sortable( Sortable.YES )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort2 = root.field( \"sort2\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.sortable( Sortable.YES )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort3 = root.field( \"sort3\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.sortable( Sortable.YES )\n\t\t\t\t\t.createAccessor();\n\t\t}\n\t}\n\n\tprivate static void contributeNativeField(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( absoluteFieldPath, value.toString(), Store.YES ) );\n\t\tcollector.accept( new NumericDocValuesField( absoluteFieldPath, value.longValue() ) );\n\t}\n\n\tprivate static Integer fromNativeField(IndexableField field) {\n\t\treturn Integer.parseInt( field.stringValue() );\n\t}\n\n\tprivate static void contributeNativeFieldInvalidFieldPath(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( \"not the expected path\", value.toString(), Store.YES ) );\n\t}\n}\n",
        "filePathAfter": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/ExtensionIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.lucene;\n\nimport static org.hibernate.search.util.impl.integrationtest.common.assertion.DocumentReferencesSearchResultAssert.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMapperUtils.referenceProvider;\n\nimport java.util.List;\nimport java.util.function.Consumer;\n\nimport org.apache.lucene.document.Field.Store;\nimport org.apache.lucene.document.IntPoint;\nimport org.apache.lucene.document.LatLonPoint;\nimport org.apache.lucene.document.NumericDocValuesField;\nimport org.apache.lucene.document.StringField;\nimport org.apache.lucene.index.IndexableField;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.search.SortField.Type;\nimport org.apache.lucene.search.TermQuery;\nimport org.assertj.core.api.Assertions;\n\nimport org.hibernate.search.backend.lucene.LuceneBackend;\nimport org.hibernate.search.backend.lucene.index.LuceneIndexManager;\nimport org.hibernate.search.engine.backend.Backend;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldAccessor;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.document.model.dsl.Sortable;\nimport org.hibernate.search.engine.backend.index.IndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\nimport org.hibernate.search.engine.common.spi.SearchIntegration;\nimport org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexSearchTarget;\nimport org.hibernate.search.backend.lucene.LuceneExtension;\nimport org.hibernate.search.engine.common.spi.SessionContext;\nimport org.hibernate.search.integrationtest.backend.tck.util.rule.SearchSetupHelper;\nimport org.hibernate.search.engine.logging.spi.EventContexts;\nimport org.hibernate.search.engine.search.DocumentReference;\nimport org.hibernate.search.engine.search.SearchPredicate;\nimport org.hibernate.search.engine.search.SearchQuery;\nimport org.hibernate.search.engine.search.SearchSort;\nimport org.hibernate.search.engine.spatial.GeoPoint;\nimport org.hibernate.search.util.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.FailureReportUtils;\nimport org.hibernate.search.util.impl.integrationtest.common.assertion.ProjectionsSearchResultAssert;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.StubSessionContext;\nimport org.hibernate.search.util.impl.test.SubTest;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\n\npublic class ExtensionIT {\n\n\tprivate static final String BACKEND_NAME = \"myLuceneBackend\";\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\n\tprivate static final String FIRST_ID = \"1\";\n\tprivate static final String SECOND_ID = \"2\";\n\tprivate static final String THIRD_ID = \"3\";\n\tprivate static final String FOURTH_ID = \"4\";\n\tprivate static final String FIFTH_ID = \"5\";\n\n\t@Rule\n\tpublic SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic ExpectedException thrown = ExpectedException.none();\n\n\tprivate SearchIntegration integration;\n\tprivate IndexAccessors indexAccessors;\n\tprivate MappedIndexManager<?> indexManager;\n\tprivate SessionContext sessionContext = new StubSessionContext();\n\n\t@Before\n\tpublic void setup() {\n\t\tthis.integration = setupHelper.withDefaultConfiguration( BACKEND_NAME )\n\t\t\t\t.withIndex(\n\t\t\t\t\t\t\"MappedType\", INDEX_NAME,\n\t\t\t\t\t\tctx -> this.indexAccessors = new IndexAccessors( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.indexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.setup();\n\n\t\tinitData();\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.bool( b -> {\n\t\t\t\t\tb.should( c -> c.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) )\n\t\t\t\t\t);\n\t\t\t\t} ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery_separatePredicate() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchPredicate predicate1 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) ).toPredicate();\n\t\tSearchPredicate predicate2 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) ).toPredicate();\n\t\tSearchPredicate predicate3 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) ).toPredicate();\n\t\tSearchPredicate booleanPredicate = searchTarget.predicate().bool( b -> {\n\t\t\tb.should( predicate1 );\n\t\t\tb.should( predicate2 );\n\t\t\tb.should( predicate3 );\n\t\t} ).toPredicate();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( booleanPredicate )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension().ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSort( new Sort(\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tTHIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField_separateSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchSort sort1 = searchTarget.sort().extension()\n\t\t\t\t\t\t.ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.orElseFail()\n\t\t\t\t.toSort();\n\t\tSearchSort sort2 = searchTarget.sort().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t.toSort();\n\t\tSearchSort sort3 = searchTarget.sort().extension()\n\t\t\t\t.ifSupported(\n\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.orElseFail()\n\t\t\t\t.toSort();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1 ).then().by( sort2 ).then().by( sort3 ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID );\n\n\t\tSearchSort sort = searchTarget.sort()\n\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneSort( new Sort(\n\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.toSort();\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, THIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_throwsException() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"match() predicate on unsupported native field\",\n\t\t\t\t() -> searchTarget.query( sessionContext )\n\t\t\t\t\t\t.asReferences()\n\t\t\t\t\t\t.predicate( root -> root\n\t\t\t\t\t\t\t\t.match().onField( \"nativeField\" ).matching( \"37\" )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining predicates with the DSL: use the Lucene extension and a native query.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void sort_nativeField_throwsException() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"sort on unsupported native field\",\n\t\t\t\t() -> searchTarget.query( sessionContext )\n\t\t\t\t\t\t.asReferences()\n\t\t\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t\t\t.sort( c -> c.byField( \"nativeField\" ) )\n\t\t\t\t\t\t.build()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining sorts with the DSL: use the Lucene extension and a native sort.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_nativeQuery() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root\n\t\t\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneQuery( new TermQuery( new Term( \"nativeField\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<List<?>> query = searchTarget.query( sessionContext )\n\t\t\t\t.asProjections( searchTarget.projection().field( \"nativeField\", Integer.class ).toProjection() )\n\t\t\t\t.predicate( root -> root.match().onField( \"string\" ).matching( \"text 1\" ) )\n\t\t\t\t.build();\n\n\t\tProjectionsSearchResultAssert.assertThat( query ).hasProjectionsHitsAnyOrder( c -> {\n\t\t\tc.projection( 37 );\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField_unsupportedProjection() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\t// let's check that it's possible to query the field beforehand\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root\n\t\t\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneQuery( new TermQuery( new Term( \"nativeField_unsupportedProjection\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\n\t\t// now, let's check that projecting on the field throws an exception\n\t\tSubTest.expectException(\n\t\t\t\t\"projection on native field not supporting projections\",\n\t\t\t\t() -> {\n\t\t\t\t\t\tSearchQuery<List<?>> projectionQuery = searchTarget.query( sessionContext )\n\t\t\t\t\t\t\t\t.asProjections( searchTarget.projection().field( \"nativeField_unsupportedProjection\", Integer.class ).toProjection() )\n\t\t\t\t\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t\t\t\t\t.build();\n\t\t\t\t\t\tprojectionQuery.execute();\n\t\t\t\t} )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Projections are not enabled for field\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField_unsupportedProjection\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_nativeSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.extension( LuceneExtension.get() ).fromLuceneSortField( new SortField( \"nativeField\", Type.LONG ) ) )\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, THIRD_ID, FIRST_ID, FIFTH_ID, SECOND_ID, FOURTH_ID );\n\t}\n\n\t@Test\n\tpublic void nativeField_invalidFieldPath() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan( sessionContext );\n\n\t\tSubTest.expectException(\n\t\t\t\t\"native field contributing field with invalid field path\",\n\t\t\t\t() -> workPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\t\t\tindexAccessors.nativeField_invalidFieldPath.write( document, 45 );\n\t\t\t\t} ) )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Invalid field path; expected path 'nativeField_invalidFieldPath', got 'not the expected path'.\" );\n\t}\n\n\t@Test\n\tpublic void backend_unwrap() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tAssertions.assertThat( backend.unwrap( LuceneBackend.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void backend_unwrap_error_unknownType() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene backend to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this backend can only be unwrapped to '\" + LuceneBackend.class.getName() + \"'\" );\n\n\t\tbackend.unwrap( String.class );\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\t\tAssertions.assertThat( indexManager.unwrap( LuceneIndexManager.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap_error_unknownType() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene index manager to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this index manager can only be unwrapped to '\" + LuceneIndexManager.class.getName() + \"'\" );\n\n\t\tindexManager.unwrap( String.class );\n\t}\n\n\tprivate void initData() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan( sessionContext );\n\t\tworkPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tindexAccessors.string.write( document, \"text 1\" );\n\n\t\t\tindexAccessors.nativeField.write( document, 37 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 37 );\n\n\t\t\tindexAccessors.sort1.write( document, \"a\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tindexAccessors.integer.write( document, 2 );\n\n\t\t\tindexAccessors.nativeField.write( document, 78 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 78 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"a\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tindexAccessors.geoPoint.write( document, GeoPoint.of( 40.12, -71.34 ) );\n\n\t\t\tindexAccessors.nativeField.write( document, 13 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 13 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tindexAccessors.nativeField.write( document, 89 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 89 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tindexAccessors.string.write( document, \"text 2\" );\n\t\t\tindexAccessors.integer.write( document, 1 );\n\t\t\tindexAccessors.geoPoint.write( document, GeoPoint.of( 45.12, -75.34 ) );\n\n\t\t\tindexAccessors.nativeField.write( document, 53 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 53 );\n\n\t\t\tindexAccessors.sort1.write( document, \"zz\" );\n\t\t\tindexAccessors.sort2.write( document, \"zz\" );\n\t\t\tindexAccessors.sort3.write( document, \"zz\" );\n\t\t} );\n\n\t\tworkPlan.execute().join();\n\n\t\t// Check that all documents are searchable\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsAnyOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}\n\n\tprivate static class IndexAccessors {\n\t\tfinal IndexFieldAccessor<Integer> integer;\n\t\tfinal IndexFieldAccessor<String> string;\n\t\tfinal IndexFieldAccessor<GeoPoint> geoPoint;\n\t\tfinal IndexFieldAccessor<Integer> nativeField;\n\t\tfinal IndexFieldAccessor<Integer> nativeField_unsupportedProjection;\n\t\tfinal IndexFieldAccessor<Integer> nativeField_invalidFieldPath;\n\n\t\tfinal IndexFieldAccessor<String> sort1;\n\t\tfinal IndexFieldAccessor<String> sort2;\n\t\tfinal IndexFieldAccessor<String> sort3;\n\n\t\tIndexAccessors(IndexSchemaElement root) {\n\t\t\tinteger = root.field( \"integer\" )\n\t\t\t\t\t.asInteger()\n\t\t\t\t\t.createAccessor();\n\t\t\tstring = root.field( \"string\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.createAccessor();\n\t\t\tgeoPoint = root.field( \"geoPoint\" )\n\t\t\t\t\t.asGeoPoint()\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField = root.field( \"nativeField\" )\n\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t.asLuceneField( Integer.class, ExtensionIT::contributeNativeField, ExtensionIT::fromNativeField )\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField_unsupportedProjection = root.field( \"nativeField_unsupportedProjection\" )\n\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t.asLuceneField( Integer.class, ExtensionIT::contributeNativeField )\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField_invalidFieldPath = root.field( \"nativeField_invalidFieldPath\" )\n\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t.asLuceneField( Integer.class, ExtensionIT::contributeNativeFieldInvalidFieldPath )\n\t\t\t\t\t.createAccessor();\n\n\t\t\tsort1 = root.field( \"sort1\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.sortable( Sortable.YES )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort2 = root.field( \"sort2\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.sortable( Sortable.YES )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort3 = root.field( \"sort3\" )\n\t\t\t\t\t.asString()\n\t\t\t\t\t.sortable( Sortable.YES )\n\t\t\t\t\t.createAccessor();\n\t\t}\n\t}\n\n\tprivate static void contributeNativeField(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( absoluteFieldPath, value.toString(), Store.YES ) );\n\t\tcollector.accept( new NumericDocValuesField( absoluteFieldPath, value.longValue() ) );\n\t}\n\n\tprivate static Integer fromNativeField(IndexableField field) {\n\t\treturn Integer.parseInt( field.stringValue() );\n\t}\n\n\tprivate static void contributeNativeFieldInvalidFieldPath(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( \"not the expected path\", value.toString(), Store.YES ) );\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext#toPredicate\n methodBody: SearchPredicate toPredicate();"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void predicate_fromLuceneQuery_separatePredicate() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchPredicate predicate1 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) ).toPredicate();\n\t\tSearchPredicate predicate2 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) ).toPredicate();\n\t\tSearchPredicate predicate3 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) ).toPredicate();\n\t\tSearchPredicate booleanPredicate = searchTarget.predicate().bool( b -> {\n\t\t\tb.should( predicate1 );\n\t\t\tb.should( predicate2 );\n\t\t\tb.should( predicate3 );\n\t\t} ).toPredicate();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( booleanPredicate )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasHitCount( 3 );\n\t}",
        "diffSourceCode": "    17: import org.apache.lucene.document.LatLonPoint;\n    18: import org.apache.lucene.document.NumericDocValuesField;\n    19: import org.apache.lucene.document.StringField;\n    20: import org.apache.lucene.index.IndexableField;\n    21: import org.apache.lucene.index.Term;\n    22: import org.apache.lucene.search.Sort;\n    23: import org.apache.lucene.search.SortField;\n    24: import org.apache.lucene.search.SortField.Type;\n   117: \t@Test\n   118: \tpublic void predicate_fromLuceneQuery_separatePredicate() {\n   119: \t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n   120: \n   121: \t\tSearchPredicate predicate1 = searchTarget.predicate().extension( LuceneExtension.get() )\n-  122: \t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) ).end();\n+  122: \t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) ).toPredicate();\n   123: \t\tSearchPredicate predicate2 = searchTarget.predicate().extension( LuceneExtension.get() )\n-  124: \t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) ).end();\n+  124: \t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) ).toPredicate();\n   125: \t\tSearchPredicate predicate3 = searchTarget.predicate().extension( LuceneExtension.get() )\n-  126: \t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) ).end();\n+  126: \t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) ).toPredicate();\n   127: \t\tSearchPredicate booleanPredicate = searchTarget.predicate().bool( b -> {\n   128: \t\t\tb.should( predicate1 );\n   129: \t\t\tb.should( predicate2 );\n   130: \t\t\tb.should( predicate3 );\n-  131: \t\t} ).end();\n+  131: \t\t} ).toPredicate();\n   132: \n   133: \t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n   134: \t\t\t\t.asReferences()\n   135: \t\t\t\t.predicate( booleanPredicate )\n   136: \t\t\t\t.build();\n   137: \t\tassertThat( query )\n   138: \t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n   139: \t\t\t\t.hasHitCount( 3 );\n   140: \t}\n",
        "uniqueId": "b67c74bdf7a5a1a8102fb0b0179bdc0774366c29_117_140__117_140_17_24",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Move And Inline Method",
        "description": "Move And Inline Method\tpublic end() : SearchPredicate moved from class org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext to class org.hibernate.search.integrationtest.backend.elasticsearch.ExtensionIT & inlined to public predicate_fromJsonString_separatePredicate() : void",
        "diffLocations": [
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/ExtensionIT.java",
                "startLine": 114,
                "endLine": 152,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/ExtensionIT.java",
                "startLine": 114,
                "endLine": 152,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/ExtensionIT.java",
                "startLine": 17,
                "endLine": 24,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "import org.hibernate.search.engine.backend.document.IndexFieldAccessor;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.index.IndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexSearchTarget;\nimport org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\nimport org.hibernate.search.engine.common.spi.SearchIntegration;\nimport org.hibernate.search.engine.common.spi.SessionContext;\nimport org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;",
        "filePathBefore": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/ExtensionIT.java",
        "isPureRefactoring": true,
        "commitId": "b67c74bdf7a5a1a8102fb0b0179bdc0774366c29",
        "packageNameBefore": "org.hibernate.search.engine.search.dsl.predicate",
        "classNameBefore": "org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext",
        "methodNameBefore": "org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext#end",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext#toPredicate\n methodBody: SearchPredicate toPredicate();",
        "classSignatureBefore": "public interface SearchPredicateTerminalContext ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext#end"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext"
        ],
        "classSignatureBeforeSet": [
            "public interface SearchPredicateTerminalContext "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.elasticsearch;\n\nimport static org.hibernate.search.util.impl.integrationtest.common.assertion.DocumentReferencesSearchResultAssert.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMapperUtils.referenceProvider;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchBackend;\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchExtension;\nimport org.hibernate.search.backend.elasticsearch.index.ElasticsearchIndexManager;\nimport org.hibernate.search.engine.backend.Backend;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldAccessor;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.index.IndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexSearchTarget;\nimport org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\nimport org.hibernate.search.engine.common.spi.SearchIntegration;\nimport org.hibernate.search.engine.common.spi.SessionContext;\nimport org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;\nimport org.hibernate.search.engine.search.DocumentReference;\nimport org.hibernate.search.engine.search.SearchPredicate;\nimport org.hibernate.search.engine.search.SearchQuery;\nimport org.hibernate.search.engine.search.SearchSort;\nimport org.hibernate.search.integrationtest.backend.tck.util.rule.SearchSetupHelper;\nimport org.hibernate.search.util.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.StubSessionContext;\n\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\n\nimport org.apache.http.nio.client.HttpAsyncClient;\nimport org.assertj.core.api.Assertions;\nimport org.elasticsearch.client.Response;\nimport org.elasticsearch.client.RestClient;\n\npublic class ExtensionIT {\n\n\tprivate static final String BACKEND_NAME = \"myElasticsearchBackend\";\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\n\tprivate static final String FIRST_ID = \"1\";\n\tprivate static final String SECOND_ID = \"2\";\n\tprivate static final String THIRD_ID = \"3\";\n\tprivate static final String FOURTH_ID = \"4\";\n\tprivate static final String FIFTH_ID = \"5\";\n\tprivate static final String EMPTY_ID = \"empty\";\n\n\t@Rule\n\tpublic SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic ExpectedException thrown = ExpectedException.none();\n\n\tprivate SearchIntegration integration;\n\tprivate IndexAccessors indexAccessors;\n\tprivate MappedIndexManager<?> indexManager;\n\tprivate SessionContext sessionContext = new StubSessionContext();\n\n\t@Before\n\tpublic void setup() {\n\t\tthis.integration = setupHelper.withDefaultConfiguration( BACKEND_NAME )\n\t\t\t\t.withIndex(\n\t\t\t\t\t\t\"MappedType\", INDEX_NAME,\n\t\t\t\t\t\tctx -> this.indexAccessors = new IndexAccessors( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.indexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.setup();\n\n\t\tinitData();\n\t}\n\n\t@Test\n\tpublic void predicate_fromJsonString() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.bool( b -> {\n\t\t\t\t\tb.should( c -> c.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t.fromJsonString( \"{'match': {'string': 'text 1'}}\" )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t.fromJsonString( \"{'match': {'integer': 2}}\" )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t.fromJsonString(\n\t\t\t\t\t\t\t\t\t\"{\"\n\t\t\t\t\t\t\t\t\t\t+ \"'geo_distance': {\"\n\t\t\t\t\t\t\t\t\t\t\t+ \"'distance': '200km',\"\n\t\t\t\t\t\t\t\t\t\t\t+ \"'geoPoint': {\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'lat': 40,\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'lon': -70\"\n\t\t\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t)\n\t\t\t\t\t);\n\t\t\t\t\t// Also test using the standard DSL on a field defined with the extension\n\t\t\t\t\tb.should( c -> c.match().onField( \"yearDays\" ).matching( \"'2018:12'\" ) );\n\t\t\t\t} ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID )\n\t\t\t\t.hasHitCount( 4 );\n\t}\n\n\t@Test\n\tpublic void predicate_fromJsonString_separatePredicate() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchPredicate predicate1 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'match': {'string': 'text 1'}}\" ).end();\n\t\tSearchPredicate predicate2 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'match': {'integer': 2}}\" ).end();\n\t\tSearchPredicate predicate3 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString(\n\t\t\t\t\t\t\"{\"\n\t\t\t\t\t\t\t+ \"'geo_distance': {\"\n\t\t\t\t\t\t\t\t+ \"'distance': '200km',\"\n\t\t\t\t\t\t\t\t+ \"'geoPoint': {\"\n\t\t\t\t\t\t\t\t\t+ \"'lat': 40,\"\n\t\t\t\t\t\t\t\t\t+ \"'lon': -70\"\n\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t\t\t.end();\n\t\t// Also test using the standard DSL on a field defined with the extension\n\t\tSearchPredicate predicate4 = searchTarget.predicate().match().onField( \"yearDays\" )\n\t\t\t\t.matching( \"'2018:12'\" ).end();\n\t\tSearchPredicate booleanPredicate = searchTarget.predicate().bool( b -> {\n\t\t\tb.should( predicate1 );\n\t\t\tb.should( predicate2 );\n\t\t\tb.should( predicate3 );\n\t\t\tb.should( predicate4 );\n\t\t} ).end();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( booleanPredicate )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID )\n\t\t\t\t.hasHitCount( 4 );\n\t}\n\n\t@Test\n\tpublic void sort_fromJsonString() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort1': 'asc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort2': 'asc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort3': 'asc'}\" )\n\t\t\t\t\t\t// Also test using the standard DSL on a field defined with the extension\n\t\t\t\t\t\t.then().byField( \"sort4\" ).asc().onMissingValue().sortLast()\n\t\t\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, EMPTY_ID, FIFTH_ID\n\t\t);\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort1': 'desc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort2': 'desc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort3': 'desc'}\" )\n\t\t\t\t\t\t.then().byField( \"sort4\" ).desc().onMissingValue().sortLast()\n\t\t\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFOURTH_ID, THIRD_ID, SECOND_ID, FIRST_ID, EMPTY_ID, FIFTH_ID\n\t\t);\n\t}\n\n\t@Test\n\tpublic void sort_fromJsonString_separateSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchSort sort1Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort1': 'asc'}\" )\n\t\t\t\t.end();\n\t\tSearchSort sort2Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort2': 'asc'}\" )\n\t\t\t\t.end();\n\t\tSearchSort sort3Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort3': 'asc'}\" )\n\t\t\t\t.end();\n\t\t// Also test using the standard DSL on a field defined with the extension\n\t\tSearchSort sort4Asc = searchTarget.sort()\n\t\t\t\t.byField( \"sort4\" ).asc().onMissingValue().sortLast()\n\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t.end();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1Asc ).then().by( sort2Asc ).then().by( sort3Asc ).then().by( sort4Asc ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, EMPTY_ID, FIFTH_ID );\n\n\t\tSearchSort sort1Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort1': 'desc'}\" )\n\t\t\t\t.end();\n\t\tSearchSort sort2Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort2': 'desc'}\" )\n\t\t\t\t.end();\n\t\tSearchSort sort3Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort3': 'desc'}\" )\n\t\t\t\t.end();\n\t\tSearchSort sort4Desc = searchTarget.sort()\n\t\t\t\t.byField( \"sort4\" ).desc().onMissingValue().sortLast()\n\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t.end();\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1Desc ).then().by( sort2Desc ).then().by( sort3Desc ).then().by( sort4Desc ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FOURTH_ID, THIRD_ID, SECOND_ID, FIRST_ID, EMPTY_ID, FIFTH_ID );\n\t}\n\n\t@Test\n\tpublic void backend_unwrap() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tAssertions.assertThat( backend.unwrap( ElasticsearchBackend.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void backend_unwrap_error_unknownType() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap an Elasticsearch backend to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this backend can only be unwrapped to '\" + ElasticsearchBackend.class.getName() + \"'\" );\n\n\t\tbackend.unwrap( String.class );\n\t}\n\n\t@Test\n\tpublic void backend_getClient() throws Exception {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tElasticsearchBackend elasticsearchBackend = backend.unwrap( ElasticsearchBackend.class );\n\t\tRestClient restClient = elasticsearchBackend.getClient( RestClient.class );\n\n\t\t// Test that the client actually works\n\t\tResponse response = restClient.performRequest( \"GET\", \"/\" );\n\t\tAssertions.assertThat( response.getStatusLine().getStatusCode() ).isEqualTo( 200 );\n\t}\n\n\t@Test\n\tpublic void backend_getClient_error_invalidClass() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tElasticsearchBackend elasticsearchBackend = backend.unwrap( ElasticsearchBackend.class );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( HttpAsyncClient.class.getName() );\n\t\tthrown.expectMessage( \"the client can only be unwrapped to\" );\n\t\tthrown.expectMessage( RestClient.class.getName() );\n\n\t\telasticsearchBackend.getClient( HttpAsyncClient.class );\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\t\tAssertions.assertThat( indexManager.unwrap( ElasticsearchIndexManager.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap_error_unknownType() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap an Elasticsearch index manager to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this index manager can only be unwrapped to '\" + ElasticsearchIndexManager.class.getName() + \"'\" );\n\n\t\tindexManager.unwrap( String.class );\n\t}\n\n\tprivate void initData() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan( sessionContext );\n\t\tworkPlan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tindexAccessors.integer.write( document, \"2\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"a\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t\tindexAccessors.sort4.write( document, \"z\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tindexAccessors.string.write( document, \"'text 1'\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"a\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t\tindexAccessors.sort4.write( document, \"z\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tindexAccessors.geoPoint.write( document, \"{'lat': 40.12, 'lon': -71.34}\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"a\" );\n\t\t\tindexAccessors.sort4.write( document, \"z\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tindexAccessors.yearDays.write( document, \"'2018:012'\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t\tindexAccessors.sort4.write( document, \"a\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tindexAccessors.string.write( document, \"'text 2'\" );\n\t\t\tindexAccessors.integer.write( document, \"1\" );\n\t\t\tindexAccessors.geoPoint.write( document, \"{'lat': 45.12, 'lon': -75.34}\" );\n\t\t\tindexAccessors.yearDays.write( document, \"'2018:025'\" );\n\n\t\t\tindexAccessors.sort5.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( EMPTY_ID ), document -> { } );\n\n\t\tworkPlan.execute().join();\n\n\t\t// Check that all documents are searchable\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsAnyOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID, EMPTY_ID\n\t\t);\n\t}\n\n\tprivate static class IndexAccessors {\n\t\tfinal IndexFieldAccessor<String> integer;\n\t\tfinal IndexFieldAccessor<String> string;\n\t\tfinal IndexFieldAccessor<String> geoPoint;\n\t\tfinal IndexFieldAccessor<String> yearDays;\n\n\t\tfinal IndexFieldAccessor<String> sort1;\n\t\tfinal IndexFieldAccessor<String> sort2;\n\t\tfinal IndexFieldAccessor<String> sort3;\n\t\tfinal IndexFieldAccessor<String> sort4;\n\t\tfinal IndexFieldAccessor<String> sort5;\n\n\t\tIndexAccessors(IndexSchemaElement root) {\n\t\t\tinteger = root.field( \"integer\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'integer'}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tstring = root.field( \"string\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword'}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tgeoPoint = root.field( \"geoPoint\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'geo_point'}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tyearDays = root.field( \"yearDays\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'date', 'format': 'yyyy:DDD'}\" )\n\t\t\t\t\t.createAccessor();\n\n\t\t\tsort1 = root.field( \"sort1\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort2 = root.field( \"sort2\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort3 = root.field( \"sort3\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort4 = root.field( \"sort4\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort5 = root.field( \"sort5\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t}\n\t}\n\n}",
        "filePathAfter": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/ExtensionIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.elasticsearch;\n\nimport static org.hibernate.search.util.impl.integrationtest.common.assertion.DocumentReferencesSearchResultAssert.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMapperUtils.referenceProvider;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchBackend;\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchExtension;\nimport org.hibernate.search.backend.elasticsearch.index.ElasticsearchIndexManager;\nimport org.hibernate.search.engine.backend.Backend;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldAccessor;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.index.IndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexSearchTarget;\nimport org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\nimport org.hibernate.search.engine.common.spi.SearchIntegration;\nimport org.hibernate.search.engine.common.spi.SessionContext;\nimport org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;\nimport org.hibernate.search.engine.search.DocumentReference;\nimport org.hibernate.search.engine.search.SearchPredicate;\nimport org.hibernate.search.engine.search.SearchQuery;\nimport org.hibernate.search.engine.search.SearchSort;\nimport org.hibernate.search.integrationtest.backend.tck.util.rule.SearchSetupHelper;\nimport org.hibernate.search.util.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.StubSessionContext;\n\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\n\nimport org.apache.http.nio.client.HttpAsyncClient;\nimport org.assertj.core.api.Assertions;\nimport org.elasticsearch.client.Response;\nimport org.elasticsearch.client.RestClient;\n\npublic class ExtensionIT {\n\n\tprivate static final String BACKEND_NAME = \"myElasticsearchBackend\";\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\n\tprivate static final String FIRST_ID = \"1\";\n\tprivate static final String SECOND_ID = \"2\";\n\tprivate static final String THIRD_ID = \"3\";\n\tprivate static final String FOURTH_ID = \"4\";\n\tprivate static final String FIFTH_ID = \"5\";\n\tprivate static final String EMPTY_ID = \"empty\";\n\n\t@Rule\n\tpublic SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic ExpectedException thrown = ExpectedException.none();\n\n\tprivate SearchIntegration integration;\n\tprivate IndexAccessors indexAccessors;\n\tprivate MappedIndexManager<?> indexManager;\n\tprivate SessionContext sessionContext = new StubSessionContext();\n\n\t@Before\n\tpublic void setup() {\n\t\tthis.integration = setupHelper.withDefaultConfiguration( BACKEND_NAME )\n\t\t\t\t.withIndex(\n\t\t\t\t\t\t\"MappedType\", INDEX_NAME,\n\t\t\t\t\t\tctx -> this.indexAccessors = new IndexAccessors( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.indexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.setup();\n\n\t\tinitData();\n\t}\n\n\t@Test\n\tpublic void predicate_fromJsonString() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.bool( b -> {\n\t\t\t\t\tb.should( c -> c.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t.fromJsonString( \"{'match': {'string': 'text 1'}}\" )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t.fromJsonString( \"{'match': {'integer': 2}}\" )\n\t\t\t\t\t);\n\t\t\t\t\tb.should( c -> c.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t.fromJsonString(\n\t\t\t\t\t\t\t\t\t\"{\"\n\t\t\t\t\t\t\t\t\t\t+ \"'geo_distance': {\"\n\t\t\t\t\t\t\t\t\t\t\t+ \"'distance': '200km',\"\n\t\t\t\t\t\t\t\t\t\t\t+ \"'geoPoint': {\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'lat': 40,\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'lon': -70\"\n\t\t\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t)\n\t\t\t\t\t);\n\t\t\t\t\t// Also test using the standard DSL on a field defined with the extension\n\t\t\t\t\tb.should( c -> c.match().onField( \"yearDays\" ).matching( \"'2018:12'\" ) );\n\t\t\t\t} ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID )\n\t\t\t\t.hasHitCount( 4 );\n\t}\n\n\t@Test\n\tpublic void predicate_fromJsonString_separatePredicate() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchPredicate predicate1 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'match': {'string': 'text 1'}}\" ).toPredicate();\n\t\tSearchPredicate predicate2 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'match': {'integer': 2}}\" ).toPredicate();\n\t\tSearchPredicate predicate3 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString(\n\t\t\t\t\t\t\"{\"\n\t\t\t\t\t\t\t+ \"'geo_distance': {\"\n\t\t\t\t\t\t\t\t+ \"'distance': '200km',\"\n\t\t\t\t\t\t\t\t+ \"'geoPoint': {\"\n\t\t\t\t\t\t\t\t\t+ \"'lat': 40,\"\n\t\t\t\t\t\t\t\t\t+ \"'lon': -70\"\n\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t\t\t.toPredicate();\n\t\t// Also test using the standard DSL on a field defined with the extension\n\t\tSearchPredicate predicate4 = searchTarget.predicate().match().onField( \"yearDays\" )\n\t\t\t\t.matching( \"'2018:12'\" ).toPredicate();\n\t\tSearchPredicate booleanPredicate = searchTarget.predicate().bool( b -> {\n\t\t\tb.should( predicate1 );\n\t\t\tb.should( predicate2 );\n\t\t\tb.should( predicate3 );\n\t\t\tb.should( predicate4 );\n\t\t} ).toPredicate();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( booleanPredicate )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID )\n\t\t\t\t.hasHitCount( 4 );\n\t}\n\n\t@Test\n\tpublic void sort_fromJsonString() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort1': 'asc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort2': 'asc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort3': 'asc'}\" )\n\t\t\t\t\t\t// Also test using the standard DSL on a field defined with the extension\n\t\t\t\t\t\t.then().byField( \"sort4\" ).asc().onMissingValue().sortLast()\n\t\t\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, EMPTY_ID, FIFTH_ID\n\t\t);\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort1': 'desc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort2': 'desc'}\" )\n\t\t\t\t\t\t.then().extension( ElasticsearchExtension.get() )\n\t\t\t\t\t\t\t\t.fromJsonString( \"{'sort3': 'desc'}\" )\n\t\t\t\t\t\t.then().byField( \"sort4\" ).desc().onMissingValue().sortLast()\n\t\t\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFOURTH_ID, THIRD_ID, SECOND_ID, FIRST_ID, EMPTY_ID, FIFTH_ID\n\t\t);\n\t}\n\n\t@Test\n\tpublic void sort_fromJsonString_separateSort() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchSort sort1Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort1': 'asc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort2Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort2': 'asc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort3Asc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort3': 'asc'}\" )\n\t\t\t\t.toSort();\n\t\t// Also test using the standard DSL on a field defined with the extension\n\t\tSearchSort sort4Asc = searchTarget.sort()\n\t\t\t\t.byField( \"sort4\" ).asc().onMissingValue().sortLast()\n\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t.toSort();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1Asc ).then().by( sort2Asc ).then().by( sort3Asc ).then().by( sort4Asc ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, EMPTY_ID, FIFTH_ID );\n\n\t\tSearchSort sort1Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort1': 'desc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort2Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort2': 'desc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort3Desc = searchTarget.sort().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'sort3': 'desc'}\" )\n\t\t\t\t.toSort();\n\t\tSearchSort sort4Desc = searchTarget.sort()\n\t\t\t\t.byField( \"sort4\" ).desc().onMissingValue().sortLast()\n\t\t\t\t.then().byField( \"sort5\" ).asc().onMissingValue().sortFirst()\n\t\t\t\t.toSort();\n\n\t\tquery = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1Desc ).then().by( sort2Desc ).then().by( sort3Desc ).then().by( sort4Desc ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsExactOrder( INDEX_NAME, FOURTH_ID, THIRD_ID, SECOND_ID, FIRST_ID, EMPTY_ID, FIFTH_ID );\n\t}\n\n\t@Test\n\tpublic void backend_unwrap() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tAssertions.assertThat( backend.unwrap( ElasticsearchBackend.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void backend_unwrap_error_unknownType() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap an Elasticsearch backend to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this backend can only be unwrapped to '\" + ElasticsearchBackend.class.getName() + \"'\" );\n\n\t\tbackend.unwrap( String.class );\n\t}\n\n\t@Test\n\tpublic void backend_getClient() throws Exception {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tElasticsearchBackend elasticsearchBackend = backend.unwrap( ElasticsearchBackend.class );\n\t\tRestClient restClient = elasticsearchBackend.getClient( RestClient.class );\n\n\t\t// Test that the client actually works\n\t\tResponse response = restClient.performRequest( \"GET\", \"/\" );\n\t\tAssertions.assertThat( response.getStatusLine().getStatusCode() ).isEqualTo( 200 );\n\t}\n\n\t@Test\n\tpublic void backend_getClient_error_invalidClass() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tElasticsearchBackend elasticsearchBackend = backend.unwrap( ElasticsearchBackend.class );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( HttpAsyncClient.class.getName() );\n\t\tthrown.expectMessage( \"the client can only be unwrapped to\" );\n\t\tthrown.expectMessage( RestClient.class.getName() );\n\n\t\telasticsearchBackend.getClient( HttpAsyncClient.class );\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\t\tAssertions.assertThat( indexManager.unwrap( ElasticsearchIndexManager.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap_error_unknownType() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap an Elasticsearch index manager to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this index manager can only be unwrapped to '\" + ElasticsearchIndexManager.class.getName() + \"'\" );\n\n\t\tindexManager.unwrap( String.class );\n\t}\n\n\tprivate void initData() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan( sessionContext );\n\t\tworkPlan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tindexAccessors.integer.write( document, \"2\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"a\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t\tindexAccessors.sort4.write( document, \"z\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tindexAccessors.string.write( document, \"'text 1'\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"a\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t\tindexAccessors.sort4.write( document, \"z\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tindexAccessors.geoPoint.write( document, \"{'lat': 40.12, 'lon': -71.34}\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"a\" );\n\t\t\tindexAccessors.sort4.write( document, \"z\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tindexAccessors.yearDays.write( document, \"'2018:012'\" );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t\tindexAccessors.sort4.write( document, \"a\" );\n\t\t\tindexAccessors.sort5.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tindexAccessors.string.write( document, \"'text 2'\" );\n\t\t\tindexAccessors.integer.write( document, \"1\" );\n\t\t\tindexAccessors.geoPoint.write( document, \"{'lat': 45.12, 'lon': -75.34}\" );\n\t\t\tindexAccessors.yearDays.write( document, \"'2018:025'\" );\n\n\t\t\tindexAccessors.sort5.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( EMPTY_ID ), document -> { } );\n\n\t\tworkPlan.execute().join();\n\n\t\t// Check that all documents are searchable\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( root -> root.matchAll() )\n\t\t\t\t.build();\n\t\tassertThat( query ).hasReferencesHitsAnyOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID, EMPTY_ID\n\t\t);\n\t}\n\n\tprivate static class IndexAccessors {\n\t\tfinal IndexFieldAccessor<String> integer;\n\t\tfinal IndexFieldAccessor<String> string;\n\t\tfinal IndexFieldAccessor<String> geoPoint;\n\t\tfinal IndexFieldAccessor<String> yearDays;\n\n\t\tfinal IndexFieldAccessor<String> sort1;\n\t\tfinal IndexFieldAccessor<String> sort2;\n\t\tfinal IndexFieldAccessor<String> sort3;\n\t\tfinal IndexFieldAccessor<String> sort4;\n\t\tfinal IndexFieldAccessor<String> sort5;\n\n\t\tIndexAccessors(IndexSchemaElement root) {\n\t\t\tinteger = root.field( \"integer\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'integer'}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tstring = root.field( \"string\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword'}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tgeoPoint = root.field( \"geoPoint\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'geo_point'}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tyearDays = root.field( \"yearDays\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'date', 'format': 'yyyy:DDD'}\" )\n\t\t\t\t\t.createAccessor();\n\n\t\t\tsort1 = root.field( \"sort1\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort2 = root.field( \"sort2\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort3 = root.field( \"sort3\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort4 = root.field( \"sort4\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort5 = root.field( \"sort5\" )\n\t\t\t\t\t.extension( ElasticsearchExtension.get() )\n\t\t\t\t\t.asJsonString( \"{'type': 'keyword', 'doc_values': true}\" )\n\t\t\t\t\t.createAccessor();\n\t\t}\n\t}\n\n}",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.search.dsl.predicate.SearchPredicateTerminalContext#toPredicate\n methodBody: SearchPredicate toPredicate();"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void predicate_fromJsonString_separatePredicate() {\n\t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n\n\t\tSearchPredicate predicate1 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'match': {'string': 'text 1'}}\" ).toPredicate();\n\t\tSearchPredicate predicate2 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString( \"{'match': {'integer': 2}}\" ).toPredicate();\n\t\tSearchPredicate predicate3 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n\t\t\t\t.fromJsonString(\n\t\t\t\t\t\t\"{\"\n\t\t\t\t\t\t\t+ \"'geo_distance': {\"\n\t\t\t\t\t\t\t\t+ \"'distance': '200km',\"\n\t\t\t\t\t\t\t\t+ \"'geoPoint': {\"\n\t\t\t\t\t\t\t\t\t+ \"'lat': 40,\"\n\t\t\t\t\t\t\t\t\t+ \"'lon': -70\"\n\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t\t\t.toPredicate();\n\t\t// Also test using the standard DSL on a field defined with the extension\n\t\tSearchPredicate predicate4 = searchTarget.predicate().match().onField( \"yearDays\" )\n\t\t\t\t.matching( \"'2018:12'\" ).toPredicate();\n\t\tSearchPredicate booleanPredicate = searchTarget.predicate().bool( b -> {\n\t\t\tb.should( predicate1 );\n\t\t\tb.should( predicate2 );\n\t\t\tb.should( predicate3 );\n\t\t\tb.should( predicate4 );\n\t\t} ).toPredicate();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n\t\t\t\t.asReferences()\n\t\t\t\t.predicate( booleanPredicate )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID )\n\t\t\t\t.hasHitCount( 4 );\n\t}",
        "diffSourceCode": "    17: import org.hibernate.search.engine.backend.document.IndexFieldAccessor;\n    18: import org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\n    19: import org.hibernate.search.engine.backend.index.IndexManager;\n    20: import org.hibernate.search.engine.backend.index.spi.IndexSearchTarget;\n    21: import org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\n    22: import org.hibernate.search.engine.common.spi.SearchIntegration;\n    23: import org.hibernate.search.engine.common.spi.SessionContext;\n    24: import org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;\n   114: \t@Test\n   115: \tpublic void predicate_fromJsonString_separatePredicate() {\n   116: \t\tIndexSearchTarget searchTarget = indexManager.createSearchTarget().build();\n   117: \n   118: \t\tSearchPredicate predicate1 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n-  119: \t\t\t\t.fromJsonString( \"{'match': {'string': 'text 1'}}\" ).end();\n+  119: \t\t\t\t.fromJsonString( \"{'match': {'string': 'text 1'}}\" ).toPredicate();\n   120: \t\tSearchPredicate predicate2 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n-  121: \t\t\t\t.fromJsonString( \"{'match': {'integer': 2}}\" ).end();\n+  121: \t\t\t\t.fromJsonString( \"{'match': {'integer': 2}}\" ).toPredicate();\n   122: \t\tSearchPredicate predicate3 = searchTarget.predicate().extension( ElasticsearchExtension.get() )\n   123: \t\t\t\t.fromJsonString(\n   124: \t\t\t\t\t\t\"{\"\n   125: \t\t\t\t\t\t\t+ \"'geo_distance': {\"\n   126: \t\t\t\t\t\t\t\t+ \"'distance': '200km',\"\n   127: \t\t\t\t\t\t\t\t+ \"'geoPoint': {\"\n   128: \t\t\t\t\t\t\t\t\t+ \"'lat': 40,\"\n   129: \t\t\t\t\t\t\t\t\t+ \"'lon': -70\"\n   130: \t\t\t\t\t\t\t\t+ \"}\"\n   131: \t\t\t\t\t\t\t+ \"}\"\n   132: \t\t\t\t\t\t+ \"}\"\n   133: \t\t\t\t)\n-  134: \t\t\t\t.end();\n+  134: \t\t\t\t.toPredicate();\n   135: \t\t// Also test using the standard DSL on a field defined with the extension\n   136: \t\tSearchPredicate predicate4 = searchTarget.predicate().match().onField( \"yearDays\" )\n-  137: \t\t\t\t.matching( \"'2018:12'\" ).end();\n+  137: \t\t\t\t.matching( \"'2018:12'\" ).toPredicate();\n   138: \t\tSearchPredicate booleanPredicate = searchTarget.predicate().bool( b -> {\n   139: \t\t\tb.should( predicate1 );\n   140: \t\t\tb.should( predicate2 );\n   141: \t\t\tb.should( predicate3 );\n   142: \t\t\tb.should( predicate4 );\n-  143: \t\t} ).end();\n+  143: \t\t} ).toPredicate();\n   144: \n   145: \t\tSearchQuery<DocumentReference> query = searchTarget.query( sessionContext )\n   146: \t\t\t\t.asReferences()\n   147: \t\t\t\t.predicate( booleanPredicate )\n   148: \t\t\t\t.build();\n   149: \t\tassertThat( query )\n   150: \t\t\t\t.hasReferencesHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID )\n   151: \t\t\t\t.hasHitCount( 4 );\n   152: \t}\n",
        "uniqueId": "b67c74bdf7a5a1a8102fb0b0179bdc0774366c29_114_152__114_152_17_24",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Move And Rename Method",
        "description": "Move And Rename Method\tpublic tearDownAfterClass() : void from class org.hibernate.search.jsr352.massindexing.MassIndexingJobWithCompositeIdTest to public shutDown() : void from class org.hibernate.search.jsr352.massindexing.impl.steps.lucene.EntityReaderTest",
        "diffLocations": [
            {
                "filePath": "jsr352/core/src/test/java/org/hibernate/search/jsr352/massindexing/MassIndexingJobWithCompositeIdTest.java",
                "startLine": 48,
                "endLine": 51,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "jsr352/core/src/test/java/org/hibernate/search/jsr352/massindexing/impl/steps/lucene/EntityReaderTest.java",
                "startLine": 103,
                "endLine": 108,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@AfterClass\n\tpublic static void tearDownAfterClass() throws Exception {\n\t\temf.close();\n\t}",
        "filePathBefore": "jsr352/core/src/test/java/org/hibernate/search/jsr352/massindexing/MassIndexingJobWithCompositeIdTest.java",
        "isPureRefactoring": true,
        "commitId": "ea4e402fd1d991e8c01ae3918047796dd2f3ab8b",
        "packageNameBefore": "org.hibernate.search.jsr352.massindexing",
        "classNameBefore": "org.hibernate.search.jsr352.massindexing.MassIndexingJobWithCompositeIdTest",
        "methodNameBefore": "org.hibernate.search.jsr352.massindexing.MassIndexingJobWithCompositeIdTest#tearDownAfterClass",
        "classSignatureBefore": "public class MassIndexingJobWithCompositeIdTest ",
        "methodNameBeforeSet": [
            "org.hibernate.search.jsr352.massindexing.MassIndexingJobWithCompositeIdTest#tearDownAfterClass"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.jsr352.massindexing.MassIndexingJobWithCompositeIdTest"
        ],
        "classSignatureBeforeSet": [
            "public class MassIndexingJobWithCompositeIdTest "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "",
                "description": "Extra print lines - with non-mapped leaves",
                "mappingState": 5
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.jsr352.massindexing;\n\nimport static org.fest.assertions.Assertions.assertThat;\n\nimport java.time.LocalDate;\nimport java.time.temporal.ChronoUnit;\nimport java.util.Properties;\n\nimport javax.persistence.EntityManagerFactory;\nimport javax.persistence.Persistence;\n\nimport org.hibernate.criterion.Restrictions;\nimport org.hibernate.search.jpa.FullTextEntityManager;\nimport org.hibernate.search.jpa.Search;\nimport org.hibernate.search.jsr352.massindexing.test.entity.EntityWithEmbeddedId;\nimport org.hibernate.search.jsr352.massindexing.test.entity.EntityWithIdClass;\nimport org.hibernate.search.jsr352.test.util.JobTestUtil;\nimport org.hibernate.search.testsupport.TestForIssue;\nimport org.junit.After;\nimport org.junit.AfterClass;\nimport org.junit.Before;\nimport org.junit.Test;\n\n/**\n * Tests that mass indexing job can handle entity having\n * {@link javax.persistence.EmbeddedId} annotation, or\n * {@link javax.persistence.IdClass} annotation.\n *\n * @author Mincong Huang\n */\n@TestForIssue(jiraKey = \"HSEARCH-2615\")\npublic class MassIndexingJobWithCompositeIdTest {\n\n\tprivate static final LocalDate START = LocalDate.of( 2017, 6, 1 );\n\n\tprivate static final LocalDate END = LocalDate.of( 2017, 8, 1 );\n\n\tprivate static final EntityManagerFactory emf = Persistence.createEntityManagerFactory( \"h2\" );\n\n\tprivate FullTextEntityManager ftem;\n\n\t@AfterClass\n\tpublic static void tearDownAfterClass() throws Exception {\n\t\temf.close();\n\t}\n\n\t@Before\n\tpublic void setUp() throws Exception {\n\t\tftem = Search.getFullTextEntityManager( emf.createEntityManager() );\n\t\tftem.getTransaction().begin();\n\t\tfor ( LocalDate d = START; d.isBefore( END ); d = d.plusDays( 1 ) ) {\n\t\t\tftem.persist( new EntityWithIdClass( d ) );\n\t\t\tftem.persist( new EntityWithEmbeddedId( d ) );\n\t\t}\n\t\tftem.getTransaction().commit();\n\n\t\tassertThat( JobTestUtil.nbDocumentsInIndex( emf, EntityWithIdClass.class ) ).isEqualTo( 0 );\n\t\tassertThat( JobTestUtil.nbDocumentsInIndex( emf, EntityWithEmbeddedId.class ) ).isEqualTo( 0 );\n\t}\n\n\t@After\n\tpublic void tearDown() throws Exception {\n\t\tftem.getTransaction().begin();\n\n\t\tftem.createQuery( \"delete from \" + EntityWithIdClass.class.getSimpleName() ).executeUpdate();\n\t\tftem.createQuery( \"delete from \" + EntityWithEmbeddedId.class.getSimpleName() ).executeUpdate();\n\n\t\tftem.purgeAll( EntityWithIdClass.class );\n\t\tftem.purgeAll( EntityWithEmbeddedId.class );\n\t\tftem.flushToIndexes();\n\n\t\tftem.getTransaction().commit();\n\t\tftem.close();\n\t}\n\n\t@Test\n\tpublic void canHandleIdClass_strategyFull() throws Exception {\n\t\tProperties props = MassIndexingJob.parameters()\n\t\t\t\t.forEntities( EntityWithIdClass.class )\n\t\t\t\t.rowsPerPartition( 13 ) // Ensure there're more than 1 partition, so that a WHERE clause is applied.\n\t\t\t\t.checkpointInterval( 4 )\n\t\t\t\t.build();\n\t\tJobTestUtil.startJobAndWait( MassIndexingJob.NAME, props );\n\n\t\tint expectedDays = (int) ChronoUnit.DAYS.between( START, END );\n\t\tassertThat( JobTestUtil.nbDocumentsInIndex( emf, EntityWithIdClass.class ) ).isEqualTo( expectedDays );\n\t}\n\n\t@Test\n\tpublic void canHandleIdClass_strategyCriteria() throws Exception {\n\t\tProperties props = MassIndexingJob.parameters()\n\t\t\t\t.forEntities( EntityWithIdClass.class )\n\t\t\t\t.restrictedBy( Restrictions.gt( \"month\", 6 ) )\n\t\t\t\t.rowsPerPartition( 13 ) // Ensure there're more than 1 partition, so that a WHERE clause is applied.\n\t\t\t\t.checkpointInterval( 4 )\n\t\t\t\t.build();\n\t\tJobTestUtil.startJobAndWait( MassIndexingJob.NAME, props );\n\n\t\tint expectedDays = (int) ChronoUnit.DAYS.between( LocalDate.of( 2017, 7, 1 ), END );\n\t\tint actualDays = JobTestUtil.nbDocumentsInIndex( emf, EntityWithIdClass.class );\n\t\tassertThat( actualDays ).isEqualTo( expectedDays );\n\t}\n\n\t@Test\n\tpublic void canHandleEmbeddedId_strategyFull() throws Exception {\n\t\tProperties props = MassIndexingJob.parameters()\n\t\t\t\t.forEntities( EntityWithEmbeddedId.class )\n\t\t\t\t.rowsPerPartition( 13 ) // Ensure there're more than 1 partition, so that a WHERE clause is applied.\n\t\t\t\t.checkpointInterval( 4 )\n\t\t\t\t.build();\n\n\t\tJobTestUtil.startJobAndWait( MassIndexingJob.NAME, props );\n\n\t\tint expectedDays = (int) ChronoUnit.DAYS.between( START, END );\n\t\tint actualDays = JobTestUtil.nbDocumentsInIndex( emf, EntityWithEmbeddedId.class );\n\t\tassertThat( actualDays ).isEqualTo( expectedDays );\n\t}\n\n\t@Test\n\tpublic void canHandleEmbeddedId_strategyCriteria() throws Exception {\n\t\tProperties props = MassIndexingJob.parameters()\n\t\t\t\t.forEntities( EntityWithEmbeddedId.class )\n\t\t\t\t.restrictedBy( Restrictions.gt( \"embeddableDateId.month\", 6 ) )\n\t\t\t\t.rowsPerPartition( 13 ) // Ensure there're more than 1 partition, so that a WHERE clause is applied.\n\t\t\t\t.checkpointInterval( 4 )\n\t\t\t\t.build();\n\t\tJobTestUtil.startJobAndWait( MassIndexingJob.NAME, props );\n\n\t\tint expectedDays = (int) ChronoUnit.DAYS.between( LocalDate.of( 2017, 7, 1 ), END );\n\t\tint actualDays = JobTestUtil.nbDocumentsInIndex( emf, EntityWithEmbeddedId.class );\n\t\tassertThat( actualDays ).isEqualTo( expectedDays );\n\t}\n\n}\n",
        "filePathAfter": "jsr352/core/src/test/java/org/hibernate/search/jsr352/massindexing/impl/steps/lucene/EntityReaderTest.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.jsr352.massindexing.impl.steps.lucene;\n\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertNull;\n\nimport java.util.Arrays;\nimport java.util.HashSet;\n\nimport javax.batch.runtime.context.JobContext;\nimport javax.batch.runtime.context.StepContext;\nimport javax.persistence.EntityManager;\nimport javax.persistence.EntityManagerFactory;\nimport javax.persistence.Persistence;\n\nimport org.hibernate.CacheMode;\nimport org.hibernate.search.jsr352.logging.impl.Log;\nimport org.hibernate.search.jsr352.massindexing.impl.JobContextData;\nimport org.hibernate.search.jsr352.massindexing.test.entity.Company;\nimport org.hibernate.search.jsr352.test.util.JobTestUtil;\nimport org.hibernate.search.util.logging.impl.LoggerFactory;\n\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\nimport org.mockito.InjectMocks;\nimport org.mockito.Mock;\nimport org.mockito.Mockito;\nimport org.mockito.MockitoAnnotations;\n\n/**\n * Unit test for item reader validation.\n *\n * @author Mincong Huang\n */\npublic class EntityReaderTest {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class );\n\n\tprivate static final String PERSISTENCE_UNIT_NAME = \"primary_pu\";\n\tprivate static final Company[] COMPANIES = new Company[]{\n\t\t\tnew Company( \"Red Hat\" ),\n\t\t\tnew Company( \"Google\" ),\n\t\t\tnew Company( \"Microsoft\" ) };\n\tprivate EntityManagerFactory emf;\n\n\t@Mock\n\tprivate JobContext mockedJobContext;\n\n\t@Mock\n\tprivate StepContext mockedStepContext;\n\n\t@InjectMocks\n\tprivate EntityReader entityReader;\n\n\t@Before\n\tpublic void setUp() {\n\t\tEntityManager em = null;\n\t\ttry {\n\t\t\temf = Persistence.createEntityManagerFactory( PERSISTENCE_UNIT_NAME );\n\t\t\tem = emf.createEntityManager();\n\t\t\tem.getTransaction().begin();\n\t\t\tfor ( Company c : COMPANIES ) {\n\t\t\t\tem.persist( c );\n\t\t\t}\n\t\t\tem.getTransaction().commit();\n\t\t}\n\t\tfinally {\n\t\t\ttry {\n\t\t\t\tem.close();\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\tlog.error( e );\n\t\t\t}\n\t\t}\n\n\t\tfinal String cacheMode = CacheMode.IGNORE.name();\n\t\tfinal String entityName = Company.class.getName();\n\t\tfinal String entityFetchSize = String.valueOf( 1000 );\n\t\tfinal String checkpointInterval = String.valueOf( 1000 );\n\t\tfinal String hql = null;\n\t\tfinal String maxResults = String.valueOf( Integer.MAX_VALUE );\n\t\tfinal String partitionId = String.valueOf( 0 );\n\t\tentityReader = new EntityReader( cacheMode,\n\t\t\t\tentityName,\n\t\t\t\tentityFetchSize,\n\t\t\t\tcheckpointInterval,\n\t\t\t\thql,\n\t\t\t\tmaxResults,\n\t\t\t\tpartitionId,\n\t\t\t\tnull,\n\t\t\t\tnull,\n\t\t\t\tIndexScope.FULL_ENTITY.name() );\n\n\t\tMockitoAnnotations.initMocks( this );\n\t}\n\n\t@After\n\tpublic void shutDown() {\n\t\tif ( emf.isOpen() ) {\n\t\t\temf.close();\n\t\t}\n\t}\n\n\t@Test\n\tpublic void testReadItem_withoutBoundary() throws Exception {\n\t\t// mock job context\n\t\tJobContextData jobData = new JobContextData();\n\t\tjobData.setEntityManagerFactory( emf );\n\t\tjobData.setCustomQueryCriteria( new HashSet<>() );\n\t\tjobData.setEntityTypeDescriptors( Arrays.asList( JobTestUtil.createSimpleEntityTypeDescriptor( emf, Company.class ) ) );\n\t\tMockito.when( mockedJobContext.getTransientUserData() ).thenReturn( jobData );\n\n\t\t// mock step context\n\t\tMockito.doNothing().when( mockedStepContext ).setTransientUserData( Mockito.any() );\n\n\t\ttry {\n\t\t\tentityReader.open( null );\n\t\t\tfor ( int i = 0; i < COMPANIES.length; i++ ) {\n\t\t\t\tCompany c = (Company) entityReader.readItem();\n\t\t\t\tassertEquals( COMPANIES[i].getName(), c.getName() );\n\t\t\t}\n\t\t\t// no more item\n\t\t\tassertNull( entityReader.readItem() );\n\t\t}\n\t\tfinally {\n\t\t\tentityReader.close();\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "@After\n\tpublic void shutDown() {\n\t\tif ( emf.isOpen() ) {\n\t\t\temf.close();\n\t\t}\n\t}",
        "diffSourceCode": "-   48: \t@AfterClass\n-   49: \tpublic static void tearDownAfterClass() throws Exception {\n-   50: \t\temf.close();\n-   51: \t}\n-  103: \t\tJobTestUtil.startJobAndWait( MassIndexingJob.NAME, props );\n-  104: \n-  105: \t\tint expectedDays = (int) ChronoUnit.DAYS.between( LocalDate.of( 2017, 7, 1 ), END );\n-  106: \t\tint actualDays = JobTestUtil.nbDocumentsInIndex( emf, EntityWithIdClass.class );\n-  107: \t\tassertThat( actualDays ).isEqualTo( expectedDays );\n+   48: \t\t\tnew Company( \"Google\" ),\n+   49: \t\t\tnew Company( \"Microsoft\" ) };\n+   50: \tprivate EntityManagerFactory emf;\n+   51: \n+  103: \t@After\n+  104: \tpublic void shutDown() {\n+  105: \t\tif ( emf.isOpen() ) {\n+  106: \t\t\temf.close();\n+  107: \t\t}\n   108: \t}\n",
        "uniqueId": "ea4e402fd1d991e8c01ae3918047796dd2f3ab8b_48_51__103_108",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate composeExplicitlyIncludedPaths(relativePrefix String, nullSafeIncludePaths Set<String>, currentRemainingDepth Integer, nestedRemainingDepth Integer) : Set<String> extracted from public composeWithNested(parentTypeModel MappableTypeModel, relativePrefix String, maxDepth Integer, includePaths Set<String>) : IndexSchemaFilter in class org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter",
        "diffLocations": [
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/IndexSchemaFilter.java",
                "startLine": 145,
                "endLine": 223,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/IndexSchemaFilter.java",
                "startLine": 145,
                "endLine": 194,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/IndexSchemaFilter.java",
                "startLine": 196,
                "endLine": 231,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public IndexSchemaFilter composeWithNested(MappableTypeModel parentTypeModel, String relativePrefix,\n\t\t\tInteger maxDepth, Set<String> includePaths) {\n\t\tString cyclicRecursionPath = getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n\t\tif ( cyclicRecursionPath != null ) {\n\t\t\tcyclicRecursionPath += relativePrefix;\n\t\t\tthrow new SearchException( \"Found an infinite IndexedEmbedded recursion involving path '\"\n\t\t\t\t\t+ cyclicRecursionPath + \"' on type '\" + parentTypeModel + \"'\" );\n\t\t}\n\n\t\tSet<String> nullSafeIncludePaths = includePaths == null ? Collections.emptySet() : includePaths;\n\n\t\t// The remaining composition depth according to \"this\" only\n\t\tInteger currentRemainingDepth = remainingCompositionDepth == null ? null : remainingCompositionDepth - 1;\n\n\t\t// The remaining composition depth according to the nested IndexedEmbedded only\n\t\tInteger nestedRemainingDepth = maxDepth;\n\t\tif ( maxDepth == null ) {\n\t\t\tif ( !nullSafeIncludePaths.isEmpty() ) {\n\t\t\t\t/*\n\t\t\t\t * If no max depth was provided and \"includePaths\" was provided,\n\t\t\t\t * the remaining composition depth is implicitly set to 0,\n\t\t\t\t * meaning no composition is allowed and paths are excluded unless\n\t\t\t\t * explicitly listed in \"includePaths\".\n\t\t\t\t */\n\t\t\t\tnestedRemainingDepth = 0;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * By default, a composed filters' remaining composition depth is its parent's minus one\n\t\t * (or null if the remaining composition depth was not set in the parent)...\n\t\t */\n\t\tInteger composedRemainingDepth = currentRemainingDepth;\n\t\tif ( composedRemainingDepth == null\n\t\t\t\t|| nestedRemainingDepth != null && composedRemainingDepth > nestedRemainingDepth ) {\n\t\t\t/*\n\t\t\t * ... but the nested filter can override it.\n\t\t\t */\n\t\t\tcomposedRemainingDepth = nestedRemainingDepth;\n\t\t}\n\n\t\tSet<String> composedFilterExplicitlyIncludedPaths = new HashSet<>();\n\t\t/*\n\t\t * Add the nested filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they are not filtered out by the current filter.\n\t\t */\n\t\tfor ( String path : nullSafeIncludePaths ) {\n\t\t\tif ( isPathIncluded( currentRemainingDepth, explicitlyIncludedPaths, relativePrefix + path ) ) {\n\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( path );\n\t\t\t\t// Also add paths leading to this path (so that object nodes are not excluded)\n\t\t\t\tint afterPreviousDotIndex = 0;\n\t\t\t\tint nextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\twhile ( nextDotIndex >= 0 ) {\n\t\t\t\t\tString subPath = path.substring( 0, nextDotIndex );\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( subPath );\n\t\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n\t\t\t\t\tnextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Add the current filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they start with the nested filter's prefix and are not filtered out by the nested filter.\n\t\t */\n\t\tint relativePrefixLength = relativePrefix.length();\n\t\tfor ( String path : explicitlyIncludedPaths ) {\n\t\t\tif ( path.startsWith( relativePrefix ) ) {\n\t\t\t\tString pathRelativeToNestedFilter = path.substring( relativePrefixLength );\n\t\t\t\tif ( isPathIncluded( nestedRemainingDepth, nullSafeIncludePaths, pathRelativeToNestedFilter ) ) {\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( pathRelativeToNestedFilter );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn new IndexSchemaFilter(\n\t\t\t\tthis, parentTypeModel, relativePrefix,\n\t\t\t\tcomposedRemainingDepth, composedFilterExplicitlyIncludedPaths\n\t\t);\n\t}",
        "filePathBefore": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/IndexSchemaFilter.java",
        "isPureRefactoring": true,
        "commitId": "35593714e0e5ffec22e4e9e93e12eaf3ab24d1a5",
        "packageNameBefore": "org.hibernate.search.engine.mapper.mapping.building.impl",
        "classNameBefore": "org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter",
        "methodNameBefore": "org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#composeWithNested",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#isPathIncluded\n methodBody: private static boolean isPathIncluded(Integer remainingDepth, Set<String> explicitlyIncludedPaths, String relativePath) {\nreturn isEveryPathIncludedByDefault(remainingDepth) || explicitlyIncludedPaths.contains(relativePath);\n}\nmethodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#getPathFromSameIndexedEmbeddedSinceNoCompositionLimits\n methodBody: private String getPathFromSameIndexedEmbeddedSinceNoCompositionLimits(MappableTypeModel parentTypeModel, String relativePrefix) {\nif(hasCompositionLimits()){return null;\n}if(parent != null){if(this.relativePrefix.equals(relativePrefix) && this.parentTypeModel.isSubTypeOf(parentTypeModel)){return this.relativePrefix;\n}{String path=parent.getPathFromSameIndexedEmbeddedSinceNoCompositionLimits(parentTypeModel,relativePrefix);\nreturn path == null ? null : path + this.relativePrefix;\n}}{return null;\n}}",
        "classSignatureBefore": "class IndexSchemaFilter ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#composeWithNested"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter"
        ],
        "classSignatureBeforeSet": [
            "class IndexSchemaFilter "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics Severe changes",
                "description": "Return expression has been added within the Extract Method mechanics - with non-mapped leaves",
                "mappingState": 2
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.mapper.mapping.building.impl;\n\nimport java.util.Collections;\nimport java.util.HashSet;\nimport java.util.Set;\n\nimport org.hibernate.search.engine.mapper.model.spi.MappableTypeModel;\nimport org.hibernate.search.util.SearchException;\n\n/**\n * A schema filter, responsible for deciding which parts of a mapping will actually make it to the index schema.\n * <p>\n * A schema filter is created at the root of a Pojo mapping (it accepts everything),\n * and also each time index embedding ({@code @IndexedEmbedded}) is used.\n *\n * <h3 id=\"filter-usage\">Filter usage</h3>\n * <p>\n * A schema filter is asked to provide advice about whether or not to trim down the schema in two cases:\n * <ul>\n *     <li>When a field is added by a bridge, the filter decides whether to include this field or not\n *     through its {@link #isPathIncluded(String)} method</li>\n *     <li>When a nested {@code @IndexedEmbedded} is requested, a new filter is created through the\n *     {@link #composeWithNested(MappableTypeModel, String, Integer, Set)} method, which may return a filter\n *     that {@link #isEveryPathExcluded() excludes every path}, meaning the {@code @IndexedEmbedded} will\n *     be ignored</li>\n * </ul>\n *\n * <h3 id=\"filter-properties\">Filter properties</h3>\n * <p>\n * A filter decides whether to include a path or not according to its two main properties:\n * <ul>\n *     <li>the {@link #remainingCompositionDepth remaining composition depth}</li>\n *     <li>the {@link #explicitlyIncludedPaths explicitly included paths}</li>\n * </ul>\n * <p>\n * The explicitly included paths, as their name suggests, define which paths\n * should be accepted by this filter no matter what.\n * <p>\n * The composition depth defines\n * {@link #isEveryPathIncludedByDefault(Integer) how paths that do not appear in the explicitly included paths should be treated}:\n * <ul>\n *     <li>if {@code <= 0}, paths are excluded by default</li>\n *     <li>if {@code null} or {@code > 0}, paths are included by default</li>\n * </ul>\n *\n * <h3 id=\"filter-composition\">Filter composition</h3>\n * <p>\n * Composed filters are created whenever a nested {@code @IndexedEmbedded} is encountered.\n * A composed filter will always enforce the restrictions of its parent filter,\n * plus some added restrictions depending on the properties of the nested {@code IndexedEmbedded}.\n * <p>\n * For more information about how filters are composed, see\n * {@link #composeWithNested(MappableTypeModel, String, Integer, Set)}.\n *\n * @author Yoann Rodiere\n */\nclass IndexSchemaFilter {\n\n\tprivate static final IndexSchemaFilter ROOT = new IndexSchemaFilter(\n\t\t\tnull, null, null, null, Collections.emptySet()\n\t);\n\n\tpublic static IndexSchemaFilter root() {\n\t\treturn ROOT;\n\t}\n\n\tprivate final IndexSchemaFilter parent;\n\tprivate final MappableTypeModel parentTypeModel;\n\tprivate final String relativePrefix;\n\n\t/**\n\t * Defines how deep indexed embedded are allowed to be composed.\n\t *\n\t * Note that composition depth only relates to IndexedEmbedded composition;\n\t * bridge-declared fields are only affected by path filtering,\n\t * whose default behavior (include or exclude) is determined by {@link #isEveryPathIncludedByDefault(Integer)}.\n\t */\n\tprivate final Integer remainingCompositionDepth;\n\n\t/**\n\t * Defines paths to be included even when the default behavior is to exclude paths.\n\t */\n\tprivate final Set<String> explicitlyIncludedPaths;\n\n\tprivate IndexSchemaFilter(IndexSchemaFilter parent, MappableTypeModel parentTypeModel, String relativePrefix,\n\t\t\tInteger remainingCompositionDepth, Set<String> explicitlyIncludedPaths) {\n\t\tthis.parent = parent;\n\t\tthis.parentTypeModel = parentTypeModel;\n\t\tthis.relativePrefix = relativePrefix;\n\t\tthis.remainingCompositionDepth = remainingCompositionDepth;\n\t\tthis.explicitlyIncludedPaths = explicitlyIncludedPaths;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn new StringBuilder( getClass().getSimpleName() )\n\t\t\t\t.append( \"[\" )\n\t\t\t\t.append( \"parentTypeModel=\" ).append( parentTypeModel )\n\t\t\t\t.append( \",relativePrefix=\" ).append( relativePrefix )\n\t\t\t\t.append( \",remainingCompositionDepth=\" ).append( remainingCompositionDepth )\n\t\t\t\t.append( \",explicitlyIncludedPaths=\" ).append( explicitlyIncludedPaths )\n\t\t\t\t.append( \"]\" )\n\t\t\t\t.toString();\n\t}\n\n\tpublic boolean isPathIncluded(String relativePath) {\n\t\treturn isPathIncluded( remainingCompositionDepth, explicitlyIncludedPaths, relativePath );\n\t}\n\n\tpublic boolean isEveryPathExcluded() {\n\t\treturn !isEveryPathIncludedByDefault( remainingCompositionDepth ) && !isAnyPathExplicitlyIncluded();\n\t}\n\n\tprivate String getPathFromSameIndexedEmbeddedSinceNoCompositionLimits(MappableTypeModel parentTypeModel, String relativePrefix) {\n\t\tif ( hasCompositionLimits() ) {\n\t\t\treturn null;\n\t\t}\n\t\telse if ( parent != null ) {\n\t\t\tif ( this.relativePrefix.equals( relativePrefix )\n\t\t\t\t\t&& this.parentTypeModel.isSubTypeOf( parentTypeModel ) ) {\n\t\t\t\t// Same IndexedEmbedded as the one passed as a parameter\n\t\t\t\treturn this.relativePrefix;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tString path = parent.getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n\t\t\t\treturn path == null ? null : path + this.relativePrefix;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t/*\n\t\t\t * No recursion limits, no parent: this is the root.\n\t\t\t * I we reach this point, it means there was no recursion limit at all,\n\t\t\t * but we did not encounter the IndexedEmbedded we were looking for.\n\t\t\t */\n\t\t\treturn null;\n\t\t}\n\t}\n\n\tpublic IndexSchemaFilter composeWithNested(MappableTypeModel parentTypeModel, String relativePrefix,\n\t\t\tInteger maxDepth, Set<String> includePaths) {\n\t\tString cyclicRecursionPath = getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n\t\tif ( cyclicRecursionPath != null ) {\n\t\t\tcyclicRecursionPath += relativePrefix;\n\t\t\tthrow new SearchException( \"Found an infinite IndexedEmbedded recursion involving path '\"\n\t\t\t\t\t+ cyclicRecursionPath + \"' on type '\" + parentTypeModel + \"'\" );\n\t\t}\n\n\t\tSet<String> nullSafeIncludePaths = includePaths == null ? Collections.emptySet() : includePaths;\n\n\t\t// The remaining composition depth according to \"this\" only\n\t\tInteger currentRemainingDepth = remainingCompositionDepth == null ? null : remainingCompositionDepth - 1;\n\n\t\t// The remaining composition depth according to the nested IndexedEmbedded only\n\t\tInteger nestedRemainingDepth = maxDepth;\n\t\tif ( maxDepth == null ) {\n\t\t\tif ( !nullSafeIncludePaths.isEmpty() ) {\n\t\t\t\t/*\n\t\t\t\t * If no max depth was provided and \"includePaths\" was provided,\n\t\t\t\t * the remaining composition depth is implicitly set to 0,\n\t\t\t\t * meaning no composition is allowed and paths are excluded unless\n\t\t\t\t * explicitly listed in \"includePaths\".\n\t\t\t\t */\n\t\t\t\tnestedRemainingDepth = 0;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * By default, a composed filters' remaining composition depth is its parent's minus one\n\t\t * (or null if the remaining composition depth was not set in the parent)...\n\t\t */\n\t\tInteger composedRemainingDepth = currentRemainingDepth;\n\t\tif ( composedRemainingDepth == null\n\t\t\t\t|| nestedRemainingDepth != null && composedRemainingDepth > nestedRemainingDepth ) {\n\t\t\t/*\n\t\t\t * ... but the nested filter can override it.\n\t\t\t */\n\t\t\tcomposedRemainingDepth = nestedRemainingDepth;\n\t\t}\n\n\t\tSet<String> composedFilterExplicitlyIncludedPaths = new HashSet<>();\n\t\t/*\n\t\t * Add the nested filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they are not filtered out by the current filter.\n\t\t */\n\t\tfor ( String path : nullSafeIncludePaths ) {\n\t\t\tif ( isPathIncluded( currentRemainingDepth, explicitlyIncludedPaths, relativePrefix + path ) ) {\n\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( path );\n\t\t\t\t// Also add paths leading to this path (so that object nodes are not excluded)\n\t\t\t\tint afterPreviousDotIndex = 0;\n\t\t\t\tint nextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\twhile ( nextDotIndex >= 0 ) {\n\t\t\t\t\tString subPath = path.substring( 0, nextDotIndex );\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( subPath );\n\t\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n\t\t\t\t\tnextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Add the current filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they start with the nested filter's prefix and are not filtered out by the nested filter.\n\t\t */\n\t\tint relativePrefixLength = relativePrefix.length();\n\t\tfor ( String path : explicitlyIncludedPaths ) {\n\t\t\tif ( path.startsWith( relativePrefix ) ) {\n\t\t\t\tString pathRelativeToNestedFilter = path.substring( relativePrefixLength );\n\t\t\t\tif ( isPathIncluded( nestedRemainingDepth, nullSafeIncludePaths, pathRelativeToNestedFilter ) ) {\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( pathRelativeToNestedFilter );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn new IndexSchemaFilter(\n\t\t\t\tthis, parentTypeModel, relativePrefix,\n\t\t\t\tcomposedRemainingDepth, composedFilterExplicitlyIncludedPaths\n\t\t);\n\t}\n\n\tprivate static boolean isPathIncluded(Integer remainingDepth, Set<String> explicitlyIncludedPaths, String relativePath) {\n\t\treturn isEveryPathIncludedByDefault( remainingDepth )\n\t\t\t\t|| explicitlyIncludedPaths.contains( relativePath );\n\t}\n\n\tprivate static boolean isEveryPathIncludedByDefault(Integer remainingDepth) {\n\t\t/*\n\t\t * A remaining composition depth of 0 or below means\n\t\t * paths should be excluded when filtering unless mentioned in explicitlyIncludedPaths.\n\t\t */\n\t\treturn remainingDepth == null || remainingDepth > 0;\n\t}\n\n\tprivate boolean isAnyPathExplicitlyIncluded() {\n\t\treturn !explicitlyIncludedPaths.isEmpty();\n\t}\n\n\tprivate boolean hasCompositionLimits() {\n\t\treturn remainingCompositionDepth != null || !explicitlyIncludedPaths.isEmpty();\n\t}\n}",
        "filePathAfter": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/IndexSchemaFilter.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.mapper.mapping.building.impl;\n\nimport java.util.Collections;\nimport java.util.HashSet;\nimport java.util.Set;\n\nimport org.hibernate.search.engine.mapper.model.spi.MappableTypeModel;\nimport org.hibernate.search.util.SearchException;\n\n/**\n * A schema filter, responsible for deciding which parts of a mapping will actually make it to the index schema.\n * <p>\n * A schema filter is created at the root of a Pojo mapping (it accepts everything),\n * and also each time index embedding ({@code @IndexedEmbedded}) is used.\n *\n * <h3 id=\"filter-usage\">Filter usage</h3>\n * <p>\n * A schema filter is asked to provide advice about whether or not to trim down the schema in two cases:\n * <ul>\n *     <li>When a field is added by a bridge, the filter decides whether to include this field or not\n *     through its {@link #isPathIncluded(String)} method</li>\n *     <li>When a nested {@code @IndexedEmbedded} is requested, a new filter is created through the\n *     {@link #composeWithNested(MappableTypeModel, String, Integer, Set)} method, which may return a filter\n *     that {@link #isEveryPathExcluded() excludes every path}, meaning the {@code @IndexedEmbedded} will\n *     be ignored</li>\n * </ul>\n *\n * <h3 id=\"filter-properties\">Filter properties</h3>\n * <p>\n * A filter decides whether to include a path or not according to its two main properties:\n * <ul>\n *     <li>the {@link #remainingCompositionDepth remaining composition depth}</li>\n *     <li>the {@link #explicitlyIncludedPaths explicitly included paths}</li>\n * </ul>\n * <p>\n * The explicitly included paths, as their name suggests, define which paths\n * should be accepted by this filter no matter what.\n * <p>\n * The composition depth defines\n * {@link #isEveryPathIncludedByDefault(Integer) how paths that do not appear in the explicitly included paths should be treated}:\n * <ul>\n *     <li>if {@code <= 0}, paths are excluded by default</li>\n *     <li>if {@code null} or {@code > 0}, paths are included by default</li>\n * </ul>\n *\n * <h3 id=\"filter-composition\">Filter composition</h3>\n * <p>\n * Composed filters are created whenever a nested {@code @IndexedEmbedded} is encountered.\n * A composed filter will always enforce the restrictions of its parent filter,\n * plus some added restrictions depending on the properties of the nested {@code IndexedEmbedded}.\n * <p>\n * For more information about how filters are composed, see\n * {@link #composeWithNested(MappableTypeModel, String, Integer, Set)}.\n *\n * @author Yoann Rodiere\n */\nclass IndexSchemaFilter {\n\n\tprivate static final IndexSchemaFilter ROOT = new IndexSchemaFilter(\n\t\t\tnull, null, null, null, Collections.emptySet()\n\t);\n\n\tpublic static IndexSchemaFilter root() {\n\t\treturn ROOT;\n\t}\n\n\tprivate final IndexSchemaFilter parent;\n\tprivate final MappableTypeModel parentTypeModel;\n\tprivate final String relativePrefix;\n\n\t/**\n\t * Defines how deep indexed embedded are allowed to be composed.\n\t *\n\t * Note that composition depth only relates to IndexedEmbedded composition;\n\t * bridge-declared fields are only affected by path filtering,\n\t * whose default behavior (include or exclude) is determined by {@link #isEveryPathIncludedByDefault(Integer)}.\n\t */\n\tprivate final Integer remainingCompositionDepth;\n\n\t/**\n\t * Defines paths to be included even when the default behavior is to exclude paths.\n\t */\n\tprivate final Set<String> explicitlyIncludedPaths;\n\n\tprivate IndexSchemaFilter(IndexSchemaFilter parent, MappableTypeModel parentTypeModel, String relativePrefix,\n\t\t\tInteger remainingCompositionDepth, Set<String> explicitlyIncludedPaths) {\n\t\tthis.parent = parent;\n\t\tthis.parentTypeModel = parentTypeModel;\n\t\tthis.relativePrefix = relativePrefix;\n\t\tthis.remainingCompositionDepth = remainingCompositionDepth;\n\t\tthis.explicitlyIncludedPaths = explicitlyIncludedPaths;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn new StringBuilder( getClass().getSimpleName() )\n\t\t\t\t.append( \"[\" )\n\t\t\t\t.append( \"parentTypeModel=\" ).append( parentTypeModel )\n\t\t\t\t.append( \",relativePrefix=\" ).append( relativePrefix )\n\t\t\t\t.append( \",remainingCompositionDepth=\" ).append( remainingCompositionDepth )\n\t\t\t\t.append( \",explicitlyIncludedPaths=\" ).append( explicitlyIncludedPaths )\n\t\t\t\t.append( \"]\" )\n\t\t\t\t.toString();\n\t}\n\n\tpublic boolean isPathIncluded(String relativePath) {\n\t\treturn isPathIncluded( remainingCompositionDepth, explicitlyIncludedPaths, relativePath );\n\t}\n\n\tpublic boolean isEveryPathExcluded() {\n\t\treturn !isEveryPathIncludedByDefault( remainingCompositionDepth ) && !isAnyPathExplicitlyIncluded();\n\t}\n\n\tprivate String getPathFromSameIndexedEmbeddedSinceNoCompositionLimits(MappableTypeModel parentTypeModel, String relativePrefix) {\n\t\tif ( hasCompositionLimits() ) {\n\t\t\treturn null;\n\t\t}\n\t\telse if ( parent != null ) {\n\t\t\tif ( this.relativePrefix.equals( relativePrefix )\n\t\t\t\t\t&& this.parentTypeModel.isSubTypeOf( parentTypeModel ) ) {\n\t\t\t\t// Same IndexedEmbedded as the one passed as a parameter\n\t\t\t\treturn this.relativePrefix;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tString path = parent.getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n\t\t\t\treturn path == null ? null : path + this.relativePrefix;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t/*\n\t\t\t * No recursion limits, no parent: this is the root.\n\t\t\t * I we reach this point, it means there was no recursion limit at all,\n\t\t\t * but we did not encounter the IndexedEmbedded we were looking for.\n\t\t\t */\n\t\t\treturn null;\n\t\t}\n\t}\n\n\tpublic IndexSchemaFilter composeWithNested(MappableTypeModel parentTypeModel, String relativePrefix,\n\t\t\tInteger maxDepth, Set<String> includePaths) {\n\t\tString cyclicRecursionPath = getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n\t\tif ( cyclicRecursionPath != null ) {\n\t\t\tcyclicRecursionPath += relativePrefix;\n\t\t\tthrow new SearchException( \"Found an infinite IndexedEmbedded recursion involving path '\"\n\t\t\t\t\t+ cyclicRecursionPath + \"' on type '\" + parentTypeModel + \"'\" );\n\t\t}\n\n\t\tSet<String> nullSafeIncludePaths = includePaths == null ? Collections.emptySet() : includePaths;\n\n\t\t// The remaining composition depth according to \"this\" only\n\t\tInteger currentRemainingDepth = remainingCompositionDepth == null ? null : remainingCompositionDepth - 1;\n\n\t\t// The remaining composition depth according to the nested IndexedEmbedded only\n\t\tInteger nestedRemainingDepth = maxDepth;\n\t\tif ( maxDepth == null ) {\n\t\t\tif ( !nullSafeIncludePaths.isEmpty() ) {\n\t\t\t\t/*\n\t\t\t\t * If no max depth was provided and \"includePaths\" was provided,\n\t\t\t\t * the remaining composition depth is implicitly set to 0,\n\t\t\t\t * meaning no composition is allowed and paths are excluded unless\n\t\t\t\t * explicitly listed in \"includePaths\".\n\t\t\t\t */\n\t\t\t\tnestedRemainingDepth = 0;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * By default, a composed filters' remaining composition depth is its parent's minus one\n\t\t * (or null if the remaining composition depth was not set in the parent)...\n\t\t */\n\t\tInteger composedRemainingDepth = currentRemainingDepth;\n\t\tif ( composedRemainingDepth == null\n\t\t\t\t|| nestedRemainingDepth != null && composedRemainingDepth > nestedRemainingDepth ) {\n\t\t\t/*\n\t\t\t * ... but the nested filter can override it.\n\t\t\t */\n\t\t\tcomposedRemainingDepth = nestedRemainingDepth;\n\t\t}\n\n\t\tSet<String> composedFilterExplicitlyIncludedPaths = composeExplicitlyIncludedPaths(\n\t\t\t\trelativePrefix, nullSafeIncludePaths, currentRemainingDepth, nestedRemainingDepth\n\t\t);\n\n\t\treturn new IndexSchemaFilter(\n\t\t\t\tthis, parentTypeModel, relativePrefix,\n\t\t\t\tcomposedRemainingDepth, composedFilterExplicitlyIncludedPaths\n\t\t);\n\t}\n\n\tprivate Set<String> composeExplicitlyIncludedPaths(String relativePrefix, Set<String> nullSafeIncludePaths,\n\t\t\tInteger currentRemainingDepth, Integer nestedRemainingDepth) {\n\t\tSet<String> composedFilterExplicitlyIncludedPaths = new HashSet<>();\n\t\t/*\n\t\t * Add the nested filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they are not filtered out by the current filter.\n\t\t */\n\t\tfor ( String path : nullSafeIncludePaths ) {\n\t\t\tif ( isPathIncluded( currentRemainingDepth, explicitlyIncludedPaths, relativePrefix + path ) ) {\n\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( path );\n\t\t\t\t// Also add paths leading to this path (so that object nodes are not excluded)\n\t\t\t\tint afterPreviousDotIndex = 0;\n\t\t\t\tint nextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\twhile ( nextDotIndex >= 0 ) {\n\t\t\t\t\tString subPath = path.substring( 0, nextDotIndex );\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( subPath );\n\t\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n\t\t\t\t\tnextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Add the current filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they start with the nested filter's prefix and are not filtered out by the nested filter.\n\t\t */\n\t\tint relativePrefixLength = relativePrefix.length();\n\t\tfor ( String path : explicitlyIncludedPaths ) {\n\t\t\tif ( path.startsWith( relativePrefix ) ) {\n\t\t\t\tString pathRelativeToNestedFilter = path.substring( relativePrefixLength );\n\t\t\t\tif ( isPathIncluded( nestedRemainingDepth, nullSafeIncludePaths, pathRelativeToNestedFilter ) ) {\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( pathRelativeToNestedFilter );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn composedFilterExplicitlyIncludedPaths;\n\t}\n\n\tprivate static boolean isPathIncluded(Integer remainingDepth, Set<String> explicitlyIncludedPaths, String relativePath) {\n\t\treturn isEveryPathIncludedByDefault( remainingDepth )\n\t\t\t\t|| explicitlyIncludedPaths.contains( relativePath );\n\t}\n\n\tprivate static boolean isEveryPathIncludedByDefault(Integer remainingDepth) {\n\t\t/*\n\t\t * A remaining composition depth of 0 or below means\n\t\t * paths should be excluded when filtering unless mentioned in explicitlyIncludedPaths.\n\t\t */\n\t\treturn remainingDepth == null || remainingDepth > 0;\n\t}\n\n\tprivate boolean isAnyPathExplicitlyIncluded() {\n\t\treturn !explicitlyIncludedPaths.isEmpty();\n\t}\n\n\tprivate boolean hasCompositionLimits() {\n\t\treturn remainingCompositionDepth != null || !explicitlyIncludedPaths.isEmpty();\n\t}\n}",
        "diffSourceCodeSet": [
            "private Set<String> composeExplicitlyIncludedPaths(String relativePrefix, Set<String> nullSafeIncludePaths,\n\t\t\tInteger currentRemainingDepth, Integer nestedRemainingDepth) {\n\t\tSet<String> composedFilterExplicitlyIncludedPaths = new HashSet<>();\n\t\t/*\n\t\t * Add the nested filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they are not filtered out by the current filter.\n\t\t */\n\t\tfor ( String path : nullSafeIncludePaths ) {\n\t\t\tif ( isPathIncluded( currentRemainingDepth, explicitlyIncludedPaths, relativePrefix + path ) ) {\n\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( path );\n\t\t\t\t// Also add paths leading to this path (so that object nodes are not excluded)\n\t\t\t\tint afterPreviousDotIndex = 0;\n\t\t\t\tint nextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\twhile ( nextDotIndex >= 0 ) {\n\t\t\t\t\tString subPath = path.substring( 0, nextDotIndex );\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( subPath );\n\t\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n\t\t\t\t\tnextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Add the current filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they start with the nested filter's prefix and are not filtered out by the nested filter.\n\t\t */\n\t\tint relativePrefixLength = relativePrefix.length();\n\t\tfor ( String path : explicitlyIncludedPaths ) {\n\t\t\tif ( path.startsWith( relativePrefix ) ) {\n\t\t\t\tString pathRelativeToNestedFilter = path.substring( relativePrefixLength );\n\t\t\t\tif ( isPathIncluded( nestedRemainingDepth, nullSafeIncludePaths, pathRelativeToNestedFilter ) ) {\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( pathRelativeToNestedFilter );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn composedFilterExplicitlyIncludedPaths;\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#isPathIncluded\n methodBody: private static boolean isPathIncluded(Integer remainingDepth, Set<String> explicitlyIncludedPaths, String relativePath) {\nreturn isEveryPathIncludedByDefault(remainingDepth) || explicitlyIncludedPaths.contains(relativePath);\n}",
            "methodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#getPathFromSameIndexedEmbeddedSinceNoCompositionLimits\n methodBody: private String getPathFromSameIndexedEmbeddedSinceNoCompositionLimits(MappableTypeModel parentTypeModel, String relativePrefix) {\nif(hasCompositionLimits()){return null;\n}if(parent != null){if(this.relativePrefix.equals(relativePrefix) && this.parentTypeModel.isSubTypeOf(parentTypeModel)){return this.relativePrefix;\n}{String path=parent.getPathFromSameIndexedEmbeddedSinceNoCompositionLimits(parentTypeModel,relativePrefix);\nreturn path == null ? null : path + this.relativePrefix;\n}}{return null;\n}}"
        ],
        "sourceCodeAfterRefactoring": "public IndexSchemaFilter composeWithNested(MappableTypeModel parentTypeModel, String relativePrefix,\n\t\t\tInteger maxDepth, Set<String> includePaths) {\n\t\tString cyclicRecursionPath = getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n\t\tif ( cyclicRecursionPath != null ) {\n\t\t\tcyclicRecursionPath += relativePrefix;\n\t\t\tthrow new SearchException( \"Found an infinite IndexedEmbedded recursion involving path '\"\n\t\t\t\t\t+ cyclicRecursionPath + \"' on type '\" + parentTypeModel + \"'\" );\n\t\t}\n\n\t\tSet<String> nullSafeIncludePaths = includePaths == null ? Collections.emptySet() : includePaths;\n\n\t\t// The remaining composition depth according to \"this\" only\n\t\tInteger currentRemainingDepth = remainingCompositionDepth == null ? null : remainingCompositionDepth - 1;\n\n\t\t// The remaining composition depth according to the nested IndexedEmbedded only\n\t\tInteger nestedRemainingDepth = maxDepth;\n\t\tif ( maxDepth == null ) {\n\t\t\tif ( !nullSafeIncludePaths.isEmpty() ) {\n\t\t\t\t/*\n\t\t\t\t * If no max depth was provided and \"includePaths\" was provided,\n\t\t\t\t * the remaining composition depth is implicitly set to 0,\n\t\t\t\t * meaning no composition is allowed and paths are excluded unless\n\t\t\t\t * explicitly listed in \"includePaths\".\n\t\t\t\t */\n\t\t\t\tnestedRemainingDepth = 0;\n\t\t\t}\n\t\t}\n\n\t\t/*\n\t\t * By default, a composed filters' remaining composition depth is its parent's minus one\n\t\t * (or null if the remaining composition depth was not set in the parent)...\n\t\t */\n\t\tInteger composedRemainingDepth = currentRemainingDepth;\n\t\tif ( composedRemainingDepth == null\n\t\t\t\t|| nestedRemainingDepth != null && composedRemainingDepth > nestedRemainingDepth ) {\n\t\t\t/*\n\t\t\t * ... but the nested filter can override it.\n\t\t\t */\n\t\t\tcomposedRemainingDepth = nestedRemainingDepth;\n\t\t}\n\n\t\tSet<String> composedFilterExplicitlyIncludedPaths = composeExplicitlyIncludedPaths(\n\t\t\t\trelativePrefix, nullSafeIncludePaths, currentRemainingDepth, nestedRemainingDepth\n\t\t);\n\n\t\treturn new IndexSchemaFilter(\n\t\t\t\tthis, parentTypeModel, relativePrefix,\n\t\t\t\tcomposedRemainingDepth, composedFilterExplicitlyIncludedPaths\n\t\t);\n\t}\nprivate Set<String> composeExplicitlyIncludedPaths(String relativePrefix, Set<String> nullSafeIncludePaths,\n\t\t\tInteger currentRemainingDepth, Integer nestedRemainingDepth) {\n\t\tSet<String> composedFilterExplicitlyIncludedPaths = new HashSet<>();\n\t\t/*\n\t\t * Add the nested filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they are not filtered out by the current filter.\n\t\t */\n\t\tfor ( String path : nullSafeIncludePaths ) {\n\t\t\tif ( isPathIncluded( currentRemainingDepth, explicitlyIncludedPaths, relativePrefix + path ) ) {\n\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( path );\n\t\t\t\t// Also add paths leading to this path (so that object nodes are not excluded)\n\t\t\t\tint afterPreviousDotIndex = 0;\n\t\t\t\tint nextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\twhile ( nextDotIndex >= 0 ) {\n\t\t\t\t\tString subPath = path.substring( 0, nextDotIndex );\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( subPath );\n\t\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n\t\t\t\t\tnextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Add the current filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they start with the nested filter's prefix and are not filtered out by the nested filter.\n\t\t */\n\t\tint relativePrefixLength = relativePrefix.length();\n\t\tfor ( String path : explicitlyIncludedPaths ) {\n\t\t\tif ( path.startsWith( relativePrefix ) ) {\n\t\t\t\tString pathRelativeToNestedFilter = path.substring( relativePrefixLength );\n\t\t\t\tif ( isPathIncluded( nestedRemainingDepth, nullSafeIncludePaths, pathRelativeToNestedFilter ) ) {\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( pathRelativeToNestedFilter );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn composedFilterExplicitlyIncludedPaths;\n\t}",
        "diffSourceCode": "   145: \tpublic IndexSchemaFilter composeWithNested(MappableTypeModel parentTypeModel, String relativePrefix,\n   146: \t\t\tInteger maxDepth, Set<String> includePaths) {\n   147: \t\tString cyclicRecursionPath = getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n   148: \t\tif ( cyclicRecursionPath != null ) {\n   149: \t\t\tcyclicRecursionPath += relativePrefix;\n   150: \t\t\tthrow new SearchException( \"Found an infinite IndexedEmbedded recursion involving path '\"\n   151: \t\t\t\t\t+ cyclicRecursionPath + \"' on type '\" + parentTypeModel + \"'\" );\n   152: \t\t}\n   153: \n   154: \t\tSet<String> nullSafeIncludePaths = includePaths == null ? Collections.emptySet() : includePaths;\n   155: \n   156: \t\t// The remaining composition depth according to \"this\" only\n   157: \t\tInteger currentRemainingDepth = remainingCompositionDepth == null ? null : remainingCompositionDepth - 1;\n   158: \n   159: \t\t// The remaining composition depth according to the nested IndexedEmbedded only\n   160: \t\tInteger nestedRemainingDepth = maxDepth;\n   161: \t\tif ( maxDepth == null ) {\n   162: \t\t\tif ( !nullSafeIncludePaths.isEmpty() ) {\n   163: \t\t\t\t/*\n   164: \t\t\t\t * If no max depth was provided and \"includePaths\" was provided,\n   165: \t\t\t\t * the remaining composition depth is implicitly set to 0,\n   166: \t\t\t\t * meaning no composition is allowed and paths are excluded unless\n   167: \t\t\t\t * explicitly listed in \"includePaths\".\n   168: \t\t\t\t */\n   169: \t\t\t\tnestedRemainingDepth = 0;\n   170: \t\t\t}\n   171: \t\t}\n   172: \n   173: \t\t/*\n   174: \t\t * By default, a composed filters' remaining composition depth is its parent's minus one\n   175: \t\t * (or null if the remaining composition depth was not set in the parent)...\n   176: \t\t */\n   177: \t\tInteger composedRemainingDepth = currentRemainingDepth;\n   178: \t\tif ( composedRemainingDepth == null\n   179: \t\t\t\t|| nestedRemainingDepth != null && composedRemainingDepth > nestedRemainingDepth ) {\n   180: \t\t\t/*\n   181: \t\t\t * ... but the nested filter can override it.\n   182: \t\t\t */\n   183: \t\t\tcomposedRemainingDepth = nestedRemainingDepth;\n   184: \t\t}\n   185: \n-  186: \t\tSet<String> composedFilterExplicitlyIncludedPaths = new HashSet<>();\n-  187: \t\t/*\n-  188: \t\t * Add the nested filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n-  189: \t\t * provided they are not filtered out by the current filter.\n-  190: \t\t */\n-  191: \t\tfor ( String path : nullSafeIncludePaths ) {\n-  192: \t\t\tif ( isPathIncluded( currentRemainingDepth, explicitlyIncludedPaths, relativePrefix + path ) ) {\n-  193: \t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( path );\n-  194: \t\t\t\t// Also add paths leading to this path (so that object nodes are not excluded)\n-  195: \t\t\t\tint afterPreviousDotIndex = 0;\n-  196: \t\t\t\tint nextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n-  197: \t\t\t\twhile ( nextDotIndex >= 0 ) {\n-  198: \t\t\t\t\tString subPath = path.substring( 0, nextDotIndex );\n-  199: \t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( subPath );\n-  200: \t\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n-  201: \t\t\t\t\tnextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n-  202: \t\t\t\t}\n-  203: \t\t\t}\n-  204: \t\t}\n-  205: \t\t/*\n-  206: \t\t * Add the current filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n-  207: \t\t * provided they start with the nested filter's prefix and are not filtered out by the nested filter.\n-  208: \t\t */\n-  209: \t\tint relativePrefixLength = relativePrefix.length();\n-  210: \t\tfor ( String path : explicitlyIncludedPaths ) {\n-  211: \t\t\tif ( path.startsWith( relativePrefix ) ) {\n-  212: \t\t\t\tString pathRelativeToNestedFilter = path.substring( relativePrefixLength );\n-  213: \t\t\t\tif ( isPathIncluded( nestedRemainingDepth, nullSafeIncludePaths, pathRelativeToNestedFilter ) ) {\n-  214: \t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( pathRelativeToNestedFilter );\n-  215: \t\t\t\t}\n-  216: \t\t\t}\n-  217: \t\t}\n-  218: \n-  219: \t\treturn new IndexSchemaFilter(\n-  220: \t\t\t\tthis, parentTypeModel, relativePrefix,\n-  221: \t\t\t\tcomposedRemainingDepth, composedFilterExplicitlyIncludedPaths\n-  222: \t\t);\n-  223: \t}\n-  224: \n-  225: \tprivate static boolean isPathIncluded(Integer remainingDepth, Set<String> explicitlyIncludedPaths, String relativePath) {\n-  226: \t\treturn isEveryPathIncludedByDefault( remainingDepth )\n-  227: \t\t\t\t|| explicitlyIncludedPaths.contains( relativePath );\n-  228: \t}\n-  229: \n-  230: \tprivate static boolean isEveryPathIncludedByDefault(Integer remainingDepth) {\n-  231: \t\t/*\n+  186: \t\tSet<String> composedFilterExplicitlyIncludedPaths = composeExplicitlyIncludedPaths(\n+  187: \t\t\t\trelativePrefix, nullSafeIncludePaths, currentRemainingDepth, nestedRemainingDepth\n+  188: \t\t);\n+  189: \n+  190: \t\treturn new IndexSchemaFilter(\n+  191: \t\t\t\tthis, parentTypeModel, relativePrefix,\n+  192: \t\t\t\tcomposedRemainingDepth, composedFilterExplicitlyIncludedPaths\n+  193: \t\t);\n+  194: \t}\n+  195: \n+  196: \tprivate Set<String> composeExplicitlyIncludedPaths(String relativePrefix, Set<String> nullSafeIncludePaths,\n+  197: \t\t\tInteger currentRemainingDepth, Integer nestedRemainingDepth) {\n+  198: \t\tSet<String> composedFilterExplicitlyIncludedPaths = new HashSet<>();\n+  199: \t\t/*\n+  200: \t\t * Add the nested filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n+  201: \t\t * provided they are not filtered out by the current filter.\n+  202: \t\t */\n+  203: \t\tfor ( String path : nullSafeIncludePaths ) {\n+  204: \t\t\tif ( isPathIncluded( currentRemainingDepth, explicitlyIncludedPaths, relativePrefix + path ) ) {\n+  205: \t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( path );\n+  206: \t\t\t\t// Also add paths leading to this path (so that object nodes are not excluded)\n+  207: \t\t\t\tint afterPreviousDotIndex = 0;\n+  208: \t\t\t\tint nextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n+  209: \t\t\t\twhile ( nextDotIndex >= 0 ) {\n+  210: \t\t\t\t\tString subPath = path.substring( 0, nextDotIndex );\n+  211: \t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( subPath );\n+  212: \t\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n+  213: \t\t\t\t\tnextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n+  214: \t\t\t\t}\n+  215: \t\t\t}\n+  216: \t\t}\n+  217: \t\t/*\n+  218: \t\t * Add the current filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n+  219: \t\t * provided they start with the nested filter's prefix and are not filtered out by the nested filter.\n+  220: \t\t */\n+  221: \t\tint relativePrefixLength = relativePrefix.length();\n+  222: \t\tfor ( String path : explicitlyIncludedPaths ) {\n+  223: \t\t\tif ( path.startsWith( relativePrefix ) ) {\n+  224: \t\t\t\tString pathRelativeToNestedFilter = path.substring( relativePrefixLength );\n+  225: \t\t\t\tif ( isPathIncluded( nestedRemainingDepth, nullSafeIncludePaths, pathRelativeToNestedFilter ) ) {\n+  226: \t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( pathRelativeToNestedFilter );\n+  227: \t\t\t\t}\n+  228: \t\t\t}\n+  229: \t\t}\n+  230: \t\treturn composedFilterExplicitlyIncludedPaths;\n+  231: \t}\n",
        "uniqueId": "35593714e0e5ffec22e4e9e93e12eaf3ab24d1a5_145_223_196_231_145_194",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 91
            },
            "BRANCH": {
                "missed": 0,
                "covered": 16
            },
            "LINE": {
                "missed": 0,
                "covered": 16
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 9
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate assertEventUUIDVersion(session Session, expectedVersion int) : void extracted from public validMappingWithCustomUuidGenerator() : void in class org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm-coordination-outbox-polling/src/test/java/org/hibernate/search/integrationtest/mapper/orm/coordination/outboxpolling/automaticindexing/OutboxPollingCustomEntityMappingIT.java",
                "startLine": 270,
                "endLine": 315,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm-coordination-outbox-polling/src/test/java/org/hibernate/search/integrationtest/mapper/orm/coordination/outboxpolling/automaticindexing/OutboxPollingCustomEntityMappingIT.java",
                "startLine": 268,
                "endLine": 310,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm-coordination-outbox-polling/src/test/java/org/hibernate/search/integrationtest/mapper/orm/coordination/outboxpolling/automaticindexing/OutboxPollingCustomEntityMappingIT.java",
                "startLine": 373,
                "endLine": 380,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void validMappingWithCustomUuidGenerator() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy\", \"time\" )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\t\t\tassertThat( events )\n\t\t\t\t\t\t.hasSize( 1 )\n\t\t\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t\t\t.extracting( UUID::version )\n\t\t\t\t\t\t.contains( 1 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t}",
        "filePathBefore": "integrationtest/mapper/orm-coordination-outbox-polling/src/test/java/org/hibernate/search/integrationtest/mapper/orm/coordination/outboxpolling/automaticindexing/OutboxPollingCustomEntityMappingIT.java",
        "isPureRefactoring": true,
        "commitId": "fe9d94f5875ba28017d584653e397cffe57d7555",
        "packageNameBefore": "org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing",
        "classNameBefore": "org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT#validMappingWithCustomUuidGenerator",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.IndexedEntity#getId\n methodBody: public Integer getId() {\nreturn id;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.IndexedEntity#setIndexedField\n methodBody: public void setIndexedField(String indexedField) {\nthis.indexedField=indexedField;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.KeysStatementInspector#countByKey\n methodBody: public int countByKey(String key) {\nreturn sqlByKey.get(key).size();\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.IndexedEntity#setId\n methodBody: public void setId(Integer id) {\nthis.id=id;\n}",
        "classSignatureBefore": "public class OutboxPollingCustomEntityMappingIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT#validMappingWithCustomUuidGenerator"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT"
        ],
        "classSignatureBeforeSet": [
            "public class OutboxPollingCustomEntityMappingIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.awaitility.Awaitility.await;\nimport static org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils.with;\nimport static org.junit.Assume.assumeTrue;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.UUID;\nimport javax.persistence.Basic;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.boot.MappingException;\nimport org.hibernate.dialect.Dialect;\nimport org.hibernate.engine.jdbc.env.spi.NameQualifierSupport;\nimport org.hibernate.engine.spi.SessionFactoryImplementor;\nimport org.hibernate.resource.jdbc.spi.StatementInspector;\nimport org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.FilteringOutboxEventFinder;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.cfg.HibernateOrmMapperOutboxPollingSettings;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.cluster.impl.OutboxPollingAgentAdditionalJaxbMappingProducer;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.OutboxEvent;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.OutboxPollingOutboxEventAdditionalJaxbMappingProducer;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.CoordinationStrategyExpectations;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\npublic class OutboxPollingCustomEntityMappingIT {\n\n\tprivate static final String CUSTOM_SCHEMA = \"CUSTOM_SCHEMA\";\n\tprivate static final String ORIGINAL_OUTBOX_EVENT_TABLE_NAME = HibernateOrmMapperOutboxPollingSettings.Defaults.COORDINATION_ENTITY_MAPPING_OUTBOX_EVENT_TABLE;\n\tprivate static final String CUSTOM_OUTBOX_EVENT_TABLE_NAME = \"CUSTOM_OUTBOX_EVENT\";\n\n\tprivate static final String ORIGINAL_AGENT_TABLE_NAME = HibernateOrmMapperOutboxPollingSettings.Defaults.COORDINATION_ENTITY_MAPPING_AGENT_TABLE;\n\tprivate static final String CUSTOM_AGENT_TABLE_NAME = \"CUSTOM_AGENT\";\n\tprivate static final String VALID_OUTBOX_EVENT_MAPPING;\n\tprivate static final String VALID_AGENT_EVENT_MAPPING;\n\n\tprivate static final String[] SQL_KEYS;\n\n\tstatic {\n\t\tVALID_OUTBOX_EVENT_MAPPING = OutboxPollingOutboxEventAdditionalJaxbMappingProducer.ENTITY_DEFINITION\n\t\t\t\t.replace( ORIGINAL_OUTBOX_EVENT_TABLE_NAME, CUSTOM_OUTBOX_EVENT_TABLE_NAME );\n\n\t\tVALID_AGENT_EVENT_MAPPING = OutboxPollingAgentAdditionalJaxbMappingProducer.ENTITY_DEFINITION\n\t\t\t\t.replace( ORIGINAL_AGENT_TABLE_NAME, CUSTOM_AGENT_TABLE_NAME );\n\n\t\tSQL_KEYS = new String[] {\n\t\t\t\tORIGINAL_OUTBOX_EVENT_TABLE_NAME, CUSTOM_OUTBOX_EVENT_TABLE_NAME,\n\t\t\t\tORIGINAL_AGENT_TABLE_NAME, CUSTOM_AGENT_TABLE_NAME,\n\t\t\t\tCUSTOM_SCHEMA,\n\t\t};\n\t}\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock();\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock )\n\t\t\t.coordinationStrategy( CoordinationStrategyExpectations.outboxPolling() );\n\n\tprivate SessionFactory sessionFactory;\n\n\tprivate final FilteringOutboxEventFinder outboxEventFinder = new FilteringOutboxEventFinder();\n\n\t@Test\n\tpublic void wrongOutboxEventMapping() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outboxevent.entity.mapping\", \"<entity-mappings><ciao></ciao></entity-mappings>\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( MappingException.class )\n\t\t\t\t.hasMessageContainingAll( \"Unable to perform unmarshalling\", \"unexpected element\" );\n\t}\n\n\t@Test\n\tpublic void wrongAgentMapping() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.agent.entity.mapping\", \"<entity-mappings><ciao></ciao></entity-mappings>\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( MappingException.class )\n\t\t\t\t.hasMessageContainingAll( \"Unable to perform unmarshalling\", \"unexpected element\" );\n\t}\n\n\t@Test\n\tpublic void validOutboxEventMapping() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outboxevent.entity.mapping\", VALID_OUTBOX_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isZero();\n\t}\n\n\t@Test\n\tpublic void validAgentMapping() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.agent.entity.mapping\", VALID_AGENT_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void conflictingAgentMappingConfiguration() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.agent.entity.mapping\", VALID_AGENT_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", \"break_it_all\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Outbox polling agent configuration property conflict.\" );\n\t}\n\n\t@Test\n\tpublic void conflictingOutboxeventMappingConfiguration() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outboxevent.entity.mapping\", VALID_OUTBOX_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", \"break_it_all\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Outbox event configuration property conflict.\" );\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomNames() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", CUSTOM_AGENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", CUSTOM_OUTBOX_EVENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomNamesAndSchema() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.schema\", CUSTOM_SCHEMA )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", CUSTOM_AGENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.schema\", CUSTOM_SCHEMA )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", CUSTOM_OUTBOX_EVENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassumeTrue( \"This test only makes sense if the database supports schemas\",\n\t\t\t\tgetNameQualifierSupport().supportsSchemas() );\n\t\tassumeTrue( \"This test only makes sense if the dialect supports creating schemas\",\n\t\t\t\tgetDialect().canCreateSchema() );\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\t\t\tassertThat( events )\n\t\t\t\t\t\t.hasSize( 1 )\n\t\t\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t\t\t.extracting( UUID::version )\n\t\t\t\t\t\t.contains( 4 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\n\t\tassertThat( statementInspector.countByKey( CUSTOM_SCHEMA ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomUuidGenerator() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy\", \"time\" )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\t\t\tassertThat( events )\n\t\t\t\t\t\t.hasSize( 1 )\n\t\t\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t\t\t.extracting( UUID::version )\n\t\t\t\t\t\t.contains( 1 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomUuidDataType() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.uuid_jdbc_type\", \"uuid-char\" )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\t\t\tassertThat( events )\n\t\t\t\t\t\t.hasSize( 1 )\n\t\t\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t\t\t.extracting( UUID::version )\n\t\t\t\t\t\t.contains( 4 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomFailingUuidGenerator() {\n\t\tassertThatThrownBy(\n\t\t\t\t() -> ormSetupHelper.start()\n\t\t\t\t\t\t.withProperty(\n\t\t\t\t\t\t\t\t\"hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy\",\n\t\t\t\t\t\t\t\t\"something-incompatible\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.setup( IndexedEntity.class )\n\t\t).isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\"Invalid value for configuration property 'hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy'\",\n\t\t\t\t\t\t\"something-incompatible\",\n\t\t\t\t\t\t\"Valid names are: [auto, random, time]\"\n\t\t\t\t);\n\t}\n\n\tprivate Dialect getDialect() {\n\t\treturn sessionFactory.unwrap( SessionFactoryImplementor.class ).getJdbcServices()\n\t\t\t\t.getJdbcEnvironment().getDialect();\n\t}\n\n\tprivate NameQualifierSupport getNameQualifierSupport() {\n\t\treturn sessionFactory.unwrap( SessionFactoryImplementor.class ).getJdbcServices()\n\t\t\t\t.getJdbcEnvironment().getNameQualifierSupport();\n\t}\n\n\t@Entity(name = IndexedEntity.INDEX)\n\t@Indexed(index = IndexedEntity.INDEX)\n\tpublic static class IndexedEntity {\n\t\tstatic final String INDEX = \"IndexedEntity\";\n\n\t\t@Id\n\t\tprivate Integer id;\n\n\t\t@Basic\n\t\t@GenericField\n\t\tprivate String indexedField;\n\n\t\tpublic IndexedEntity() {\n\t\t}\n\n\t\tpublic IndexedEntity(Integer id, String indexedField) {\n\t\t\tthis.id = id;\n\t\t\tthis.indexedField = indexedField;\n\t\t}\n\n\t\tpublic Integer getId() {\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\tpublic String getIndexedField() {\n\t\t\treturn indexedField;\n\t\t}\n\n\t\tpublic void setIndexedField(String indexedField) {\n\t\t\tthis.indexedField = indexedField;\n\t\t}\n\t}\n\n\tpublic static class KeysStatementInspector implements StatementInspector {\n\n\t\tprivate Map<String, List<String>> sqlByKey = new HashMap<>();\n\n\t\tpublic KeysStatementInspector() {\n\t\t\tfor ( String key : SQL_KEYS ) {\n\t\t\t\tsqlByKey.put( key, new ArrayList<>() );\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic String inspect(String sql) {\n\t\t\tfor ( String key : SQL_KEYS ) {\n\t\t\t\tif ( Arrays.stream( sql.split( \"[^A-Za-z0-9_-]\" ) ).anyMatch( token -> key.equals( token ) ) ) {\n\t\t\t\t\tsqlByKey.get( key ).add( sql );\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn sql;\n\t\t}\n\n\t\tpublic int countByKey(String key) {\n\t\t\treturn sqlByKey.get( key ).size();\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/orm-coordination-outbox-polling/src/test/java/org/hibernate/search/integrationtest/mapper/orm/coordination/outboxpolling/automaticindexing/OutboxPollingCustomEntityMappingIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.awaitility.Awaitility.await;\nimport static org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils.with;\nimport static org.junit.Assume.assumeTrue;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.UUID;\nimport javax.persistence.Basic;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.Session;\nimport org.hibernate.SessionFactory;\nimport org.hibernate.boot.MappingException;\nimport org.hibernate.dialect.Dialect;\nimport org.hibernate.engine.jdbc.env.spi.NameQualifierSupport;\nimport org.hibernate.engine.spi.SessionFactoryImplementor;\nimport org.hibernate.resource.jdbc.spi.StatementInspector;\nimport org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.FilteringOutboxEventFinder;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.cfg.HibernateOrmMapperOutboxPollingSettings;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.cluster.impl.Agent;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.cluster.impl.OutboxPollingAgentAdditionalJaxbMappingProducer;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.OutboxEvent;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.OutboxPollingOutboxEventAdditionalJaxbMappingProducer;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.CoordinationStrategyExpectations;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\npublic class OutboxPollingCustomEntityMappingIT {\n\n\tprivate static final String CUSTOM_SCHEMA = \"CUSTOM_SCHEMA\";\n\tprivate static final String ORIGINAL_OUTBOX_EVENT_TABLE_NAME = HibernateOrmMapperOutboxPollingSettings.Defaults.COORDINATION_ENTITY_MAPPING_OUTBOX_EVENT_TABLE;\n\tprivate static final String CUSTOM_OUTBOX_EVENT_TABLE_NAME = \"CUSTOM_OUTBOX_EVENT\";\n\n\tprivate static final String ORIGINAL_AGENT_TABLE_NAME = HibernateOrmMapperOutboxPollingSettings.Defaults.COORDINATION_ENTITY_MAPPING_AGENT_TABLE;\n\tprivate static final String CUSTOM_AGENT_TABLE_NAME = \"CUSTOM_AGENT\";\n\tprivate static final String VALID_OUTBOX_EVENT_MAPPING;\n\tprivate static final String VALID_AGENT_EVENT_MAPPING;\n\n\tprivate static final String[] SQL_KEYS;\n\n\tstatic {\n\t\tVALID_OUTBOX_EVENT_MAPPING = OutboxPollingOutboxEventAdditionalJaxbMappingProducer.ENTITY_DEFINITION\n\t\t\t\t.replace( ORIGINAL_OUTBOX_EVENT_TABLE_NAME, CUSTOM_OUTBOX_EVENT_TABLE_NAME );\n\n\t\tVALID_AGENT_EVENT_MAPPING = OutboxPollingAgentAdditionalJaxbMappingProducer.ENTITY_DEFINITION\n\t\t\t\t.replace( ORIGINAL_AGENT_TABLE_NAME, CUSTOM_AGENT_TABLE_NAME );\n\n\t\tSQL_KEYS = new String[] {\n\t\t\t\tORIGINAL_OUTBOX_EVENT_TABLE_NAME, CUSTOM_OUTBOX_EVENT_TABLE_NAME,\n\t\t\t\tORIGINAL_AGENT_TABLE_NAME, CUSTOM_AGENT_TABLE_NAME,\n\t\t\t\tCUSTOM_SCHEMA,\n\t\t};\n\t}\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock();\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock )\n\t\t\t.coordinationStrategy( CoordinationStrategyExpectations.outboxPolling() );\n\n\tprivate SessionFactory sessionFactory;\n\n\tprivate final FilteringOutboxEventFinder outboxEventFinder = new FilteringOutboxEventFinder();\n\n\t@Test\n\tpublic void wrongOutboxEventMapping() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outboxevent.entity.mapping\", \"<entity-mappings><ciao></ciao></entity-mappings>\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( MappingException.class )\n\t\t\t\t.hasMessageContainingAll( \"Unable to perform unmarshalling\", \"unexpected element\" );\n\t}\n\n\t@Test\n\tpublic void wrongAgentMapping() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.agent.entity.mapping\", \"<entity-mappings><ciao></ciao></entity-mappings>\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( MappingException.class )\n\t\t\t\t.hasMessageContainingAll( \"Unable to perform unmarshalling\", \"unexpected element\" );\n\t}\n\n\t@Test\n\tpublic void validOutboxEventMapping() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outboxevent.entity.mapping\", VALID_OUTBOX_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isZero();\n\t}\n\n\t@Test\n\tpublic void validAgentMapping() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.agent.entity.mapping\", VALID_AGENT_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void conflictingAgentMappingConfiguration() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.agent.entity.mapping\", VALID_AGENT_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", \"break_it_all\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Outbox polling agent configuration property conflict.\" );\n\t}\n\n\t@Test\n\tpublic void conflictingOutboxeventMappingConfiguration() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outboxevent.entity.mapping\", VALID_OUTBOX_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", \"break_it_all\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Outbox event configuration property conflict.\" );\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomNames() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", CUSTOM_AGENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", CUSTOM_OUTBOX_EVENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomNamesAndSchema() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.schema\", CUSTOM_SCHEMA )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", CUSTOM_AGENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.schema\", CUSTOM_SCHEMA )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", CUSTOM_OUTBOX_EVENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassumeTrue( \"This test only makes sense if the database supports schemas\",\n\t\t\t\tgetNameQualifierSupport().supportsSchemas() );\n\t\tassumeTrue( \"This test only makes sense if the dialect supports creating schemas\",\n\t\t\t\tgetDialect().canCreateSchema() );\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tassertEventUUIDVersion( session, 4 );\n\t\t\t\tassertAgentUUIDVersion( session, 4 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\n\t\tassertThat( statementInspector.countByKey( CUSTOM_SCHEMA ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomUuidGenerator() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.uuid_gen_strategy\", \"time\" )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy\", \"time\" )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tassertEventUUIDVersion( session, 1 );\n\t\t\t\tassertAgentUUIDVersion( session, 1 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomUuidDataType() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.uuid_jdbc_type\", \"uuid-char\" )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.uuid_jdbc_type\", \"uuid-char\" )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tassertEventUUIDVersion( session, 4 );\n\t\t\t\tassertAgentUUIDVersion( session, 4 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomFailingUuidGenerator() {\n\t\tassertThatThrownBy(\n\t\t\t\t() -> ormSetupHelper.start()\n\t\t\t\t\t\t.withProperty(\n\t\t\t\t\t\t\t\t\"hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy\",\n\t\t\t\t\t\t\t\t\"something-incompatible\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.setup( IndexedEntity.class )\n\t\t).isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\"Invalid value for configuration property 'hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy'\",\n\t\t\t\t\t\t\"something-incompatible\",\n\t\t\t\t\t\t\"Valid names are: [auto, random, time]\"\n\t\t\t\t);\n\t}\n\n\tprivate void assertEventUUIDVersion(Session session, int expectedVersion) {\n\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\tassertThat( events )\n\t\t\t\t.hasSize( 1 )\n\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t.extracting( UUID::version )\n\t\t\t\t.containsOnly( expectedVersion );\n\t}\n\n\tprivate void assertAgentUUIDVersion(Session session, int expectedVersion) {\n\t\tassertThat(\n\t\t\t\tsession.createQuery(\n\t\t\t\t\t\t\t\t\"select a from \" + OutboxPollingAgentAdditionalJaxbMappingProducer.ENTITY_NAME + \" a \",\n\t\t\t\t\t\t\t\tAgent.class\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.getResultList()\n\t\t).hasSizeGreaterThan( 0 )\n\t\t\t\t.extracting( Agent::getId )\n\t\t\t\t.extracting( UUID::version )\n\t\t\t\t.containsOnly( expectedVersion );\n\t}\n\n\tprivate Dialect getDialect() {\n\t\treturn sessionFactory.unwrap( SessionFactoryImplementor.class ).getJdbcServices()\n\t\t\t\t.getJdbcEnvironment().getDialect();\n\t}\n\n\tprivate NameQualifierSupport getNameQualifierSupport() {\n\t\treturn sessionFactory.unwrap( SessionFactoryImplementor.class ).getJdbcServices()\n\t\t\t\t.getJdbcEnvironment().getNameQualifierSupport();\n\t}\n\n\t@Entity(name = IndexedEntity.INDEX)\n\t@Indexed(index = IndexedEntity.INDEX)\n\tpublic static class IndexedEntity {\n\t\tstatic final String INDEX = \"IndexedEntity\";\n\n\t\t@Id\n\t\tprivate Integer id;\n\n\t\t@Basic\n\t\t@GenericField\n\t\tprivate String indexedField;\n\n\t\tpublic IndexedEntity() {\n\t\t}\n\n\t\tpublic IndexedEntity(Integer id, String indexedField) {\n\t\t\tthis.id = id;\n\t\t\tthis.indexedField = indexedField;\n\t\t}\n\n\t\tpublic Integer getId() {\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\tpublic String getIndexedField() {\n\t\t\treturn indexedField;\n\t\t}\n\n\t\tpublic void setIndexedField(String indexedField) {\n\t\t\tthis.indexedField = indexedField;\n\t\t}\n\t}\n\n\tpublic static class KeysStatementInspector implements StatementInspector {\n\n\t\tprivate Map<String, List<String>> sqlByKey = new HashMap<>();\n\n\t\tpublic KeysStatementInspector() {\n\t\t\tfor ( String key : SQL_KEYS ) {\n\t\t\t\tsqlByKey.put( key, new ArrayList<>() );\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic String inspect(String sql) {\n\t\t\tfor ( String key : SQL_KEYS ) {\n\t\t\t\tif ( Arrays.stream( sql.split( \"[^A-Za-z0-9_-]\" ) ).anyMatch( token -> key.equals( token ) ) ) {\n\t\t\t\t\tsqlByKey.get( key ).add( sql );\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn sql;\n\t\t}\n\n\t\tpublic int countByKey(String key) {\n\t\t\treturn sqlByKey.get( key ).size();\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private void assertEventUUIDVersion(Session session, int expectedVersion) {\n\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\tassertThat( events )\n\t\t\t\t.hasSize( 1 )\n\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t.extracting( UUID::version )\n\t\t\t\t.containsOnly( expectedVersion );\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.IndexedEntity#getId\n methodBody: public Integer getId() {\nreturn id;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.IndexedEntity#setIndexedField\n methodBody: public void setIndexedField(String indexedField) {\nthis.indexedField=indexedField;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.KeysStatementInspector#countByKey\n methodBody: public int countByKey(String key) {\nreturn sqlByKey.get(key).size();\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.IndexedEntity#setId\n methodBody: public void setId(Integer id) {\nthis.id=id;\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void validMappingWithCustomUuidGenerator() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.uuid_gen_strategy\", \"time\" )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy\", \"time\" )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tassertEventUUIDVersion( session, 1 );\n\t\t\t\tassertAgentUUIDVersion( session, 1 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t}\nprivate void assertEventUUIDVersion(Session session, int expectedVersion) {\n\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\tassertThat( events )\n\t\t\t\t.hasSize( 1 )\n\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t.extracting( UUID::version )\n\t\t\t\t.containsOnly( expectedVersion );\n\t}",
        "diffSourceCode": "-  268: \t}\n-  269: \n-  270: \t@Test\n-  271: \tpublic void validMappingWithCustomUuidGenerator() {\n-  272: \t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n-  273: \n-  274: \t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n-  275: \t\tsessionFactory = ormSetupHelper.start()\n-  276: \t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n-  277: \t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n-  278: \t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n-  279: \t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy\", \"time\" )\n-  280: \t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n-  281: \t\t\t\t.setup( IndexedEntity.class );\n-  282: \t\tbackendMock.verifyExpectationsMet();\n-  283: \n-  284: \t\tint id = 1;\n-  285: \t\twith( sessionFactory ).runInTransaction( session -> {\n-  286: \t\t\tIndexedEntity entity = new IndexedEntity();\n-  287: \t\t\tentity.setId( id );\n-  288: \t\t\tentity.setIndexedField( \"value for the field\" );\n-  289: \t\t\tsession.persist( entity );\n-  290: \n-  291: \t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n-  292: \t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n-  293: \t\t} );\n-  294: \n-  295: \t\tawait().untilAsserted( () -> {\n-  296: \t\t\twith( sessionFactory ).runInTransaction( session -> {\n-  297: \t\t\t\t// check that correct UUIDs are generated by asserting the version:\n-  298: \t\t\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n-  299: \t\t\t\tassertThat( events )\n-  300: \t\t\t\t\t\t.hasSize( 1 )\n-  301: \t\t\t\t\t\t.extracting( OutboxEvent::getId )\n-  302: \t\t\t\t\t\t.extracting( UUID::version )\n-  303: \t\t\t\t\t\t.contains( 1 );\n-  304: \t\t\t} );\n-  305: \t\t} );\n-  306: \t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n-  307: \t\t// Make them visible to Hibernate Search now.\n-  308: \t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n-  309: \t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n-  310: \n-  311: \t\tbackendMock.verifyExpectationsMet();\n-  312: \n-  313: \t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n-  314: \t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n-  315: \t}\n-  373: \t\t).isInstanceOf( SearchException.class )\n-  374: \t\t\t\t.hasMessageContainingAll(\n-  375: \t\t\t\t\t\t\"Invalid value for configuration property 'hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy'\",\n-  376: \t\t\t\t\t\t\"something-incompatible\",\n-  377: \t\t\t\t\t\t\"Valid names are: [auto, random, time]\"\n-  378: \t\t\t\t);\n-  379: \t}\n-  380: \n+  268: \t@Test\n+  269: \tpublic void validMappingWithCustomUuidGenerator() {\n+  270: \t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n+  271: \n+  272: \t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n+  273: \t\tsessionFactory = ormSetupHelper.start()\n+  274: \t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n+  275: \t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n+  276: \t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n+  277: \t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.uuid_gen_strategy\", \"time\" )\n+  278: \t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy\", \"time\" )\n+  279: \t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n+  280: \t\t\t\t.setup( IndexedEntity.class );\n+  281: \t\tbackendMock.verifyExpectationsMet();\n+  282: \n+  283: \t\tint id = 1;\n+  284: \t\twith( sessionFactory ).runInTransaction( session -> {\n+  285: \t\t\tIndexedEntity entity = new IndexedEntity();\n+  286: \t\t\tentity.setId( id );\n+  287: \t\t\tentity.setIndexedField( \"value for the field\" );\n+  288: \t\t\tsession.persist( entity );\n+  289: \n+  290: \t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n+  291: \t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n+  292: \t\t} );\n+  293: \n+  294: \t\tawait().untilAsserted( () -> {\n+  295: \t\t\twith( sessionFactory ).runInTransaction( session -> {\n+  296: \t\t\t\t// check that correct UUIDs are generated by asserting the version:\n+  297: \t\t\t\tassertEventUUIDVersion( session, 1 );\n+  298: \t\t\t\tassertAgentUUIDVersion( session, 1 );\n+  299: \t\t\t} );\n+  300: \t\t} );\n+  301: \t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n+  302: \t\t// Make them visible to Hibernate Search now.\n+  303: \t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n+  304: \t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n+  305: \n+  306: \t\tbackendMock.verifyExpectationsMet();\n+  307: \n+  308: \t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n+  309: \t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n+  310: \t}\n+  311: \n+  312: \t@Test\n+  313: \tpublic void validMappingWithCustomUuidDataType() {\n+  314: \t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n+  315: \n+  373: \tprivate void assertEventUUIDVersion(Session session, int expectedVersion) {\n+  374: \t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n+  375: \t\tassertThat( events )\n+  376: \t\t\t\t.hasSize( 1 )\n+  377: \t\t\t\t.extracting( OutboxEvent::getId )\n+  378: \t\t\t\t.extracting( UUID::version )\n+  379: \t\t\t\t.containsOnly( expectedVersion );\n+  380: \t}\n",
        "uniqueId": "fe9d94f5875ba28017d584653e397cffe57d7555_270_315_373_380_268_310",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate assertEventUUIDVersion(session Session, expectedVersion int) : void extracted from public validMappingWithCustomNamesAndSchema() : void in class org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm-coordination-outbox-polling/src/test/java/org/hibernate/search/integrationtest/mapper/orm/coordination/outboxpolling/automaticindexing/OutboxPollingCustomEntityMappingIT.java",
                "startLine": 212,
                "endLine": 268,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm-coordination-outbox-polling/src/test/java/org/hibernate/search/integrationtest/mapper/orm/coordination/outboxpolling/automaticindexing/OutboxPollingCustomEntityMappingIT.java",
                "startLine": 214,
                "endLine": 266,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm-coordination-outbox-polling/src/test/java/org/hibernate/search/integrationtest/mapper/orm/coordination/outboxpolling/automaticindexing/OutboxPollingCustomEntityMappingIT.java",
                "startLine": 373,
                "endLine": 380,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void validMappingWithCustomNamesAndSchema() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.schema\", CUSTOM_SCHEMA )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", CUSTOM_AGENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.schema\", CUSTOM_SCHEMA )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", CUSTOM_OUTBOX_EVENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassumeTrue( \"This test only makes sense if the database supports schemas\",\n\t\t\t\tgetNameQualifierSupport().supportsSchemas() );\n\t\tassumeTrue( \"This test only makes sense if the dialect supports creating schemas\",\n\t\t\t\tgetDialect().canCreateSchema() );\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\t\t\tassertThat( events )\n\t\t\t\t\t\t.hasSize( 1 )\n\t\t\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t\t\t.extracting( UUID::version )\n\t\t\t\t\t\t.contains( 4 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\n\t\tassertThat( statementInspector.countByKey( CUSTOM_SCHEMA ) ).isPositive();\n\t}",
        "filePathBefore": "integrationtest/mapper/orm-coordination-outbox-polling/src/test/java/org/hibernate/search/integrationtest/mapper/orm/coordination/outboxpolling/automaticindexing/OutboxPollingCustomEntityMappingIT.java",
        "isPureRefactoring": true,
        "commitId": "fe9d94f5875ba28017d584653e397cffe57d7555",
        "packageNameBefore": "org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing",
        "classNameBefore": "org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT#validMappingWithCustomNamesAndSchema",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.IndexedEntity#getId\n methodBody: public Integer getId() {\nreturn id;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT#getDialect\n methodBody: private Dialect getDialect() {\nreturn sessionFactory.unwrap(SessionFactoryImplementor.class).getJdbcServices().getJdbcEnvironment().getDialect();\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.IndexedEntity#setIndexedField\n methodBody: public void setIndexedField(String indexedField) {\nthis.indexedField=indexedField;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.KeysStatementInspector#countByKey\n methodBody: public int countByKey(String key) {\nreturn sqlByKey.get(key).size();\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.IndexedEntity#setId\n methodBody: public void setId(Integer id) {\nthis.id=id;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT#getNameQualifierSupport\n methodBody: private NameQualifierSupport getNameQualifierSupport() {\nreturn sessionFactory.unwrap(SessionFactoryImplementor.class).getJdbcServices().getJdbcEnvironment().getNameQualifierSupport();\n}",
        "classSignatureBefore": "public class OutboxPollingCustomEntityMappingIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT#validMappingWithCustomNamesAndSchema"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT"
        ],
        "classSignatureBeforeSet": [
            "public class OutboxPollingCustomEntityMappingIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.awaitility.Awaitility.await;\nimport static org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils.with;\nimport static org.junit.Assume.assumeTrue;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.UUID;\nimport javax.persistence.Basic;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.boot.MappingException;\nimport org.hibernate.dialect.Dialect;\nimport org.hibernate.engine.jdbc.env.spi.NameQualifierSupport;\nimport org.hibernate.engine.spi.SessionFactoryImplementor;\nimport org.hibernate.resource.jdbc.spi.StatementInspector;\nimport org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.FilteringOutboxEventFinder;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.cfg.HibernateOrmMapperOutboxPollingSettings;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.cluster.impl.OutboxPollingAgentAdditionalJaxbMappingProducer;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.OutboxEvent;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.OutboxPollingOutboxEventAdditionalJaxbMappingProducer;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.CoordinationStrategyExpectations;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\npublic class OutboxPollingCustomEntityMappingIT {\n\n\tprivate static final String CUSTOM_SCHEMA = \"CUSTOM_SCHEMA\";\n\tprivate static final String ORIGINAL_OUTBOX_EVENT_TABLE_NAME = HibernateOrmMapperOutboxPollingSettings.Defaults.COORDINATION_ENTITY_MAPPING_OUTBOX_EVENT_TABLE;\n\tprivate static final String CUSTOM_OUTBOX_EVENT_TABLE_NAME = \"CUSTOM_OUTBOX_EVENT\";\n\n\tprivate static final String ORIGINAL_AGENT_TABLE_NAME = HibernateOrmMapperOutboxPollingSettings.Defaults.COORDINATION_ENTITY_MAPPING_AGENT_TABLE;\n\tprivate static final String CUSTOM_AGENT_TABLE_NAME = \"CUSTOM_AGENT\";\n\tprivate static final String VALID_OUTBOX_EVENT_MAPPING;\n\tprivate static final String VALID_AGENT_EVENT_MAPPING;\n\n\tprivate static final String[] SQL_KEYS;\n\n\tstatic {\n\t\tVALID_OUTBOX_EVENT_MAPPING = OutboxPollingOutboxEventAdditionalJaxbMappingProducer.ENTITY_DEFINITION\n\t\t\t\t.replace( ORIGINAL_OUTBOX_EVENT_TABLE_NAME, CUSTOM_OUTBOX_EVENT_TABLE_NAME );\n\n\t\tVALID_AGENT_EVENT_MAPPING = OutboxPollingAgentAdditionalJaxbMappingProducer.ENTITY_DEFINITION\n\t\t\t\t.replace( ORIGINAL_AGENT_TABLE_NAME, CUSTOM_AGENT_TABLE_NAME );\n\n\t\tSQL_KEYS = new String[] {\n\t\t\t\tORIGINAL_OUTBOX_EVENT_TABLE_NAME, CUSTOM_OUTBOX_EVENT_TABLE_NAME,\n\t\t\t\tORIGINAL_AGENT_TABLE_NAME, CUSTOM_AGENT_TABLE_NAME,\n\t\t\t\tCUSTOM_SCHEMA,\n\t\t};\n\t}\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock();\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock )\n\t\t\t.coordinationStrategy( CoordinationStrategyExpectations.outboxPolling() );\n\n\tprivate SessionFactory sessionFactory;\n\n\tprivate final FilteringOutboxEventFinder outboxEventFinder = new FilteringOutboxEventFinder();\n\n\t@Test\n\tpublic void wrongOutboxEventMapping() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outboxevent.entity.mapping\", \"<entity-mappings><ciao></ciao></entity-mappings>\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( MappingException.class )\n\t\t\t\t.hasMessageContainingAll( \"Unable to perform unmarshalling\", \"unexpected element\" );\n\t}\n\n\t@Test\n\tpublic void wrongAgentMapping() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.agent.entity.mapping\", \"<entity-mappings><ciao></ciao></entity-mappings>\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( MappingException.class )\n\t\t\t\t.hasMessageContainingAll( \"Unable to perform unmarshalling\", \"unexpected element\" );\n\t}\n\n\t@Test\n\tpublic void validOutboxEventMapping() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outboxevent.entity.mapping\", VALID_OUTBOX_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isZero();\n\t}\n\n\t@Test\n\tpublic void validAgentMapping() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.agent.entity.mapping\", VALID_AGENT_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void conflictingAgentMappingConfiguration() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.agent.entity.mapping\", VALID_AGENT_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", \"break_it_all\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Outbox polling agent configuration property conflict.\" );\n\t}\n\n\t@Test\n\tpublic void conflictingOutboxeventMappingConfiguration() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outboxevent.entity.mapping\", VALID_OUTBOX_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", \"break_it_all\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Outbox event configuration property conflict.\" );\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomNames() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", CUSTOM_AGENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", CUSTOM_OUTBOX_EVENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomNamesAndSchema() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.schema\", CUSTOM_SCHEMA )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", CUSTOM_AGENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.schema\", CUSTOM_SCHEMA )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", CUSTOM_OUTBOX_EVENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassumeTrue( \"This test only makes sense if the database supports schemas\",\n\t\t\t\tgetNameQualifierSupport().supportsSchemas() );\n\t\tassumeTrue( \"This test only makes sense if the dialect supports creating schemas\",\n\t\t\t\tgetDialect().canCreateSchema() );\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\t\t\tassertThat( events )\n\t\t\t\t\t\t.hasSize( 1 )\n\t\t\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t\t\t.extracting( UUID::version )\n\t\t\t\t\t\t.contains( 4 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\n\t\tassertThat( statementInspector.countByKey( CUSTOM_SCHEMA ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomUuidGenerator() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy\", \"time\" )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\t\t\tassertThat( events )\n\t\t\t\t\t\t.hasSize( 1 )\n\t\t\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t\t\t.extracting( UUID::version )\n\t\t\t\t\t\t.contains( 1 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomUuidDataType() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.uuid_jdbc_type\", \"uuid-char\" )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\t\t\tassertThat( events )\n\t\t\t\t\t\t.hasSize( 1 )\n\t\t\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t\t\t.extracting( UUID::version )\n\t\t\t\t\t\t.contains( 4 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomFailingUuidGenerator() {\n\t\tassertThatThrownBy(\n\t\t\t\t() -> ormSetupHelper.start()\n\t\t\t\t\t\t.withProperty(\n\t\t\t\t\t\t\t\t\"hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy\",\n\t\t\t\t\t\t\t\t\"something-incompatible\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.setup( IndexedEntity.class )\n\t\t).isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\"Invalid value for configuration property 'hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy'\",\n\t\t\t\t\t\t\"something-incompatible\",\n\t\t\t\t\t\t\"Valid names are: [auto, random, time]\"\n\t\t\t\t);\n\t}\n\n\tprivate Dialect getDialect() {\n\t\treturn sessionFactory.unwrap( SessionFactoryImplementor.class ).getJdbcServices()\n\t\t\t\t.getJdbcEnvironment().getDialect();\n\t}\n\n\tprivate NameQualifierSupport getNameQualifierSupport() {\n\t\treturn sessionFactory.unwrap( SessionFactoryImplementor.class ).getJdbcServices()\n\t\t\t\t.getJdbcEnvironment().getNameQualifierSupport();\n\t}\n\n\t@Entity(name = IndexedEntity.INDEX)\n\t@Indexed(index = IndexedEntity.INDEX)\n\tpublic static class IndexedEntity {\n\t\tstatic final String INDEX = \"IndexedEntity\";\n\n\t\t@Id\n\t\tprivate Integer id;\n\n\t\t@Basic\n\t\t@GenericField\n\t\tprivate String indexedField;\n\n\t\tpublic IndexedEntity() {\n\t\t}\n\n\t\tpublic IndexedEntity(Integer id, String indexedField) {\n\t\t\tthis.id = id;\n\t\t\tthis.indexedField = indexedField;\n\t\t}\n\n\t\tpublic Integer getId() {\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\tpublic String getIndexedField() {\n\t\t\treturn indexedField;\n\t\t}\n\n\t\tpublic void setIndexedField(String indexedField) {\n\t\t\tthis.indexedField = indexedField;\n\t\t}\n\t}\n\n\tpublic static class KeysStatementInspector implements StatementInspector {\n\n\t\tprivate Map<String, List<String>> sqlByKey = new HashMap<>();\n\n\t\tpublic KeysStatementInspector() {\n\t\t\tfor ( String key : SQL_KEYS ) {\n\t\t\t\tsqlByKey.put( key, new ArrayList<>() );\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic String inspect(String sql) {\n\t\t\tfor ( String key : SQL_KEYS ) {\n\t\t\t\tif ( Arrays.stream( sql.split( \"[^A-Za-z0-9_-]\" ) ).anyMatch( token -> key.equals( token ) ) ) {\n\t\t\t\t\tsqlByKey.get( key ).add( sql );\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn sql;\n\t\t}\n\n\t\tpublic int countByKey(String key) {\n\t\t\treturn sqlByKey.get( key ).size();\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/orm-coordination-outbox-polling/src/test/java/org/hibernate/search/integrationtest/mapper/orm/coordination/outboxpolling/automaticindexing/OutboxPollingCustomEntityMappingIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.awaitility.Awaitility.await;\nimport static org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils.with;\nimport static org.junit.Assume.assumeTrue;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.UUID;\nimport javax.persistence.Basic;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.Session;\nimport org.hibernate.SessionFactory;\nimport org.hibernate.boot.MappingException;\nimport org.hibernate.dialect.Dialect;\nimport org.hibernate.engine.jdbc.env.spi.NameQualifierSupport;\nimport org.hibernate.engine.spi.SessionFactoryImplementor;\nimport org.hibernate.resource.jdbc.spi.StatementInspector;\nimport org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.FilteringOutboxEventFinder;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.cfg.HibernateOrmMapperOutboxPollingSettings;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.cluster.impl.Agent;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.cluster.impl.OutboxPollingAgentAdditionalJaxbMappingProducer;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.OutboxEvent;\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.OutboxPollingOutboxEventAdditionalJaxbMappingProducer;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.CoordinationStrategyExpectations;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\npublic class OutboxPollingCustomEntityMappingIT {\n\n\tprivate static final String CUSTOM_SCHEMA = \"CUSTOM_SCHEMA\";\n\tprivate static final String ORIGINAL_OUTBOX_EVENT_TABLE_NAME = HibernateOrmMapperOutboxPollingSettings.Defaults.COORDINATION_ENTITY_MAPPING_OUTBOX_EVENT_TABLE;\n\tprivate static final String CUSTOM_OUTBOX_EVENT_TABLE_NAME = \"CUSTOM_OUTBOX_EVENT\";\n\n\tprivate static final String ORIGINAL_AGENT_TABLE_NAME = HibernateOrmMapperOutboxPollingSettings.Defaults.COORDINATION_ENTITY_MAPPING_AGENT_TABLE;\n\tprivate static final String CUSTOM_AGENT_TABLE_NAME = \"CUSTOM_AGENT\";\n\tprivate static final String VALID_OUTBOX_EVENT_MAPPING;\n\tprivate static final String VALID_AGENT_EVENT_MAPPING;\n\n\tprivate static final String[] SQL_KEYS;\n\n\tstatic {\n\t\tVALID_OUTBOX_EVENT_MAPPING = OutboxPollingOutboxEventAdditionalJaxbMappingProducer.ENTITY_DEFINITION\n\t\t\t\t.replace( ORIGINAL_OUTBOX_EVENT_TABLE_NAME, CUSTOM_OUTBOX_EVENT_TABLE_NAME );\n\n\t\tVALID_AGENT_EVENT_MAPPING = OutboxPollingAgentAdditionalJaxbMappingProducer.ENTITY_DEFINITION\n\t\t\t\t.replace( ORIGINAL_AGENT_TABLE_NAME, CUSTOM_AGENT_TABLE_NAME );\n\n\t\tSQL_KEYS = new String[] {\n\t\t\t\tORIGINAL_OUTBOX_EVENT_TABLE_NAME, CUSTOM_OUTBOX_EVENT_TABLE_NAME,\n\t\t\t\tORIGINAL_AGENT_TABLE_NAME, CUSTOM_AGENT_TABLE_NAME,\n\t\t\t\tCUSTOM_SCHEMA,\n\t\t};\n\t}\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock();\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock )\n\t\t\t.coordinationStrategy( CoordinationStrategyExpectations.outboxPolling() );\n\n\tprivate SessionFactory sessionFactory;\n\n\tprivate final FilteringOutboxEventFinder outboxEventFinder = new FilteringOutboxEventFinder();\n\n\t@Test\n\tpublic void wrongOutboxEventMapping() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outboxevent.entity.mapping\", \"<entity-mappings><ciao></ciao></entity-mappings>\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( MappingException.class )\n\t\t\t\t.hasMessageContainingAll( \"Unable to perform unmarshalling\", \"unexpected element\" );\n\t}\n\n\t@Test\n\tpublic void wrongAgentMapping() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.agent.entity.mapping\", \"<entity-mappings><ciao></ciao></entity-mappings>\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( MappingException.class )\n\t\t\t\t.hasMessageContainingAll( \"Unable to perform unmarshalling\", \"unexpected element\" );\n\t}\n\n\t@Test\n\tpublic void validOutboxEventMapping() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outboxevent.entity.mapping\", VALID_OUTBOX_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isZero();\n\t}\n\n\t@Test\n\tpublic void validAgentMapping() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.agent.entity.mapping\", VALID_AGENT_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void conflictingAgentMappingConfiguration() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.agent.entity.mapping\", VALID_AGENT_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", \"break_it_all\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Outbox polling agent configuration property conflict.\" );\n\t}\n\n\t@Test\n\tpublic void conflictingOutboxeventMappingConfiguration() {\n\t\tassertThatThrownBy( () -> ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outboxevent.entity.mapping\", VALID_OUTBOX_EVENT_MAPPING )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", \"break_it_all\" )\n\t\t\t\t.setup( IndexedEntity.class ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Outbox event configuration property conflict.\" );\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomNames() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", CUSTOM_AGENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", CUSTOM_OUTBOX_EVENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomNamesAndSchema() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.schema\", CUSTOM_SCHEMA )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", CUSTOM_AGENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.schema\", CUSTOM_SCHEMA )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", CUSTOM_OUTBOX_EVENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassumeTrue( \"This test only makes sense if the database supports schemas\",\n\t\t\t\tgetNameQualifierSupport().supportsSchemas() );\n\t\tassumeTrue( \"This test only makes sense if the dialect supports creating schemas\",\n\t\t\t\tgetDialect().canCreateSchema() );\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tassertEventUUIDVersion( session, 4 );\n\t\t\t\tassertAgentUUIDVersion( session, 4 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\n\t\tassertThat( statementInspector.countByKey( CUSTOM_SCHEMA ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomUuidGenerator() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.uuid_gen_strategy\", \"time\" )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy\", \"time\" )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tassertEventUUIDVersion( session, 1 );\n\t\t\t\tassertAgentUUIDVersion( session, 1 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomUuidDataType() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.uuid_jdbc_type\", \"uuid-char\" )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.uuid_jdbc_type\", \"uuid-char\" )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tassertEventUUIDVersion( session, 4 );\n\t\t\t\tassertAgentUUIDVersion( session, 4 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isPositive();\n\t}\n\n\t@Test\n\tpublic void validMappingWithCustomFailingUuidGenerator() {\n\t\tassertThatThrownBy(\n\t\t\t\t() -> ormSetupHelper.start()\n\t\t\t\t\t\t.withProperty(\n\t\t\t\t\t\t\t\t\"hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy\",\n\t\t\t\t\t\t\t\t\"something-incompatible\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.setup( IndexedEntity.class )\n\t\t).isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\"Invalid value for configuration property 'hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy'\",\n\t\t\t\t\t\t\"something-incompatible\",\n\t\t\t\t\t\t\"Valid names are: [auto, random, time]\"\n\t\t\t\t);\n\t}\n\n\tprivate void assertEventUUIDVersion(Session session, int expectedVersion) {\n\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\tassertThat( events )\n\t\t\t\t.hasSize( 1 )\n\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t.extracting( UUID::version )\n\t\t\t\t.containsOnly( expectedVersion );\n\t}\n\n\tprivate void assertAgentUUIDVersion(Session session, int expectedVersion) {\n\t\tassertThat(\n\t\t\t\tsession.createQuery(\n\t\t\t\t\t\t\t\t\"select a from \" + OutboxPollingAgentAdditionalJaxbMappingProducer.ENTITY_NAME + \" a \",\n\t\t\t\t\t\t\t\tAgent.class\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.getResultList()\n\t\t).hasSizeGreaterThan( 0 )\n\t\t\t\t.extracting( Agent::getId )\n\t\t\t\t.extracting( UUID::version )\n\t\t\t\t.containsOnly( expectedVersion );\n\t}\n\n\tprivate Dialect getDialect() {\n\t\treturn sessionFactory.unwrap( SessionFactoryImplementor.class ).getJdbcServices()\n\t\t\t\t.getJdbcEnvironment().getDialect();\n\t}\n\n\tprivate NameQualifierSupport getNameQualifierSupport() {\n\t\treturn sessionFactory.unwrap( SessionFactoryImplementor.class ).getJdbcServices()\n\t\t\t\t.getJdbcEnvironment().getNameQualifierSupport();\n\t}\n\n\t@Entity(name = IndexedEntity.INDEX)\n\t@Indexed(index = IndexedEntity.INDEX)\n\tpublic static class IndexedEntity {\n\t\tstatic final String INDEX = \"IndexedEntity\";\n\n\t\t@Id\n\t\tprivate Integer id;\n\n\t\t@Basic\n\t\t@GenericField\n\t\tprivate String indexedField;\n\n\t\tpublic IndexedEntity() {\n\t\t}\n\n\t\tpublic IndexedEntity(Integer id, String indexedField) {\n\t\t\tthis.id = id;\n\t\t\tthis.indexedField = indexedField;\n\t\t}\n\n\t\tpublic Integer getId() {\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\tpublic String getIndexedField() {\n\t\t\treturn indexedField;\n\t\t}\n\n\t\tpublic void setIndexedField(String indexedField) {\n\t\t\tthis.indexedField = indexedField;\n\t\t}\n\t}\n\n\tpublic static class KeysStatementInspector implements StatementInspector {\n\n\t\tprivate Map<String, List<String>> sqlByKey = new HashMap<>();\n\n\t\tpublic KeysStatementInspector() {\n\t\t\tfor ( String key : SQL_KEYS ) {\n\t\t\t\tsqlByKey.put( key, new ArrayList<>() );\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic String inspect(String sql) {\n\t\t\tfor ( String key : SQL_KEYS ) {\n\t\t\t\tif ( Arrays.stream( sql.split( \"[^A-Za-z0-9_-]\" ) ).anyMatch( token -> key.equals( token ) ) ) {\n\t\t\t\t\tsqlByKey.get( key ).add( sql );\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn sql;\n\t\t}\n\n\t\tpublic int countByKey(String key) {\n\t\t\treturn sqlByKey.get( key ).size();\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private void assertEventUUIDVersion(Session session, int expectedVersion) {\n\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\tassertThat( events )\n\t\t\t\t.hasSize( 1 )\n\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t.extracting( UUID::version )\n\t\t\t\t.containsOnly( expectedVersion );\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.IndexedEntity#getId\n methodBody: public Integer getId() {\nreturn id;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT#getDialect\n methodBody: private Dialect getDialect() {\nreturn sessionFactory.unwrap(SessionFactoryImplementor.class).getJdbcServices().getJdbcEnvironment().getDialect();\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.IndexedEntity#setIndexedField\n methodBody: public void setIndexedField(String indexedField) {\nthis.indexedField=indexedField;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.KeysStatementInspector#countByKey\n methodBody: public int countByKey(String key) {\nreturn sqlByKey.get(key).size();\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT.IndexedEntity#setId\n methodBody: public void setId(Integer id) {\nthis.id=id;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.OutboxPollingCustomEntityMappingIT#getNameQualifierSupport\n methodBody: private NameQualifierSupport getNameQualifierSupport() {\nreturn sessionFactory.unwrap(SessionFactoryImplementor.class).getJdbcServices().getJdbcEnvironment().getNameQualifierSupport();\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void validMappingWithCustomNamesAndSchema() {\n\t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n\n\t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n\t\tsessionFactory = ormSetupHelper.start()\n\t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n\t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n\t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.schema\", CUSTOM_SCHEMA )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", CUSTOM_AGENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.schema\", CUSTOM_SCHEMA )\n\t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", CUSTOM_OUTBOX_EVENT_TABLE_NAME )\n\t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n\t\t\t\t.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassumeTrue( \"This test only makes sense if the database supports schemas\",\n\t\t\t\tgetNameQualifierSupport().supportsSchemas() );\n\t\tassumeTrue( \"This test only makes sense if the dialect supports creating schemas\",\n\t\t\t\tgetDialect().canCreateSchema() );\n\n\t\tint id = 1;\n\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.setId( id );\n\t\t\tentity.setIndexedField( \"value for the field\" );\n\t\t\tsession.persist( entity );\n\n\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n\t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n\t\t} );\n\t\tawait().untilAsserted( () -> {\n\t\t\twith( sessionFactory ).runInTransaction( session -> {\n\t\t\t\t// check that correct UUIDs are generated by asserting the version:\n\t\t\t\tassertEventUUIDVersion( session, 4 );\n\t\t\t\tassertAgentUUIDVersion( session, 4 );\n\t\t\t} );\n\t\t} );\n\t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n\t\t// Make them visible to Hibernate Search now.\n\t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n\t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n\t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n\t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n\n\t\tassertThat( statementInspector.countByKey( CUSTOM_SCHEMA ) ).isPositive();\n\t}\nprivate void assertEventUUIDVersion(Session session, int expectedVersion) {\n\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n\t\tassertThat( events )\n\t\t\t\t.hasSize( 1 )\n\t\t\t\t.extracting( OutboxEvent::getId )\n\t\t\t\t.extracting( UUID::version )\n\t\t\t\t.containsOnly( expectedVersion );\n\t}",
        "diffSourceCode": "-  212: \t@Test\n-  213: \tpublic void validMappingWithCustomNamesAndSchema() {\n-  214: \t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n-  215: \n-  216: \t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n-  217: \t\tsessionFactory = ormSetupHelper.start()\n-  218: \t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n-  219: \t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n-  220: \t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n-  221: \t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.schema\", CUSTOM_SCHEMA )\n-  222: \t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", CUSTOM_AGENT_TABLE_NAME )\n-  223: \t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.schema\", CUSTOM_SCHEMA )\n-  224: \t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", CUSTOM_OUTBOX_EVENT_TABLE_NAME )\n-  225: \t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n-  226: \t\t\t\t.setup( IndexedEntity.class );\n-  227: \t\tbackendMock.verifyExpectationsMet();\n-  228: \n-  229: \t\tassumeTrue( \"This test only makes sense if the database supports schemas\",\n-  230: \t\t\t\tgetNameQualifierSupport().supportsSchemas() );\n-  231: \t\tassumeTrue( \"This test only makes sense if the dialect supports creating schemas\",\n-  232: \t\t\t\tgetDialect().canCreateSchema() );\n-  233: \n-  234: \t\tint id = 1;\n-  235: \t\twith( sessionFactory ).runInTransaction( session -> {\n-  236: \t\t\tIndexedEntity entity = new IndexedEntity();\n-  237: \t\t\tentity.setId( id );\n-  238: \t\t\tentity.setIndexedField( \"value for the field\" );\n-  239: \t\t\tsession.persist( entity );\n-  240: \n-  241: \t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n-  242: \t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n-  243: \t\t} );\n-  244: \t\tawait().untilAsserted( () -> {\n-  245: \t\t\twith( sessionFactory ).runInTransaction( session -> {\n-  246: \t\t\t\t// check that correct UUIDs are generated by asserting the version:\n-  247: \t\t\t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n-  248: \t\t\t\tassertThat( events )\n-  249: \t\t\t\t\t\t.hasSize( 1 )\n-  250: \t\t\t\t\t\t.extracting( OutboxEvent::getId )\n-  251: \t\t\t\t\t\t.extracting( UUID::version )\n-  252: \t\t\t\t\t\t.contains( 4 );\n-  253: \t\t\t} );\n-  254: \t\t} );\n-  255: \t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n-  256: \t\t// Make them visible to Hibernate Search now.\n-  257: \t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n-  258: \t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n+  212: \t}\n+  213: \n+  214: \t@Test\n+  215: \tpublic void validMappingWithCustomNamesAndSchema() {\n+  216: \t\tKeysStatementInspector statementInspector = new KeysStatementInspector();\n+  217: \n+  218: \t\tbackendMock.expectAnySchema( IndexedEntity.INDEX );\n+  219: \t\tsessionFactory = ormSetupHelper.start()\n+  220: \t\t\t\t.withProperty( \"hibernate.search.coordination.outbox_event_finder.provider\", outboxEventFinder.provider() )\n+  221: \t\t\t\t// Allow ORM to create schema as we want to use non-default for this testcase:\n+  222: \t\t\t\t.withProperty( \"javax.persistence.create-database-schemas\", true )\n+  223: \t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.schema\", CUSTOM_SCHEMA )\n+  224: \t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.agent.table\", CUSTOM_AGENT_TABLE_NAME )\n+  225: \t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.schema\", CUSTOM_SCHEMA )\n+  226: \t\t\t\t.withProperty( \"hibernate.search.coordination.entity.mapping.outboxevent.table\", CUSTOM_OUTBOX_EVENT_TABLE_NAME )\n+  227: \t\t\t\t.withProperty( \"hibernate.session_factory.statement_inspector\", statementInspector )\n+  228: \t\t\t\t.setup( IndexedEntity.class );\n+  229: \t\tbackendMock.verifyExpectationsMet();\n+  230: \n+  231: \t\tassumeTrue( \"This test only makes sense if the database supports schemas\",\n+  232: \t\t\t\tgetNameQualifierSupport().supportsSchemas() );\n+  233: \t\tassumeTrue( \"This test only makes sense if the dialect supports creating schemas\",\n+  234: \t\t\t\tgetDialect().canCreateSchema() );\n+  235: \n+  236: \t\tint id = 1;\n+  237: \t\twith( sessionFactory ).runInTransaction( session -> {\n+  238: \t\t\tIndexedEntity entity = new IndexedEntity();\n+  239: \t\t\tentity.setId( id );\n+  240: \t\t\tentity.setIndexedField( \"value for the field\" );\n+  241: \t\t\tsession.persist( entity );\n+  242: \n+  243: \t\t\tbackendMock.expectWorks( IndexedEntity.INDEX )\n+  244: \t\t\t\t\t.add( \"1\", f -> f.field( \"indexedField\", \"value for the field\" ) );\n+  245: \t\t} );\n+  246: \t\tawait().untilAsserted( () -> {\n+  247: \t\t\twith( sessionFactory ).runInTransaction( session -> {\n+  248: \t\t\t\t// check that correct UUIDs are generated by asserting the version:\n+  249: \t\t\t\tassertEventUUIDVersion( session, 4 );\n+  250: \t\t\t\tassertAgentUUIDVersion( session, 4 );\n+  251: \t\t\t} );\n+  252: \t\t} );\n+  253: \t\t// The events were hidden until now, to ensure they were not processed in separate batches.\n+  254: \t\t// Make them visible to Hibernate Search now.\n+  255: \t\toutboxEventFinder.showAllEventsUpToNow( sessionFactory );\n+  256: \t\toutboxEventFinder.awaitUntilNoMoreVisibleEvents( sessionFactory );\n+  257: \n+  258: \t\tbackendMock.verifyExpectationsMet();\n   259: \n-  260: \t\tbackendMock.verifyExpectationsMet();\n-  261: \n-  262: \t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n-  263: \t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n-  264: \t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n-  265: \t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n-  266: \n-  267: \t\tassertThat( statementInspector.countByKey( CUSTOM_SCHEMA ) ).isPositive();\n-  268: \t}\n-  373: \t\t).isInstanceOf( SearchException.class )\n-  374: \t\t\t\t.hasMessageContainingAll(\n-  375: \t\t\t\t\t\t\"Invalid value for configuration property 'hibernate.search.coordination.entity.mapping.outboxevent.uuid_gen_strategy'\",\n-  376: \t\t\t\t\t\t\"something-incompatible\",\n-  377: \t\t\t\t\t\t\"Valid names are: [auto, random, time]\"\n-  378: \t\t\t\t);\n-  379: \t}\n-  380: \n+  260: \t\tassertThat( statementInspector.countByKey( ORIGINAL_OUTBOX_EVENT_TABLE_NAME ) ).isZero();\n+  261: \t\tassertThat( statementInspector.countByKey( CUSTOM_OUTBOX_EVENT_TABLE_NAME ) ).isPositive();\n+  262: \t\tassertThat( statementInspector.countByKey( ORIGINAL_AGENT_TABLE_NAME ) ).isZero();\n+  263: \t\tassertThat( statementInspector.countByKey( CUSTOM_AGENT_TABLE_NAME ) ).isPositive();\n+  264: \n+  265: \t\tassertThat( statementInspector.countByKey( CUSTOM_SCHEMA ) ).isPositive();\n+  266: \t}\n+  267: \n+  268: \t@Test\n+  373: \tprivate void assertEventUUIDVersion(Session session, int expectedVersion) {\n+  374: \t\tList<OutboxEvent> events = outboxEventFinder.findOutboxEventsNoFilter( session );\n+  375: \t\tassertThat( events )\n+  376: \t\t\t\t.hasSize( 1 )\n+  377: \t\t\t\t.extracting( OutboxEvent::getId )\n+  378: \t\t\t\t.extracting( UUID::version )\n+  379: \t\t\t\t.containsOnly( expectedVersion );\n+  380: \t}\n",
        "uniqueId": "fe9d94f5875ba28017d584653e397cffe57d7555_212_268_373_380_214_266",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Move Method",
        "description": "Move Method\tpublic mappingContainsExpectedEntities() : void from class org.hibernate.search.documentation.mapper.pojo.standalone.entrypoints.StandalonePojoConfigurerIT to public mappingContainsExpectedEntities() : void from class org.hibernate.search.documentation.mapper.pojo.standalone.entrypoints.StandalonePojoEntryPointsIT",
        "diffLocations": [
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/mapper/pojo/standalone/entrypoints/StandalonePojoConfigurerIT.java",
                "startLine": 60,
                "endLine": 66,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/mapper/pojo/standalone/entrypoints/StandalonePojoEntryPointsIT.java",
                "startLine": 72,
                "endLine": 78,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void mappingContainsExpectedEntities() {\n\t\tassertThat( theSearchMapping.allIndexedEntities() )\n\t\t\t\t.extracting( SearchIndexedEntity::name )\n\t\t\t\t.contains( \"Book\", \"Associate\", \"Manager\" )\n\t\t;\n\t}",
        "filePathBefore": "documentation/src/test/java/org/hibernate/search/documentation/mapper/pojo/standalone/entrypoints/StandalonePojoConfigurerIT.java",
        "isPureRefactoring": true,
        "commitId": "2b1681e3aa7954b7bc13ca4a75761f30d915b03a",
        "packageNameBefore": "org.hibernate.search.documentation.mapper.pojo.standalone.entrypoints",
        "classNameBefore": "org.hibernate.search.documentation.mapper.pojo.standalone.entrypoints.StandalonePojoConfigurerIT",
        "methodNameBefore": "org.hibernate.search.documentation.mapper.pojo.standalone.entrypoints.StandalonePojoConfigurerIT#mappingContainsExpectedEntities",
        "classSignatureBefore": "public class StandalonePojoConfigurerIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.documentation.mapper.pojo.standalone.entrypoints.StandalonePojoConfigurerIT#mappingContainsExpectedEntities"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.documentation.mapper.pojo.standalone.entrypoints.StandalonePojoConfigurerIT"
        ],
        "classSignatureBeforeSet": [
            "public class StandalonePojoConfigurerIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.documentation.mapper.pojo.standalone.entrypoints;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport org.hibernate.search.documentation.testsupport.BackendConfigurations;\nimport org.hibernate.search.documentation.testsupport.TestConfiguration;\nimport org.hibernate.search.mapper.pojo.standalone.entity.SearchIndexedEntity;\nimport org.hibernate.search.mapper.pojo.standalone.mapping.CloseableSearchMapping;\nimport org.hibernate.search.mapper.pojo.standalone.mapping.SearchMapping;\nimport org.hibernate.search.util.impl.integrationtest.common.TestConfigurationProvider;\n\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\n\npublic class StandalonePojoConfigurerIT {\n\n\tprivate CloseableSearchMapping theSearchMapping;\n\n\t@Rule\n\tpublic TestConfigurationProvider configurationProvider = new TestConfigurationProvider();\n\n\t@Before\n\tpublic void setup() {\n\t\t// tag::setup[]\n\t\tCloseableSearchMapping searchMapping = SearchMapping.builder() // <1>\n\t\t\t\t.property(\n\t\t\t\t\t\t\"hibernate.search.mapping.configurer\", // <2>\n\t\t\t\t\t\t\"class:org.hibernate.search.documentation.mapper.pojo.standalone.entrypoints.StandalonePojoConfigurer\"\n\t\t\t\t)\n\t\t\t\t.property(\n\t\t\t\t\t\t\"hibernate.search.backend.hosts\", // <3>\n\t\t\t\t\t\t\"elasticsearch.mycompany.com\"\n\t\t\t\t).property( \"hibernate.search.mapping.multi_tenancy.enabled\", true )\n\t\t\t\t// end::setup[]\n\t\t\t\t.properties( TestConfiguration.standalonePojoMapperProperties(\n\t\t\t\t\t\tconfigurationProvider,\n\t\t\t\t\t\tBackendConfigurations.simple()\n\t\t\t\t) )\n\t\t\t\t// tag::setup[]\n\t\t\t\t.build(); // <4>\n\t\t// end::setup[]\n\t\tthis.theSearchMapping = searchMapping;\n\t}\n\n\t@After\n\tpublic void cleanup() {\n\t\tif ( theSearchMapping != null ) {\n\t\t\ttheSearchMapping.close();\n\t\t}\n\t}\n\n\t@Test\n\tpublic void mappingContainsExpectedEntities() {\n\t\tassertThat( theSearchMapping.allIndexedEntities() )\n\t\t\t\t.extracting( SearchIndexedEntity::name )\n\t\t\t\t.contains( \"Book\", \"Associate\", \"Manager\" )\n\t\t;\n\t}\n}\n",
        "filePathAfter": "documentation/src/test/java/org/hibernate/search/documentation/mapper/pojo/standalone/entrypoints/StandalonePojoEntryPointsIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.documentation.mapper.pojo.standalone.entrypoints;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport java.util.Arrays;\n\nimport org.hibernate.search.documentation.testsupport.BackendConfigurations;\nimport org.hibernate.search.documentation.testsupport.TestConfiguration;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.mapper.pojo.standalone.entity.SearchIndexedEntity;\nimport org.hibernate.search.mapper.pojo.standalone.mapping.CloseableSearchMapping;\nimport org.hibernate.search.mapper.pojo.standalone.mapping.SearchMapping;\nimport org.hibernate.search.mapper.pojo.standalone.scope.SearchScope;\nimport org.hibernate.search.mapper.pojo.standalone.session.SearchSession;\nimport org.hibernate.search.util.impl.integrationtest.common.TestConfigurationProvider;\n\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\n\npublic class StandalonePojoEntryPointsIT {\n\n\tprivate CloseableSearchMapping theSearchMapping;\n\n\t@Rule\n\tpublic TestConfigurationProvider configurationProvider = new TestConfigurationProvider();\n\n\t@Before\n\tpublic void setup() {\n\t\t// tag::setup[]\n\t\tCloseableSearchMapping searchMapping = SearchMapping.builder() // <1>\n\t\t\t\t.property(\n\t\t\t\t\t\t\"hibernate.search.mapping.configurer\", // <2>\n\t\t\t\t\t\t\"class:org.hibernate.search.documentation.mapper.pojo.standalone.entrypoints.StandalonePojoConfigurer\"\n\t\t\t\t)\n\t\t\t\t.property(\n\t\t\t\t\t\t\"hibernate.search.backend.hosts\", // <3>\n\t\t\t\t\t\t\"elasticsearch.mycompany.com\"\n\t\t\t\t)\n\t\t\t\t// end::setup[]\n\t\t\t\t.properties( TestConfiguration.standalonePojoMapperProperties(\n\t\t\t\t\t\tconfigurationProvider,\n\t\t\t\t\t\tBackendConfigurations.simple()\n\t\t\t\t) )\n\t\t\t\t// tag::setup[]\n\t\t\t\t.build(); // <4>\n\t\t// end::setup[]\n\t\tthis.theSearchMapping = searchMapping;\n\t}\n\n\t@After\n\tpublic void cleanup() {\n\t\tif ( theSearchMapping != null ) {\n\t\t\t// tag::shutdown[]\n\t\t\tCloseableSearchMapping searchMapping = /* ... */ // <1>\n\t\t\t\t\t// end::shutdown[]\n\t\t\t\t\ttheSearchMapping;\n\t\t\t// tag::shutdown[]\n\t\t\tsearchMapping.close(); // <2>\n\t\t\t// end::shutdown[]\n\t\t}\n\t}\n\n\t@Test\n\tpublic void mappingContainsExpectedEntities() {\n\t\tassertThat( theSearchMapping.allIndexedEntities() )\n\t\t\t\t.extracting( SearchIndexedEntity::name )\n\t\t\t\t.contains( \"Book\", \"Associate\", \"Manager\" )\n\t\t;\n\t}\n\n\t@Test\n\tpublic void searchSession() {\n\t\t// tag::searchSession-simple[]\n\t\tSearchMapping searchMapping = /* ... */ // <1>\n\t\t\t\t// end::searchSession-simple[]\n\t\t\t\ttheSearchMapping;\n\t\t// tag::searchSession-simple[]\n\t\ttry ( SearchSession searchSession = searchMapping.createSession() ) { // <2>\n\t\t\t// ...\n\t\t\t// end::searchSession-simple[]\n\t\t\tassertThat( searchSession ).isNotNull();\n\t\t\tassertThat( searchSession.isOpen() ).isTrue();\n\t\t\t// tag::searchSession-simple[]\n\t\t}\n\t\t// end::searchSession-simple[]\n\t}\n\n\t@Test\n\tpublic void searchSession_withOptions() {\n\t\t// tag::searchSession-withOptions[]\n\t\tSearchMapping searchMapping = /* ... */ // <1>\n\t\t\t\t// end::searchSession-withOptions[]\n\t\t\t\ttheSearchMapping;\n\t\t// tag::searchSession-withOptions[]\n\t\ttry ( SearchSession searchSession = searchMapping.createSessionWithOptions() // <2>\n\t\t\t\t.commitStrategy( DocumentCommitStrategy.FORCE ) // <3>\n\t\t\t\t.refreshStrategy( DocumentRefreshStrategy.FORCE )\n\t\t\t\t.tenantId( \"myTenant\" )\n\t\t\t\t.build() ) { // <4>\n\t\t\t// ...\n\t\t\t// end::searchSession-withOptions[]\n\t\t\tassertThat( searchSession ).isNotNull();\n\t\t\tassertThat( searchSession.isOpen() ).isTrue();\n\t\t\t// tag::searchSession-withOptions[]\n\t\t}\n\t\t// end::searchSession-withOptions[]\n\t}\n\n\t@Test\n\tpublic void searchScope_fromSearchMapping() {\n\t\tSearchMapping searchMapping = theSearchMapping;\n\t\t// tag::searchScope-fromSearchMapping[]\n\t\tSearchScope<Book> bookScope = searchMapping.scope( Book.class );\n\t\tSearchScope<Person> associateAndManagerScope = searchMapping.scope( Arrays.asList( Associate.class, Manager.class ) );\n\t\tSearchScope<Person> personScope = searchMapping.scope( Person.class );\n\t\tSearchScope<Object> allScope = searchMapping.scope( Object.class );\n\t\t// end::searchScope-fromSearchMapping[]\n\t\tassertThat( bookScope.includedTypes() )\n\t\t\t\t.extracting( SearchIndexedEntity::name )\n\t\t\t\t.containsExactlyInAnyOrder( \"Book\" );\n\t\tassertThat( associateAndManagerScope.includedTypes() )\n\t\t\t\t.extracting( SearchIndexedEntity::name )\n\t\t\t\t.containsExactlyInAnyOrder( \"Manager\", \"Associate\" );\n\t\tassertThat( personScope.includedTypes() )\n\t\t\t\t.extracting( SearchIndexedEntity::name )\n\t\t\t\t.containsExactlyInAnyOrder( \"Manager\", \"Associate\" );\n\t\tassertThat( allScope.includedTypes() )\n\t\t\t\t.extracting( SearchIndexedEntity::name )\n\t\t\t\t.containsExactlyInAnyOrder( \"Book\", \"Manager\", \"Associate\" );\n\t}\n\n\t@Test\n\tpublic void searchScope_fromSearchSession() {\n\t\tSearchMapping searchMapping = theSearchMapping;\n\t\ttry ( SearchSession searchSession = searchMapping.createSession() ) {\n\t\t\tSearchScope<Book> bookScope = searchSession.scope( Book.class );\n\t\t\tSearchScope<Person> associateAndManagerScope = searchSession.scope( Arrays.asList( Associate.class, Manager.class ) );\n\t\t\tSearchScope<Person> personScope = searchSession.scope( Person.class );\n\t\t\tSearchScope<Object> allScope = searchSession.scope( Object.class );\n\t\t\t// end::searchScope-fromSearchSession[]\n\t\t\tassertThat( bookScope.includedTypes() )\n\t\t\t\t\t.extracting( SearchIndexedEntity::name )\n\t\t\t\t\t.containsExactlyInAnyOrder( \"Book\" );\n\t\t\tassertThat( associateAndManagerScope.includedTypes() )\n\t\t\t\t\t.extracting( SearchIndexedEntity::name )\n\t\t\t\t\t.containsExactlyInAnyOrder( \"Manager\", \"Associate\" );\n\t\t\tassertThat( personScope.includedTypes() )\n\t\t\t\t\t.extracting( SearchIndexedEntity::name )\n\t\t\t\t\t.containsExactlyInAnyOrder( \"Manager\", \"Associate\" );\n\t\t\tassertThat( allScope.includedTypes() )\n\t\t\t\t\t.extracting( SearchIndexedEntity::name )\n\t\t\t\t\t.containsExactlyInAnyOrder( \"Book\", \"Manager\", \"Associate\" );\n\t\t}\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void mappingContainsExpectedEntities() {\n\t\tassertThat( theSearchMapping.allIndexedEntities() )\n\t\t\t\t.extracting( SearchIndexedEntity::name )\n\t\t\t\t.contains( \"Book\", \"Associate\", \"Manager\" )\n\t\t;\n\t}",
        "diffSourceCode": "-   60: \t@Test\n-   61: \tpublic void mappingContainsExpectedEntities() {\n-   62: \t\tassertThat( theSearchMapping.allIndexedEntities() )\n-   63: \t\t\t\t.extracting( SearchIndexedEntity::name )\n-   64: \t\t\t\t.contains( \"Book\", \"Associate\", \"Manager\" )\n-   65: \t\t;\n-   66: \t}\n+   60: \tpublic void cleanup() {\n+   61: \t\tif ( theSearchMapping != null ) {\n+   62: \t\t\t// tag::shutdown[]\n+   63: \t\t\tCloseableSearchMapping searchMapping = /* ... */ // <1>\n+   64: \t\t\t\t\t// end::shutdown[]\n+   65: \t\t\t\t\ttheSearchMapping;\n+   66: \t\t\t// tag::shutdown[]\n+   72: \t@Test\n+   73: \tpublic void mappingContainsExpectedEntities() {\n+   74: \t\tassertThat( theSearchMapping.allIndexedEntities() )\n+   75: \t\t\t\t.extracting( SearchIndexedEntity::name )\n+   76: \t\t\t\t.contains( \"Book\", \"Associate\", \"Manager\" )\n+   77: \t\t;\n+   78: \t}\n",
        "uniqueId": "2b1681e3aa7954b7bc13ca4a75761f30d915b03a_60_66__72_78",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tpublic testParamsForBothAnnotationsAndProgrammatic(backendSetupStrategy BackendSetupStrategy, defaultBackendConfiguration BackendConfiguration, additionalAnnotatedClasses Set<Class<?>>, programmaticMappingContributor Consumer<ProgrammaticMappingConfigurationContext>) : List<DocumentationSetupHelper> extracted from public testParamsForBothAnnotationsAndProgrammatic(backendConfiguration BackendConfiguration, programmaticMappingContributor Consumer<ProgrammaticMappingConfigurationContext>) : List<DocumentationSetupHelper> in class org.hibernate.search.documentation.testsupport.DocumentationSetupHelper",
        "diffLocations": [
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
                "startLine": 32,
                "endLine": 43,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
                "startLine": 69,
                "endLine": 87,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
                "startLine": 69,
                "endLine": 87,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendConfiguration backendConfiguration,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\tHibernateOrmSearchMappingConfigurer mappingConfigurer =\n\t\t\t\tcontext -> programmaticMappingContributor.accept( context.programmaticMapping() );\n\t\tList<DocumentationSetupHelper> result = new ArrayList<>();\n\t\t// Annotation-based mapping\n\t\tresult.add( withSingleBackend( backendConfiguration, null ) );\n\t\t// Programmatic mapping\n\t\tresult.add( withSingleBackend( backendConfiguration, mappingConfigurer ) );\n\t\treturn result;\n\t}",
        "filePathBefore": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
        "isPureRefactoring": true,
        "commitId": "8eae7b49e26c97b229d1ae2942bcfddb466c6ed2",
        "packageNameBefore": "org.hibernate.search.documentation.testsupport",
        "classNameBefore": "org.hibernate.search.documentation.testsupport.DocumentationSetupHelper",
        "methodNameBefore": "org.hibernate.search.documentation.testsupport.DocumentationSetupHelper#testParamsForBothAnnotationsAndProgrammatic",
        "invokedMethod": "methodSignature: org.hibernate.search.documentation.testsupport.DocumentationSetupHelper#withSingleBackend\n methodBody: public static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration,\n\t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\nreturn new DocumentationSetupHelper(BackendSetupStrategy.withSingleBackend(backendConfiguration),backendConfiguration,mappingConfigurerOrNull);\n}",
        "classSignatureBefore": "public final class DocumentationSetupHelper\n\t\textends MappingSetupHelper<DocumentationSetupHelper.SetupContext, SimpleSessionFactoryBuilder, SessionFactory> ",
        "methodNameBeforeSet": [
            "org.hibernate.search.documentation.testsupport.DocumentationSetupHelper#testParamsForBothAnnotationsAndProgrammatic"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.documentation.testsupport.DocumentationSetupHelper"
        ],
        "classSignatureBeforeSet": [
            "public final class DocumentationSetupHelper\n\t\textends MappingSetupHelper<DocumentationSetupHelper.SetupContext, SimpleSessionFactoryBuilder, SessionFactory> "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Overlapped refactoring - can be identical by undoing the overlapped refactoring\n- Rename Variable- Severe changes",
                "description": "Mapped statements in other refactorings - with non-mapped leaves",
                "mappingState": 2
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.documentation.testsupport;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.LinkedHashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.function.Consumer;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.mapper.orm.automaticindexing.session.AutomaticIndexingSynchronizationStrategyNames;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.mapping.HibernateOrmSearchMappingConfigurer;\nimport org.hibernate.search.mapper.orm.schema.management.SchemaManagementStrategyName;\nimport org.hibernate.search.mapper.pojo.mapping.definition.programmatic.ProgrammaticMappingConfigurationContext;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendConfiguration;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendSetupStrategy;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.BackendMappingHandle;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.MappingSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.HibernateOrmMappingHandle;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.SimpleSessionFactoryBuilder;\n\npublic final class DocumentationSetupHelper\n\t\textends MappingSetupHelper<DocumentationSetupHelper.SetupContext, SimpleSessionFactoryBuilder, SessionFactory> {\n\n\tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendConfiguration backendConfiguration,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\tHibernateOrmSearchMappingConfigurer mappingConfigurer =\n\t\t\t\tcontext -> programmaticMappingContributor.accept( context.programmaticMapping() );\n\t\tList<DocumentationSetupHelper> result = new ArrayList<>();\n\t\t// Annotation-based mapping\n\t\tresult.add( withSingleBackend( backendConfiguration, null ) );\n\t\t// Programmatic mapping\n\t\tresult.add( withSingleBackend( backendConfiguration, mappingConfigurer ) );\n\t\treturn result;\n\t}\n\n\tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n\t\t\t\tbackendConfiguration,\n\t\t\t\tnull\n\t\t);\n\t}\n\n\tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration,\n\t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n\t\t\t\tbackendConfiguration,\n\t\t\t\tmappingConfigurerOrNull\n\t\t);\n\t}\n\n\tpublic static DocumentationSetupHelper withMultipleBackends(BackendConfiguration defaultBackendConfiguration,\n\t\t\tMap<String, BackendConfiguration> namedBackendConfigurations,\n\t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withMultipleBackends( defaultBackendConfiguration, namedBackendConfigurations ),\n\t\t\t\tdefaultBackendConfiguration,\n\t\t\t\tmappingConfigurerOrNull\n\t\t);\n\t}\n\n\tprivate final BackendConfiguration defaultBackendConfiguration;\n\n\tprivate final HibernateOrmSearchMappingConfigurer mappingConfigurerOrNull;\n\n\tprivate DocumentationSetupHelper(BackendSetupStrategy backendSetupStrategy,\n\t\t\tBackendConfiguration defaultBackendConfiguration,\n\t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n\t\tsuper( backendSetupStrategy );\n\t\tthis.defaultBackendConfiguration = defaultBackendConfiguration;\n\t\tthis.mappingConfigurerOrNull = mappingConfigurerOrNull;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn defaultBackendConfiguration.toString()\n\t\t\t\t+ (mappingConfigurerOrNull != null ? \" - programmatic mapping\" : \"\");\n\t}\n\n\t@Override\n\tprotected SetupContext createSetupContext() {\n\t\treturn new SetupContext( mappingConfigurerOrNull );\n\t}\n\n\t@Override\n\tprotected void close(SessionFactory toClose) {\n\t\ttoClose.close();\n\t}\n\n\tpublic final class SetupContext\n\t\t\textends MappingSetupHelper<SetupContext, SimpleSessionFactoryBuilder, SessionFactory>.AbstractSetupContext {\n\n\t\t// Use a LinkedHashMap for deterministic iteration\n\t\tprivate final Map<String, Object> overriddenProperties = new LinkedHashMap<>();\n\n\t\tSetupContext(HibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n\t\t\t// Real backend => ensure we clean up everything before and after the tests\n\t\t\twithProperty( HibernateOrmMapperSettings.SCHEMA_MANAGEMENT_STRATEGY,\n\t\t\t\t\tSchemaManagementStrategyName.DROP_AND_CREATE_AND_DROP );\n\t\t\t// Override the automatic indexing synchronization strategy according to our needs for testing\n\t\t\twithProperty( HibernateOrmMapperSettings.AUTOMATIC_INDEXING_SYNCHRONIZATION_STRATEGY,\n\t\t\t\t\tAutomaticIndexingSynchronizationStrategyNames.SYNC );\n\t\t\t// Set up programmatic mapping if necessary\n\t\t\tif ( mappingConfigurerOrNull != null ) {\n\t\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_PROCESS_ANNOTATIONS, false );\n\t\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_CONFIGURER, mappingConfigurerOrNull );\n\t\t\t}\n\t\t\t// Ensure we don't build Jandex indexes needlessly:\n\t\t\t// discovery based on Jandex ought to be tested in real projects that don't use this setup helper.\n\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_BUILD_MISSING_DISCOVERED_JANDEX_INDEXES, false );\n\t\t\t// Ensure overridden properties will be applied\n\t\t\twithConfiguration( builder -> overriddenProperties.forEach( builder::setProperty ) );\n\t\t}\n\n\t\t@Override\n\t\tpublic SetupContext withProperty(String key, Object value) {\n\t\t\toverriddenProperties.put( key, value );\n\t\t\treturn thisAsC();\n\t\t}\n\n\t\tpublic SessionFactory setup(Class<?> ... annotatedTypes) {\n\t\t\treturn withConfiguration( builder -> builder.addAnnotatedClasses( Arrays.asList( annotatedTypes ) ) )\n\t\t\t\t\t.setup();\n\t\t}\n\n\t\t@Override\n\t\tprotected SimpleSessionFactoryBuilder createBuilder() {\n\t\t\treturn new SimpleSessionFactoryBuilder();\n\t\t}\n\n\t\t@Override\n\t\tprotected SessionFactory build(SimpleSessionFactoryBuilder builder) {\n\t\t\treturn builder.build();\n\t\t}\n\n\t\t@Override\n\t\tprotected BackendMappingHandle toBackendMappingHandle(SessionFactory result) {\n\t\t\treturn new HibernateOrmMappingHandle( result );\n\t\t}\n\n\t\t@Override\n\t\tprotected SetupContext thisAsC() {\n\t\t\treturn this;\n\t\t}\n\t}\n\n}",
        "filePathAfter": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.documentation.testsupport;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.LinkedHashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.function.Consumer;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.mapper.orm.automaticindexing.session.AutomaticIndexingSynchronizationStrategyNames;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.mapping.HibernateOrmSearchMappingConfigurer;\nimport org.hibernate.search.mapper.orm.schema.management.SchemaManagementStrategyName;\nimport org.hibernate.search.mapper.pojo.mapping.definition.programmatic.ProgrammaticMappingConfigurationContext;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendConfiguration;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendSetupStrategy;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.MappingSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.BackendMappingHandle;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.HibernateOrmMappingHandle;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.SimpleSessionFactoryBuilder;\n\npublic final class DocumentationSetupHelper\n\t\textends MappingSetupHelper<DocumentationSetupHelper.SetupContext, SimpleSessionFactoryBuilder, SessionFactory> {\n\n\tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendConfiguration backendConfiguration,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\treturn testParamsForBothAnnotationsAndProgrammatic( backendConfiguration,\n\t\t\t\tCollections.emptySet(), programmaticMappingContributor );\n\t}\n\n\tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendConfiguration backendConfiguration,\n\t\t\tSet<Class<?>> additionalAnnotatedClasses,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\treturn testParamsForBothAnnotationsAndProgrammatic(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ), backendConfiguration,\n\t\t\t\tadditionalAnnotatedClasses, programmaticMappingContributor );\n\t}\n\n\tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendConfiguration defaultBackendConfiguration,\n\t\t\tMap<String, BackendConfiguration> namedBackendConfigurations,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\treturn testParamsForBothAnnotationsAndProgrammatic( defaultBackendConfiguration, namedBackendConfigurations,\n\t\t\t\tCollections.emptySet(), programmaticMappingContributor );\n\t}\n\n\tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendConfiguration defaultBackendConfiguration,\n\t\t\tMap<String, BackendConfiguration> namedBackendConfigurations,\n\t\t\tSet<Class<?>> additionalAnnotatedClasses,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\treturn testParamsForBothAnnotationsAndProgrammatic(\n\t\t\t\tBackendSetupStrategy.withMultipleBackends( defaultBackendConfiguration, namedBackendConfigurations ),\n\t\t\t\tdefaultBackendConfiguration,\n\t\t\t\tadditionalAnnotatedClasses, programmaticMappingContributor );\n\t}\n\n\tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendSetupStrategy backendSetupStrategy,\n\t\t\tBackendConfiguration defaultBackendConfiguration,\n\t\t\tSet<Class<?>> additionalAnnotatedClasses,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\tList<DocumentationSetupHelper> result = new ArrayList<>();\n\t\t// Annotation-based mapping\n\t\tHibernateOrmSearchMappingConfigurer annotationMappingConfigurer =\n\t\t\t\tadditionalAnnotatedClasses.isEmpty() ? null\n\t\t\t\t\t\t: context -> context.annotationMapping().add( additionalAnnotatedClasses );\n\t\tresult.add( new DocumentationSetupHelper( backendSetupStrategy,\n\t\t\t\tnull, annotationMappingConfigurer ) );\n\t\t// Programmatic mapping\n\t\tHibernateOrmSearchMappingConfigurer programmaticMappingConfigurer =\n\t\t\t\tcontext -> programmaticMappingContributor.accept( context.programmaticMapping() );\n\t\tresult.add( new DocumentationSetupHelper( backendSetupStrategy,\n\t\t\t\tfalse, programmaticMappingConfigurer ) );\n\t\treturn result;\n\t}\n\n\tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n\t\t\t\tnull, null\n\t\t);\n\t}\n\n\tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration,\n\t\t\tBoolean annotationProcessingEnabled, HibernateOrmSearchMappingConfigurer defaultMappingConfigurer) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n\t\t\t\tannotationProcessingEnabled, defaultMappingConfigurer\n\t\t);\n\t}\n\n\tprivate final Boolean annotationProcessingEnabled;\n\n\tprivate final HibernateOrmSearchMappingConfigurer defaultMappingConfigurer;\n\n\tprivate DocumentationSetupHelper(BackendSetupStrategy backendSetupStrategy,\n\t\t\tBoolean annotationProcessingEnabled,\n\t\t\tHibernateOrmSearchMappingConfigurer defaultMappingConfigurer) {\n\t\tsuper( backendSetupStrategy );\n\t\tthis.annotationProcessingEnabled = annotationProcessingEnabled;\n\t\tthis.defaultMappingConfigurer = defaultMappingConfigurer;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn super.toString()\n\t\t\t\t+ ( annotationProcessingEnabled == Boolean.FALSE ? \" - programmatic mapping\" : \"\");\n\t}\n\n\t@Override\n\tprotected SetupContext createSetupContext() {\n\t\treturn new SetupContext( annotationProcessingEnabled, defaultMappingConfigurer );\n\t}\n\n\t@Override\n\tprotected void close(SessionFactory toClose) {\n\t\ttoClose.close();\n\t}\n\n\tpublic final class SetupContext\n\t\t\textends MappingSetupHelper<SetupContext, SimpleSessionFactoryBuilder, SessionFactory>.AbstractSetupContext {\n\n\t\t// Use a LinkedHashMap for deterministic iteration\n\t\tprivate final Map<String, Object> overriddenProperties = new LinkedHashMap<>();\n\n\t\tSetupContext(Boolean annotationProcessingEnabled, HibernateOrmSearchMappingConfigurer defaultMappingConfigurer) {\n\t\t\t// Real backend => ensure we clean up everything before and after the tests\n\t\t\twithProperty( HibernateOrmMapperSettings.SCHEMA_MANAGEMENT_STRATEGY,\n\t\t\t\t\tSchemaManagementStrategyName.DROP_AND_CREATE_AND_DROP );\n\t\t\t// Override the automatic indexing synchronization strategy according to our needs for testing\n\t\t\twithProperty( HibernateOrmMapperSettings.AUTOMATIC_INDEXING_SYNCHRONIZATION_STRATEGY,\n\t\t\t\t\tAutomaticIndexingSynchronizationStrategyNames.SYNC );\n\t\t\t// Set up default mapping if necessary\n\t\t\tif ( annotationProcessingEnabled != null ) {\n\t\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_PROCESS_ANNOTATIONS, annotationProcessingEnabled );\n\t\t\t}\n\t\t\tif ( defaultMappingConfigurer != null ) {\n\t\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_CONFIGURER, defaultMappingConfigurer );\n\t\t\t}\n\t\t\t// Ensure we don't build Jandex indexes needlessly:\n\t\t\t// discovery based on Jandex ought to be tested in real projects that don't use this setup helper.\n\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_BUILD_MISSING_DISCOVERED_JANDEX_INDEXES, false );\n\t\t\t// Ensure overridden properties will be applied\n\t\t\twithConfiguration( builder -> overriddenProperties.forEach( builder::setProperty ) );\n\t\t}\n\n\t\t@Override\n\t\tpublic SetupContext withProperty(String key, Object value) {\n\t\t\toverriddenProperties.put( key, value );\n\t\t\treturn thisAsC();\n\t\t}\n\n\t\tpublic SessionFactory setup(Class<?> ... annotatedTypes) {\n\t\t\treturn withConfiguration( builder -> builder.addAnnotatedClasses( Arrays.asList( annotatedTypes ) ) )\n\t\t\t\t\t.setup();\n\t\t}\n\n\t\t@Override\n\t\tprotected SimpleSessionFactoryBuilder createBuilder() {\n\t\t\treturn new SimpleSessionFactoryBuilder();\n\t\t}\n\n\t\t@Override\n\t\tprotected SessionFactory build(SimpleSessionFactoryBuilder builder) {\n\t\t\treturn builder.build();\n\t\t}\n\n\t\t@Override\n\t\tprotected BackendMappingHandle toBackendMappingHandle(SessionFactory result) {\n\t\t\treturn new HibernateOrmMappingHandle( result );\n\t\t}\n\n\t\t@Override\n\t\tprotected SetupContext thisAsC() {\n\t\t\treturn this;\n\t\t}\n\t}\n\n}",
        "diffSourceCodeSet": [
            "public static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendSetupStrategy backendSetupStrategy,\n\t\t\tBackendConfiguration defaultBackendConfiguration,\n\t\t\tSet<Class<?>> additionalAnnotatedClasses,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\tList<DocumentationSetupHelper> result = new ArrayList<>();\n\t\t// Annotation-based mapping\n\t\tHibernateOrmSearchMappingConfigurer annotationMappingConfigurer =\n\t\t\t\tadditionalAnnotatedClasses.isEmpty() ? null\n\t\t\t\t\t\t: context -> context.annotationMapping().add( additionalAnnotatedClasses );\n\t\tresult.add( new DocumentationSetupHelper( backendSetupStrategy,\n\t\t\t\tnull, annotationMappingConfigurer ) );\n\t\t// Programmatic mapping\n\t\tHibernateOrmSearchMappingConfigurer programmaticMappingConfigurer =\n\t\t\t\tcontext -> programmaticMappingContributor.accept( context.programmaticMapping() );\n\t\tresult.add( new DocumentationSetupHelper( backendSetupStrategy,\n\t\t\t\tfalse, programmaticMappingConfigurer ) );\n\t\treturn result;\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.documentation.testsupport.DocumentationSetupHelper#withSingleBackend\n methodBody: public static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration,\n\t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\nreturn new DocumentationSetupHelper(BackendSetupStrategy.withSingleBackend(backendConfiguration),backendConfiguration,mappingConfigurerOrNull);\n}"
        ],
        "sourceCodeAfterRefactoring": "public static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendSetupStrategy backendSetupStrategy,\n\t\t\tBackendConfiguration defaultBackendConfiguration,\n\t\t\tSet<Class<?>> additionalAnnotatedClasses,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\tList<DocumentationSetupHelper> result = new ArrayList<>();\n\t\t// Annotation-based mapping\n\t\tHibernateOrmSearchMappingConfigurer annotationMappingConfigurer =\n\t\t\t\tadditionalAnnotatedClasses.isEmpty() ? null\n\t\t\t\t\t\t: context -> context.annotationMapping().add( additionalAnnotatedClasses );\n\t\tresult.add( new DocumentationSetupHelper( backendSetupStrategy,\n\t\t\t\tnull, annotationMappingConfigurer ) );\n\t\t// Programmatic mapping\n\t\tHibernateOrmSearchMappingConfigurer programmaticMappingConfigurer =\n\t\t\t\tcontext -> programmaticMappingContributor.accept( context.programmaticMapping() );\n\t\tresult.add( new DocumentationSetupHelper( backendSetupStrategy,\n\t\t\t\tfalse, programmaticMappingConfigurer ) );\n\t\treturn result;\n\t}\npublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendSetupStrategy backendSetupStrategy,\n\t\t\tBackendConfiguration defaultBackendConfiguration,\n\t\t\tSet<Class<?>> additionalAnnotatedClasses,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\tList<DocumentationSetupHelper> result = new ArrayList<>();\n\t\t// Annotation-based mapping\n\t\tHibernateOrmSearchMappingConfigurer annotationMappingConfigurer =\n\t\t\t\tadditionalAnnotatedClasses.isEmpty() ? null\n\t\t\t\t\t\t: context -> context.annotationMapping().add( additionalAnnotatedClasses );\n\t\tresult.add( new DocumentationSetupHelper( backendSetupStrategy,\n\t\t\t\tnull, annotationMappingConfigurer ) );\n\t\t// Programmatic mapping\n\t\tHibernateOrmSearchMappingConfigurer programmaticMappingConfigurer =\n\t\t\t\tcontext -> programmaticMappingContributor.accept( context.programmaticMapping() );\n\t\tresult.add( new DocumentationSetupHelper( backendSetupStrategy,\n\t\t\t\tfalse, programmaticMappingConfigurer ) );\n\t\treturn result;\n\t}",
        "diffSourceCode": "-   32: \tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n-   33: \t\t\tBackendConfiguration backendConfiguration,\n-   34: \t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n-   35: \t\tHibernateOrmSearchMappingConfigurer mappingConfigurer =\n-   36: \t\t\t\tcontext -> programmaticMappingContributor.accept( context.programmaticMapping() );\n-   37: \t\tList<DocumentationSetupHelper> result = new ArrayList<>();\n-   38: \t\t// Annotation-based mapping\n-   39: \t\tresult.add( withSingleBackend( backendConfiguration, null ) );\n-   40: \t\t// Programmatic mapping\n-   41: \t\tresult.add( withSingleBackend( backendConfiguration, mappingConfigurer ) );\n-   42: \t\treturn result;\n-   43: \t}\n-   69: \t\t);\n-   70: \t}\n-   71: \n-   72: \tprivate final BackendConfiguration defaultBackendConfiguration;\n-   73: \n-   74: \tprivate final HibernateOrmSearchMappingConfigurer mappingConfigurerOrNull;\n-   75: \n-   76: \tprivate DocumentationSetupHelper(BackendSetupStrategy backendSetupStrategy,\n-   77: \t\t\tBackendConfiguration defaultBackendConfiguration,\n-   78: \t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n-   79: \t\tsuper( backendSetupStrategy );\n-   80: \t\tthis.defaultBackendConfiguration = defaultBackendConfiguration;\n-   81: \t\tthis.mappingConfigurerOrNull = mappingConfigurerOrNull;\n-   82: \t}\n-   83: \n-   84: \t@Override\n-   85: \tpublic String toString() {\n-   86: \t\treturn defaultBackendConfiguration.toString()\n-   87: \t\t\t\t+ (mappingConfigurerOrNull != null ? \" - programmatic mapping\" : \"\");\n+   32: \t\textends MappingSetupHelper<DocumentationSetupHelper.SetupContext, SimpleSessionFactoryBuilder, SessionFactory> {\n+   33: \n+   34: \tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n+   35: \t\t\tBackendConfiguration backendConfiguration,\n+   36: \t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n+   37: \t\treturn testParamsForBothAnnotationsAndProgrammatic( backendConfiguration,\n+   38: \t\t\t\tCollections.emptySet(), programmaticMappingContributor );\n+   39: \t}\n+   40: \n+   41: \tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n+   42: \t\t\tBackendConfiguration backendConfiguration,\n+   43: \t\t\tSet<Class<?>> additionalAnnotatedClasses,\n+   69: \tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n+   70: \t\t\tBackendSetupStrategy backendSetupStrategy,\n+   71: \t\t\tBackendConfiguration defaultBackendConfiguration,\n+   72: \t\t\tSet<Class<?>> additionalAnnotatedClasses,\n+   73: \t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n+   74: \t\tList<DocumentationSetupHelper> result = new ArrayList<>();\n+   75: \t\t// Annotation-based mapping\n+   76: \t\tHibernateOrmSearchMappingConfigurer annotationMappingConfigurer =\n+   77: \t\t\t\tadditionalAnnotatedClasses.isEmpty() ? null\n+   78: \t\t\t\t\t\t: context -> context.annotationMapping().add( additionalAnnotatedClasses );\n+   79: \t\tresult.add( new DocumentationSetupHelper( backendSetupStrategy,\n+   80: \t\t\t\tnull, annotationMappingConfigurer ) );\n+   81: \t\t// Programmatic mapping\n+   82: \t\tHibernateOrmSearchMappingConfigurer programmaticMappingConfigurer =\n+   83: \t\t\t\tcontext -> programmaticMappingContributor.accept( context.programmaticMapping() );\n+   84: \t\tresult.add( new DocumentationSetupHelper( backendSetupStrategy,\n+   85: \t\t\t\tfalse, programmaticMappingConfigurer ) );\n+   86: \t\treturn result;\n+   87: \t}\n",
        "uniqueId": "8eae7b49e26c97b229d1ae2942bcfddb466c6ed2_32_43_69_87_69_87",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate testSynchronous(sessionFactory SessionFactory, customStrategy AutomaticIndexingSynchronizationStrategy, expectedRefreshStrategy DocumentRefreshStrategy) : void extracted from private testSynchronous(sessionFactory SessionFactory, expectedRefreshStrategy DocumentRefreshStrategy) : void in class org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/automaticindexing/AutomaticIndexingSynchronizationStrategyIT.java",
                "startLine": 68,
                "endLine": 96,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/automaticindexing/AutomaticIndexingSynchronizationStrategyIT.java",
                "startLine": 135,
                "endLine": 139,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/automaticindexing/AutomaticIndexingSynchronizationStrategyIT.java",
                "startLine": 141,
                "endLine": 171,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void testSynchronous(SessionFactory sessionFactory,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\texpectedRefreshStrategy,\n\t\t\t\tworkFuture,\n\t\t\t\t/*\n\t\t\t\t * With this synchronization strategy, the transaction may NOT unblock the thread\n\t\t\t\t * until the work future is complete.\n\t\t\t\t */\n\t\t\t\t() -> assertThat( workFuture ).isSuccessful()\n\t\t);\n\n\t\t// We expect the transaction to block forever, because the work future isn't complete\n\t\tALMOST_FOREVER_UNIT.sleep( ALMOST_FOREVER_VALUE );\n\t\tassertThat( transactionFuture ).isPending();\n\n\t\t// Completing the work should allow the transaction to unblock the thread\n\t\tworkFuture.complete( null );\n\t\t/*\n\t\t * Note that this will throw an ExecutionException it the transaction failed\n\t\t * or an assertion failed in the other thread.\n\t\t */\n\t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\t\tassertThat( transactionFuture ).isSuccessful();\n\t}",
        "filePathBefore": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/automaticindexing/AutomaticIndexingSynchronizationStrategyIT.java",
        "isPureRefactoring": true,
        "commitId": "f465d75f6bb1794abc32ce53371f49da4af13926",
        "packageNameBefore": "org.hibernate.search.integrationtest.mapper.orm.automaticindexing",
        "classNameBefore": "org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT#testSynchronous",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT#runTransactionInDifferentThread\n methodBody: private CompletableFuture<?> runTransactionInDifferentThread(SessionFactory sessionFactory,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy,\n\t\t\tCompletableFuture<?> workFuture,\n\t\t\tRunnable afterTransactionAssertion)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\nCompletableFuture<?> justBeforeTransactionCommitFuture=new CompletableFuture<>();\nCompletableFuture<?> transactionFuture=CompletableFuture.runAsync(() -> {\n  OrmUtils.withinTransaction(sessionFactory,session -> {\n    IndexedEntity entity1=new IndexedEntity();\n    entity1.setId(1);\n    entity1.setIndexedField(\"initialValue\");\n    session.persist(entity1);\n    backendMock.expectWorks(IndexedEntity.INDEX,expectedRefreshStrategy).add(\"1\",b -> b.field(\"indexedField\",entity1.getIndexedField())).preparedThenExecuted(workFuture);\n    justBeforeTransactionCommitFuture.complete(null);\n  }\n);\n  backendMock.verifyExpectationsMet();\n  afterTransactionAssertion.run();\n}\n);\njustBeforeTransactionCommitFuture.get(ALMOST_FOREVER_VALUE,ALMOST_FOREVER_UNIT);\nreturn transactionFuture;\n}",
        "classSignatureBefore": "public class AutomaticIndexingSynchronizationStrategyIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT#testSynchronous"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT"
        ],
        "classSignatureBeforeSet": [
            "public class AutomaticIndexingSynchronizationStrategyIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.automaticindexing;\n\nimport static org.hibernate.search.util.impl.test.FutureAssert.assertThat;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport javax.persistence.Basic;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.index.DocumentRefreshStrategy;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmAutomaticIndexingSynchronizationStrategyName;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.orm.OrmUtils;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\npublic class AutomaticIndexingSynchronizationStrategyIT {\n\n\t// Let's say 3 seconds are long enough to consider that, if nothing changed after this time, nothing ever will.\n\tprivate static final long ALMOST_FOREVER_VALUE = 3L;\n\tprivate static final TimeUnit ALMOST_FOREVER_UNIT = TimeUnit.SECONDS;\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = new OrmSetupHelper();\n\n\t@Test\n\tpublic void queued() throws InterruptedException, ExecutionException, TimeoutException {\n\t\tSessionFactory sessionFactory = setup( HibernateOrmAutomaticIndexingSynchronizationStrategyName.QUEUED );\n\t\ttestAsynchronous( sessionFactory, DocumentRefreshStrategy.NONE );\n\t}\n\n\t@Test\n\tpublic void committed_default() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( null );\n\t\ttestSynchronous( sessionFactory, DocumentRefreshStrategy.NONE );\n\t}\n\n\t@Test\n\tpublic void committed_explicit() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( HibernateOrmAutomaticIndexingSynchronizationStrategyName.COMMITTED );\n\t\ttestSynchronous( sessionFactory, DocumentRefreshStrategy.NONE );\n\t}\n\n\t@Test\n\tpublic void searchable() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( HibernateOrmAutomaticIndexingSynchronizationStrategyName.SEARCHABLE );\n\t\ttestSynchronous( sessionFactory, DocumentRefreshStrategy.FORCE );\n\t}\n\n\tprivate void testSynchronous(SessionFactory sessionFactory,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\texpectedRefreshStrategy,\n\t\t\t\tworkFuture,\n\t\t\t\t/*\n\t\t\t\t * With this synchronization strategy, the transaction may NOT unblock the thread\n\t\t\t\t * until the work future is complete.\n\t\t\t\t */\n\t\t\t\t() -> assertThat( workFuture ).isSuccessful()\n\t\t);\n\n\t\t// We expect the transaction to block forever, because the work future isn't complete\n\t\tALMOST_FOREVER_UNIT.sleep( ALMOST_FOREVER_VALUE );\n\t\tassertThat( transactionFuture ).isPending();\n\n\t\t// Completing the work should allow the transaction to unblock the thread\n\t\tworkFuture.complete( null );\n\t\t/*\n\t\t * Note that this will throw an ExecutionException it the transaction failed\n\t\t * or an assertion failed in the other thread.\n\t\t */\n\t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\t\tassertThat( transactionFuture ).isSuccessful();\n\t}\n\n\tprivate void testAsynchronous(SessionFactory sessionFactory,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\texpectedRefreshStrategy,\n\t\t\t\tworkFuture,\n\t\t\t\t/*\n\t\t\t\t * With this synchronization strategy, the transaction will unblock the thread\n\t\t\t\t * before the work future is complete.\n\t\t\t\t */\n\t\t\t\t() -> assertThat( workFuture ).isPending()\n\t\t);\n\n\t\t/*\n\t\t * We didn't complete the work, but the transaction should unblock the thread anyway.\n\t\t * Note that this will throw an ExecutionException it the transaction failed\n\t\t * or an assertion failed in the other thread.\n\t\t */\n\t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\t}\n\n\t/*\n\t * Run a transaction in a different thread so that its progress can be inspected from the current thread.\n\t */\n\tprivate CompletableFuture<?> runTransactionInDifferentThread(SessionFactory sessionFactory,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy,\n\t\t\tCompletableFuture<?> workFuture,\n\t\t\tRunnable afterTransactionAssertion)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> justBeforeTransactionCommitFuture = new CompletableFuture<>();\n\t\tCompletableFuture<?> transactionFuture = CompletableFuture.runAsync( () -> {\n\t\t\tOrmUtils.withinTransaction( sessionFactory, session -> {\n\t\t\t\tIndexedEntity entity1 = new IndexedEntity();\n\t\t\t\tentity1.setId( 1 );\n\t\t\t\tentity1.setIndexedField( \"initialValue\" );\n\n\t\t\t\tsession.persist( entity1 );\n\n\t\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX, expectedRefreshStrategy )\n\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t.field( \"indexedField\", entity1.getIndexedField() )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.preparedThenExecuted( workFuture );\n\t\t\t\tjustBeforeTransactionCommitFuture.complete( null );\n\t\t\t} );\n\t\t\tbackendMock.verifyExpectationsMet();\n\n\t\t\tafterTransactionAssertion.run();\n\t\t} );\n\n\t\t// Ensure the transaction at least reached the point just before commit\n\t\tjustBeforeTransactionCommitFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\n\t\treturn transactionFuture;\n\t}\n\n\tprivate SessionFactory setup(HibernateOrmAutomaticIndexingSynchronizationStrategyName strategyName) {\n\t\tOrmSetupHelper.SetupContext setupContext = ormSetupHelper.withBackendMock( backendMock );\n\t\tif ( strategyName != null ) {\n\t\t\tsetupContext.withProperty(\n\t\t\t\t\tHibernateOrmMapperSettings.AUTOMATIC_INDEXING_SYNCHRONIZATION_STRATEGY,\n\t\t\t\t\tstrategyName\n\t\t\t);\n\t\t}\n\n\t\tbackendMock.expectSchema( IndexedEntity.INDEX, b -> b\n\t\t\t\t.field( \"indexedField\", String.class )\n\t\t);\n\t\tSessionFactory sessionFactory = setupContext.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\treturn sessionFactory;\n\t}\n\n\t@Entity(name = \"indexed\")\n\t@Indexed(index = IndexedEntity.INDEX)\n\tpublic static class IndexedEntity {\n\n\t\tstatic final String INDEX = \"IndexedEntity\";\n\n\t\t@Id\n\t\tprivate Integer id;\n\n\t\t@Basic\n\t\t@GenericField\n\t\tprivate String indexedField;\n\n\t\tpublic Integer getId() {\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\tpublic String getIndexedField() {\n\t\t\treturn indexedField;\n\t\t}\n\n\t\tpublic void setIndexedField(String indexedField) {\n\t\t\tthis.indexedField = indexedField;\n\t\t}\n\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/automaticindexing/AutomaticIndexingSynchronizationStrategyIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.automaticindexing;\n\nimport static org.hibernate.search.util.impl.test.FutureAssert.assertThat;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicReference;\nimport javax.persistence.Basic;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.index.DocumentRefreshStrategy;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmAutomaticIndexingSynchronizationStrategyName;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.session.AutomaticIndexingSynchronizationStrategy;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.orm.OrmUtils;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.assertj.core.api.Assertions;\n\npublic class AutomaticIndexingSynchronizationStrategyIT {\n\n\t// Let's say 3 seconds are long enough to consider that, if nothing changed after this time, nothing ever will.\n\tprivate static final long ALMOST_FOREVER_VALUE = 3L;\n\tprivate static final TimeUnit ALMOST_FOREVER_UNIT = TimeUnit.SECONDS;\n\n\tprivate static final long SMALL_DURATION_VALUE = 100L;\n\tprivate static final TimeUnit SMALL_DURATION_UNIT = TimeUnit.MILLISECONDS;\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = new OrmSetupHelper();\n\n\t@Test\n\tpublic void queued() throws InterruptedException, ExecutionException, TimeoutException {\n\t\tSessionFactory sessionFactory = setup( HibernateOrmAutomaticIndexingSynchronizationStrategyName.QUEUED );\n\t\ttestAsynchronous( sessionFactory, DocumentRefreshStrategy.NONE );\n\t}\n\n\t@Test\n\tpublic void committed_default() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( null );\n\t\ttestSynchronous( sessionFactory, DocumentRefreshStrategy.NONE );\n\t}\n\n\t@Test\n\tpublic void committed_explicit() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( HibernateOrmAutomaticIndexingSynchronizationStrategyName.COMMITTED );\n\t\ttestSynchronous( sessionFactory, DocumentRefreshStrategy.NONE );\n\t}\n\n\t@Test\n\tpublic void searchable() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( HibernateOrmAutomaticIndexingSynchronizationStrategyName.SEARCHABLE );\n\t\ttestSynchronous( sessionFactory, DocumentRefreshStrategy.FORCE );\n\t}\n\n\t@Test\n\tpublic void override_committedToSearchable() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( HibernateOrmAutomaticIndexingSynchronizationStrategyName.COMMITTED );\n\t\ttestSynchronous( sessionFactory, AutomaticIndexingSynchronizationStrategy.searchable(), DocumentRefreshStrategy.FORCE );\n\t}\n\n\t@Test\n\tpublic void override_committedToCustom() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( HibernateOrmAutomaticIndexingSynchronizationStrategyName.COMMITTED );\n\t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n\n\t\tAtomicReference<CompletableFuture<?>> futurePushedToBackgroundServiceReference = new AtomicReference<>( null );\n\n\t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\tnew AutomaticIndexingSynchronizationStrategy() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic DocumentRefreshStrategy getDocumentRefreshStrategy() {\n\t\t\t\t\t\treturn DocumentRefreshStrategy.FORCE;\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void handleFuture(CompletableFuture<?> future) {\n\t\t\t\t\t\t// try to wait for the future to complete for a small duration...\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tfuture.get( SMALL_DURATION_VALUE, SMALL_DURATION_UNIT );\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcatch (TimeoutException e) {\n\t\t\t\t\t\t\t/*\n\t\t\t\t\t\t\t * If it takes too long, push the the completable future to some background service\n\t\t\t\t\t\t\t * to wait on it and report errors asynchronously if necessary.\n\t\t\t\t\t\t\t * Here we just simulate this by setting an AtomicReference.\n\t\t\t\t\t\t\t */\n\t\t\t\t\t\t\tfuturePushedToBackgroundServiceReference.set( future );\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcatch (InterruptedException | ExecutionException e) {\n\t\t\t\t\t\t\tAssertions.fail( \"Unexpected exception: \" + e, e );\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\tDocumentRefreshStrategy.FORCE,\n\t\t\t\tworkFuture,\n\t\t\t\t/*\n\t\t\t\t * With this synchronization strategy, the transaction will unblock the thread\n\t\t\t\t * before the work future is complete.\n\t\t\t\t */\n\t\t\t\t() -> assertThat( workFuture ).isPending()\n\t\t);\n\n\t\t/*\n\t\t * We didn't complete the work, but the transaction should unblock the thread anyway after some time.\n\t\t * Note that this will throw an ExecutionException it the transaction failed\n\t\t * or an assertion failed in the other thread.\n\t\t */\n\t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\t\t// The strategy should have timed out and it should have set the future on this reference\n\t\tAssertions.assertThat( futurePushedToBackgroundServiceReference ).isNotNull();\n\t}\n\n\tprivate void testSynchronous(SessionFactory sessionFactory,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\ttestSynchronous( sessionFactory, null, expectedRefreshStrategy );\n\t}\n\n\tprivate void testSynchronous(SessionFactory sessionFactory,\n\t\t\tAutomaticIndexingSynchronizationStrategy customStrategy,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\tcustomStrategy,\n\t\t\t\texpectedRefreshStrategy,\n\t\t\t\tworkFuture,\n\t\t\t\t/*\n\t\t\t\t * With this synchronization strategy, the transaction may NOT unblock the thread\n\t\t\t\t * until the work future is complete.\n\t\t\t\t */\n\t\t\t\t() -> assertThat( workFuture ).isSuccessful()\n\t\t);\n\n\t\t// We expect the transaction to block forever, because the work future isn't complete\n\t\tALMOST_FOREVER_UNIT.sleep( ALMOST_FOREVER_VALUE );\n\t\tassertThat( transactionFuture ).isPending();\n\n\t\t// Completing the work should allow the transaction to unblock the thread\n\t\tworkFuture.complete( null );\n\t\t/*\n\t\t * Note that this will throw an ExecutionException it the transaction failed\n\t\t * or an assertion failed in the other thread.\n\t\t */\n\t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\t\tassertThat( transactionFuture ).isSuccessful();\n\t}\n\n\tprivate void testAsynchronous(SessionFactory sessionFactory,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\tnull,\n\t\t\t\texpectedRefreshStrategy,\n\t\t\t\tworkFuture,\n\t\t\t\t/*\n\t\t\t\t * With this synchronization strategy, the transaction will unblock the thread\n\t\t\t\t * before the work future is complete.\n\t\t\t\t */\n\t\t\t\t() -> assertThat( workFuture ).isPending()\n\t\t);\n\n\t\t/*\n\t\t * We didn't complete the work, but the transaction should unblock the thread anyway.\n\t\t * Note that this will throw an ExecutionException it the transaction failed\n\t\t * or an assertion failed in the other thread.\n\t\t */\n\t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\t}\n\n\t/*\n\t * Run a transaction in a different thread so that its progress can be inspected from the current thread.\n\t */\n\tprivate CompletableFuture<?> runTransactionInDifferentThread(SessionFactory sessionFactory,\n\t\t\tAutomaticIndexingSynchronizationStrategy customStrategy,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy,\n\t\t\tCompletableFuture<?> workFuture,\n\t\t\tRunnable afterTransactionAssertion)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> justBeforeTransactionCommitFuture = new CompletableFuture<>();\n\t\tCompletableFuture<?> transactionFuture = CompletableFuture.runAsync( () -> {\n\t\t\tOrmUtils.withinTransaction( sessionFactory, session -> {\n\t\t\t\tif ( customStrategy != null ) {\n\t\t\t\t\tSearch.getSearchSession( session ).setAutomaticIndexingSynchronizationStrategy( customStrategy );\n\t\t\t\t}\n\t\t\t\tIndexedEntity entity1 = new IndexedEntity();\n\t\t\t\tentity1.setId( 1 );\n\t\t\t\tentity1.setIndexedField( \"initialValue\" );\n\n\t\t\t\tsession.persist( entity1 );\n\n\t\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX, expectedRefreshStrategy )\n\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t.field( \"indexedField\", entity1.getIndexedField() )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.preparedThenExecuted( workFuture );\n\t\t\t\tjustBeforeTransactionCommitFuture.complete( null );\n\t\t\t} );\n\t\t\tbackendMock.verifyExpectationsMet();\n\n\t\t\tafterTransactionAssertion.run();\n\t\t} );\n\n\t\t// Ensure the transaction at least reached the point just before commit\n\t\tjustBeforeTransactionCommitFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\n\t\treturn transactionFuture;\n\t}\n\n\tprivate SessionFactory setup(HibernateOrmAutomaticIndexingSynchronizationStrategyName strategyName) {\n\t\tOrmSetupHelper.SetupContext setupContext = ormSetupHelper.withBackendMock( backendMock );\n\t\tif ( strategyName != null ) {\n\t\t\tsetupContext.withProperty(\n\t\t\t\t\tHibernateOrmMapperSettings.AUTOMATIC_INDEXING_SYNCHRONIZATION_STRATEGY,\n\t\t\t\t\tstrategyName\n\t\t\t);\n\t\t}\n\n\t\tbackendMock.expectSchema( IndexedEntity.INDEX, b -> b\n\t\t\t\t.field( \"indexedField\", String.class )\n\t\t);\n\t\tSessionFactory sessionFactory = setupContext.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\treturn sessionFactory;\n\t}\n\n\t@Entity(name = \"indexed\")\n\t@Indexed(index = IndexedEntity.INDEX)\n\tpublic static class IndexedEntity {\n\n\t\tstatic final String INDEX = \"IndexedEntity\";\n\n\t\t@Id\n\t\tprivate Integer id;\n\n\t\t@Basic\n\t\t@GenericField\n\t\tprivate String indexedField;\n\n\t\tpublic Integer getId() {\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\tpublic String getIndexedField() {\n\t\t\treturn indexedField;\n\t\t}\n\n\t\tpublic void setIndexedField(String indexedField) {\n\t\t\tthis.indexedField = indexedField;\n\t\t}\n\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private void testSynchronous(SessionFactory sessionFactory,\n\t\t\tAutomaticIndexingSynchronizationStrategy customStrategy,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\tcustomStrategy,\n\t\t\t\texpectedRefreshStrategy,\n\t\t\t\tworkFuture,\n\t\t\t\t/*\n\t\t\t\t * With this synchronization strategy, the transaction may NOT unblock the thread\n\t\t\t\t * until the work future is complete.\n\t\t\t\t */\n\t\t\t\t() -> assertThat( workFuture ).isSuccessful()\n\t\t);\n\n\t\t// We expect the transaction to block forever, because the work future isn't complete\n\t\tALMOST_FOREVER_UNIT.sleep( ALMOST_FOREVER_VALUE );\n\t\tassertThat( transactionFuture ).isPending();\n\n\t\t// Completing the work should allow the transaction to unblock the thread\n\t\tworkFuture.complete( null );\n\t\t/*\n\t\t * Note that this will throw an ExecutionException it the transaction failed\n\t\t * or an assertion failed in the other thread.\n\t\t */\n\t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\t\tassertThat( transactionFuture ).isSuccessful();\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT#runTransactionInDifferentThread\n methodBody: private CompletableFuture<?> runTransactionInDifferentThread(SessionFactory sessionFactory,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy,\n\t\t\tCompletableFuture<?> workFuture,\n\t\t\tRunnable afterTransactionAssertion)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\nCompletableFuture<?> justBeforeTransactionCommitFuture=new CompletableFuture<>();\nCompletableFuture<?> transactionFuture=CompletableFuture.runAsync(() -> {\n  OrmUtils.withinTransaction(sessionFactory,session -> {\n    IndexedEntity entity1=new IndexedEntity();\n    entity1.setId(1);\n    entity1.setIndexedField(\"initialValue\");\n    session.persist(entity1);\n    backendMock.expectWorks(IndexedEntity.INDEX,expectedRefreshStrategy).add(\"1\",b -> b.field(\"indexedField\",entity1.getIndexedField())).preparedThenExecuted(workFuture);\n    justBeforeTransactionCommitFuture.complete(null);\n  }\n);\n  backendMock.verifyExpectationsMet();\n  afterTransactionAssertion.run();\n}\n);\njustBeforeTransactionCommitFuture.get(ALMOST_FOREVER_VALUE,ALMOST_FOREVER_UNIT);\nreturn transactionFuture;\n}"
        ],
        "sourceCodeAfterRefactoring": "private void testSynchronous(SessionFactory sessionFactory,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\ttestSynchronous( sessionFactory, null, expectedRefreshStrategy );\n\t}\nprivate void testSynchronous(SessionFactory sessionFactory,\n\t\t\tAutomaticIndexingSynchronizationStrategy customStrategy,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\tcustomStrategy,\n\t\t\t\texpectedRefreshStrategy,\n\t\t\t\tworkFuture,\n\t\t\t\t/*\n\t\t\t\t * With this synchronization strategy, the transaction may NOT unblock the thread\n\t\t\t\t * until the work future is complete.\n\t\t\t\t */\n\t\t\t\t() -> assertThat( workFuture ).isSuccessful()\n\t\t);\n\n\t\t// We expect the transaction to block forever, because the work future isn't complete\n\t\tALMOST_FOREVER_UNIT.sleep( ALMOST_FOREVER_VALUE );\n\t\tassertThat( transactionFuture ).isPending();\n\n\t\t// Completing the work should allow the transaction to unblock the thread\n\t\tworkFuture.complete( null );\n\t\t/*\n\t\t * Note that this will throw an ExecutionException it the transaction failed\n\t\t * or an assertion failed in the other thread.\n\t\t */\n\t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\t\tassertThat( transactionFuture ).isSuccessful();\n\t}",
        "diffSourceCode": "-   68: \tprivate void testSynchronous(SessionFactory sessionFactory,\n-   69: \t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n-   70: \t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n-   71: \t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n-   72: \n-   73: \t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n-   74: \t\t\t\tsessionFactory,\n-   75: \t\t\t\texpectedRefreshStrategy,\n-   76: \t\t\t\tworkFuture,\n-   77: \t\t\t\t/*\n-   78: \t\t\t\t * With this synchronization strategy, the transaction may NOT unblock the thread\n-   79: \t\t\t\t * until the work future is complete.\n-   80: \t\t\t\t */\n-   81: \t\t\t\t() -> assertThat( workFuture ).isSuccessful()\n-   82: \t\t);\n-   83: \n-   84: \t\t// We expect the transaction to block forever, because the work future isn't complete\n-   85: \t\tALMOST_FOREVER_UNIT.sleep( ALMOST_FOREVER_VALUE );\n-   86: \t\tassertThat( transactionFuture ).isPending();\n-   87: \n-   88: \t\t// Completing the work should allow the transaction to unblock the thread\n-   89: \t\tworkFuture.complete( null );\n-   90: \t\t/*\n-   91: \t\t * Note that this will throw an ExecutionException it the transaction failed\n-   92: \t\t * or an assertion failed in the other thread.\n-   93: \t\t */\n-   94: \t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n-   95: \t\tassertThat( transactionFuture ).isSuccessful();\n-   96: \t}\n-  135: \t\t\t\tentity1.setIndexedField( \"initialValue\" );\n-  136: \n-  137: \t\t\t\tsession.persist( entity1 );\n-  138: \n-  139: \t\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX, expectedRefreshStrategy )\n-  141: \t\t\t\t\t\t\t\t.field( \"indexedField\", entity1.getIndexedField() )\n-  142: \t\t\t\t\t\t)\n-  143: \t\t\t\t\t\t.preparedThenExecuted( workFuture );\n-  144: \t\t\t\tjustBeforeTransactionCommitFuture.complete( null );\n-  145: \t\t\t} );\n-  146: \t\t\tbackendMock.verifyExpectationsMet();\n-  147: \n-  148: \t\t\tafterTransactionAssertion.run();\n-  149: \t\t} );\n-  150: \n-  151: \t\t// Ensure the transaction at least reached the point just before commit\n-  152: \t\tjustBeforeTransactionCommitFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n-  153: \n-  154: \t\treturn transactionFuture;\n-  155: \t}\n-  156: \n-  157: \tprivate SessionFactory setup(HibernateOrmAutomaticIndexingSynchronizationStrategyName strategyName) {\n-  158: \t\tOrmSetupHelper.SetupContext setupContext = ormSetupHelper.withBackendMock( backendMock );\n-  159: \t\tif ( strategyName != null ) {\n-  160: \t\t\tsetupContext.withProperty(\n-  161: \t\t\t\t\tHibernateOrmMapperSettings.AUTOMATIC_INDEXING_SYNCHRONIZATION_STRATEGY,\n-  162: \t\t\t\t\tstrategyName\n-  163: \t\t\t);\n-  164: \t\t}\n-  165: \n-  166: \t\tbackendMock.expectSchema( IndexedEntity.INDEX, b -> b\n-  167: \t\t\t\t.field( \"indexedField\", String.class )\n-  168: \t\t);\n-  169: \t\tSessionFactory sessionFactory = setupContext.setup( IndexedEntity.class );\n-  170: \t\tbackendMock.verifyExpectationsMet();\n-  171: \n+   68: \t}\n+   69: \n+   70: \t@Test\n+   71: \tpublic void searchable() throws InterruptedException, TimeoutException, ExecutionException {\n+   72: \t\tSessionFactory sessionFactory = setup( HibernateOrmAutomaticIndexingSynchronizationStrategyName.SEARCHABLE );\n+   73: \t\ttestSynchronous( sessionFactory, DocumentRefreshStrategy.FORCE );\n+   74: \t}\n+   75: \n+   76: \t@Test\n+   77: \tpublic void override_committedToSearchable() throws InterruptedException, TimeoutException, ExecutionException {\n+   78: \t\tSessionFactory sessionFactory = setup( HibernateOrmAutomaticIndexingSynchronizationStrategyName.COMMITTED );\n+   79: \t\ttestSynchronous( sessionFactory, AutomaticIndexingSynchronizationStrategy.searchable(), DocumentRefreshStrategy.FORCE );\n+   80: \t}\n+   81: \n+   82: \t@Test\n+   83: \tpublic void override_committedToCustom() throws InterruptedException, TimeoutException, ExecutionException {\n+   84: \t\tSessionFactory sessionFactory = setup( HibernateOrmAutomaticIndexingSynchronizationStrategyName.COMMITTED );\n+   85: \t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n+   86: \n+   87: \t\tAtomicReference<CompletableFuture<?>> futurePushedToBackgroundServiceReference = new AtomicReference<>( null );\n+   88: \n+   89: \t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n+   90: \t\t\t\tsessionFactory,\n+   91: \t\t\t\tnew AutomaticIndexingSynchronizationStrategy() {\n+   92: \t\t\t\t\t@Override\n+   93: \t\t\t\t\tpublic DocumentRefreshStrategy getDocumentRefreshStrategy() {\n+   94: \t\t\t\t\t\treturn DocumentRefreshStrategy.FORCE;\n+   95: \t\t\t\t\t}\n+   96: \n+  135: \tprivate void testSynchronous(SessionFactory sessionFactory,\n+  136: \t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n+  137: \t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n+  138: \t\ttestSynchronous( sessionFactory, null, expectedRefreshStrategy );\n+  139: \t}\n+  141: \tprivate void testSynchronous(SessionFactory sessionFactory,\n+  142: \t\t\tAutomaticIndexingSynchronizationStrategy customStrategy,\n+  143: \t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n+  144: \t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n+  145: \t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n+  146: \n+  147: \t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n+  148: \t\t\t\tsessionFactory,\n+  149: \t\t\t\tcustomStrategy,\n+  150: \t\t\t\texpectedRefreshStrategy,\n+  151: \t\t\t\tworkFuture,\n+  152: \t\t\t\t/*\n+  153: \t\t\t\t * With this synchronization strategy, the transaction may NOT unblock the thread\n+  154: \t\t\t\t * until the work future is complete.\n+  155: \t\t\t\t */\n+  156: \t\t\t\t() -> assertThat( workFuture ).isSuccessful()\n+  157: \t\t);\n+  158: \n+  159: \t\t// We expect the transaction to block forever, because the work future isn't complete\n+  160: \t\tALMOST_FOREVER_UNIT.sleep( ALMOST_FOREVER_VALUE );\n+  161: \t\tassertThat( transactionFuture ).isPending();\n+  162: \n+  163: \t\t// Completing the work should allow the transaction to unblock the thread\n+  164: \t\tworkFuture.complete( null );\n+  165: \t\t/*\n+  166: \t\t * Note that this will throw an ExecutionException it the transaction failed\n+  167: \t\t * or an assertion failed in the other thread.\n+  168: \t\t */\n+  169: \t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n+  170: \t\tassertThat( transactionFuture ).isSuccessful();\n+  171: \t}\n",
        "uniqueId": "f465d75f6bb1794abc32ce53371f49da4af13926_68_96_141_171_135_139",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate getDefaultBackendName() : String extracted from private getBackendName(indexName String, indexPropertySource ConfigurationPropertySource) : String in class org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolder",
        "diffLocations": [
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/common/impl/IndexManagerBuildingStateHolder.java",
                "startLine": 92,
                "endLine": 111,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/common/impl/IndexManagerBuildingStateHolder.java",
                "startLine": 71,
                "endLine": 76,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/common/impl/IndexManagerBuildingStateHolder.java",
                "startLine": 78,
                "endLine": 89,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private String getBackendName(String indexName, ConfigurationPropertySource indexPropertySource) {\n\t\tOptional<String> backendNameOptional = INDEX_BACKEND_NAME.get( indexPropertySource );\n\t\tif ( backendNameOptional.isPresent() ) {\n\t\t\treturn backendNameOptional.get();\n\t\t}\n\t\telse {\n\t\t\tOptional<String> defaultBackendNameOptional = DEFAULT_INDEX_BACKEND_NAME.get( propertySource );\n\t\t\tif ( defaultBackendNameOptional.isPresent() ) {\n\t\t\t\treturn defaultBackendNameOptional.get();\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow log.indexBackendCannotBeNullOrEmpty(\n\t\t\t\t\t\tindexName,\n\t\t\t\t\t\tINDEX_BACKEND_NAME.resolveOrRaw( indexPropertySource ),\n\t\t\t\t\t\t// Retrieve the resolved *default* key (*.default_backend) from the global (non-index-scoped) source\n\t\t\t\t\t\tDEFAULT_INDEX_BACKEND_NAME.resolveOrRaw( propertySource )\n\t\t\t\t);\n\t\t\t}\n\t\t}\n\t}",
        "filePathBefore": "engine/src/main/java/org/hibernate/search/engine/common/impl/IndexManagerBuildingStateHolder.java",
        "isPureRefactoring": true,
        "commitId": "8163c142f60d58fb295123d6a2ba72b99c7f020f",
        "packageNameBefore": "org.hibernate.search.engine.common.impl",
        "classNameBefore": "org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolder",
        "methodNameBefore": "org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolder#getBackendName",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.logging.impl.Log#indexBackendCannotBeNullOrEmpty\n methodBody: SearchException indexBackendCannotBeNullOrEmpty(String indexName, String key, String defaultKey);",
        "classSignatureBefore": "class IndexManagerBuildingStateHolder ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolder#getBackendName"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolder"
        ],
        "classSignatureBeforeSet": [
            "class IndexManagerBuildingStateHolder "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Overlapped refactoring - can be identical by undoing the overlapped refactoring\n- Rename Method-",
                "description": "Rename Method Refactoring on the top of the extracted method - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.common.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.LinkedHashMap;\nimport java.util.Map;\nimport java.util.Optional;\n\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.model.dsl.spi.IndexSchemaRootNodeBuilder;\nimport org.hibernate.search.engine.cfg.BackendSettings;\nimport org.hibernate.search.engine.cfg.EngineSettings;\nimport org.hibernate.search.engine.cfg.IndexSettings;\nimport org.hibernate.search.engine.cfg.impl.EngineConfigurationUtils;\nimport org.hibernate.search.engine.cfg.spi.OptionalConfigurationProperty;\nimport org.hibernate.search.engine.environment.bean.BeanReference;\nimport org.hibernate.search.engine.environment.bean.BeanHolder;\nimport org.hibernate.search.engine.logging.impl.Log;\nimport org.hibernate.search.engine.mapper.mapping.building.spi.IndexedEntityBindingContext;\nimport org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexManagerBuilder;\nimport org.hibernate.search.engine.backend.index.spi.IndexManagerImplementor;\nimport org.hibernate.search.engine.backend.spi.BackendImplementor;\nimport org.hibernate.search.engine.backend.spi.BackendFactory;\nimport org.hibernate.search.engine.cfg.ConfigurationPropertySource;\nimport org.hibernate.search.engine.cfg.spi.ConfigurationProperty;\nimport org.hibernate.search.engine.environment.bean.BeanProvider;\nimport org.hibernate.search.engine.backend.spi.BackendBuildContext;\nimport org.hibernate.search.engine.mapper.mapping.building.impl.IndexedEntityBindingContextImpl;\nimport org.hibernate.search.engine.mapper.mapping.building.spi.IndexManagerBuildingState;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\nimport org.hibernate.search.util.common.impl.SuppressingCloser;\n\n\n/**\n * @author Yoann Rodiere\n */\nclass IndexManagerBuildingStateHolder {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate static final OptionalConfigurationProperty<String> DEFAULT_INDEX_BACKEND_NAME =\n\t\t\tConfigurationProperty.forKey( EngineSettings.DEFAULT_BACKEND ).asString().build();\n\n\tprivate static final OptionalConfigurationProperty<String> INDEX_BACKEND_NAME =\n\t\t\tConfigurationProperty.forKey( IndexSettings.BACKEND ).asString().build();\n\n\tprivate static final OptionalConfigurationProperty<BeanReference<? extends BackendFactory>> BACKEND_TYPE =\n\t\t\tConfigurationProperty.forKey( BackendSettings.TYPE ).asBeanReference( BackendFactory.class )\n\t\t\t\t\t.build();\n\n\tprivate final BeanProvider beanProvider;\n\tprivate final ConfigurationPropertySource propertySource;\n\tprivate final RootBuildContext rootBuildContext;\n\n\t// Use a LinkedHashMap for deterministic iteration\n\tprivate final Map<String, BackendInitialBuildState<?>> backendBuildStateByName = new LinkedHashMap<>();\n\t// Use a LinkedHashMap for deterministic iteration\n\tprivate final Map<String, IndexManagerInitialBuildState<?>> indexManagerBuildStateByName = new LinkedHashMap<>();\n\n\tIndexManagerBuildingStateHolder(BeanProvider beanProvider, ConfigurationPropertySource propertySource,\n\t\t\tRootBuildContext rootBuildContext) {\n\t\tthis.beanProvider = beanProvider;\n\t\tthis.propertySource = propertySource;\n\t\tthis.rootBuildContext = rootBuildContext;\n\t}\n\n\tpublic IndexManagerBuildingState<?> startBuilding(String indexName, boolean multiTenancyEnabled) {\n\t\tConfigurationPropertySource indexPropertySource =\n\t\t\t\tEngineConfigurationUtils.getIndexWithoutDefaults( propertySource, indexName );\n\t\tString backendName = getBackendName( indexName, indexPropertySource );\n\n\t\tBackendInitialBuildState<?> backendBuildingstate =\n\t\t\t\tbackendBuildStateByName.computeIfAbsent( backendName, this::createBackend );\n\n\t\tIndexManagerInitialBuildState<?> state = indexManagerBuildStateByName.get( indexName );\n\t\tif ( state == null ) {\n\t\t\tstate = backendBuildingstate.createIndexManagerBuildingState(\n\t\t\t\t\tindexName, multiTenancyEnabled, indexPropertySource\n\t\t\t);\n\t\t\tindexManagerBuildStateByName.put( indexName, state );\n\t\t}\n\t\treturn state;\n\t}\n\n\tprivate String getBackendName(String indexName, ConfigurationPropertySource indexPropertySource) {\n\t\tOptional<String> backendNameOptional = INDEX_BACKEND_NAME.get( indexPropertySource );\n\t\tif ( backendNameOptional.isPresent() ) {\n\t\t\treturn backendNameOptional.get();\n\t\t}\n\t\telse {\n\t\t\tOptional<String> defaultBackendNameOptional = DEFAULT_INDEX_BACKEND_NAME.get( propertySource );\n\t\t\tif ( defaultBackendNameOptional.isPresent() ) {\n\t\t\t\treturn defaultBackendNameOptional.get();\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow log.indexBackendCannotBeNullOrEmpty(\n\t\t\t\t\t\tindexName,\n\t\t\t\t\t\tINDEX_BACKEND_NAME.resolveOrRaw( indexPropertySource ),\n\t\t\t\t\t\t// Retrieve the resolved *default* key (*.default_backend) from the global (non-index-scoped) source\n\t\t\t\t\t\tDEFAULT_INDEX_BACKEND_NAME.resolveOrRaw( propertySource )\n\t\t\t\t);\n\t\t\t}\n\t\t}\n\t}\n\n\tMap<String, BackendPartialBuildState> getBackendPartialBuildStates() {\n\t\t// Use a LinkedHashMap for deterministic iteration\n\t\tMap<String, BackendPartialBuildState> backendsByName = new LinkedHashMap<>();\n\t\tfor ( Map.Entry<String, BackendInitialBuildState<?>> entry : backendBuildStateByName.entrySet() ) {\n\t\t\tbackendsByName.put( entry.getKey(), entry.getValue().getPartiallyBuilt() );\n\t\t}\n\t\treturn backendsByName;\n\t}\n\n\tMap<String, IndexManagerPartialBuildState> getIndexManagersByName() {\n\t\t// Use a LinkedHashMap for deterministic iteration\n\t\tMap<String, IndexManagerPartialBuildState> indexManagersByName = new LinkedHashMap<>();\n\t\tfor ( Map.Entry<String, IndexManagerInitialBuildState<?>> entry : indexManagerBuildStateByName.entrySet() ) {\n\t\t\tindexManagersByName.put( entry.getKey(), entry.getValue().getPartialBuildState() );\n\t\t}\n\t\treturn indexManagersByName;\n\t}\n\n\tvoid closeOnFailure(SuppressingCloser closer) {\n\t\tcloser.pushAll( state -> state.closeOnFailure( closer ), indexManagerBuildStateByName.values() );\n\t\tcloser.pushAll( BackendInitialBuildState::closeOnFailure, backendBuildStateByName.values() );\n\t}\n\n\tprivate BackendInitialBuildState<?> createBackend(String backendName) {\n\t\tConfigurationPropertySource backendPropertySource =\n\t\t\t\tEngineConfigurationUtils.getBackend( propertySource, backendName );\n\t\ttry ( BeanHolder<? extends BackendFactory> backendFactoryHolder =\n\t\t\t\tBACKEND_TYPE.getAndMapOrThrow(\n\t\t\t\t\t\tbackendPropertySource,\n\t\t\t\t\t\tbeanProvider::getBean,\n\t\t\t\t\t\tkey -> log.backendTypeCannotBeNullOrEmpty( backendName, key )\n\t\t\t\t) ) {\n\t\t\tBackendBuildContext backendBuildContext = new BackendBuildContextImpl( rootBuildContext );\n\n\t\t\tBackendImplementor<?> backend = backendFactoryHolder.get()\n\t\t\t\t\t.create( backendName, backendBuildContext, backendPropertySource );\n\t\t\treturn new BackendInitialBuildState<>( backendName, backendBuildContext, backendPropertySource, backend );\n\t\t}\n\t}\n\n\tprivate class BackendInitialBuildState<D extends DocumentElement> {\n\t\tprivate final String backendName;\n\t\tprivate final BackendBuildContext backendBuildContext;\n\t\tprivate final ConfigurationPropertySource defaultIndexPropertySource;\n\t\tprivate final BackendImplementor<D> backend;\n\n\t\tprivate BackendInitialBuildState(\n\t\t\t\tString backendName,\n\t\t\t\tBackendBuildContext backendBuildContext,\n\t\t\t\tConfigurationPropertySource backendPropertySource,\n\t\t\t\tBackendImplementor<D> backend) {\n\t\t\tthis.backendName = backendName;\n\t\t\tthis.backendBuildContext = backendBuildContext;\n\t\t\tthis.defaultIndexPropertySource =\n\t\t\t\t\tEngineConfigurationUtils.getIndexDefaults( backendPropertySource );\n\t\t\tthis.backend = backend;\n\t\t}\n\n\t\tIndexManagerInitialBuildState<D> createIndexManagerBuildingState(\n\t\t\t\tString indexName, boolean multiTenancyEnabled,\n\t\t\t\tConfigurationPropertySource indexPropertySource) {\n\t\t\tConfigurationPropertySource defaultedIndexPropertySource =\n\t\t\t\t\tEngineConfigurationUtils.addIndexDefaults( indexPropertySource, defaultIndexPropertySource );\n\t\t\tIndexManagerBuilder<D> builder = backend.createIndexManagerBuilder(\n\t\t\t\t\tindexName, multiTenancyEnabled, backendBuildContext, defaultedIndexPropertySource\n\t\t\t);\n\t\t\tIndexSchemaRootNodeBuilder schemaRootNodeBuilder = builder.getSchemaRootNodeBuilder();\n\t\t\tIndexedEntityBindingContext bindingContext = new IndexedEntityBindingContextImpl( schemaRootNodeBuilder );\n\t\t\treturn new IndexManagerInitialBuildState<>( backendName, indexName, builder, bindingContext );\n\t\t}\n\n\t\tvoid closeOnFailure() {\n\t\t\tbackend.close();\n\t\t}\n\n\t\tBackendPartialBuildState getPartiallyBuilt() {\n\t\t\treturn new BackendPartialBuildState( backendName, backend );\n\t\t}\n\t}\n\n\tprivate class IndexManagerInitialBuildState<D extends DocumentElement> implements IndexManagerBuildingState<D> {\n\n\t\tprivate final String backendName;\n\t\tprivate final String indexName;\n\t\tprivate final IndexManagerBuilder<D> builder;\n\t\tprivate final IndexedEntityBindingContext bindingContext;\n\n\t\tprivate IndexManagerImplementor<D> indexManager;\n\n\t\tIndexManagerInitialBuildState(String backendName, String indexName,\n\t\t\t\tIndexManagerBuilder<D> builder,\n\t\t\t\tIndexedEntityBindingContext bindingContext) {\n\t\t\tthis.backendName = backendName;\n\t\t\tthis.indexName = indexName;\n\t\t\tthis.builder = builder;\n\t\t\tthis.bindingContext = bindingContext;\n\t\t}\n\n\t\tvoid closeOnFailure(SuppressingCloser closer) {\n\t\t\tif ( indexManager != null ) {\n\t\t\t\tcloser.push( IndexManagerImplementor::close, indexManager );\n\t\t\t}\n\t\t\telse {\n\t\t\t\tcloser.push( IndexManagerBuilder::closeOnFailure, builder );\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic String getIndexName() {\n\t\t\treturn indexName;\n\t\t}\n\n\t\t@Override\n\t\tpublic IndexedEntityBindingContext getIndexedEntityBindingContext() {\n\t\t\treturn bindingContext;\n\t\t}\n\n\t\t@Override\n\t\tpublic MappedIndexManager<D> build() {\n\t\t\tif ( indexManager != null ) {\n\t\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\t\"Trying to build index manager \" + indexName + \" twice.\"\n\t\t\t\t\t\t+ \" There is probably a bug in the mapper implementation.\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tindexManager = builder.build();\n\t\t\treturn new MappedIndexManagerImpl<>( indexManager );\n\t\t}\n\n\t\tIndexManagerPartialBuildState getPartialBuildState() {\n\t\t\tif ( indexManager == null ) {\n\t\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\t\"Index manager \" + indexName + \" was not built by the mapper as expected.\"\n\t\t\t\t\t\t+ \" There is probably a bug in the mapper implementation.\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new IndexManagerPartialBuildState( backendName, indexName, indexManager );\n\t\t}\n\t}\n\n}\n",
        "filePathAfter": "engine/src/main/java/org/hibernate/search/engine/common/impl/IndexManagerBuildingStateHolder.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.common.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.LinkedHashMap;\nimport java.util.Map;\nimport java.util.Optional;\n\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.model.dsl.spi.IndexSchemaRootNodeBuilder;\nimport org.hibernate.search.engine.cfg.BackendSettings;\nimport org.hibernate.search.engine.cfg.EngineSettings;\nimport org.hibernate.search.engine.cfg.impl.EngineConfigurationUtils;\nimport org.hibernate.search.engine.cfg.spi.OptionalConfigurationProperty;\nimport org.hibernate.search.engine.environment.bean.BeanReference;\nimport org.hibernate.search.engine.environment.bean.BeanHolder;\nimport org.hibernate.search.engine.logging.impl.Log;\nimport org.hibernate.search.engine.mapper.mapping.building.spi.IndexedEntityBindingContext;\nimport org.hibernate.search.engine.mapper.mapping.spi.MappedIndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexManagerBuilder;\nimport org.hibernate.search.engine.backend.index.spi.IndexManagerImplementor;\nimport org.hibernate.search.engine.backend.spi.BackendImplementor;\nimport org.hibernate.search.engine.backend.spi.BackendFactory;\nimport org.hibernate.search.engine.cfg.ConfigurationPropertySource;\nimport org.hibernate.search.engine.cfg.spi.ConfigurationProperty;\nimport org.hibernate.search.engine.environment.bean.BeanProvider;\nimport org.hibernate.search.engine.backend.spi.BackendBuildContext;\nimport org.hibernate.search.engine.mapper.mapping.building.impl.IndexedEntityBindingContextImpl;\nimport org.hibernate.search.engine.mapper.mapping.building.spi.IndexManagerBuildingState;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.impl.StringHelper;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\nimport org.hibernate.search.util.common.impl.SuppressingCloser;\n\n\n/**\n * @author Yoann Rodiere\n */\nclass IndexManagerBuildingStateHolder {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate static final OptionalConfigurationProperty<String> DEFAULT_INDEX_BACKEND_NAME =\n\t\t\tConfigurationProperty.forKey( EngineSettings.DEFAULT_BACKEND ).asString().build();\n\n\tprivate static final OptionalConfigurationProperty<BeanReference<? extends BackendFactory>> BACKEND_TYPE =\n\t\t\tConfigurationProperty.forKey( BackendSettings.TYPE ).asBeanReference( BackendFactory.class )\n\t\t\t\t\t.build();\n\n\tprivate final BeanProvider beanProvider;\n\tprivate final ConfigurationPropertySource propertySource;\n\tprivate final RootBuildContext rootBuildContext;\n\n\t// Use a LinkedHashMap for deterministic iteration\n\tprivate final Map<String, BackendInitialBuildState<?>> backendBuildStateByName = new LinkedHashMap<>();\n\t// Use a LinkedHashMap for deterministic iteration\n\tprivate final Map<String, IndexManagerInitialBuildState<?>> indexManagerBuildStateByName = new LinkedHashMap<>();\n\n\tIndexManagerBuildingStateHolder(BeanProvider beanProvider, ConfigurationPropertySource propertySource,\n\t\t\tRootBuildContext rootBuildContext) {\n\t\tthis.beanProvider = beanProvider;\n\t\tthis.propertySource = propertySource;\n\t\tthis.rootBuildContext = rootBuildContext;\n\t}\n\n\tBackendInitialBuildState<?> getBackend(String backendName) {\n\t\tif ( StringHelper.isEmpty( backendName ) ) {\n\t\t\tbackendName = getDefaultBackendName();\n\t\t}\n\t\treturn backendBuildStateByName.computeIfAbsent( backendName, this::createBackend );\n\t}\n\n\tprivate String getDefaultBackendName() {\n\t\tOptional<String> defaultBackendNameOptional = DEFAULT_INDEX_BACKEND_NAME.get( propertySource );\n\t\tif ( defaultBackendNameOptional.isPresent() ) {\n\t\t\treturn defaultBackendNameOptional.get();\n\t\t}\n\t\telse {\n\t\t\tthrow log.defaultBackendNameNotSet(\n\t\t\t\t\t// Retrieve the resolved *default* key (*.default_backend) from the global (non-index-scoped) source\n\t\t\t\t\tDEFAULT_INDEX_BACKEND_NAME.resolveOrRaw( propertySource )\n\t\t\t);\n\t\t}\n\t}\n\n\tMap<String, BackendPartialBuildState> getBackendPartialBuildStates() {\n\t\t// Use a LinkedHashMap for deterministic iteration\n\t\tMap<String, BackendPartialBuildState> backendsByName = new LinkedHashMap<>();\n\t\tfor ( Map.Entry<String, BackendInitialBuildState<?>> entry : backendBuildStateByName.entrySet() ) {\n\t\t\tbackendsByName.put( entry.getKey(), entry.getValue().getPartiallyBuilt() );\n\t\t}\n\t\treturn backendsByName;\n\t}\n\n\tMap<String, IndexManagerPartialBuildState> getIndexManagersByName() {\n\t\t// Use a LinkedHashMap for deterministic iteration\n\t\tMap<String, IndexManagerPartialBuildState> indexManagersByName = new LinkedHashMap<>();\n\t\tfor ( Map.Entry<String, IndexManagerInitialBuildState<?>> entry : indexManagerBuildStateByName.entrySet() ) {\n\t\t\tindexManagersByName.put( entry.getKey(), entry.getValue().getPartialBuildState() );\n\t\t}\n\t\treturn indexManagersByName;\n\t}\n\n\tvoid closeOnFailure(SuppressingCloser closer) {\n\t\tcloser.pushAll( state -> state.closeOnFailure( closer ), indexManagerBuildStateByName.values() );\n\t\tcloser.pushAll( BackendInitialBuildState::closeOnFailure, backendBuildStateByName.values() );\n\t}\n\n\tprivate BackendInitialBuildState<?> createBackend(String backendName) {\n\t\tConfigurationPropertySource backendPropertySource =\n\t\t\t\tEngineConfigurationUtils.getBackend( propertySource, backendName );\n\t\ttry ( BeanHolder<? extends BackendFactory> backendFactoryHolder =\n\t\t\t\tBACKEND_TYPE.getAndMapOrThrow(\n\t\t\t\t\t\tbackendPropertySource,\n\t\t\t\t\t\tbeanProvider::getBean,\n\t\t\t\t\t\tkey -> log.backendTypeCannotBeNullOrEmpty( backendName, key )\n\t\t\t\t) ) {\n\t\t\tBackendBuildContext backendBuildContext = new BackendBuildContextImpl( rootBuildContext );\n\n\t\t\tBackendImplementor<?> backend = backendFactoryHolder.get()\n\t\t\t\t\t.create( backendName, backendBuildContext, backendPropertySource );\n\t\t\treturn new BackendInitialBuildState<>( backendName, backendBuildContext, backendPropertySource, backend );\n\t\t}\n\t}\n\n\tclass BackendInitialBuildState<D extends DocumentElement> {\n\t\tprivate final String backendName;\n\t\tprivate final BackendBuildContext backendBuildContext;\n\t\tprivate final ConfigurationPropertySource defaultIndexPropertySource;\n\t\tprivate final BackendImplementor<D> backend;\n\n\t\tprivate BackendInitialBuildState(\n\t\t\t\tString backendName,\n\t\t\t\tBackendBuildContext backendBuildContext,\n\t\t\t\tConfigurationPropertySource backendPropertySource,\n\t\t\t\tBackendImplementor<D> backend) {\n\t\t\tthis.backendName = backendName;\n\t\t\tthis.backendBuildContext = backendBuildContext;\n\t\t\tthis.defaultIndexPropertySource =\n\t\t\t\t\tEngineConfigurationUtils.getIndexDefaults( backendPropertySource );\n\t\t\tthis.backend = backend;\n\t\t}\n\n\t\tIndexManagerInitialBuildState<?> getIndexManagerBuildingState(\n\t\t\t\tString indexName, boolean multiTenancyEnabled) {\n\t\t\tIndexManagerInitialBuildState<?> state = indexManagerBuildStateByName.get( indexName );\n\t\t\tif ( state == null ) {\n\t\t\t\tConfigurationPropertySource indexPropertySource =\n\t\t\t\t\t\tEngineConfigurationUtils.getIndexWithoutDefaults( propertySource, indexName );\n\t\t\t\tConfigurationPropertySource defaultedIndexPropertySource =\n\t\t\t\t\t\tEngineConfigurationUtils.addIndexDefaults( indexPropertySource, defaultIndexPropertySource );\n\n\t\t\t\tIndexManagerBuilder<D> builder = backend.createIndexManagerBuilder(\n\t\t\t\t\t\tindexName, multiTenancyEnabled, backendBuildContext, defaultedIndexPropertySource\n\t\t\t\t);\n\t\t\t\tIndexSchemaRootNodeBuilder schemaRootNodeBuilder = builder.getSchemaRootNodeBuilder();\n\t\t\t\tIndexedEntityBindingContext bindingContext = new IndexedEntityBindingContextImpl( schemaRootNodeBuilder );\n\n\t\t\t\tstate = new IndexManagerInitialBuildState<>( backendName, indexName, builder, bindingContext );\n\t\t\t\tindexManagerBuildStateByName.put( indexName, state );\n\t\t\t}\n\t\t\treturn state;\n\n\t\t}\n\n\t\tvoid closeOnFailure() {\n\t\t\tbackend.close();\n\t\t}\n\n\t\tBackendPartialBuildState getPartiallyBuilt() {\n\t\t\treturn new BackendPartialBuildState( backendName, backend );\n\t\t}\n\t}\n\n\tprivate class IndexManagerInitialBuildState<D extends DocumentElement> implements IndexManagerBuildingState<D> {\n\n\t\tprivate final String backendName;\n\t\tprivate final String indexName;\n\t\tprivate final IndexManagerBuilder<D> builder;\n\t\tprivate final IndexedEntityBindingContext bindingContext;\n\n\t\tprivate IndexManagerImplementor<D> indexManager;\n\n\t\tIndexManagerInitialBuildState(String backendName, String indexName,\n\t\t\t\tIndexManagerBuilder<D> builder,\n\t\t\t\tIndexedEntityBindingContext bindingContext) {\n\t\t\tthis.backendName = backendName;\n\t\t\tthis.indexName = indexName;\n\t\t\tthis.builder = builder;\n\t\t\tthis.bindingContext = bindingContext;\n\t\t}\n\n\t\tvoid closeOnFailure(SuppressingCloser closer) {\n\t\t\tif ( indexManager != null ) {\n\t\t\t\tcloser.push( IndexManagerImplementor::close, indexManager );\n\t\t\t}\n\t\t\telse {\n\t\t\t\tcloser.push( IndexManagerBuilder::closeOnFailure, builder );\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic String getIndexName() {\n\t\t\treturn indexName;\n\t\t}\n\n\t\t@Override\n\t\tpublic IndexedEntityBindingContext getIndexedEntityBindingContext() {\n\t\t\treturn bindingContext;\n\t\t}\n\n\t\t@Override\n\t\tpublic MappedIndexManager<D> build() {\n\t\t\tif ( indexManager != null ) {\n\t\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\t\"Trying to build index manager \" + indexName + \" twice.\"\n\t\t\t\t\t\t+ \" There is probably a bug in the mapper implementation.\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tindexManager = builder.build();\n\t\t\treturn new MappedIndexManagerImpl<>( indexManager );\n\t\t}\n\n\t\tIndexManagerPartialBuildState getPartialBuildState() {\n\t\t\tif ( indexManager == null ) {\n\t\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\t\"Index manager \" + indexName + \" was not built by the mapper as expected.\"\n\t\t\t\t\t\t+ \" There is probably a bug in the mapper implementation.\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new IndexManagerPartialBuildState( backendName, indexName, indexManager );\n\t\t}\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "private String getDefaultBackendName() {\n\t\tOptional<String> defaultBackendNameOptional = DEFAULT_INDEX_BACKEND_NAME.get( propertySource );\n\t\tif ( defaultBackendNameOptional.isPresent() ) {\n\t\t\treturn defaultBackendNameOptional.get();\n\t\t}\n\t\telse {\n\t\t\tthrow log.defaultBackendNameNotSet(\n\t\t\t\t\t// Retrieve the resolved *default* key (*.default_backend) from the global (non-index-scoped) source\n\t\t\t\t\tDEFAULT_INDEX_BACKEND_NAME.resolveOrRaw( propertySource )\n\t\t\t);\n\t\t}\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.logging.impl.Log#indexBackendCannotBeNullOrEmpty\n methodBody: SearchException indexBackendCannotBeNullOrEmpty(String indexName, String key, String defaultKey);"
        ],
        "sourceCodeAfterRefactoring": "BackendInitialBuildState<?> getBackend(String backendName) {\n\t\tif ( StringHelper.isEmpty( backendName ) ) {\n\t\t\tbackendName = getDefaultBackendName();\n\t\t}\n\t\treturn backendBuildStateByName.computeIfAbsent( backendName, this::createBackend );\n\t}\nprivate String getDefaultBackendName() {\n\t\tOptional<String> defaultBackendNameOptional = DEFAULT_INDEX_BACKEND_NAME.get( propertySource );\n\t\tif ( defaultBackendNameOptional.isPresent() ) {\n\t\t\treturn defaultBackendNameOptional.get();\n\t\t}\n\t\telse {\n\t\t\tthrow log.defaultBackendNameNotSet(\n\t\t\t\t\t// Retrieve the resolved *default* key (*.default_backend) from the global (non-index-scoped) source\n\t\t\t\t\tDEFAULT_INDEX_BACKEND_NAME.resolveOrRaw( propertySource )\n\t\t\t);\n\t\t}\n\t}",
        "diffSourceCode": "-   71: \t\tthis.rootBuildContext = rootBuildContext;\n-   72: \t}\n-   73: \n-   74: \tpublic IndexManagerBuildingState<?> startBuilding(String indexName, boolean multiTenancyEnabled) {\n-   75: \t\tConfigurationPropertySource indexPropertySource =\n-   76: \t\t\t\tEngineConfigurationUtils.getIndexWithoutDefaults( propertySource, indexName );\n-   78: \n-   79: \t\tBackendInitialBuildState<?> backendBuildingstate =\n-   80: \t\t\t\tbackendBuildStateByName.computeIfAbsent( backendName, this::createBackend );\n-   81: \n-   82: \t\tIndexManagerInitialBuildState<?> state = indexManagerBuildStateByName.get( indexName );\n-   83: \t\tif ( state == null ) {\n-   84: \t\t\tstate = backendBuildingstate.createIndexManagerBuildingState(\n-   85: \t\t\t\t\tindexName, multiTenancyEnabled, indexPropertySource\n-   86: \t\t\t);\n-   87: \t\t\tindexManagerBuildStateByName.put( indexName, state );\n+   71: \tBackendInitialBuildState<?> getBackend(String backendName) {\n+   72: \t\tif ( StringHelper.isEmpty( backendName ) ) {\n+   73: \t\t\tbackendName = getDefaultBackendName();\n+   74: \t\t}\n+   75: \t\treturn backendBuildStateByName.computeIfAbsent( backendName, this::createBackend );\n+   76: \t}\n+   78: \tprivate String getDefaultBackendName() {\n+   79: \t\tOptional<String> defaultBackendNameOptional = DEFAULT_INDEX_BACKEND_NAME.get( propertySource );\n+   80: \t\tif ( defaultBackendNameOptional.isPresent() ) {\n+   81: \t\t\treturn defaultBackendNameOptional.get();\n+   82: \t\t}\n+   83: \t\telse {\n+   84: \t\t\tthrow log.defaultBackendNameNotSet(\n+   85: \t\t\t\t\t// Retrieve the resolved *default* key (*.default_backend) from the global (non-index-scoped) source\n+   86: \t\t\t\t\tDEFAULT_INDEX_BACKEND_NAME.resolveOrRaw( propertySource )\n+   87: \t\t\t);\n    88: \t\t}\n-   89: \t\treturn state;\n-   92: \tprivate String getBackendName(String indexName, ConfigurationPropertySource indexPropertySource) {\n-   93: \t\tOptional<String> backendNameOptional = INDEX_BACKEND_NAME.get( indexPropertySource );\n-   94: \t\tif ( backendNameOptional.isPresent() ) {\n-   95: \t\t\treturn backendNameOptional.get();\n+   89: \t}\n+   92: \t\t// Use a LinkedHashMap for deterministic iteration\n+   93: \t\tMap<String, BackendPartialBuildState> backendsByName = new LinkedHashMap<>();\n+   94: \t\tfor ( Map.Entry<String, BackendInitialBuildState<?>> entry : backendBuildStateByName.entrySet() ) {\n+   95: \t\t\tbackendsByName.put( entry.getKey(), entry.getValue().getPartiallyBuilt() );\n    96: \t\t}\n-   97: \t\telse {\n-   98: \t\t\tOptional<String> defaultBackendNameOptional = DEFAULT_INDEX_BACKEND_NAME.get( propertySource );\n-   99: \t\t\tif ( defaultBackendNameOptional.isPresent() ) {\n-  100: \t\t\t\treturn defaultBackendNameOptional.get();\n-  101: \t\t\t}\n-  102: \t\t\telse {\n-  103: \t\t\t\tthrow log.indexBackendCannotBeNullOrEmpty(\n-  104: \t\t\t\t\t\tindexName,\n-  105: \t\t\t\t\t\tINDEX_BACKEND_NAME.resolveOrRaw( indexPropertySource ),\n-  106: \t\t\t\t\t\t// Retrieve the resolved *default* key (*.default_backend) from the global (non-index-scoped) source\n-  107: \t\t\t\t\t\tDEFAULT_INDEX_BACKEND_NAME.resolveOrRaw( propertySource )\n-  108: \t\t\t\t);\n-  109: \t\t\t}\n-  110: \t\t}\n-  111: \t}\n+   97: \t\treturn backendsByName;\n+   98: \t}\n+   99: \n+  100: \tMap<String, IndexManagerPartialBuildState> getIndexManagersByName() {\n+  101: \t\t// Use a LinkedHashMap for deterministic iteration\n+  102: \t\tMap<String, IndexManagerPartialBuildState> indexManagersByName = new LinkedHashMap<>();\n+  103: \t\tfor ( Map.Entry<String, IndexManagerInitialBuildState<?>> entry : indexManagerBuildStateByName.entrySet() ) {\n+  104: \t\t\tindexManagersByName.put( entry.getKey(), entry.getValue().getPartialBuildState() );\n+  105: \t\t}\n+  106: \t\treturn indexManagersByName;\n+  107: \t}\n+  108: \n+  109: \tvoid closeOnFailure(SuppressingCloser closer) {\n+  110: \t\tcloser.pushAll( state -> state.closeOnFailure( closer ), indexManagerBuildStateByName.values() );\n+  111: \t\tcloser.pushAll( BackendInitialBuildState::closeOnFailure, backendBuildStateByName.values() );\n",
        "uniqueId": "8163c142f60d58fb295123d6a2ba72b99c7f020f_92_111_78_89_71_76",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 4,
                "covered": 32
            },
            "BRANCH": {
                "missed": 1,
                "covered": 3
            },
            "LINE": {
                "missed": 1,
                "covered": 8
            },
            "COMPLEXITY": {
                "missed": 1,
                "covered": 2
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Inline Method",
        "description": "Inline Method\tprivate composeExplicitlyIncludedPaths(relativePrefix String, nullSafeIncludePaths Set<String>, currentRemainingDepth Integer, nestedRemainingDepth Integer) : Set<String> inlined to public composeWithNested(parentTypeModel MappableTypeModel, relativePrefix String, maxDepth Integer, includePaths Set<String>) : IndexSchemaFilter in class org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter",
        "diffLocations": [
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/IndexSchemaFilter.java",
                "startLine": 149,
                "endLine": 195,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/IndexSchemaFilter.java",
                "startLine": 184,
                "endLine": 266,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/IndexSchemaFilter.java",
                "startLine": 197,
                "endLine": 232,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private Set<String> composeExplicitlyIncludedPaths(String relativePrefix, Set<String> nullSafeIncludePaths,\n\t\t\tInteger currentRemainingDepth, Integer nestedRemainingDepth) {\n\t\tSet<String> composedFilterExplicitlyIncludedPaths = new HashSet<>();\n\t\t/*\n\t\t * Add the nested filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they are not filtered out by the current filter.\n\t\t */\n\t\tfor ( String path : nullSafeIncludePaths ) {\n\t\t\tif ( isPathIncluded( currentRemainingDepth, explicitlyIncludedPaths, relativePrefix + path ) ) {\n\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( path );\n\t\t\t\t// Also add paths leading to this path (so that object nodes are not excluded)\n\t\t\t\tint afterPreviousDotIndex = 0;\n\t\t\t\tint nextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\twhile ( nextDotIndex >= 0 ) {\n\t\t\t\t\tString subPath = path.substring( 0, nextDotIndex );\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( subPath );\n\t\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n\t\t\t\t\tnextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Add the current filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they start with the nested filter's prefix and are not filtered out by the nested filter.\n\t\t */\n\t\tint relativePrefixLength = relativePrefix.length();\n\t\tfor ( String path : explicitlyIncludedPaths ) {\n\t\t\tif ( path.startsWith( relativePrefix ) ) {\n\t\t\t\tString pathRelativeToNestedFilter = path.substring( relativePrefixLength );\n\t\t\t\tif ( isPathIncluded( nestedRemainingDepth, nullSafeIncludePaths, pathRelativeToNestedFilter ) ) {\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( pathRelativeToNestedFilter );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn composedFilterExplicitlyIncludedPaths;\n\t}",
        "filePathBefore": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/IndexSchemaFilter.java",
        "isPureRefactoring": true,
        "commitId": "d5122d457575873143ef1afe52a80a4322bc5181",
        "packageNameBefore": "org.hibernate.search.engine.mapper.mapping.building.impl",
        "classNameBefore": "org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter",
        "methodNameBefore": "org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#composeExplicitlyIncludedPaths",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#isPathIncluded\n methodBody: private static boolean isPathIncluded(Integer remainingDepth, Set<String> explicitlyIncludedPaths, String relativePath) {\nreturn isEveryPathIncludedByDefault(remainingDepth) || explicitlyIncludedPaths.contains(relativePath);\n}",
        "classSignatureBefore": "class IndexSchemaFilter ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#composeExplicitlyIncludedPaths"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter"
        ],
        "classSignatureBeforeSet": [
            "class IndexSchemaFilter "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "",
                "description": "Return statements added",
                "mappingState": 2
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.mapper.mapping.building.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.Collections;\nimport java.util.HashSet;\nimport java.util.Set;\n\nimport org.hibernate.search.engine.logging.impl.Log;\nimport org.hibernate.search.engine.mapper.model.spi.MappableTypeModel;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * A schema filter, responsible for deciding which parts of a mapping will actually make it to the index schema.\n * <p>\n * A schema filter is created at the root of a Pojo mapping (it accepts everything),\n * and also each time index embedding ({@code @IndexedEmbedded}) is used.\n *\n * <h3 id=\"filter-usage\">Filter usage</h3>\n * <p>\n * A schema filter is asked to provide advice about whether or not to trim down the schema in two cases:\n * <ul>\n *     <li>When a field is added by a bridge, the filter decides whether to include this field or not\n *     through its {@link #isPathIncluded(String)} method</li>\n *     <li>When a nested {@code @IndexedEmbedded} is requested, a new filter is created through the\n *     {@link #composeWithNested(MappableTypeModel, String, Integer, Set)} method, which may return a filter\n *     that {@link #isEveryPathExcluded() excludes every path}, meaning the {@code @IndexedEmbedded} will\n *     be ignored</li>\n * </ul>\n *\n * <h3 id=\"filter-properties\">Filter properties</h3>\n * <p>\n * A filter decides whether to include a path or not according to its two main properties:\n * <ul>\n *     <li>the {@link #remainingCompositionDepth remaining composition depth}</li>\n *     <li>the {@link #explicitlyIncludedPaths explicitly included paths}</li>\n * </ul>\n * <p>\n * The explicitly included paths, as their name suggests, define which paths\n * should be accepted by this filter no matter what.\n * <p>\n * The composition depth defines\n * {@link #isEveryPathIncludedByDefault(Integer) how paths that do not appear in the explicitly included paths should be treated}:\n * <ul>\n *     <li>if {@code <= 0}, paths are excluded by default</li>\n *     <li>if {@code null} or {@code > 0}, paths are included by default</li>\n * </ul>\n *\n * <h3 id=\"filter-composition\">Filter composition</h3>\n * <p>\n * Composed filters are created whenever a nested {@code @IndexedEmbedded} is encountered.\n * A composed filter will always enforce the restrictions of its parent filter,\n * plus some added restrictions depending on the properties of the nested {@code IndexedEmbedded}.\n * <p>\n * For more information about how filters are composed, see\n * {@link #composeWithNested(MappableTypeModel, String, Integer, Set)}.\n *\n * @author Yoann Rodiere\n */\nclass IndexSchemaFilter {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate static final IndexSchemaFilter ROOT = new IndexSchemaFilter(\n\t\t\tnull, null, null, null, Collections.emptySet()\n\t);\n\n\tpublic static IndexSchemaFilter root() {\n\t\treturn ROOT;\n\t}\n\n\tprivate final IndexSchemaFilter parent;\n\tprivate final MappableTypeModel parentTypeModel;\n\tprivate final String relativePrefix;\n\n\t/**\n\t * Defines how deep indexed embedded are allowed to be composed.\n\t *\n\t * Note that composition depth only relates to IndexedEmbedded composition;\n\t * bridge-declared fields are only affected by path filtering,\n\t * whose default behavior (include or exclude) is determined by {@link #isEveryPathIncludedByDefault(Integer)}.\n\t */\n\tprivate final Integer remainingCompositionDepth;\n\n\t/**\n\t * Defines paths to be included even when the default behavior is to exclude paths.\n\t */\n\tprivate final Set<String> explicitlyIncludedPaths;\n\n\tprivate IndexSchemaFilter(IndexSchemaFilter parent, MappableTypeModel parentTypeModel, String relativePrefix,\n\t\t\tInteger remainingCompositionDepth, Set<String> explicitlyIncludedPaths) {\n\t\tthis.parent = parent;\n\t\tthis.parentTypeModel = parentTypeModel;\n\t\tthis.relativePrefix = relativePrefix;\n\t\tthis.remainingCompositionDepth = remainingCompositionDepth;\n\t\tthis.explicitlyIncludedPaths = explicitlyIncludedPaths;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn new StringBuilder( getClass().getSimpleName() )\n\t\t\t\t.append( \"[\" )\n\t\t\t\t.append( \"parentTypeModel=\" ).append( parentTypeModel )\n\t\t\t\t.append( \",relativePrefix=\" ).append( relativePrefix )\n\t\t\t\t.append( \",remainingCompositionDepth=\" ).append( remainingCompositionDepth )\n\t\t\t\t.append( \",explicitlyIncludedPaths=\" ).append( explicitlyIncludedPaths )\n\t\t\t\t.append( \"]\" )\n\t\t\t\t.toString();\n\t}\n\n\tpublic boolean isPathIncluded(String relativePath) {\n\t\treturn isPathIncluded( remainingCompositionDepth, explicitlyIncludedPaths, relativePath );\n\t}\n\n\tpublic boolean isEveryPathExcluded() {\n\t\treturn !isEveryPathIncludedByDefault( remainingCompositionDepth ) && !isAnyPathExplicitlyIncluded();\n\t}\n\n\tprivate String getPathFromSameIndexedEmbeddedSinceNoCompositionLimits(MappableTypeModel parentTypeModel, String relativePrefix) {\n\t\tif ( hasCompositionLimits() ) {\n\t\t\treturn null;\n\t\t}\n\t\telse if ( parent != null ) {\n\t\t\tif ( this.relativePrefix.equals( relativePrefix )\n\t\t\t\t\t&& this.parentTypeModel.isSubTypeOf( parentTypeModel ) ) {\n\t\t\t\t// Same IndexedEmbedded as the one passed as a parameter\n\t\t\t\treturn this.relativePrefix;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tString path = parent.getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n\t\t\t\treturn path == null ? null : path + this.relativePrefix;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t/*\n\t\t\t * No recursion limits, no parent: this is the root.\n\t\t\t * I we reach this point, it means there was no recursion limit at all,\n\t\t\t * but we did not encounter the IndexedEmbedded we were looking for.\n\t\t\t */\n\t\t\treturn null;\n\t\t}\n\t}\n\n\tpublic IndexSchemaFilter composeWithNested(MappableTypeModel parentTypeModel, String relativePrefix,\n\t\t\tInteger maxDepth, Set<String> includePaths) {\n\t\tString cyclicRecursionPath = getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n\t\tif ( cyclicRecursionPath != null ) {\n\t\t\tcyclicRecursionPath += relativePrefix;\n\t\t\tthrow log.indexedEmbeddedCyclicRecursion( cyclicRecursionPath, parentTypeModel );\n\t\t}\n\n\t\tSet<String> nullSafeIncludePaths = includePaths == null ? Collections.emptySet() : includePaths;\n\n\t\t// The remaining composition depth according to \"this\" only\n\t\tInteger currentRemainingDepth = remainingCompositionDepth == null ? null : remainingCompositionDepth - 1;\n\n\t\t// The remaining composition depth according to the nested IndexedEmbedded only\n\t\tInteger nestedRemainingDepth = maxDepth;\n\t\tif ( maxDepth == null && !nullSafeIncludePaths.isEmpty() ) {\n\t\t\t/*\n\t\t\t * If no max depth was provided and \"includePaths\" was provided,\n\t\t\t * the remaining composition depth is implicitly set to 0,\n\t\t\t * meaning no composition is allowed and paths are excluded unless\n\t\t\t * explicitly listed in \"includePaths\".\n\t\t\t */\n\t\t\tnestedRemainingDepth = 0;\n\t\t}\n\n\t\t/*\n\t\t * By default, a composed filters' remaining composition depth is its parent's minus one\n\t\t * (or null if the remaining composition depth was not set in the parent)...\n\t\t */\n\t\tInteger composedRemainingDepth = currentRemainingDepth;\n\t\tif ( composedRemainingDepth == null\n\t\t\t\t|| nestedRemainingDepth != null && composedRemainingDepth > nestedRemainingDepth ) {\n\t\t\t/*\n\t\t\t * ... but the nested filter can override it.\n\t\t\t */\n\t\t\tcomposedRemainingDepth = nestedRemainingDepth;\n\t\t}\n\n\t\tSet<String> composedFilterExplicitlyIncludedPaths = composeExplicitlyIncludedPaths(\n\t\t\t\trelativePrefix, nullSafeIncludePaths, currentRemainingDepth, nestedRemainingDepth\n\t\t);\n\n\t\treturn new IndexSchemaFilter(\n\t\t\t\tthis, parentTypeModel, relativePrefix,\n\t\t\t\tcomposedRemainingDepth, composedFilterExplicitlyIncludedPaths\n\t\t);\n\t}\n\n\tprivate Set<String> composeExplicitlyIncludedPaths(String relativePrefix, Set<String> nullSafeIncludePaths,\n\t\t\tInteger currentRemainingDepth, Integer nestedRemainingDepth) {\n\t\tSet<String> composedFilterExplicitlyIncludedPaths = new HashSet<>();\n\t\t/*\n\t\t * Add the nested filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they are not filtered out by the current filter.\n\t\t */\n\t\tfor ( String path : nullSafeIncludePaths ) {\n\t\t\tif ( isPathIncluded( currentRemainingDepth, explicitlyIncludedPaths, relativePrefix + path ) ) {\n\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( path );\n\t\t\t\t// Also add paths leading to this path (so that object nodes are not excluded)\n\t\t\t\tint afterPreviousDotIndex = 0;\n\t\t\t\tint nextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\twhile ( nextDotIndex >= 0 ) {\n\t\t\t\t\tString subPath = path.substring( 0, nextDotIndex );\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( subPath );\n\t\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n\t\t\t\t\tnextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Add the current filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they start with the nested filter's prefix and are not filtered out by the nested filter.\n\t\t */\n\t\tint relativePrefixLength = relativePrefix.length();\n\t\tfor ( String path : explicitlyIncludedPaths ) {\n\t\t\tif ( path.startsWith( relativePrefix ) ) {\n\t\t\t\tString pathRelativeToNestedFilter = path.substring( relativePrefixLength );\n\t\t\t\tif ( isPathIncluded( nestedRemainingDepth, nullSafeIncludePaths, pathRelativeToNestedFilter ) ) {\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( pathRelativeToNestedFilter );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn composedFilterExplicitlyIncludedPaths;\n\t}\n\n\tprivate static boolean isPathIncluded(Integer remainingDepth, Set<String> explicitlyIncludedPaths, String relativePath) {\n\t\treturn isEveryPathIncludedByDefault( remainingDepth )\n\t\t\t\t|| explicitlyIncludedPaths.contains( relativePath );\n\t}\n\n\tprivate static boolean isEveryPathIncludedByDefault(Integer remainingDepth) {\n\t\t/*\n\t\t * A remaining composition depth of 0 or below means\n\t\t * paths should be excluded when filtering unless mentioned in explicitlyIncludedPaths.\n\t\t */\n\t\treturn remainingDepth == null || remainingDepth > 0;\n\t}\n\n\tprivate boolean isAnyPathExplicitlyIncluded() {\n\t\treturn !explicitlyIncludedPaths.isEmpty();\n\t}\n\n\tprivate boolean hasCompositionLimits() {\n\t\treturn remainingCompositionDepth != null || !explicitlyIncludedPaths.isEmpty();\n\t}\n}",
        "filePathAfter": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/IndexSchemaFilter.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.mapper.mapping.building.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.Collections;\nimport java.util.HashSet;\nimport java.util.LinkedHashMap;\nimport java.util.LinkedHashSet;\nimport java.util.Map;\nimport java.util.Set;\n\nimport org.hibernate.search.engine.logging.impl.Log;\nimport org.hibernate.search.engine.mapper.model.spi.MappableTypeModel;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * A schema filter, responsible for deciding which parts of a mapping will actually make it to the index schema.\n * <p>\n * A schema filter is created at the root of a Pojo mapping (it accepts everything),\n * and also each time index embedding ({@code @IndexedEmbedded}) is used.\n *\n * <h3 id=\"filter-usage\">Filter usage</h3>\n * <p>\n * A schema filter is asked to provide advice about whether or not to trim down the schema in two cases:\n * <ul>\n *     <li>When a field is added by a bridge, the filter decides whether to include this field or not\n *     through its {@link #isPathIncluded(String)} method</li>\n *     <li>When a nested {@code @IndexedEmbedded} is requested, a new filter is created through the\n *     {@link #composeWithNested(MappableTypeModel, String, Integer, Set)} method, which may return a filter\n *     that {@link #isEveryPathExcluded() excludes every path}, meaning the {@code @IndexedEmbedded} will\n *     be ignored</li>\n * </ul>\n *\n * <h3 id=\"filter-properties\">Filter properties</h3>\n * <p>\n * A filter decides whether to include a path or not according to its two main properties:\n * <ul>\n *     <li>the {@link #remainingCompositionDepth remaining composition depth}</li>\n *     <li>the {@link #explicitlyIncludedPaths explicitly included paths}</li>\n * </ul>\n * <p>\n * The explicitly included paths, as their name suggests, define which paths\n * should be accepted by this filter no matter what.\n * <p>\n * The composition depth defines\n * {@link #isEveryPathIncludedByDefault(Integer) how paths that do not appear in the explicitly included paths should be treated}:\n * <ul>\n *     <li>if {@code <= 0}, paths are excluded by default</li>\n *     <li>if {@code null} or {@code > 0}, paths are included by default</li>\n * </ul>\n *\n * <h3 id=\"filter-composition\">Filter composition</h3>\n * <p>\n * Composed filters are created whenever a nested {@code @IndexedEmbedded} is encountered.\n * A composed filter will always enforce the restrictions of its parent filter,\n * plus some added restrictions depending on the properties of the nested {@code IndexedEmbedded}.\n * <p>\n * For more information about how filters are composed, see\n * {@link #composeWithNested(MappableTypeModel, String, Integer, Set)}.\n *\n * @author Yoann Rodiere\n */\nclass IndexSchemaFilter {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate static final IndexSchemaFilter ROOT = new IndexSchemaFilter(\n\t\t\tnull, null, null,\n\t\t\tnull, Collections.emptySet(), Collections.emptySet()\n\t);\n\n\tpublic static IndexSchemaFilter root() {\n\t\treturn ROOT;\n\t}\n\n\tprivate final IndexSchemaFilter parent;\n\tprivate final MappableTypeModel parentTypeModel;\n\tprivate final String relativePrefix;\n\n\t/**\n\t * Defines how deep indexed embedded are allowed to be composed.\n\t *\n\t * Note that composition depth only relates to IndexedEmbedded composition;\n\t * bridge-declared fields are only affected by path filtering,\n\t * whose default behavior (include or exclude) is determined by {@link #isEveryPathIncludedByDefault(Integer)}.\n\t */\n\tprivate final Integer remainingCompositionDepth;\n\n\t/**\n\t * Defines paths to be included even when the default behavior is to exclude paths.\n\t */\n\tprivate final Set<String> explicitlyIncludedPaths;\n\n\t/**\n\t * The {@code explicitlyIncludedPaths} that were included by this filter explicitly (not a parent filter).\n\t */\n\tprivate final Set<String> properExplicitlyIncludedPaths;\n\n\t/**\n\t * The {@code paths} that were encountered, i.e. passed to {@link #isPathIncluded(String)}\n\t * or to the same method of a child filter.\n\t */\n\t// Use a LinkedHashSet, since the set will be exposed through a getter and may be iterated on\n\tprivate final Map<String, Boolean> encounteredFieldPaths = new LinkedHashMap<>();\n\n\tprivate IndexSchemaFilter(IndexSchemaFilter parent, MappableTypeModel parentTypeModel, String relativePrefix,\n\t\t\tInteger remainingCompositionDepth, Set<String> explicitlyIncludedPaths,\n\t\t\tSet<String> properExplicitlyIncludedPaths) {\n\t\tthis.parent = parent;\n\t\tthis.parentTypeModel = parentTypeModel;\n\t\tthis.relativePrefix = relativePrefix;\n\t\tthis.remainingCompositionDepth = remainingCompositionDepth;\n\t\tthis.explicitlyIncludedPaths = explicitlyIncludedPaths;\n\t\tthis.properExplicitlyIncludedPaths = properExplicitlyIncludedPaths;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn new StringBuilder( getClass().getSimpleName() )\n\t\t\t\t.append( \"[\" )\n\t\t\t\t.append( \"parentTypeModel=\" ).append( parentTypeModel )\n\t\t\t\t.append( \",relativePrefix=\" ).append( relativePrefix )\n\t\t\t\t.append( \",remainingCompositionDepth=\" ).append( remainingCompositionDepth )\n\t\t\t\t.append( \",explicitlyIncludedPaths=\" ).append( explicitlyIncludedPaths )\n\t\t\t\t.append( \"]\" )\n\t\t\t\t.toString();\n\t}\n\n\tpublic boolean isPathIncluded(String relativePath) {\n\t\tboolean included = isPathIncluded( remainingCompositionDepth, explicitlyIncludedPaths, relativePath );\n\t\tmarkAsEncountered( relativePath, included );\n\t\treturn included;\n\t}\n\n\tprivate void markAsEncountered(String relativePath, boolean included) {\n\t\tencounteredFieldPaths.put( relativePath, included );\n\t\tif ( parent != null ) {\n\t\t\tparent.markAsEncountered( relativePrefix + relativePath, included );\n\t\t}\n\t}\n\n\tpublic boolean isEveryPathExcluded() {\n\t\treturn !isEveryPathIncludedByDefault( remainingCompositionDepth ) && !isAnyPathExplicitlyIncluded();\n\t}\n\n\tpublic Set<String> getProperExplicitlyIncludedPaths() {\n\t\treturn properExplicitlyIncludedPaths;\n\t}\n\n\tpublic Map<String, Boolean> getEncounteredFieldPaths() {\n\t\treturn encounteredFieldPaths;\n\t}\n\n\tprivate String getPathFromSameIndexedEmbeddedSinceNoCompositionLimits(MappableTypeModel parentTypeModel, String relativePrefix) {\n\t\tif ( hasCompositionLimits() ) {\n\t\t\treturn null;\n\t\t}\n\t\telse if ( parent != null ) {\n\t\t\tif ( this.relativePrefix.equals( relativePrefix )\n\t\t\t\t\t&& this.parentTypeModel.isSubTypeOf( parentTypeModel ) ) {\n\t\t\t\t// Same IndexedEmbedded as the one passed as a parameter\n\t\t\t\treturn this.relativePrefix;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tString path = parent.getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n\t\t\t\treturn path == null ? null : path + this.relativePrefix;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t/*\n\t\t\t * No recursion limits, no parent: this is the root.\n\t\t\t * I we reach this point, it means there was no recursion limit at all,\n\t\t\t * but we did not encounter the IndexedEmbedded we were looking for.\n\t\t\t */\n\t\t\treturn null;\n\t\t}\n\t}\n\n\tpublic IndexSchemaFilter composeWithNested(MappableTypeModel parentTypeModel, String relativePrefix,\n\t\t\tInteger maxDepth, Set<String> includePaths) {\n\t\tString cyclicRecursionPath = getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n\t\tif ( cyclicRecursionPath != null ) {\n\t\t\tcyclicRecursionPath += relativePrefix;\n\t\t\tthrow log.indexedEmbeddedCyclicRecursion( cyclicRecursionPath, parentTypeModel );\n\t\t}\n\n\t\tSet<String> nullSafeIncludePaths = includePaths == null ? Collections.emptySet() : includePaths;\n\n\t\t// The remaining composition depth according to \"this\" only\n\t\tInteger currentRemainingDepth = remainingCompositionDepth == null ? null : remainingCompositionDepth - 1;\n\n\t\t// The remaining composition depth according to the nested IndexedEmbedded only\n\t\tInteger nestedRemainingDepth = maxDepth;\n\t\tif ( maxDepth == null && !nullSafeIncludePaths.isEmpty() ) {\n\t\t\t/*\n\t\t\t * If no max depth was provided and \"includePaths\" was provided,\n\t\t\t * the remaining composition depth is implicitly set to 0,\n\t\t\t * meaning no composition is allowed and paths are excluded unless\n\t\t\t * explicitly listed in \"includePaths\".\n\t\t\t */\n\t\t\tnestedRemainingDepth = 0;\n\t\t}\n\n\t\t/*\n\t\t * By default, a composed filters' remaining composition depth is its parent's minus one\n\t\t * (or null if the remaining composition depth was not set in the parent)...\n\t\t */\n\t\tInteger composedRemainingDepth = currentRemainingDepth;\n\t\tif ( composedRemainingDepth == null\n\t\t\t\t|| nestedRemainingDepth != null && composedRemainingDepth > nestedRemainingDepth ) {\n\t\t\t/*\n\t\t\t * ... but the nested filter can override it.\n\t\t\t */\n\t\t\tcomposedRemainingDepth = nestedRemainingDepth;\n\t\t}\n\n\t\t// The included paths that will be used to determine inclusion\n\t\tSet<String> composedFilterExplicitlyIncludedPaths = new HashSet<>();\n\t\t// The subset of these included paths that were added by the nested filter (and not a parent filter)\n\t\t// Use a LinkedHashSet, since the set will be exposed through a getter and may be iterated on\n\t\tSet<String> composedFilterProperExplicitlyIncludedPaths = new LinkedHashSet<>();\n\n\t\t/*\n\t\t * Add the nested filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they are not filtered out by the current filter.\n\t\t */\n\t\tfor ( String path : nullSafeIncludePaths ) {\n\t\t\tif ( isPathIncluded( currentRemainingDepth, explicitlyIncludedPaths, relativePrefix + path ) ) {\n\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( path );\n\t\t\t\tcomposedFilterProperExplicitlyIncludedPaths.add( path );\n\t\t\t\t// Also add paths leading to this path (so that object nodes are not excluded)\n\t\t\t\tint afterPreviousDotIndex = 0;\n\t\t\t\tint nextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\twhile ( nextDotIndex >= 0 ) {\n\t\t\t\t\tString subPath = path.substring( 0, nextDotIndex );\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( subPath );\n\t\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n\t\t\t\t\tnextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Add the current filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they start with the nested filter's prefix and are not filtered out by the nested filter.\n\t\t */\n\t\tint relativePrefixLength = relativePrefix.length();\n\t\tfor ( String path : explicitlyIncludedPaths ) {\n\t\t\tif ( path.startsWith( relativePrefix ) ) {\n\t\t\t\tString pathRelativeToNestedFilter = path.substring( relativePrefixLength );\n\t\t\t\tif ( isPathIncluded( nestedRemainingDepth, nullSafeIncludePaths, pathRelativeToNestedFilter ) ) {\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( pathRelativeToNestedFilter );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn new IndexSchemaFilter(\n\t\t\t\tthis, parentTypeModel, relativePrefix,\n\t\t\t\tcomposedRemainingDepth, composedFilterExplicitlyIncludedPaths,\n\t\t\t\tcomposedFilterProperExplicitlyIncludedPaths\n\t\t);\n\t}\n\n\tprivate static boolean isPathIncluded(Integer remainingDepth, Set<String> explicitlyIncludedPaths, String relativePath) {\n\t\treturn isEveryPathIncludedByDefault( remainingDepth )\n\t\t\t\t|| explicitlyIncludedPaths.contains( relativePath );\n\t}\n\n\tprivate static boolean isEveryPathIncludedByDefault(Integer remainingDepth) {\n\t\t/*\n\t\t * A remaining composition depth of 0 or below means\n\t\t * paths should be excluded when filtering unless mentioned in explicitlyIncludedPaths.\n\t\t */\n\t\treturn remainingDepth == null || remainingDepth > 0;\n\t}\n\n\tprivate boolean isAnyPathExplicitlyIncluded() {\n\t\treturn !explicitlyIncludedPaths.isEmpty();\n\t}\n\n\tprivate boolean hasCompositionLimits() {\n\t\treturn remainingCompositionDepth != null || !explicitlyIncludedPaths.isEmpty();\n\t}\n}",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#isPathIncluded\n methodBody: private static boolean isPathIncluded(Integer remainingDepth, Set<String> explicitlyIncludedPaths, String relativePath) {\nreturn isEveryPathIncludedByDefault(remainingDepth) || explicitlyIncludedPaths.contains(relativePath);\n}"
        ],
        "sourceCodeAfterRefactoring": "public IndexSchemaFilter composeWithNested(MappableTypeModel parentTypeModel, String relativePrefix,\n\t\t\tInteger maxDepth, Set<String> includePaths) {\n\t\tString cyclicRecursionPath = getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n\t\tif ( cyclicRecursionPath != null ) {\n\t\t\tcyclicRecursionPath += relativePrefix;\n\t\t\tthrow log.indexedEmbeddedCyclicRecursion( cyclicRecursionPath, parentTypeModel );\n\t\t}\n\n\t\tSet<String> nullSafeIncludePaths = includePaths == null ? Collections.emptySet() : includePaths;\n\n\t\t// The remaining composition depth according to \"this\" only\n\t\tInteger currentRemainingDepth = remainingCompositionDepth == null ? null : remainingCompositionDepth - 1;\n\n\t\t// The remaining composition depth according to the nested IndexedEmbedded only\n\t\tInteger nestedRemainingDepth = maxDepth;\n\t\tif ( maxDepth == null && !nullSafeIncludePaths.isEmpty() ) {\n\t\t\t/*\n\t\t\t * If no max depth was provided and \"includePaths\" was provided,\n\t\t\t * the remaining composition depth is implicitly set to 0,\n\t\t\t * meaning no composition is allowed and paths are excluded unless\n\t\t\t * explicitly listed in \"includePaths\".\n\t\t\t */\n\t\t\tnestedRemainingDepth = 0;\n\t\t}\n\n\t\t/*\n\t\t * By default, a composed filters' remaining composition depth is its parent's minus one\n\t\t * (or null if the remaining composition depth was not set in the parent)...\n\t\t */\n\t\tInteger composedRemainingDepth = currentRemainingDepth;\n\t\tif ( composedRemainingDepth == null\n\t\t\t\t|| nestedRemainingDepth != null && composedRemainingDepth > nestedRemainingDepth ) {\n\t\t\t/*\n\t\t\t * ... but the nested filter can override it.\n\t\t\t */\n\t\t\tcomposedRemainingDepth = nestedRemainingDepth;\n\t\t}\n\n\t\t// The included paths that will be used to determine inclusion\n\t\tSet<String> composedFilterExplicitlyIncludedPaths = new HashSet<>();\n\t\t// The subset of these included paths that were added by the nested filter (and not a parent filter)\n\t\t// Use a LinkedHashSet, since the set will be exposed through a getter and may be iterated on\n\t\tSet<String> composedFilterProperExplicitlyIncludedPaths = new LinkedHashSet<>();\n\n\t\t/*\n\t\t * Add the nested filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they are not filtered out by the current filter.\n\t\t */\n\t\tfor ( String path : nullSafeIncludePaths ) {\n\t\t\tif ( isPathIncluded( currentRemainingDepth, explicitlyIncludedPaths, relativePrefix + path ) ) {\n\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( path );\n\t\t\t\tcomposedFilterProperExplicitlyIncludedPaths.add( path );\n\t\t\t\t// Also add paths leading to this path (so that object nodes are not excluded)\n\t\t\t\tint afterPreviousDotIndex = 0;\n\t\t\t\tint nextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\twhile ( nextDotIndex >= 0 ) {\n\t\t\t\t\tString subPath = path.substring( 0, nextDotIndex );\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( subPath );\n\t\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n\t\t\t\t\tnextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t/*\n\t\t * Add the current filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n\t\t * provided they start with the nested filter's prefix and are not filtered out by the nested filter.\n\t\t */\n\t\tint relativePrefixLength = relativePrefix.length();\n\t\tfor ( String path : explicitlyIncludedPaths ) {\n\t\t\tif ( path.startsWith( relativePrefix ) ) {\n\t\t\t\tString pathRelativeToNestedFilter = path.substring( relativePrefixLength );\n\t\t\t\tif ( isPathIncluded( nestedRemainingDepth, nullSafeIncludePaths, pathRelativeToNestedFilter ) ) {\n\t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( pathRelativeToNestedFilter );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn new IndexSchemaFilter(\n\t\t\t\tthis, parentTypeModel, relativePrefix,\n\t\t\t\tcomposedRemainingDepth, composedFilterExplicitlyIncludedPaths,\n\t\t\t\tcomposedFilterProperExplicitlyIncludedPaths\n\t\t);\n\t}",
        "diffSourceCode": "-  149: \tpublic IndexSchemaFilter composeWithNested(MappableTypeModel parentTypeModel, String relativePrefix,\n-  150: \t\t\tInteger maxDepth, Set<String> includePaths) {\n-  151: \t\tString cyclicRecursionPath = getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n-  152: \t\tif ( cyclicRecursionPath != null ) {\n-  153: \t\t\tcyclicRecursionPath += relativePrefix;\n-  154: \t\t\tthrow log.indexedEmbeddedCyclicRecursion( cyclicRecursionPath, parentTypeModel );\n-  155: \t\t}\n-  156: \n-  157: \t\tSet<String> nullSafeIncludePaths = includePaths == null ? Collections.emptySet() : includePaths;\n+  149: \t}\n+  150: \n+  151: \tpublic Set<String> getProperExplicitlyIncludedPaths() {\n+  152: \t\treturn properExplicitlyIncludedPaths;\n+  153: \t}\n+  154: \n+  155: \tpublic Map<String, Boolean> getEncounteredFieldPaths() {\n+  156: \t\treturn encounteredFieldPaths;\n+  157: \t}\n   158: \n-  159: \t\t// The remaining composition depth according to \"this\" only\n-  160: \t\tInteger currentRemainingDepth = remainingCompositionDepth == null ? null : remainingCompositionDepth - 1;\n-  161: \n-  162: \t\t// The remaining composition depth according to the nested IndexedEmbedded only\n-  163: \t\tInteger nestedRemainingDepth = maxDepth;\n-  164: \t\tif ( maxDepth == null && !nullSafeIncludePaths.isEmpty() ) {\n-  165: \t\t\t/*\n-  166: \t\t\t * If no max depth was provided and \"includePaths\" was provided,\n-  167: \t\t\t * the remaining composition depth is implicitly set to 0,\n-  168: \t\t\t * meaning no composition is allowed and paths are excluded unless\n-  169: \t\t\t * explicitly listed in \"includePaths\".\n-  170: \t\t\t */\n-  171: \t\t\tnestedRemainingDepth = 0;\n-  172: \t\t}\n-  173: \n-  174: \t\t/*\n-  175: \t\t * By default, a composed filters' remaining composition depth is its parent's minus one\n-  176: \t\t * (or null if the remaining composition depth was not set in the parent)...\n-  177: \t\t */\n-  178: \t\tInteger composedRemainingDepth = currentRemainingDepth;\n-  179: \t\tif ( composedRemainingDepth == null\n-  180: \t\t\t\t|| nestedRemainingDepth != null && composedRemainingDepth > nestedRemainingDepth ) {\n-  181: \t\t\t/*\n-  182: \t\t\t * ... but the nested filter can override it.\n-  183: \t\t\t */\n-  184: \t\t\tcomposedRemainingDepth = nestedRemainingDepth;\n-  185: \t\t}\n-  186: \n-  187: \t\tSet<String> composedFilterExplicitlyIncludedPaths = composeExplicitlyIncludedPaths(\n-  188: \t\t\t\trelativePrefix, nullSafeIncludePaths, currentRemainingDepth, nestedRemainingDepth\n-  189: \t\t);\n-  190: \n-  191: \t\treturn new IndexSchemaFilter(\n-  192: \t\t\t\tthis, parentTypeModel, relativePrefix,\n-  193: \t\t\t\tcomposedRemainingDepth, composedFilterExplicitlyIncludedPaths\n-  194: \t\t);\n-  195: \t}\n+  159: \tprivate String getPathFromSameIndexedEmbeddedSinceNoCompositionLimits(MappableTypeModel parentTypeModel, String relativePrefix) {\n+  160: \t\tif ( hasCompositionLimits() ) {\n+  161: \t\t\treturn null;\n+  162: \t\t}\n+  163: \t\telse if ( parent != null ) {\n+  164: \t\t\tif ( this.relativePrefix.equals( relativePrefix )\n+  165: \t\t\t\t\t&& this.parentTypeModel.isSubTypeOf( parentTypeModel ) ) {\n+  166: \t\t\t\t// Same IndexedEmbedded as the one passed as a parameter\n+  167: \t\t\t\treturn this.relativePrefix;\n+  168: \t\t\t}\n+  169: \t\t\telse {\n+  170: \t\t\t\tString path = parent.getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n+  171: \t\t\t\treturn path == null ? null : path + this.relativePrefix;\n+  172: \t\t\t}\n+  173: \t\t}\n+  174: \t\telse {\n+  175: \t\t\t/*\n+  176: \t\t\t * No recursion limits, no parent: this is the root.\n+  177: \t\t\t * I we reach this point, it means there was no recursion limit at all,\n+  178: \t\t\t * but we did not encounter the IndexedEmbedded we were looking for.\n+  179: \t\t\t */\n+  180: \t\t\treturn null;\n+  181: \t\t}\n+  182: \t}\n+  183: \n+  184: \tpublic IndexSchemaFilter composeWithNested(MappableTypeModel parentTypeModel, String relativePrefix,\n+  185: \t\t\tInteger maxDepth, Set<String> includePaths) {\n+  186: \t\tString cyclicRecursionPath = getPathFromSameIndexedEmbeddedSinceNoCompositionLimits( parentTypeModel, relativePrefix );\n+  187: \t\tif ( cyclicRecursionPath != null ) {\n+  188: \t\t\tcyclicRecursionPath += relativePrefix;\n+  189: \t\t\tthrow log.indexedEmbeddedCyclicRecursion( cyclicRecursionPath, parentTypeModel );\n+  190: \t\t}\n+  191: \n+  192: \t\tSet<String> nullSafeIncludePaths = includePaths == null ? Collections.emptySet() : includePaths;\n+  193: \n+  194: \t\t// The remaining composition depth according to \"this\" only\n+  195: \t\tInteger currentRemainingDepth = remainingCompositionDepth == null ? null : remainingCompositionDepth - 1;\n   196: \n-  197: \tprivate Set<String> composeExplicitlyIncludedPaths(String relativePrefix, Set<String> nullSafeIncludePaths,\n-  198: \t\t\tInteger currentRemainingDepth, Integer nestedRemainingDepth) {\n-  199: \t\tSet<String> composedFilterExplicitlyIncludedPaths = new HashSet<>();\n-  200: \t\t/*\n-  201: \t\t * Add the nested filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n-  202: \t\t * provided they are not filtered out by the current filter.\n-  203: \t\t */\n-  204: \t\tfor ( String path : nullSafeIncludePaths ) {\n-  205: \t\t\tif ( isPathIncluded( currentRemainingDepth, explicitlyIncludedPaths, relativePrefix + path ) ) {\n-  206: \t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( path );\n-  207: \t\t\t\t// Also add paths leading to this path (so that object nodes are not excluded)\n-  208: \t\t\t\tint afterPreviousDotIndex = 0;\n-  209: \t\t\t\tint nextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n-  210: \t\t\t\twhile ( nextDotIndex >= 0 ) {\n-  211: \t\t\t\t\tString subPath = path.substring( 0, nextDotIndex );\n-  212: \t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( subPath );\n-  213: \t\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n-  214: \t\t\t\t\tnextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n-  215: \t\t\t\t}\n-  216: \t\t\t}\n-  217: \t\t}\n-  218: \t\t/*\n-  219: \t\t * Add the current filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n-  220: \t\t * provided they start with the nested filter's prefix and are not filtered out by the nested filter.\n-  221: \t\t */\n-  222: \t\tint relativePrefixLength = relativePrefix.length();\n-  223: \t\tfor ( String path : explicitlyIncludedPaths ) {\n-  224: \t\t\tif ( path.startsWith( relativePrefix ) ) {\n-  225: \t\t\t\tString pathRelativeToNestedFilter = path.substring( relativePrefixLength );\n-  226: \t\t\t\tif ( isPathIncluded( nestedRemainingDepth, nullSafeIncludePaths, pathRelativeToNestedFilter ) ) {\n-  227: \t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( pathRelativeToNestedFilter );\n-  228: \t\t\t\t}\n-  229: \t\t\t}\n-  230: \t\t}\n-  231: \t\treturn composedFilterExplicitlyIncludedPaths;\n-  232: \t}\n-  233: \n-  234: \tprivate static boolean isPathIncluded(Integer remainingDepth, Set<String> explicitlyIncludedPaths, String relativePath) {\n-  235: \t\treturn isEveryPathIncludedByDefault( remainingDepth )\n-  236: \t\t\t\t|| explicitlyIncludedPaths.contains( relativePath );\n-  237: \t}\n-  238: \n-  239: \tprivate static boolean isEveryPathIncludedByDefault(Integer remainingDepth) {\n-  240: \t\t/*\n-  241: \t\t * A remaining composition depth of 0 or below means\n-  242: \t\t * paths should be excluded when filtering unless mentioned in explicitlyIncludedPaths.\n-  243: \t\t */\n-  244: \t\treturn remainingDepth == null || remainingDepth > 0;\n-  245: \t}\n-  246: \n-  247: \tprivate boolean isAnyPathExplicitlyIncluded() {\n-  248: \t\treturn !explicitlyIncludedPaths.isEmpty();\n-  249: \t}\n-  250: \n-  251: \tprivate boolean hasCompositionLimits() {\n-  252: \t\treturn remainingCompositionDepth != null || !explicitlyIncludedPaths.isEmpty();\n-  253: \t}\n-  254: }\n+  197: \t\t// The remaining composition depth according to the nested IndexedEmbedded only\n+  198: \t\tInteger nestedRemainingDepth = maxDepth;\n+  199: \t\tif ( maxDepth == null && !nullSafeIncludePaths.isEmpty() ) {\n+  200: \t\t\t/*\n+  201: \t\t\t * If no max depth was provided and \"includePaths\" was provided,\n+  202: \t\t\t * the remaining composition depth is implicitly set to 0,\n+  203: \t\t\t * meaning no composition is allowed and paths are excluded unless\n+  204: \t\t\t * explicitly listed in \"includePaths\".\n+  205: \t\t\t */\n+  206: \t\t\tnestedRemainingDepth = 0;\n+  207: \t\t}\n+  208: \n+  209: \t\t/*\n+  210: \t\t * By default, a composed filters' remaining composition depth is its parent's minus one\n+  211: \t\t * (or null if the remaining composition depth was not set in the parent)...\n+  212: \t\t */\n+  213: \t\tInteger composedRemainingDepth = currentRemainingDepth;\n+  214: \t\tif ( composedRemainingDepth == null\n+  215: \t\t\t\t|| nestedRemainingDepth != null && composedRemainingDepth > nestedRemainingDepth ) {\n+  216: \t\t\t/*\n+  217: \t\t\t * ... but the nested filter can override it.\n+  218: \t\t\t */\n+  219: \t\t\tcomposedRemainingDepth = nestedRemainingDepth;\n+  220: \t\t}\n+  221: \n+  222: \t\t// The included paths that will be used to determine inclusion\n+  223: \t\tSet<String> composedFilterExplicitlyIncludedPaths = new HashSet<>();\n+  224: \t\t// The subset of these included paths that were added by the nested filter (and not a parent filter)\n+  225: \t\t// Use a LinkedHashSet, since the set will be exposed through a getter and may be iterated on\n+  226: \t\tSet<String> composedFilterProperExplicitlyIncludedPaths = new LinkedHashSet<>();\n+  227: \n+  228: \t\t/*\n+  229: \t\t * Add the nested filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n+  230: \t\t * provided they are not filtered out by the current filter.\n+  231: \t\t */\n+  232: \t\tfor ( String path : nullSafeIncludePaths ) {\n+  233: \t\t\tif ( isPathIncluded( currentRemainingDepth, explicitlyIncludedPaths, relativePrefix + path ) ) {\n+  234: \t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( path );\n+  235: \t\t\t\tcomposedFilterProperExplicitlyIncludedPaths.add( path );\n+  236: \t\t\t\t// Also add paths leading to this path (so that object nodes are not excluded)\n+  237: \t\t\t\tint afterPreviousDotIndex = 0;\n+  238: \t\t\t\tint nextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n+  239: \t\t\t\twhile ( nextDotIndex >= 0 ) {\n+  240: \t\t\t\t\tString subPath = path.substring( 0, nextDotIndex );\n+  241: \t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( subPath );\n+  242: \t\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n+  243: \t\t\t\t\tnextDotIndex = path.indexOf( '.', afterPreviousDotIndex );\n+  244: \t\t\t\t}\n+  245: \t\t\t}\n+  246: \t\t}\n+  247: \t\t/*\n+  248: \t\t * Add the current filter's explicitly included paths to the composed filter's \"explicitlyIncludedPaths\",\n+  249: \t\t * provided they start with the nested filter's prefix and are not filtered out by the nested filter.\n+  250: \t\t */\n+  251: \t\tint relativePrefixLength = relativePrefix.length();\n+  252: \t\tfor ( String path : explicitlyIncludedPaths ) {\n+  253: \t\t\tif ( path.startsWith( relativePrefix ) ) {\n+  254: \t\t\t\tString pathRelativeToNestedFilter = path.substring( relativePrefixLength );\n+  255: \t\t\t\tif ( isPathIncluded( nestedRemainingDepth, nullSafeIncludePaths, pathRelativeToNestedFilter ) ) {\n+  256: \t\t\t\t\tcomposedFilterExplicitlyIncludedPaths.add( pathRelativeToNestedFilter );\n+  257: \t\t\t\t}\n+  258: \t\t\t}\n+  259: \t\t}\n+  260: \n+  261: \t\treturn new IndexSchemaFilter(\n+  262: \t\t\t\tthis, parentTypeModel, relativePrefix,\n+  263: \t\t\t\tcomposedRemainingDepth, composedFilterExplicitlyIncludedPaths,\n+  264: \t\t\t\tcomposedFilterProperExplicitlyIncludedPaths\n+  265: \t\t);\n+  266: \t}\n",
        "uniqueId": "d5122d457575873143ef1afe52a80a4322bc5181_149_195__184_266_197_232",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 94
            },
            "BRANCH": {
                "missed": 0,
                "covered": 12
            },
            "LINE": {
                "missed": 0,
                "covered": 21
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 7
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Move Method",
        "description": "Move Method\tpublic containsDocument(id String, assertions Consumer<DocumentAssert>) : Consumer<List<? extends Document>> from class org.hibernate.search.integrationtest.backend.lucene.LuceneExtensionIT to public containsDocument(id String, assertions Consumer<DocumentAssert>) : Consumer<List<? extends Document>> from class org.hibernate.search.integrationtest.backend.lucene.testsupport.util.DocumentAssert",
        "diffLocations": [
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/LuceneExtensionIT.java",
                "startLine": 565,
                "endLine": 575,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/testsupport/util/DocumentAssert.java",
                "startLine": 31,
                "endLine": 51,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public static Consumer<List<? extends Document>> containsDocument(String id, Consumer<DocumentAssert> assertions) {\n\t\treturn allDocuments -> {\n\t\t\tOptional<? extends Document> found = allDocuments.stream()\n\t\t\t\t\t.filter( doc -> id.equals( doc.get( LuceneFields.idFieldName() ) ) )\n\t\t\t\t\t.findFirst();\n\t\t\tAssertions.assertThat( found )\n\t\t\t\t\t.as( \"Document with ID '\" + id + \"'\" )\n\t\t\t\t\t.isNotEmpty();\n\t\t\tassertions.accept( new DocumentAssert( found.get() ).as( id ) );\n\t\t};\n\t}",
        "filePathBefore": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/LuceneExtensionIT.java",
        "isPureRefactoring": true,
        "commitId": "ed590b17174a39379c8007229feeef79e15aeac4",
        "packageNameBefore": "org.hibernate.search.integrationtest.backend.lucene",
        "classNameBefore": "org.hibernate.search.integrationtest.backend.lucene.LuceneExtensionIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.backend.lucene.LuceneExtensionIT#containsDocument",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.backend.lucene.testsupport.util.DocumentAssert#as\n methodBody: public DocumentAssert as(String name) {\nthis.name=name;\nreturn this;\n}",
        "classSignatureBefore": "public class LuceneExtensionIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.lucene.LuceneExtensionIT#containsDocument"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.lucene.LuceneExtensionIT"
        ],
        "classSignatureBeforeSet": [
            "public class LuceneExtensionIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.lucene;\n\nimport static org.hibernate.search.util.impl.integrationtest.common.assertion.SearchResultAssert.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMapperUtils.referenceProvider;\n\nimport java.util.List;\nimport java.util.Optional;\nimport java.util.function.Consumer;\nimport java.util.stream.Collectors;\n\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field.Store;\nimport org.apache.lucene.document.IntPoint;\nimport org.apache.lucene.document.LatLonPoint;\nimport org.apache.lucene.document.NumericDocValuesField;\nimport org.apache.lucene.document.StringField;\nimport org.apache.lucene.index.IndexableField;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.Explanation;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.search.SortField.Type;\nimport org.apache.lucene.search.TermQuery;\nimport org.assertj.core.api.Assertions;\n\nimport org.hibernate.search.backend.lucene.LuceneBackend;\nimport org.hibernate.search.backend.lucene.index.LuceneIndexManager;\nimport org.hibernate.search.backend.lucene.util.impl.LuceneFields;\nimport org.hibernate.search.engine.backend.Backend;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldAccessor;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.types.Projectable;\nimport org.hibernate.search.engine.backend.types.Sortable;\nimport org.hibernate.search.engine.backend.index.IndexManager;\nimport org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\nimport org.hibernate.search.engine.common.spi.SearchIntegration;\nimport org.hibernate.search.integrationtest.backend.lucene.testsupport.util.DocumentAssert;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMappingIndexManager;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMappingSearchTarget;\nimport org.hibernate.search.backend.lucene.LuceneExtension;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.rule.SearchSetupHelper;\nimport org.hibernate.search.engine.reporting.spi.EventContexts;\nimport org.hibernate.search.engine.search.DocumentReference;\nimport org.hibernate.search.engine.search.SearchPredicate;\nimport org.hibernate.search.engine.search.SearchQuery;\nimport org.hibernate.search.engine.search.SearchSort;\nimport org.hibernate.search.engine.spatial.GeoPoint;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.FailureReportUtils;\nimport org.hibernate.search.util.impl.test.SubTest;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\n\npublic class LuceneExtensionIT {\n\n\tprivate static final String BACKEND_NAME = \"myLuceneBackend\";\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\n\tprivate static final String FIRST_ID = \"1\";\n\tprivate static final String SECOND_ID = \"2\";\n\tprivate static final String THIRD_ID = \"3\";\n\tprivate static final String FOURTH_ID = \"4\";\n\tprivate static final String FIFTH_ID = \"5\";\n\n\t@Rule\n\tpublic SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic ExpectedException thrown = ExpectedException.none();\n\n\tprivate SearchIntegration integration;\n\tprivate IndexAccessors indexAccessors;\n\tprivate StubMappingIndexManager indexManager;\n\n\t@Before\n\tpublic void setup() {\n\t\tthis.integration = setupHelper.withDefaultConfiguration( BACKEND_NAME )\n\t\t\t\t.withIndex(\n\t\t\t\t\t\t\"MappedType\", INDEX_NAME,\n\t\t\t\t\t\tctx -> this.indexAccessors = new IndexAccessors( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.indexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.setup();\n\n\t\tinitData();\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery() {\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query()\n\t\t\t\t.asReference()\n\t\t\t\t.predicate( f -> f.bool()\n\t\t\t\t\t\t.should( f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.should( f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.should( f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) )\n\t\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery_separatePredicate() {\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\n\t\tSearchPredicate predicate1 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) ).toPredicate();\n\t\tSearchPredicate predicate2 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) ).toPredicate();\n\t\tSearchPredicate predicate3 = searchTarget.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) ).toPredicate();\n\t\tSearchPredicate booleanPredicate = searchTarget.predicate().bool()\n\t\t\t\t.should( predicate1 )\n\t\t\t\t.should( predicate2 )\n\t\t\t\t.should( predicate3 )\n\t\t\t\t.toPredicate();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query()\n\t\t\t\t.asReference()\n\t\t\t\t.predicate( booleanPredicate )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField() {\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query()\n\t\t\t\t.asReference()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasDocRefHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\n\t\tquery = searchTarget.query()\n\t\t\t\t.asReference()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( c -> c\n\t\t\t\t\t\t.extension().ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSort( new Sort(\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.build();\n\t\tassertThat( query ).hasDocRefHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tTHIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField_separateSort() {\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\n\t\tSearchSort sort1 = searchTarget.sort().extension()\n\t\t\t\t\t\t.ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.orElseFail()\n\t\t\t\t.toSort();\n\t\tSearchSort sort2 = searchTarget.sort().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t.toSort();\n\t\tSearchSort sort3 = searchTarget.sort().extension()\n\t\t\t\t.ifSupported(\n\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.orElseFail()\n\t\t\t\t.toSort();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query()\n\t\t\t\t.asReference()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort1 ).then().by( sort2 ).then().by( sort3 ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID );\n\n\t\tSearchSort sort = searchTarget.sort()\n\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneSort( new Sort(\n\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.toSort();\n\n\t\tquery = searchTarget.query()\n\t\t\t\t.asReference()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( c -> c.by( sort ) )\n\t\t\t\t.build();\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsExactOrder( INDEX_NAME, THIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_throwsException() {\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"match() predicate on unsupported native field\",\n\t\t\t\t() -> searchTarget.query()\n\t\t\t\t\t\t.asReference()\n\t\t\t\t\t\t.predicate( f -> f.match().onField( \"nativeField\" ).matching( \"37\" ) )\n\t\t\t\t\t\t.build()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining predicates with the DSL: use the Lucene extension and a native query.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void sort_nativeField_throwsException() {\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"sort on unsupported native field\",\n\t\t\t\t() -> searchTarget.query()\n\t\t\t\t\t\t.asReference()\n\t\t\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t\t\t.sort( c -> c.byField( \"nativeField\" ) )\n\t\t\t\t\t\t.build()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining sorts with the DSL: use the Lucene extension and a native sort.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_nativeQuery() {\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query()\n\t\t\t\t.asReference()\n\t\t\t\t.predicate( f -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"nativeField\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField() {\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\n\t\tSearchQuery<Integer> query = searchTarget.query()\n\t\t\t\t.asProjection( f -> f.field( \"nativeField\", Integer.class ) )\n\t\t\t\t.predicate( f -> f.match().onField( \"string\" ).matching( \"text 1\" ) )\n\t\t\t\t.build();\n\n\t\tassertThat( query ).hasHitsAnyOrder( 37 );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField_unsupportedProjection() {\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\n\t\t// let's check that it's possible to query the field beforehand\n\t\tSearchQuery<DocumentReference> query = searchTarget.query()\n\t\t\t\t.asReference()\n\t\t\t\t.predicate( f -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"nativeField_unsupportedProjection\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\n\t\t// now, let's check that projecting on the field throws an exception\n\t\tSubTest.expectException(\n\t\t\t\t\"projection on native field not supporting projections\",\n\t\t\t\t() -> searchTarget.projection().field( \"nativeField_unsupportedProjection\", Integer.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Projections are not enabled for field\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField_unsupportedProjection\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void sort_nativeField_nativeSort() {\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\n\t\tSearchQuery<DocumentReference> query = searchTarget.query()\n\t\t\t\t.asReference()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( c -> c.extension( LuceneExtension.get() ).fromLuceneSortField( new SortField( \"nativeField\", Type.LONG ) ) )\n\t\t\t\t.build();\n\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsExactOrder( INDEX_NAME, THIRD_ID, FIRST_ID, FIFTH_ID, SECOND_ID, FOURTH_ID );\n\t}\n\n\t@Test\n\tpublic void projection_document() {\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\n\t\tSearchQuery<Document> query = searchTarget.query()\n\t\t\t\t.asProjection(\n\t\t\t\t\t\tf -> f.extension( LuceneExtension.get() ).document()\n\t\t\t\t)\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.build();\n\n\t\tList<Document> result = query.execute().getHits();\n\t\tAssertions.assertThat( result )\n\t\t\t\t.hasSize( 5 )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tFIRST_ID,\n\t\t\t\t\t\tdoc -> doc.hasField( \"string\", \"text 1\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField\", \"37\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"37\" )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tSECOND_ID,\n\t\t\t\t\t\tdoc -> doc.hasField( \"integer\", 2 )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField\", \"78\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"78\" )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tTHIRD_ID,\n\t\t\t\t\t\tdoc -> doc.hasField( \"nativeField\", \"13\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"13\" )\n\t\t\t\t\t\t\t\t// Geo points are stored as two internal fields\n\t\t\t\t\t\t\t\t.hasInternalField( \"geoPoint_latitude\", 40.12 )\n\t\t\t\t\t\t\t\t.hasInternalField( \"geoPoint_longitude\", -71.34 )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tFOURTH_ID,\n\t\t\t\t\t\tdoc -> doc.hasField( \"nativeField\", \"89\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"89\" )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tFIFTH_ID,\n\t\t\t\t\t\tdoc -> doc.hasField( \"string\", \"text 2\" )\n\t\t\t\t\t\t\t\t.hasField( \"integer\", 1 )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField\", \"53\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"53\" )\n\t\t\t\t\t\t\t\t// Geo points are stored as two internal fields\n\t\t\t\t\t\t\t\t.hasInternalField( \"geoPoint_latitude\", 45.12 )\n\t\t\t\t\t\t\t\t.hasInternalField( \"geoPoint_longitude\", -75.34 )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) );\n\t}\n\n\t/**\n\t * Check that the projection on a document includes all fields,\n\t * even if there is a field projection, which would usually trigger document filtering.\n\t */\n\t@Test\n\tpublic void projection_documentAndField() {\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\n\t\tSearchQuery<List<?>> query = searchTarget.query()\n\t\t\t\t.asProjection( f ->\n\t\t\t\t\t\tf.composite(\n\t\t\t\t\t\t\t\tf.extension( LuceneExtension.get() ).document(),\n\t\t\t\t\t\t\t\tf.field( \"string\" )\n\t\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.build();\n\n\t\tList<Document> result = query.execute().getHits().stream()\n\t\t\t\t.map( list -> (Document) list.get( 0 ) )\n\t\t\t\t.collect( Collectors.toList() );\n\t\tAssertions.assertThat( result )\n\t\t\t\t.hasSize( 1 )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tFIRST_ID,\n\t\t\t\t\t\tdoc -> doc.hasField( \"string\", \"text 1\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField\", \"37\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"37\" )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void projection_explanation() {\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\n\t\tSearchQuery<Explanation> query = searchTarget.query()\n\t\t\t\t.asProjection( f -> f.extension( LuceneExtension.get() ).explanation() )\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.build();\n\n\t\tList<Explanation> result = query.execute().getHits();\n\t\tAssertions.assertThat( result ).hasSize( 1 );\n\t\tAssertions.assertThat( result.get( 0 ) ).isInstanceOf( Explanation.class );\n\t\tAssertions.assertThat( result.get( 0 ).toString() )\n\t\t\t\t.contains( LuceneFields.idFieldName() );\n\t}\n\n\t@Test\n\tpublic void nativeField_invalidFieldPath() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"native field contributing field with invalid field path\",\n\t\t\t\t() -> workPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\t\t\tindexAccessors.nativeField_invalidFieldPath.write( document, 45 );\n\t\t\t\t} ) )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Invalid field path; expected path 'nativeField_invalidFieldPath', got 'not the expected path'.\" );\n\t}\n\n\t@Test\n\tpublic void backend_unwrap() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tAssertions.assertThat( backend.unwrap( LuceneBackend.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void backend_unwrap_error_unknownType() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene backend to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this backend can only be unwrapped to '\" + LuceneBackend.class.getName() + \"'\" );\n\n\t\tbackend.unwrap( String.class );\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\t\tAssertions.assertThat( indexManager.unwrap( LuceneIndexManager.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap_error_unknownType() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene index manager to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this index manager can only be unwrapped to '\" + LuceneIndexManager.class.getName() + \"'\" );\n\n\t\tindexManager.unwrap( String.class );\n\t}\n\n\tprivate void initData() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan();\n\t\tworkPlan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tindexAccessors.string.write( document, \"text 1\" );\n\n\t\t\tindexAccessors.nativeField.write( document, 37 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 37 );\n\n\t\t\tindexAccessors.sort1.write( document, \"a\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tindexAccessors.integer.write( document, 2 );\n\n\t\t\tindexAccessors.nativeField.write( document, 78 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 78 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"a\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tindexAccessors.geoPoint.write( document, GeoPoint.of( 40.12, -71.34 ) );\n\n\t\t\tindexAccessors.nativeField.write( document, 13 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 13 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"a\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tindexAccessors.nativeField.write( document, 89 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 89 );\n\n\t\t\tindexAccessors.sort1.write( document, \"z\" );\n\t\t\tindexAccessors.sort2.write( document, \"z\" );\n\t\t\tindexAccessors.sort3.write( document, \"z\" );\n\t\t} );\n\t\tworkPlan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tindexAccessors.string.write( document, \"text 2\" );\n\t\t\tindexAccessors.integer.write( document, 1 );\n\t\t\tindexAccessors.geoPoint.write( document, GeoPoint.of( 45.12, -75.34 ) );\n\n\t\t\tindexAccessors.nativeField.write( document, 53 );\n\t\t\tindexAccessors.nativeField_unsupportedProjection.write( document, 53 );\n\n\t\t\tindexAccessors.sort1.write( document, \"zz\" );\n\t\t\tindexAccessors.sort2.write( document, \"zz\" );\n\t\t\tindexAccessors.sort3.write( document, \"zz\" );\n\t\t} );\n\n\t\tworkPlan.execute().join();\n\n\t\t// Check that all documents are searchable\n\t\tStubMappingSearchTarget searchTarget = indexManager.createSearchTarget();\n\t\tSearchQuery<DocumentReference> query = searchTarget.query()\n\t\t\t\t.asReference()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.build();\n\t\tassertThat( query ).hasDocRefHitsAnyOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}\n\n\tpublic static Consumer<List<? extends Document>> containsDocument(String id, Consumer<DocumentAssert> assertions) {\n\t\treturn allDocuments -> {\n\t\t\tOptional<? extends Document> found = allDocuments.stream()\n\t\t\t\t\t.filter( doc -> id.equals( doc.get( LuceneFields.idFieldName() ) ) )\n\t\t\t\t\t.findFirst();\n\t\t\tAssertions.assertThat( found )\n\t\t\t\t\t.as( \"Document with ID '\" + id + \"'\" )\n\t\t\t\t\t.isNotEmpty();\n\t\t\tassertions.accept( new DocumentAssert( found.get() ).as( id ) );\n\t\t};\n\t}\n\n\tprivate static class IndexAccessors {\n\t\tfinal IndexFieldAccessor<Integer> integer;\n\t\tfinal IndexFieldAccessor<String> string;\n\t\tfinal IndexFieldAccessor<GeoPoint> geoPoint;\n\t\tfinal IndexFieldAccessor<Integer> nativeField;\n\t\tfinal IndexFieldAccessor<Integer> nativeField_unsupportedProjection;\n\t\tfinal IndexFieldAccessor<Integer> nativeField_invalidFieldPath;\n\n\t\tfinal IndexFieldAccessor<String> sort1;\n\t\tfinal IndexFieldAccessor<String> sort2;\n\t\tfinal IndexFieldAccessor<String> sort3;\n\n\t\tIndexAccessors(IndexSchemaElement root) {\n\t\t\tinteger = root.field(\n\t\t\t\t\t\"integer\",\n\t\t\t\t\tf -> f.asInteger().projectable( Projectable.YES )\n\t\t\t)\n\t\t\t\t\t.createAccessor();\n\t\t\tstring = root.field(\n\t\t\t\t\t\"string\",\n\t\t\t\t\tf -> f.asString().projectable( Projectable.YES )\n\t\t\t)\n\t\t\t\t\t.createAccessor();\n\t\t\tgeoPoint = root.field(\n\t\t\t\t\t\"geoPoint\",\n\t\t\t\t\tf -> f.asGeoPoint().projectable( Projectable.YES )\n\t\t\t)\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField = root.field(\n\t\t\t\t\t\"nativeField\",\n\t\t\t\t\tf -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.asLuceneField( Integer.class, LuceneExtensionIT::contributeNativeField, LuceneExtensionIT::fromNativeField )\n\t\t\t)\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField_unsupportedProjection = root.field(\n\t\t\t\t\t\"nativeField_unsupportedProjection\",\n\t\t\t\t\tf -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.asLuceneField( Integer.class, LuceneExtensionIT::contributeNativeField )\n\t\t\t)\n\t\t\t\t\t.createAccessor();\n\t\t\tnativeField_invalidFieldPath = root.field(\n\t\t\t\t\t\"nativeField_invalidFieldPath\",\n\t\t\t\t\tf -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.asLuceneField( Integer.class, LuceneExtensionIT::contributeNativeFieldInvalidFieldPath )\n\t\t\t)\n\t\t\t\t\t.createAccessor();\n\n\t\t\tsort1 = root.field( \"sort1\", f -> f.asString().sortable( Sortable.YES ) )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort2 = root.field( \"sort2\", f -> f.asString().sortable( Sortable.YES ) )\n\t\t\t\t\t.createAccessor();\n\t\t\tsort3 = root.field( \"sort3\", f -> f.asString().sortable( Sortable.YES ) )\n\t\t\t\t\t.createAccessor();\n\t\t}\n\t}\n\n\tprivate static void contributeNativeField(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( absoluteFieldPath, value.toString(), Store.YES ) );\n\t\tcollector.accept( new NumericDocValuesField( absoluteFieldPath, value.longValue() ) );\n\t}\n\n\tprivate static Integer fromNativeField(IndexableField field) {\n\t\treturn Integer.parseInt( field.stringValue() );\n\t}\n\n\tprivate static void contributeNativeFieldInvalidFieldPath(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( \"not the expected path\", value.toString(), Store.YES ) );\n\t}\n}\n",
        "filePathAfter": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/testsupport/util/DocumentAssert.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.lucene.testsupport.util;\n\nimport java.util.Arrays;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Objects;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.function.Consumer;\nimport java.util.function.Predicate;\n\nimport org.hibernate.search.backend.lucene.util.impl.LuceneFields;\n\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.index.IndexableField;\nimport org.apache.lucene.util.BytesRef;\nimport org.assertj.core.api.Assertions;\nimport org.assertj.core.api.Condition;\nimport org.assertj.core.api.ListAssert;\nimport org.assertj.core.api.iterable.Extractor;\n\npublic class DocumentAssert {\n\tprivate static final String INTERNAL_FIELDS_PREFIX = \"__HSEARCH_\";\n\n\t/**\n\t * Creates a consumer that checks that a list of documents contains a document with the given ID,\n\t * and that this document passes the given assertion.\n\t * <p>\n\t * The consumer should generally be passed to {@link ListAssert#satisfies(java.util.function.Consumer)}.\n\t *\n\t * @param id The ID of the document that should be contained within the list.\n\t * @param assertions An assertion that should pass on the document with the given id.\n\t * @return A consumer to be passed to {@link ListAssert#satisfies(java.util.function.Consumer)}.\n\t */\n\tpublic static Consumer<List<? extends Document>> containsDocument(String id, Consumer<DocumentAssert> assertions) {\n\t\treturn allDocuments -> {\n\t\t\tOptional<? extends Document> found = allDocuments.stream()\n\t\t\t\t\t.filter( doc -> id.equals( doc.get( LuceneFields.idFieldName() ) ) )\n\t\t\t\t\t.findFirst();\n\t\t\tAssertions.assertThat( found )\n\t\t\t\t\t.as( \"Document with ID '\" + id + \"'\" )\n\t\t\t\t\t.isNotEmpty();\n\t\t\tassertions.accept( new DocumentAssert( found.get() ).as( id ) );\n\t\t};\n\t}\n\n\tprivate final Document actual;\n\tprivate String name;\n\n\tprivate Set<String> allCheckedPaths = new HashSet<>();\n\n\tpublic DocumentAssert(Document actual) {\n\t\tthis.actual = actual;\n\t}\n\n\tpublic DocumentAssert as(String name) {\n\t\tthis.name = name;\n\t\treturn this;\n\t}\n\n\tprivate ListAssert<IndexableField> asFields() {\n\t\treturn Assertions.assertThat( actual.getFields() );\n\t}\n\n\tpublic DocumentAssert hasField(String absoluteFieldPath, String ... values) {\n\t\treturn hasField( \"string\", absoluteFieldPath, values );\n\t}\n\n\tpublic DocumentAssert hasField(String absoluteFieldPath, Number ... values) {\n\t\treturn hasField( \"numeric\", absoluteFieldPath, values );\n\t}\n\n\tpublic DocumentAssert hasInternalField(String absoluteFieldPath, String ... values) {\n\t\treturn hasField( INTERNAL_FIELDS_PREFIX + absoluteFieldPath, values );\n\t}\n\n\tpublic DocumentAssert hasInternalField(String absoluteFieldPath, Number ... values) {\n\t\treturn hasField( INTERNAL_FIELDS_PREFIX + absoluteFieldPath, values );\n\t}\n\n\t@SafeVarargs\n\tprivate final <T> DocumentAssert hasField(String type, String absoluteFieldPath, T ... values) {\n\t\tString fieldDescription = \"field of document '\" + name + \"' at path '\" + absoluteFieldPath + \"'\"\n\t\t\t\t+ \" with type '\" + type + \"' and values '\" + Arrays.toString( values ) + \"'\";\n\t\tPredicate<IndexableField> predicate = field -> absoluteFieldPath.equals( field.name() );\n\t\tasFields()\n\t\t\t\t.areAtLeastOne( new Condition<>( predicate, fieldDescription ) )\n\t\t\t\t.filteredOn( predicate )\n\t\t\t\t.extracting( (Extractor<IndexableField, Object>) f -> {\n\t\t\t\t\t// We can't just return everything and then exclude nulls,\n\t\t\t\t\t// since .stringValue() converts a number to a string automatically...\n\t\t\t\t\tNumber number = f.numericValue();\n\t\t\t\t\tif ( number != null ) {\n\t\t\t\t\t\treturn number;\n\t\t\t\t\t}\n\t\t\t\t\tBytesRef bytesRef = f.binaryValue();\n\t\t\t\t\tif ( bytesRef != null ) {\n\t\t\t\t\t\treturn bytesRef.bytes;\n\t\t\t\t\t}\n\t\t\t\t\tString string = f.stringValue();\n\t\t\t\t\tif ( string != null ) {\n\t\t\t\t\t\treturn string;\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t} )\n\t\t\t\t.filteredOn( Objects::nonNull )\n\t\t\t\t.as( fieldDescription )\n\t\t\t\t.containsExactlyInAnyOrder( values );\n\t\tallCheckedPaths.add( absoluteFieldPath );\n\t\treturn this;\n\t}\n\n\tpublic void andOnlyInternalFields() {\n\t\tSet<String> allowedPaths = new HashSet<>( allCheckedPaths );\n\t\tallowedPaths.add( LuceneFields.idFieldName() );\n\t\tallowedPaths.add( LuceneFields.indexFieldName() );\n\t\tallowedPaths.add( LuceneFields.typeFieldName() );\n\t\tallowedPaths.add( LuceneFields.tenantIdFieldName() );\n\t\tallowedPaths.add( LuceneFields.rootIdFieldName() );\n\t\tallowedPaths.add( LuceneFields.rootIndexFieldName() );\n\t\tasFields().are( new Condition<>(\n\t\t\t\tfield -> allowedPaths.contains( field.name() ),\n\t\t\t\t\"exclusively fields with path \" + allCheckedPaths\n\t\t\t\t\t\t+ \" or prefixed with \" + INTERNAL_FIELDS_PREFIX + \" , and no other path\"\n\t\t) );\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.backend.lucene.testsupport.util.DocumentAssert#as\n methodBody: public DocumentAssert as(String name) {\nthis.name=name;\nreturn this;\n}"
        ],
        "sourceCodeAfterRefactoring": "/**\n\t * Creates a consumer that checks that a list of documents contains a document with the given ID,\n\t * and that this document passes the given assertion.\n\t * <p>\n\t * The consumer should generally be passed to {@link ListAssert#satisfies(java.util.function.Consumer)}.\n\t *\n\t * @param id The ID of the document that should be contained within the list.\n\t * @param assertions An assertion that should pass on the document with the given id.\n\t * @return A consumer to be passed to {@link ListAssert#satisfies(java.util.function.Consumer)}.\n\t */\n\tpublic static Consumer<List<? extends Document>> containsDocument(String id, Consumer<DocumentAssert> assertions) {\n\t\treturn allDocuments -> {\n\t\t\tOptional<? extends Document> found = allDocuments.stream()\n\t\t\t\t\t.filter( doc -> id.equals( doc.get( LuceneFields.idFieldName() ) ) )\n\t\t\t\t\t.findFirst();\n\t\t\tAssertions.assertThat( found )\n\t\t\t\t\t.as( \"Document with ID '\" + id + \"'\" )\n\t\t\t\t\t.isNotEmpty();\n\t\t\tassertions.accept( new DocumentAssert( found.get() ).as( id ) );\n\t\t};\n\t}",
        "diffSourceCode": "-   31: \n-   32: import org.hibernate.search.backend.lucene.LuceneBackend;\n-   33: import org.hibernate.search.backend.lucene.index.LuceneIndexManager;\n-   34: import org.hibernate.search.backend.lucene.util.impl.LuceneFields;\n-   35: import org.hibernate.search.engine.backend.Backend;\n-   36: import org.hibernate.search.engine.backend.document.DocumentElement;\n-   37: import org.hibernate.search.engine.backend.document.IndexFieldAccessor;\n-   38: import org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\n-   39: import org.hibernate.search.engine.backend.types.Projectable;\n-   40: import org.hibernate.search.engine.backend.types.Sortable;\n-   41: import org.hibernate.search.engine.backend.index.IndexManager;\n-   42: import org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\n-   43: import org.hibernate.search.engine.common.spi.SearchIntegration;\n-   44: import org.hibernate.search.integrationtest.backend.lucene.testsupport.util.DocumentAssert;\n-   45: import org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMappingIndexManager;\n-   46: import org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMappingSearchTarget;\n-   47: import org.hibernate.search.backend.lucene.LuceneExtension;\n-   48: import org.hibernate.search.integrationtest.backend.tck.testsupport.util.rule.SearchSetupHelper;\n-   49: import org.hibernate.search.engine.reporting.spi.EventContexts;\n-   50: import org.hibernate.search.engine.search.DocumentReference;\n-   51: import org.hibernate.search.engine.search.SearchPredicate;\n-  565: \tpublic static Consumer<List<? extends Document>> containsDocument(String id, Consumer<DocumentAssert> assertions) {\n-  566: \t\treturn allDocuments -> {\n-  567: \t\t\tOptional<? extends Document> found = allDocuments.stream()\n-  568: \t\t\t\t\t.filter( doc -> id.equals( doc.get( LuceneFields.idFieldName() ) ) )\n-  569: \t\t\t\t\t.findFirst();\n-  570: \t\t\tAssertions.assertThat( found )\n-  571: \t\t\t\t\t.as( \"Document with ID '\" + id + \"'\" )\n-  572: \t\t\t\t\t.isNotEmpty();\n-  573: \t\t\tassertions.accept( new DocumentAssert( found.get() ).as( id ) );\n-  574: \t\t};\n-  575: \t}\n+   31: \t/**\n+   32: \t * Creates a consumer that checks that a list of documents contains a document with the given ID,\n+   33: \t * and that this document passes the given assertion.\n+   34: \t * <p>\n+   35: \t * The consumer should generally be passed to {@link ListAssert#satisfies(java.util.function.Consumer)}.\n+   36: \t *\n+   37: \t * @param id The ID of the document that should be contained within the list.\n+   38: \t * @param assertions An assertion that should pass on the document with the given id.\n+   39: \t * @return A consumer to be passed to {@link ListAssert#satisfies(java.util.function.Consumer)}.\n+   40: \t */\n+   41: \tpublic static Consumer<List<? extends Document>> containsDocument(String id, Consumer<DocumentAssert> assertions) {\n+   42: \t\treturn allDocuments -> {\n+   43: \t\t\tOptional<? extends Document> found = allDocuments.stream()\n+   44: \t\t\t\t\t.filter( doc -> id.equals( doc.get( LuceneFields.idFieldName() ) ) )\n+   45: \t\t\t\t\t.findFirst();\n+   46: \t\t\tAssertions.assertThat( found )\n+   47: \t\t\t\t\t.as( \"Document with ID '\" + id + \"'\" )\n+   48: \t\t\t\t\t.isNotEmpty();\n+   49: \t\t\tassertions.accept( new DocumentAssert( found.get() ).as( id ) );\n+   50: \t\t};\n+   51: \t}\n",
        "uniqueId": "ed590b17174a39379c8007229feeef79e15aeac4_565_575__31_51",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Move And Rename Method",
        "description": "Move And Rename Method\tprivate SubTest(description String, thrown Throwable) from class org.hibernate.search.util.impl.test.SubTest to private ExceptionThrowingSubTest(description String, thrown Throwable) from class org.hibernate.search.util.impl.test.SubTest.ExceptionThrowingSubTest",
        "diffLocations": [
            {
                "filePath": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/SubTest.java",
                "startLine": 62,
                "endLine": 65,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/SubTest.java",
                "startLine": 81,
                "endLine": 84,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private SubTest(String description, Throwable thrown) {\n\t\tthis.description = description;\n\t\tthis.thrown = thrown;\n\t}",
        "filePathBefore": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/SubTest.java",
        "isPureRefactoring": true,
        "commitId": "f50cbfc772b75e7d7a983359b9baca383bc6ae9f",
        "packageNameBefore": "org.hibernate.search.util.impl.test",
        "classNameBefore": "org.hibernate.search.util.impl.test.SubTest",
        "methodNameBefore": "org.hibernate.search.util.impl.test.SubTest#SubTest",
        "classSignatureBefore": "public class SubTest ",
        "methodNameBeforeSet": [
            "org.hibernate.search.util.impl.test.SubTest#SubTest"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.util.impl.test.SubTest"
        ],
        "classSignatureBeforeSet": [
            "public class SubTest "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.test;\n\nimport static org.junit.Assert.fail;\n\nimport java.util.concurrent.Callable;\n\nimport org.junit.Test;\n\nimport org.assertj.core.api.AbstractThrowableAssert;\nimport org.assertj.core.api.ThrowableAssert;\n\n/**\n * A util allowing to run blocks of code (\"sub-tests\"), expecting them to throw an exception.\n * <p>\n * Useful in particular when expecting an exception for each execution of a loop,\n * in which case {@link org.junit.rules.ExpectedException} or {@link Test#expected()} cannot be used.\n * <p>\n * By default any thrown exception will be accepted; if you want to run additional checks on the thrown exception,\n * use {@link #assertThrown()}.\n */\npublic class SubTest {\n\n\tpublic static SubTest expectException(Runnable runnable) {\n\t\treturn expectException( runnable.toString(), runnable );\n\t}\n\n\tpublic static SubTest expectException(String description, Runnable runnable) {\n\t\treturn expectException(\n\t\t\t\tdescription,\n\t\t\t\t() -> {\n\t\t\t\t\trunnable.run();\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t);\n\t}\n\n\tpublic static SubTest expectException(Callable<?> callable) {\n\t\treturn expectException( callable.toString(), callable );\n\t}\n\n\tpublic static SubTest expectException(String description, Callable<?> callable) {\n\t\ttry {\n\t\t\tcallable.call();\n\t\t\tfail( \"'\" + description + \"' should have thrown an exception\" );\n\t\t\tthrow new IllegalStateException( \"This should never happen\" );\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\treturn new SubTest( \"Exception thrown by '\" + description + \"'\", e );\n\t\t}\n\t}\n\n\tprivate final String description;\n\n\tprivate final Throwable thrown;\n\n\tprivate SubTest(String description, Throwable thrown) {\n\t\tthis.description = description;\n\t\tthis.thrown = thrown;\n\t}\n\n\tpublic AbstractThrowableAssert<?, Throwable> assertThrown() {\n\t\treturn new ThrowableAssert( thrown )\n\t\t\t\t.as( description );\n\t}\n\n}\n",
        "filePathAfter": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/SubTest.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.test;\n\nimport static org.junit.Assert.fail;\n\nimport java.util.concurrent.Callable;\n\nimport org.junit.Test;\n\nimport org.assertj.core.api.AbstractThrowableAssert;\nimport org.assertj.core.api.ThrowableAssert;\n\n/**\n * A util allowing to run blocks of code as \"sub-tests\".\n * <p>\n * This class is useful when looping over several executions of the same set of assertions:\n * <ul>\n *     <li>\n *         When executing code that both produces and consumes instances of a different generic type T\n *         for each execution of a loop,\n *         you usually cannot write type-safe code easily, because of limitations and how generics work,\n *         but you can with {@link #expectSuccess(Object, ParameterizedSubTest)}.\n *     </li>\n *     <li>\n *         When expecting an exception for each execution of a loop,\n *         you cannot use {@link org.junit.rules.ExpectedException} or {@link Test#expected()},\n *         but you can use {@link #expectException(String, Runnable)}.\n *         By default any thrown exception will be accepted; if you want to run additional checks on the thrown exception,\n *         use {@link ExceptionThrowingSubTest#assertThrown()}.\n *     </li>\n * </ul>\n */\npublic class SubTest {\n\n\tpublic static <T> void expectSuccess(T parameter, ParameterizedSubTest<T> subTest) {\n\t\tsubTest.test( parameter );\n\t}\n\n\tpublic static ExceptionThrowingSubTest expectException(Runnable runnable) {\n\t\treturn expectException( runnable.toString(), runnable );\n\t}\n\n\tpublic static ExceptionThrowingSubTest expectException(String description, Runnable runnable) {\n\t\treturn expectException(\n\t\t\t\tdescription,\n\t\t\t\t() -> {\n\t\t\t\t\trunnable.run();\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t);\n\t}\n\n\tpublic static ExceptionThrowingSubTest expectException(Callable<?> callable) {\n\t\treturn expectException( callable.toString(), callable );\n\t}\n\n\tpublic static ExceptionThrowingSubTest expectException(String description, Callable<?> callable) {\n\t\ttry {\n\t\t\tcallable.call();\n\t\t\tfail( \"'\" + description + \"' should have thrown an exception\" );\n\t\t\tthrow new IllegalStateException( \"This should never happen\" );\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\treturn new ExceptionThrowingSubTest( \"Exception thrown by '\" + description + \"'\", e );\n\t\t}\n\t}\n\n\tprivate SubTest() {\n\t}\n\n\tpublic static final class ExceptionThrowingSubTest {\n\t\tprivate final String description;\n\n\t\tprivate final Throwable thrown;\n\n\t\tprivate ExceptionThrowingSubTest(String description, Throwable thrown) {\n\t\t\tthis.description = description;\n\t\t\tthis.thrown = thrown;\n\t\t}\n\n\t\tpublic AbstractThrowableAssert<?, Throwable> assertThrown() {\n\t\t\treturn new ThrowableAssert( thrown )\n\t\t\t\t\t.as( description );\n\t\t}\n\t}\n\n\t@FunctionalInterface\n\tpublic interface ParameterizedSubTest<T> {\n\t\tvoid test(T param);\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "private ExceptionThrowingSubTest(String description, Throwable thrown) {\n\t\t\tthis.description = description;\n\t\t\tthis.thrown = thrown;\n\t\t}",
        "diffSourceCode": "-   62: \tprivate SubTest(String description, Throwable thrown) {\n-   63: \t\tthis.description = description;\n-   64: \t\tthis.thrown = thrown;\n-   65: \t}\n+   62: \tpublic static ExceptionThrowingSubTest expectException(String description, Callable<?> callable) {\n+   63: \t\ttry {\n+   64: \t\t\tcallable.call();\n+   65: \t\t\tfail( \"'\" + description + \"' should have thrown an exception\" );\n+   81: \t\tprivate ExceptionThrowingSubTest(String description, Throwable thrown) {\n+   82: \t\t\tthis.description = description;\n+   83: \t\t\tthis.thrown = thrown;\n+   84: \t\t}\n",
        "uniqueId": "f50cbfc772b75e7d7a983359b9baca383bc6ae9f_62_65__81_84",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8
    },
    {
        "type": "Inline Method",
        "description": "Inline Method\tpublic withSingleBackend(backendName String, backendConfiguration BackendConfiguration) : DocumentationSetupHelper inlined to public withSingleBackend(backendConfiguration BackendConfiguration) : DocumentationSetupHelper in class org.hibernate.search.documentation.testsupport.DocumentationSetupHelper",
        "diffLocations": [
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
                "startLine": 62,
                "endLine": 64,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
                "startLine": 55,
                "endLine": 61,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
                "startLine": 66,
                "endLine": 72,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public static DocumentationSetupHelper withSingleBackend(String backendName, BackendConfiguration backendConfiguration) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendName, backendConfiguration ),\n\t\t\t\tbackendConfiguration,\n\t\t\t\tnull\n\t\t);\n\t}",
        "filePathBefore": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
        "isPureRefactoring": true,
        "commitId": "01ec43a1bfee6833da8f16dbb6739bde1cd888f9",
        "packageNameBefore": "org.hibernate.search.documentation.testsupport",
        "classNameBefore": "org.hibernate.search.documentation.testsupport.DocumentationSetupHelper",
        "methodNameBefore": "org.hibernate.search.documentation.testsupport.DocumentationSetupHelper#withSingleBackend",
        "invokedMethod": "methodSignature: org.hibernate.search.documentation.testsupport.DocumentationSetupHelper#withSingleBackend\n methodBody: public static DocumentationSetupHelper withSingleBackend(String backendName, BackendConfiguration backendConfiguration,\n\t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\nreturn new DocumentationSetupHelper(BackendSetupStrategy.withSingleBackend(backendName,backendConfiguration),backendConfiguration,mappingConfigurerOrNull);\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.common.rule.BackendSetupStrategy#withSingleBackend\n methodBody: static BackendSetupStrategy withSingleBackend(String backendName, BackendConfiguration backendConfiguration) {\nreturn withMultipleBackends(backendName,Collections.singletonMap(backendName,backendConfiguration));\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper#withSingleBackend\n methodBody: public static OrmSetupHelper withSingleBackend(String backendName, BackendConfiguration backendConfiguration) {\nreturn new OrmSetupHelper(BackendSetupStrategy.withSingleBackend(backendName,backendConfiguration),SchemaManagementStrategyName.DROP_AND_CREATE_AND_DROP);\n}",
        "classSignatureBefore": "public final class DocumentationSetupHelper\n\t\textends MappingSetupHelper<DocumentationSetupHelper.SetupContext, SimpleSessionFactoryBuilder, SessionFactory> ",
        "methodNameBeforeSet": [
            "org.hibernate.search.documentation.testsupport.DocumentationSetupHelper#withSingleBackend"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.documentation.testsupport.DocumentationSetupHelper"
        ],
        "classSignatureBeforeSet": [
            "public final class DocumentationSetupHelper\n\t\textends MappingSetupHelper<DocumentationSetupHelper.SetupContext, SimpleSessionFactoryBuilder, SessionFactory> "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Overlapped refactoring - can be identical by undoing the overlapped refactoring\n- Inline Method-",
                "description": "Inline Method on top of the inlined method - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.documentation.testsupport;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.LinkedHashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.function.Consumer;\nimport java.util.stream.Collectors;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.mapper.orm.automaticindexing.session.AutomaticIndexingSynchronizationStrategyNames;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.mapping.HibernateOrmSearchMappingConfigurer;\nimport org.hibernate.search.mapper.orm.schema.management.SchemaManagementStrategyName;\nimport org.hibernate.search.mapper.pojo.mapping.definition.programmatic.ProgrammaticMappingConfigurationContext;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendConfiguration;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendSetupStrategy;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.MappingSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.SimpleSessionFactoryBuilder;\n\nimport org.junit.Assume;\n\npublic final class DocumentationSetupHelper\n\t\textends MappingSetupHelper<DocumentationSetupHelper.SetupContext, SimpleSessionFactoryBuilder, SessionFactory> {\n\n\tprivate static final String DEFAULT_BACKEND_NAME = \"backendName\";\n\n\tpublic static List<DocumentationSetupHelper> testParamsWithSingleBackend(\n\t\t\tList<BackendConfiguration> backendConfigurations) {\n\t\treturn testParamsWithSingleBackend( DEFAULT_BACKEND_NAME, backendConfigurations );\n\t}\n\n\tpublic static List<DocumentationSetupHelper> testParamsWithSingleBackend(String backendName,\n\t\t\tList<BackendConfiguration> backendConfigurations) {\n\t\treturn backendConfigurations.stream()\n\t\t\t\t.map( config -> withSingleBackend( backendName, config ) )\n\t\t\t\t.collect( Collectors.toList() );\n\t}\n\n\tpublic static List<DocumentationSetupHelper> testParamsWithSingleBackendForBothAnnotationsAndProgrammatic(\n\t\t\tList<BackendConfiguration> backendConfigurations,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\tHibernateOrmSearchMappingConfigurer mappingConfigurer =\n\t\t\t\tcontext -> programmaticMappingContributor.accept( context.programmaticMapping() );\n\t\tList<DocumentationSetupHelper> result = new ArrayList<>();\n\t\tfor ( BackendConfiguration configuration : backendConfigurations ) {\n\t\t\t// Annotation-based mapping\n\t\t\tresult.add( withSingleBackend( DEFAULT_BACKEND_NAME, configuration, null ) );\n\t\t\t// Programmatic mapping\n\t\t\tresult.add( withSingleBackend( DEFAULT_BACKEND_NAME, configuration, mappingConfigurer ) );\n\t\t}\n\t\treturn result;\n\t}\n\n\tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration) {\n\t\treturn withSingleBackend( DEFAULT_BACKEND_NAME, backendConfiguration );\n\t}\n\n\tpublic static DocumentationSetupHelper withSingleBackend(String backendName, BackendConfiguration backendConfiguration) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendName, backendConfiguration ),\n\t\t\t\tbackendConfiguration,\n\t\t\t\tnull\n\t\t);\n\t}\n\n\tpublic static DocumentationSetupHelper withSingleBackend(String backendName, BackendConfiguration backendConfiguration,\n\t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendName, backendConfiguration ),\n\t\t\t\tbackendConfiguration,\n\t\t\t\tmappingConfigurerOrNull\n\t\t);\n\t}\n\n\tpublic static DocumentationSetupHelper withMultipleBackends(String defaultBackendName,\n\t\t\tMap<String, BackendConfiguration> backendConfigurations,\n\t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withMultipleBackends( defaultBackendName, backendConfigurations ),\n\t\t\t\tbackendConfigurations.get( defaultBackendName ),\n\t\t\t\tmappingConfigurerOrNull\n\t\t);\n\t}\n\n\tprivate final BackendConfiguration defaultBackendConfiguration;\n\n\tprivate final HibernateOrmSearchMappingConfigurer mappingConfigurerOrNull;\n\n\tprivate DocumentationSetupHelper(BackendSetupStrategy backendSetupStrategy,\n\t\t\tBackendConfiguration defaultBackendConfiguration,\n\t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n\t\tsuper( backendSetupStrategy );\n\t\tthis.defaultBackendConfiguration = defaultBackendConfiguration;\n\t\tthis.mappingConfigurerOrNull = mappingConfigurerOrNull;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn defaultBackendConfiguration.toString()\n\t\t\t\t+ (mappingConfigurerOrNull != null ? \" - programmatic mapping\" : \"\");\n\t}\n\n\t@Override\n\tprotected SetupContext createSetupContext() {\n\t\treturn new SetupContext( mappingConfigurerOrNull );\n\t}\n\n\t@Override\n\tprotected void close(SessionFactory toClose) {\n\t\ttoClose.close();\n\t}\n\n\tpublic boolean isElasticsearch() {\n\t\treturn defaultBackendConfiguration instanceof ElasticsearchBackendConfiguration;\n\t}\n\n\tpublic boolean isLucene() {\n\t\treturn defaultBackendConfiguration instanceof LuceneBackendConfiguration;\n\t}\n\n\tpublic void assumeElasticsearch() {\n\t\tAssume.assumeTrue( isElasticsearch() );\n\t}\n\n\tpublic void assumeLucene() {\n\t\tAssume.assumeTrue( isLucene() );\n\t}\n\n\tpublic final class SetupContext\n\t\t\textends MappingSetupHelper<SetupContext, SimpleSessionFactoryBuilder, SessionFactory>.AbstractSetupContext {\n\n\t\t// Use a LinkedHashMap for deterministic iteration\n\t\tprivate final Map<String, Object> overriddenProperties = new LinkedHashMap<>();\n\n\t\tSetupContext(HibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n\t\t\t// Real backend => ensure we clean up everything before and after the tests\n\t\t\twithProperty( HibernateOrmMapperSettings.SCHEMA_MANAGEMENT_STRATEGY,\n\t\t\t\t\tSchemaManagementStrategyName.DROP_AND_CREATE_AND_DROP );\n\t\t\t// Override the automatic indexing synchronization strategy according to our needs for testing\n\t\t\twithProperty( HibernateOrmMapperSettings.AUTOMATIC_INDEXING_SYNCHRONIZATION_STRATEGY,\n\t\t\t\t\tAutomaticIndexingSynchronizationStrategyNames.SYNC );\n\t\t\t// Set up programmatic mapping if necessary\n\t\t\tif ( mappingConfigurerOrNull != null ) {\n\t\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_PROCESS_ANNOTATIONS, false );\n\t\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_CONFIGURER, mappingConfigurerOrNull );\n\t\t\t}\n\t\t\t// Ensure overridden properties will be applied\n\t\t\twithConfiguration( builder -> overriddenProperties.forEach( builder::setProperty ) );\n\t\t}\n\n\t\t@Override\n\t\tpublic SetupContext withProperty(String key, Object value) {\n\t\t\toverriddenProperties.put( key, value );\n\t\t\treturn thisAsC();\n\t\t}\n\n\t\tpublic SessionFactory setup(Class<?> ... annotatedTypes) {\n\t\t\treturn withConfiguration( builder -> builder.addAnnotatedClasses( Arrays.asList( annotatedTypes ) ) )\n\t\t\t\t\t.setup();\n\t\t}\n\n\t\t@Override\n\t\tprotected SimpleSessionFactoryBuilder createBuilder() {\n\t\t\treturn new SimpleSessionFactoryBuilder();\n\t\t}\n\n\t\t@Override\n\t\tprotected SessionFactory build(SimpleSessionFactoryBuilder builder) {\n\t\t\treturn builder.build();\n\t\t}\n\n\t\t@Override\n\t\tprotected SetupContext thisAsC() {\n\t\t\treturn this;\n\t\t}\n\t}\n\n}",
        "filePathAfter": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.documentation.testsupport;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.LinkedHashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.function.Consumer;\nimport java.util.stream.Collectors;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.mapper.orm.automaticindexing.session.AutomaticIndexingSynchronizationStrategyNames;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.mapping.HibernateOrmSearchMappingConfigurer;\nimport org.hibernate.search.mapper.orm.schema.management.SchemaManagementStrategyName;\nimport org.hibernate.search.mapper.pojo.mapping.definition.programmatic.ProgrammaticMappingConfigurationContext;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendConfiguration;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendSetupStrategy;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.MappingSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.SimpleSessionFactoryBuilder;\n\nimport org.junit.Assume;\n\npublic final class DocumentationSetupHelper\n\t\textends MappingSetupHelper<DocumentationSetupHelper.SetupContext, SimpleSessionFactoryBuilder, SessionFactory> {\n\n\tpublic static List<DocumentationSetupHelper> testParamsWithSingleBackend(\n\t\t\tList<BackendConfiguration> backendConfigurations) {\n\t\treturn backendConfigurations.stream()\n\t\t\t\t.map( DocumentationSetupHelper::withSingleBackend )\n\t\t\t\t.collect( Collectors.toList() );\n\t}\n\n\tpublic static List<DocumentationSetupHelper> testParamsWithSingleBackendForBothAnnotationsAndProgrammatic(\n\t\t\tList<BackendConfiguration> backendConfigurations,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\tHibernateOrmSearchMappingConfigurer mappingConfigurer =\n\t\t\t\tcontext -> programmaticMappingContributor.accept( context.programmaticMapping() );\n\t\tList<DocumentationSetupHelper> result = new ArrayList<>();\n\t\tfor ( BackendConfiguration configuration : backendConfigurations ) {\n\t\t\t// Annotation-based mapping\n\t\t\tresult.add( withSingleBackend( configuration, null ) );\n\t\t\t// Programmatic mapping\n\t\t\tresult.add( withSingleBackend( configuration, mappingConfigurer ) );\n\t\t}\n\t\treturn result;\n\t}\n\n\tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n\t\t\t\tbackendConfiguration,\n\t\t\t\tnull\n\t\t);\n\t}\n\n\tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration,\n\t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n\t\t\t\tbackendConfiguration,\n\t\t\t\tmappingConfigurerOrNull\n\t\t);\n\t}\n\n\tpublic static DocumentationSetupHelper withMultipleBackends(BackendConfiguration defaultBackendConfiguration,\n\t\t\tMap<String, BackendConfiguration> namedBackendConfigurations,\n\t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withMultipleBackends( defaultBackendConfiguration, namedBackendConfigurations ),\n\t\t\t\tdefaultBackendConfiguration,\n\t\t\t\tmappingConfigurerOrNull\n\t\t);\n\t}\n\n\tprivate final BackendConfiguration defaultBackendConfiguration;\n\n\tprivate final HibernateOrmSearchMappingConfigurer mappingConfigurerOrNull;\n\n\tprivate DocumentationSetupHelper(BackendSetupStrategy backendSetupStrategy,\n\t\t\tBackendConfiguration defaultBackendConfiguration,\n\t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n\t\tsuper( backendSetupStrategy );\n\t\tthis.defaultBackendConfiguration = defaultBackendConfiguration;\n\t\tthis.mappingConfigurerOrNull = mappingConfigurerOrNull;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn defaultBackendConfiguration.toString()\n\t\t\t\t+ (mappingConfigurerOrNull != null ? \" - programmatic mapping\" : \"\");\n\t}\n\n\t@Override\n\tprotected SetupContext createSetupContext() {\n\t\treturn new SetupContext( mappingConfigurerOrNull );\n\t}\n\n\t@Override\n\tprotected void close(SessionFactory toClose) {\n\t\ttoClose.close();\n\t}\n\n\tpublic boolean isElasticsearch() {\n\t\treturn defaultBackendConfiguration instanceof ElasticsearchBackendConfiguration;\n\t}\n\n\tpublic boolean isLucene() {\n\t\treturn defaultBackendConfiguration instanceof LuceneBackendConfiguration;\n\t}\n\n\tpublic void assumeElasticsearch() {\n\t\tAssume.assumeTrue( isElasticsearch() );\n\t}\n\n\tpublic void assumeLucene() {\n\t\tAssume.assumeTrue( isLucene() );\n\t}\n\n\tpublic final class SetupContext\n\t\t\textends MappingSetupHelper<SetupContext, SimpleSessionFactoryBuilder, SessionFactory>.AbstractSetupContext {\n\n\t\t// Use a LinkedHashMap for deterministic iteration\n\t\tprivate final Map<String, Object> overriddenProperties = new LinkedHashMap<>();\n\n\t\tSetupContext(HibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n\t\t\t// Real backend => ensure we clean up everything before and after the tests\n\t\t\twithProperty( HibernateOrmMapperSettings.SCHEMA_MANAGEMENT_STRATEGY,\n\t\t\t\t\tSchemaManagementStrategyName.DROP_AND_CREATE_AND_DROP );\n\t\t\t// Override the automatic indexing synchronization strategy according to our needs for testing\n\t\t\twithProperty( HibernateOrmMapperSettings.AUTOMATIC_INDEXING_SYNCHRONIZATION_STRATEGY,\n\t\t\t\t\tAutomaticIndexingSynchronizationStrategyNames.SYNC );\n\t\t\t// Set up programmatic mapping if necessary\n\t\t\tif ( mappingConfigurerOrNull != null ) {\n\t\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_PROCESS_ANNOTATIONS, false );\n\t\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_CONFIGURER, mappingConfigurerOrNull );\n\t\t\t}\n\t\t\t// Ensure overridden properties will be applied\n\t\t\twithConfiguration( builder -> overriddenProperties.forEach( builder::setProperty ) );\n\t\t}\n\n\t\t@Override\n\t\tpublic SetupContext withProperty(String key, Object value) {\n\t\t\toverriddenProperties.put( key, value );\n\t\t\treturn thisAsC();\n\t\t}\n\n\t\tpublic SessionFactory setup(Class<?> ... annotatedTypes) {\n\t\t\treturn withConfiguration( builder -> builder.addAnnotatedClasses( Arrays.asList( annotatedTypes ) ) )\n\t\t\t\t\t.setup();\n\t\t}\n\n\t\t@Override\n\t\tprotected SimpleSessionFactoryBuilder createBuilder() {\n\t\t\treturn new SimpleSessionFactoryBuilder();\n\t\t}\n\n\t\t@Override\n\t\tprotected SessionFactory build(SimpleSessionFactoryBuilder builder) {\n\t\t\treturn builder.build();\n\t\t}\n\n\t\t@Override\n\t\tprotected SetupContext thisAsC() {\n\t\t\treturn this;\n\t\t}\n\t}\n\n}",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.documentation.testsupport.DocumentationSetupHelper#withSingleBackend\n methodBody: public static DocumentationSetupHelper withSingleBackend(String backendName, BackendConfiguration backendConfiguration,\n\t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\nreturn new DocumentationSetupHelper(BackendSetupStrategy.withSingleBackend(backendName,backendConfiguration),backendConfiguration,mappingConfigurerOrNull);\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.common.rule.BackendSetupStrategy#withSingleBackend\n methodBody: static BackendSetupStrategy withSingleBackend(String backendName, BackendConfiguration backendConfiguration) {\nreturn withMultipleBackends(backendName,Collections.singletonMap(backendName,backendConfiguration));\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper#withSingleBackend\n methodBody: public static OrmSetupHelper withSingleBackend(String backendName, BackendConfiguration backendConfiguration) {\nreturn new OrmSetupHelper(BackendSetupStrategy.withSingleBackend(backendName,backendConfiguration),SchemaManagementStrategyName.DROP_AND_CREATE_AND_DROP);\n}"
        ],
        "sourceCodeAfterRefactoring": "public static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n\t\t\t\tbackendConfiguration,\n\t\t\t\tnull\n\t\t);\n\t}",
        "diffSourceCode": "-   55: \t\t\tresult.add( withSingleBackend( DEFAULT_BACKEND_NAME, configuration, null ) );\n-   56: \t\t\t// Programmatic mapping\n-   57: \t\t\tresult.add( withSingleBackend( DEFAULT_BACKEND_NAME, configuration, mappingConfigurer ) );\n-   58: \t\t}\n-   59: \t\treturn result;\n-   60: \t}\n-   61: \n-   62: \tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration) {\n-   63: \t\treturn withSingleBackend( DEFAULT_BACKEND_NAME, backendConfiguration );\n-   64: \t}\n-   66: \tpublic static DocumentationSetupHelper withSingleBackend(String backendName, BackendConfiguration backendConfiguration) {\n-   67: \t\treturn new DocumentationSetupHelper(\n-   68: \t\t\t\tBackendSetupStrategy.withSingleBackend( backendName, backendConfiguration ),\n-   69: \t\t\t\tbackendConfiguration,\n-   70: \t\t\t\tnull\n-   71: \t\t);\n-   72: \t}\n+   55: \tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration) {\n+   56: \t\treturn new DocumentationSetupHelper(\n+   57: \t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n+   58: \t\t\t\tbackendConfiguration,\n+   59: \t\t\t\tnull\n+   60: \t\t);\n+   61: \t}\n+   62: \n+   63: \tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration,\n+   64: \t\t\tHibernateOrmSearchMappingConfigurer mappingConfigurerOrNull) {\n+   66: \t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n+   67: \t\t\t\tbackendConfiguration,\n+   68: \t\t\t\tmappingConfigurerOrNull\n+   69: \t\t);\n+   70: \t}\n+   71: \n+   72: \tpublic static DocumentationSetupHelper withMultipleBackends(BackendConfiguration defaultBackendConfiguration,\n",
        "uniqueId": "01ec43a1bfee6833da8f16dbb6739bde1cd888f9_62_64__55_61_66_72",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tpackage buildValidationFailureReportPattern() : FailureReportUtils.FailureReportPatternBuilder extracted from public mapping_missing() : void in class org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerValidationMappingBaseIT & moved to class org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerTestUtils",
        "diffLocations": [
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/schema/management/ElasticsearchIndexSchemaManagerValidationMappingBaseIT.java",
                "startLine": 144,
                "endLine": 165,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/schema/management/ElasticsearchIndexSchemaManagerValidationMappingBaseIT.java",
                "startLine": 142,
                "endLine": 162,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/schema/management/ElasticsearchIndexSchemaManagerValidationMappingBaseIT.java",
                "startLine": 75,
                "endLine": 79,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void mapping_missing() {\n\t\tAssume.assumeTrue(\n\t\t\t\t\"Skipping this test as there is always a mapping (be it empty) in \" + elasticSearchClient.getDialect(),\n\t\t\t\telasticSearchClient.getDialect().isEmptyMappingPossible()\n\t\t);\n\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field( \"myField\", f -> f.asLocalDate() )\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\n\t\tsetupAndValidateExpectingFailure(\n\t\t\t\tindex,\n\t\t\t\tFailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.contextLiteral( SCHEMA_VALIDATION_CONTEXT )\n\t\t\t\t\t\t.failure( \"Missing type mapping\" )\n\t\t\t\t\t\t.build()\n\t\t);\n\t}",
        "filePathBefore": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/schema/management/ElasticsearchIndexSchemaManagerValidationMappingBaseIT.java",
        "isPureRefactoring": true,
        "commitId": "0b4c69d582f4c29554d78da38daebbc49d9d40da",
        "packageNameBefore": "org.hibernate.search.integrationtest.backend.elasticsearch.schema.management",
        "classNameBefore": "org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerValidationMappingBaseIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerValidationMappingBaseIT#mapping_missing",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerValidationAnalyzerIT#setupAndValidateExpectingFailure\n methodBody: private void setupAndValidateExpectingFailure(String failureReportPattern) {\nAssertions.assertThatThrownBy(this::setupAndValidate).isInstanceOf(SearchException.class).hasMessageMatching(failureReportPattern);\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerValidationNormalizerIT#setupAndValidateExpectingFailure\n methodBody: private void setupAndValidateExpectingFailure(String failureReportPattern) {\nAssertions.assertThatThrownBy(this::setupAndValidate).isInstanceOf(SearchException.class).hasMessageMatching(failureReportPattern);\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerValidationMappingBaseIT#setupAndValidateExpectingFailure\n methodBody: private void setupAndValidateExpectingFailure(StubMappedIndex index, String failureReportRegex) {\nAssertions.assertThatThrownBy(() -> setupAndValidate(index)).isInstanceOf(SearchException.class).hasMessageMatching(failureReportRegex);\n}\nmethodSignature: org.hibernate.search.engine.reporting.impl.EngineEventContextMessages#index\n methodBody: String index(String name);",
        "classSignatureBefore": "public class ElasticsearchIndexSchemaManagerValidationMappingBaseIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerValidationMappingBaseIT#mapping_missing"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerValidationMappingBaseIT"
        ],
        "classSignatureBeforeSet": [
            "public class ElasticsearchIndexSchemaManagerValidationMappingBaseIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Tolerable changes in the body\n",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.elasticsearch.schema.management;\n\nimport static org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerTestUtils.defaultMetadataMappingAndCommaForInitialization;\nimport static org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerTestUtils.simpleMappingForInitialization;\n\nimport java.util.EnumSet;\n\nimport org.hibernate.search.backend.elasticsearch.analysis.ElasticsearchAnalysisConfigurer;\nimport org.hibernate.search.backend.elasticsearch.analysis.ElasticsearchAnalysisConfigurationContext;\nimport org.hibernate.search.backend.elasticsearch.cfg.ElasticsearchBackendSettings;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaObjectField;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.rule.TestElasticsearchClient;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.rule.SearchSetupHelper;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.FailureReportUtils;\nimport org.hibernate.search.util.impl.integrationtest.mapper.stub.StubMappedIndex;\nimport org.hibernate.search.util.impl.integrationtest.mapper.stub.StubMappingSchemaManagementStrategy;\nimport org.assertj.core.api.Assertions;\nimport org.hibernate.search.util.impl.test.annotation.PortedFromSearch5;\n\nimport org.junit.Assume;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.junit.runners.Parameterized;\n\n/**\n * Basic tests related to the mapping when validating indexes,\n * for all index-validating schema management operations.\n */\n@RunWith(Parameterized.class)\n@PortedFromSearch5(original = \"org.hibernate.search.elasticsearch.test.Elasticsearch5SchemaValidationIT\")\npublic class ElasticsearchIndexSchemaManagerValidationMappingBaseIT {\n\n\tprivate static final String SCHEMA_VALIDATION_CONTEXT = \"schema validation\";\n\n\t@Parameterized.Parameters(name = \"With operation {0}\")\n\tpublic static EnumSet<ElasticsearchIndexSchemaManagerValidationOperation> operations() {\n\t\treturn ElasticsearchIndexSchemaManagerValidationOperation.all();\n\t}\n\n\t@Rule\n\tpublic final SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic TestElasticsearchClient elasticSearchClient = new TestElasticsearchClient();\n\n\tprivate final ElasticsearchIndexSchemaManagerValidationOperation operation;\n\n\tpublic ElasticsearchIndexSchemaManagerValidationMappingBaseIT(\n\t\t\tElasticsearchIndexSchemaManagerValidationOperation operation) {\n\t\tthis.operation = operation;\n\t}\n\n\t@Test\n\tpublic void success_1() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field( \"myField\", f -> f.asLocalDate() )\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\tsimpleMappingForInitialization(\n\t\t\t\t\t\t\"'myField': {\"\n\t\t\t\t\t\t\t\t+ \"'type': 'date',\"\n\t\t\t\t\t\t\t\t+ \"'index': true,\"\n\t\t\t\t\t\t\t\t+ \"'format': '\" + elasticSearchClient.getDialect().getConcatenatedLocalDateDefaultMappingFormats() + \"',\"\n\t\t\t\t\t\t\t\t+ \"'ignore_malformed': true\" // Ignored during validation\n\t\t\t\t\t\t+ \"},\"\n\t\t\t\t\t\t+ \"'NOTmyField': {\" // Ignored during validation\n\t\t\t\t\t\t\t\t+ \"'type': 'date',\"\n\t\t\t\t\t\t\t\t+ \"'index': true\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t);\n\n\t\tsetupAndValidate( index );\n\n\t\t// If we get here, it means validation passed (no exception was thrown)\n\t}\n\n\t@Test\n\tpublic void success_2() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field( \"myField\", f -> f.asBoolean() )\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\tsimpleMappingForInitialization(\n\t\t\t\t\t\t\"'myField': {\"\n\t\t\t\t\t\t\t\t+ \"'type': 'boolean',\"\n\t\t\t\t\t\t\t\t+ \"'index': true\"\n\t\t\t\t\t\t+ \"},\"\n\t\t\t\t\t\t+ \"'NOTmyField': {\" // Ignored during validation\n\t\t\t\t\t\t\t\t+ \"'type': 'boolean',\"\n\t\t\t\t\t\t\t\t+ \"'index': true\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t);\n\n\t\tsetupAndValidate( index );\n\n\t\t// If we get here, it means validation passed (no exception was thrown)\n\t}\n\n\t@Test\n\tpublic void success_3() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field(\n\t\t\t\t\t\"myField\",\n\t\t\t\t\tf -> f.asString().analyzer( \"default\" )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\tsimpleMappingForInitialization(\n\t\t\t\t\t\t\"'myField': {\"\n\t\t\t\t\t\t\t\t+ \"'type': 'text',\"\n\t\t\t\t\t\t\t\t+ \"'index': true,\"\n\t\t\t\t\t\t\t\t+ \"'analyzer': 'default'\"\n\t\t\t\t\t\t+ \"},\"\n\t\t\t\t\t\t+ \"'NOTmyField': {\" // Ignored during validation\n\t\t\t\t\t\t\t\t+ \"'type': 'text',\"\n\t\t\t\t\t\t\t\t+ \"'index': true\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t);\n\n\t\tsetupAndValidate( index );\n\t}\n\n\t@Test\n\tpublic void mapping_missing() {\n\t\tAssume.assumeTrue(\n\t\t\t\t\"Skipping this test as there is always a mapping (be it empty) in \" + elasticSearchClient.getDialect(),\n\t\t\t\telasticSearchClient.getDialect().isEmptyMappingPossible()\n\t\t);\n\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field( \"myField\", f -> f.asLocalDate() )\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\n\t\tsetupAndValidateExpectingFailure(\n\t\t\t\tindex,\n\t\t\t\tFailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.contextLiteral( SCHEMA_VALIDATION_CONTEXT )\n\t\t\t\t\t\t.failure( \"Missing type mapping\" )\n\t\t\t\t\t\t.build()\n\t\t);\n\t}\n\n\t@Test\n\tpublic void attribute_field_notPresent() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable(\n\t\t\troot -> root.field( \"myField\", f -> f.asInteger() ).toReference()\n\t\t);\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\tsimpleMappingForInitialization(\n\t\t\t\t\t\"'notMyField': {\"\n\t\t\t\t\t\t\t\t\t+ \"'type': 'integer',\"\n\t\t\t\t\t\t\t\t\t+ \"'index': true\"\n\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t);\n\n\t\tsetupAndValidateExpectingFailure(\n\t\t\t\tindex,\n\t\t\t\tFailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.contextLiteral( SCHEMA_VALIDATION_CONTEXT )\n\t\t\t\t\t\t.indexFieldContext( \"myField\" )\n\t\t\t\t\t\t.failure( \"Missing property mapping\" )\n\t\t\t\t\t\t.build()\n\t\t);\n\t}\n\n\t/**\n\t * Tests that mappings that are more powerful than requested will pass validation.\n\t */\n\t@Test\n\tpublic void property_attribute_leniency() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field( \"myField\", f -> f.asLong() )\n\t\t\t\t\t.toReference();\n\t\t\troot.field( \"myTextField\", f -> f.asString().analyzer( \"default\" ) )\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\tsimpleMappingForInitialization(\n\t\t\t\t\t\t\"'myField': {\"\n\t\t\t\t\t\t\t\t+ \"'type': 'long',\"\n\t\t\t\t\t\t\t\t+ \"'index': true,\"\n\t\t\t\t\t\t\t\t+ \"'store': true\"\n\t\t\t\t\t\t+ \"},\"\n\t\t\t\t\t\t+ \"'myTextField': {\"\n\t\t\t\t\t\t\t\t+ \"'type': 'text',\"\n\t\t\t\t\t\t\t\t+ \"'index': true,\"\n\t\t\t\t\t\t\t\t+ \"'norms': true\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t);\n\n\t\tsetupAndValidate( index );\n\t}\n\n\t/**\n\t * Tests that properties within properties are correctly represented in the failure report.\n\t */\n\t@Test\n\tpublic void nestedProperty_attribute_invalid() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\tIndexSchemaObjectField objectField =\n\t\t\t\t\troot.objectField( \"myObjectField\" );\n\t\t\tobjectField.field( \"myField\", f -> f.asLocalDate() )\n\t\t\t\t\t.toReference();\n\t\t\tobjectField.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\tsimpleMappingForInitialization(\n\t\t\t\t\t\t\"'myObjectField': {\"\n\t\t\t\t\t\t\t\t+ \"'type': 'object',\"\n\t\t\t\t\t\t\t\t+ \"'dynamic': 'strict',\"\n\t\t\t\t\t\t\t\t+ \"'properties': {\"\n\t\t\t\t\t\t\t\t\t\t+ \"'myField': {\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'type': 'date',\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'format': '\" + elasticSearchClient.getDialect().getConcatenatedLocalDateDefaultMappingFormats() + \"',\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'index': false\"\n\t\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t);\n\n\t\tsetupAndValidateExpectingFailure(\n\t\t\t\tindex,\n\t\t\t\tFailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.contextLiteral( SCHEMA_VALIDATION_CONTEXT )\n\t\t\t\t\t\t.indexFieldContext( \"myObjectField.myField\" )\n\t\t\t\t\t\t.mappingAttributeContext( \"index\" )\n\t\t\t\t\t\t.failure( \"Invalid value. Expected 'true', actual is 'false'\" )\n\t\t\t\t\t\t.build()\n\t\t);\n\t}\n\n\t@Test\n\tpublic void multipleErrors() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field( \"myField\", f -> f.asString() )\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\t\"{\"\n\t\t\t\t\t\t+ \"'dynamic': false,\"\n\t\t\t\t\t\t+ \"'properties': {\"\n\t\t\t\t\t\t\t\t+ defaultMetadataMappingAndCommaForInitialization()\n\t\t\t\t\t\t\t\t+ \"'myField': {\"\n\t\t\t\t\t\t\t\t\t\t+ \"'type': 'integer'\"\n\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t+ \"}\"\n\t\t);\n\n\t\tsetupAndValidateExpectingFailure(\n\t\t\t\tindex,\n\t\t\t\tFailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.contextLiteral( SCHEMA_VALIDATION_CONTEXT )\n\t\t\t\t\t\t.mappingAttributeContext( \"dynamic\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Invalid value. Expected 'STRICT', actual is 'FALSE'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.indexFieldContext( \"myField\" )\n\t\t\t\t\t\t.mappingAttributeContext( \"type\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Invalid value. Expected 'keyword', actual is 'integer'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t);\n\t}\n\n\tprivate void setupAndValidateExpectingFailure(StubMappedIndex index, String failureReportRegex) {\n\t\tAssertions.assertThatThrownBy( () -> setupAndValidate( index ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( failureReportRegex );\n\t}\n\n\tprivate void setupAndValidate(StubMappedIndex index) {\n\t\tsetupHelper.start()\n\t\t\t\t.withSchemaManagement( StubMappingSchemaManagementStrategy.DROP_ON_SHUTDOWN_ONLY )\n\t\t\t\t.withBackendProperty(\n\t\t\t\t\t\t// Don't contribute any analysis definitions, migration of those is tested in another test class\n\t\t\t\t\t\tElasticsearchBackendSettings.ANALYSIS_CONFIGURER,\n\t\t\t\t\t\t(ElasticsearchAnalysisConfigurer) (ElasticsearchAnalysisConfigurationContext context) -> {\n\t\t\t\t\t\t\t// No-op\n\t\t\t\t\t\t}\n\t\t\t\t)\n\t\t\t\t.withIndex( index )\n\t\t\t\t.setup();\n\n\t\tFutures.unwrappedExceptionJoin( operation.apply( index.schemaManager() ) );\n\t}\n}\n",
        "filePathAfter": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/schema/management/ElasticsearchIndexSchemaManagerValidationMappingBaseIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.elasticsearch.schema.management;\n\nimport static org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerTestUtils.buildValidationFailureReportPattern;\nimport static org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerTestUtils.defaultMetadataMappingAndCommaForInitialization;\nimport static org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerTestUtils.simpleMappingForInitialization;\n\nimport java.util.EnumSet;\n\nimport org.hibernate.search.backend.elasticsearch.analysis.ElasticsearchAnalysisConfigurer;\nimport org.hibernate.search.backend.elasticsearch.analysis.ElasticsearchAnalysisConfigurationContext;\nimport org.hibernate.search.backend.elasticsearch.cfg.ElasticsearchBackendSettings;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaObjectField;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.rule.TestElasticsearchClient;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.rule.SearchSetupHelper;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.mapper.stub.StubMappedIndex;\nimport org.hibernate.search.util.impl.integrationtest.mapper.stub.StubMappingSchemaManagementStrategy;\nimport org.assertj.core.api.Assertions;\nimport org.hibernate.search.util.impl.test.annotation.PortedFromSearch5;\n\nimport org.junit.Assume;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.junit.runners.Parameterized;\n\n/**\n * Basic tests related to the mapping when validating indexes,\n * for all index-validating schema management operations.\n */\n@RunWith(Parameterized.class)\n@PortedFromSearch5(original = \"org.hibernate.search.elasticsearch.test.Elasticsearch5SchemaValidationIT\")\npublic class ElasticsearchIndexSchemaManagerValidationMappingBaseIT {\n\n\t@Parameterized.Parameters(name = \"With operation {0}\")\n\tpublic static EnumSet<ElasticsearchIndexSchemaManagerValidationOperation> operations() {\n\t\treturn ElasticsearchIndexSchemaManagerValidationOperation.all();\n\t}\n\n\t@Rule\n\tpublic final SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic TestElasticsearchClient elasticSearchClient = new TestElasticsearchClient();\n\n\tprivate final ElasticsearchIndexSchemaManagerValidationOperation operation;\n\n\tpublic ElasticsearchIndexSchemaManagerValidationMappingBaseIT(\n\t\t\tElasticsearchIndexSchemaManagerValidationOperation operation) {\n\t\tthis.operation = operation;\n\t}\n\n\t@Test\n\tpublic void success_1() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field( \"myField\", f -> f.asLocalDate() )\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\tsimpleMappingForInitialization(\n\t\t\t\t\t\t\"'myField': {\"\n\t\t\t\t\t\t\t\t+ \"'type': 'date',\"\n\t\t\t\t\t\t\t\t+ \"'index': true,\"\n\t\t\t\t\t\t\t\t+ \"'format': '\" + elasticSearchClient.getDialect().getConcatenatedLocalDateDefaultMappingFormats() + \"',\"\n\t\t\t\t\t\t\t\t+ \"'ignore_malformed': true\" // Ignored during validation\n\t\t\t\t\t\t+ \"},\"\n\t\t\t\t\t\t+ \"'NOTmyField': {\" // Ignored during validation\n\t\t\t\t\t\t\t\t+ \"'type': 'date',\"\n\t\t\t\t\t\t\t\t+ \"'index': true\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t);\n\n\t\tsetupAndValidate( index );\n\n\t\t// If we get here, it means validation passed (no exception was thrown)\n\t}\n\n\t@Test\n\tpublic void success_2() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field( \"myField\", f -> f.asBoolean() )\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\tsimpleMappingForInitialization(\n\t\t\t\t\t\t\"'myField': {\"\n\t\t\t\t\t\t\t\t+ \"'type': 'boolean',\"\n\t\t\t\t\t\t\t\t+ \"'index': true\"\n\t\t\t\t\t\t+ \"},\"\n\t\t\t\t\t\t+ \"'NOTmyField': {\" // Ignored during validation\n\t\t\t\t\t\t\t\t+ \"'type': 'boolean',\"\n\t\t\t\t\t\t\t\t+ \"'index': true\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t);\n\n\t\tsetupAndValidate( index );\n\n\t\t// If we get here, it means validation passed (no exception was thrown)\n\t}\n\n\t@Test\n\tpublic void success_3() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field(\n\t\t\t\t\t\"myField\",\n\t\t\t\t\tf -> f.asString().analyzer( \"default\" )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\tsimpleMappingForInitialization(\n\t\t\t\t\t\t\"'myField': {\"\n\t\t\t\t\t\t\t\t+ \"'type': 'text',\"\n\t\t\t\t\t\t\t\t+ \"'index': true,\"\n\t\t\t\t\t\t\t\t+ \"'analyzer': 'default'\"\n\t\t\t\t\t\t+ \"},\"\n\t\t\t\t\t\t+ \"'NOTmyField': {\" // Ignored during validation\n\t\t\t\t\t\t\t\t+ \"'type': 'text',\"\n\t\t\t\t\t\t\t\t+ \"'index': true\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t);\n\n\t\tsetupAndValidate( index );\n\t}\n\n\t@Test\n\tpublic void mapping_missing() {\n\t\tAssume.assumeTrue(\n\t\t\t\t\"Skipping this test as there is always a mapping (be it empty) in \" + elasticSearchClient.getDialect(),\n\t\t\t\telasticSearchClient.getDialect().isEmptyMappingPossible()\n\t\t);\n\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field( \"myField\", f -> f.asLocalDate() )\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\n\t\tsetupAndValidateExpectingFailure(\n\t\t\t\tindex,\n\t\t\t\tbuildValidationFailureReportPattern()\n\t\t\t\t\t\t.failure( \"Missing type mapping\" )\n\t\t\t\t\t\t.build()\n\t\t);\n\t}\n\n\t@Test\n\tpublic void attribute_field_notPresent() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable(\n\t\t\troot -> root.field( \"myField\", f -> f.asInteger() ).toReference()\n\t\t);\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\tsimpleMappingForInitialization(\n\t\t\t\t\t\"'notMyField': {\"\n\t\t\t\t\t\t\t\t\t+ \"'type': 'integer',\"\n\t\t\t\t\t\t\t\t\t+ \"'index': true\"\n\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t);\n\n\t\tsetupAndValidateExpectingFailure(\n\t\t\t\tindex,\n\t\t\t\tbuildValidationFailureReportPattern()\n\t\t\t\t\t\t.indexFieldContext( \"myField\" )\n\t\t\t\t\t\t.failure( \"Missing property mapping\" )\n\t\t\t\t\t\t.build()\n\t\t);\n\t}\n\n\t/**\n\t * Tests that mappings that are more powerful than requested will pass validation.\n\t */\n\t@Test\n\tpublic void property_attribute_leniency() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field( \"myField\", f -> f.asLong() )\n\t\t\t\t\t.toReference();\n\t\t\troot.field( \"myTextField\", f -> f.asString().analyzer( \"default\" ) )\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\tsimpleMappingForInitialization(\n\t\t\t\t\t\t\"'myField': {\"\n\t\t\t\t\t\t\t\t+ \"'type': 'long',\"\n\t\t\t\t\t\t\t\t+ \"'index': true,\"\n\t\t\t\t\t\t\t\t+ \"'store': true\"\n\t\t\t\t\t\t+ \"},\"\n\t\t\t\t\t\t+ \"'myTextField': {\"\n\t\t\t\t\t\t\t\t+ \"'type': 'text',\"\n\t\t\t\t\t\t\t\t+ \"'index': true,\"\n\t\t\t\t\t\t\t\t+ \"'norms': true\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t);\n\n\t\tsetupAndValidate( index );\n\t}\n\n\t/**\n\t * Tests that properties within properties are correctly represented in the failure report.\n\t */\n\t@Test\n\tpublic void nestedProperty_attribute_invalid() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\tIndexSchemaObjectField objectField =\n\t\t\t\t\troot.objectField( \"myObjectField\" );\n\t\t\tobjectField.field( \"myField\", f -> f.asLocalDate() )\n\t\t\t\t\t.toReference();\n\t\t\tobjectField.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\tsimpleMappingForInitialization(\n\t\t\t\t\t\t\"'myObjectField': {\"\n\t\t\t\t\t\t\t\t+ \"'type': 'object',\"\n\t\t\t\t\t\t\t\t+ \"'dynamic': 'strict',\"\n\t\t\t\t\t\t\t\t+ \"'properties': {\"\n\t\t\t\t\t\t\t\t\t\t+ \"'myField': {\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'type': 'date',\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'format': '\" + elasticSearchClient.getDialect().getConcatenatedLocalDateDefaultMappingFormats() + \"',\"\n\t\t\t\t\t\t\t\t\t\t\t\t+ \"'index': false\"\n\t\t\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t)\n\t\t);\n\n\t\tsetupAndValidateExpectingFailure(\n\t\t\t\tindex,\n\t\t\t\tbuildValidationFailureReportPattern()\n\t\t\t\t\t\t.indexFieldContext( \"myObjectField.myField\" )\n\t\t\t\t\t\t.mappingAttributeContext( \"index\" )\n\t\t\t\t\t\t.failure( \"Invalid value. Expected 'true', actual is 'false'\" )\n\t\t\t\t\t\t.build()\n\t\t);\n\t}\n\n\t@Test\n\tpublic void multipleErrors() {\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field( \"myField\", f -> f.asString() )\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\t\telasticSearchClient.index( index.name() ).type().putMapping(\n\t\t\t\t\"{\"\n\t\t\t\t\t\t+ \"'dynamic': false,\"\n\t\t\t\t\t\t+ \"'properties': {\"\n\t\t\t\t\t\t\t\t+ defaultMetadataMappingAndCommaForInitialization()\n\t\t\t\t\t\t\t\t+ \"'myField': {\"\n\t\t\t\t\t\t\t\t\t\t+ \"'type': 'integer'\"\n\t\t\t\t\t\t\t\t+ \"}\"\n\t\t\t\t\t\t+ \"}\"\n\t\t\t\t+ \"}\"\n\t\t);\n\n\t\tsetupAndValidateExpectingFailure(\n\t\t\t\tindex,\n\t\t\t\tbuildValidationFailureReportPattern()\n\t\t\t\t\t\t.mappingAttributeContext( \"dynamic\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Invalid value. Expected 'STRICT', actual is 'FALSE'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.indexFieldContext( \"myField\" )\n\t\t\t\t\t\t.mappingAttributeContext( \"type\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Invalid value. Expected 'keyword', actual is 'integer'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t);\n\t}\n\n\tprivate void setupAndValidateExpectingFailure(StubMappedIndex index, String failureReportRegex) {\n\t\tAssertions.assertThatThrownBy( () -> setupAndValidate( index ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( failureReportRegex );\n\t}\n\n\tprivate void setupAndValidate(StubMappedIndex index) {\n\t\tsetupHelper.start()\n\t\t\t\t.withSchemaManagement( StubMappingSchemaManagementStrategy.DROP_ON_SHUTDOWN_ONLY )\n\t\t\t\t.withBackendProperty(\n\t\t\t\t\t\t// Don't contribute any analysis definitions, migration of those is tested in another test class\n\t\t\t\t\t\tElasticsearchBackendSettings.ANALYSIS_CONFIGURER,\n\t\t\t\t\t\t(ElasticsearchAnalysisConfigurer) (ElasticsearchAnalysisConfigurationContext context) -> {\n\t\t\t\t\t\t\t// No-op\n\t\t\t\t\t\t}\n\t\t\t\t)\n\t\t\t\t.withIndex( index )\n\t\t\t\t.setup();\n\n\t\tFutures.unwrappedExceptionJoin( operation.apply( index.schemaManager() ) );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "+ \"},\"\n\t\t\t\t\t\t+ \"'NOTmyField': {\" // Ignored during validation\n\t\t\t\t\t\t\t\t+ \"'type': 'date',\"\n\t\t\t\t\t\t\t\t+ \"'index': true\"\n\t\t\t\t\t\t+ \"}\""
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerValidationAnalyzerIT#setupAndValidateExpectingFailure\n methodBody: private void setupAndValidateExpectingFailure(String failureReportPattern) {\nAssertions.assertThatThrownBy(this::setupAndValidate).isInstanceOf(SearchException.class).hasMessageMatching(failureReportPattern);\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerValidationNormalizerIT#setupAndValidateExpectingFailure\n methodBody: private void setupAndValidateExpectingFailure(String failureReportPattern) {\nAssertions.assertThatThrownBy(this::setupAndValidate).isInstanceOf(SearchException.class).hasMessageMatching(failureReportPattern);\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerValidationMappingBaseIT#setupAndValidateExpectingFailure\n methodBody: private void setupAndValidateExpectingFailure(StubMappedIndex index, String failureReportRegex) {\nAssertions.assertThatThrownBy(() -> setupAndValidate(index)).isInstanceOf(SearchException.class).hasMessageMatching(failureReportRegex);\n}",
            "methodSignature: org.hibernate.search.engine.reporting.impl.EngineEventContextMessages#index\n methodBody: String index(String name);"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void mapping_missing() {\n\t\tAssume.assumeTrue(\n\t\t\t\t\"Skipping this test as there is always a mapping (be it empty) in \" + elasticSearchClient.getDialect(),\n\t\t\t\telasticSearchClient.getDialect().isEmptyMappingPossible()\n\t\t);\n\n\t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n\t\t\troot.field( \"myField\", f -> f.asLocalDate() )\n\t\t\t\t\t.toReference();\n\t\t} );\n\n\t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n\n\t\tsetupAndValidateExpectingFailure(\n\t\t\t\tindex,\n\t\t\t\tbuildValidationFailureReportPattern()\n\t\t\t\t\t\t.failure( \"Missing type mapping\" )\n\t\t\t\t\t\t.build()\n\t\t);\n\t}\n+ \"},\"\n\t\t\t\t\t\t+ \"'NOTmyField': {\" // Ignored during validation\n\t\t\t\t\t\t\t\t+ \"'type': 'date',\"\n\t\t\t\t\t\t\t\t+ \"'index': true\"\n\t\t\t\t\t\t+ \"}\"",
        "diffSourceCode": "-   75: \t\t\t\t\t\t\t\t+ \"'format': '\" + elasticSearchClient.getDialect().getConcatenatedLocalDateDefaultMappingFormats() + \"',\"\n-   76: \t\t\t\t\t\t\t\t+ \"'ignore_malformed': true\" // Ignored during validation\n-   77: \t\t\t\t\t\t+ \"},\"\n-   78: \t\t\t\t\t\t+ \"'NOTmyField': {\" // Ignored during validation\n-   79: \t\t\t\t\t\t\t\t+ \"'type': 'date',\"\n-  142: \t}\n-  143: \n-  144: \t@Test\n-  145: \tpublic void mapping_missing() {\n-  146: \t\tAssume.assumeTrue(\n-  147: \t\t\t\t\"Skipping this test as there is always a mapping (be it empty) in \" + elasticSearchClient.getDialect(),\n-  148: \t\t\t\telasticSearchClient.getDialect().isEmptyMappingPossible()\n-  149: \t\t);\n-  150: \n-  151: \t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n-  152: \t\t\troot.field( \"myField\", f -> f.asLocalDate() )\n-  153: \t\t\t\t\t.toReference();\n-  154: \t\t} );\n+   75: \t\t\t\t\t\t+ \"},\"\n+   76: \t\t\t\t\t\t+ \"'NOTmyField': {\" // Ignored during validation\n+   77: \t\t\t\t\t\t\t\t+ \"'type': 'date',\"\n+   78: \t\t\t\t\t\t\t\t+ \"'index': true\"\n+   79: \t\t\t\t\t\t+ \"}\"\n+  142: \t@Test\n+  143: \tpublic void mapping_missing() {\n+  144: \t\tAssume.assumeTrue(\n+  145: \t\t\t\t\"Skipping this test as there is always a mapping (be it empty) in \" + elasticSearchClient.getDialect(),\n+  146: \t\t\t\telasticSearchClient.getDialect().isEmptyMappingPossible()\n+  147: \t\t);\n+  148: \n+  149: \t\tStubMappedIndex index = StubMappedIndex.ofNonRetrievable( root -> {\n+  150: \t\t\troot.field( \"myField\", f -> f.asLocalDate() )\n+  151: \t\t\t\t\t.toReference();\n+  152: \t\t} );\n+  153: \n+  154: \t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n   155: \n-  156: \t\telasticSearchClient.index( index.name() ).deleteAndCreate();\n-  157: \n-  158: \t\tsetupAndValidateExpectingFailure(\n-  159: \t\t\t\tindex,\n-  160: \t\t\t\tFailureReportUtils.buildFailureReportPattern()\n-  161: \t\t\t\t\t\t.contextLiteral( SCHEMA_VALIDATION_CONTEXT )\n-  162: \t\t\t\t\t\t.failure( \"Missing type mapping\" )\n-  163: \t\t\t\t\t\t.build()\n-  164: \t\t);\n-  165: \t}\n+  156: \t\tsetupAndValidateExpectingFailure(\n+  157: \t\t\t\tindex,\n+  158: \t\t\t\tbuildValidationFailureReportPattern()\n+  159: \t\t\t\t\t\t.failure( \"Missing type mapping\" )\n+  160: \t\t\t\t\t\t.build()\n+  161: \t\t);\n+  162: \t}\n+  163: \n+  164: \t@Test\n+  165: \tpublic void attribute_field_notPresent() {\n",
        "uniqueId": "0b4c69d582f4c29554d78da38daebbc49d9d40da_144_165_75_79_142_162",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tpublic qualifier() : Optional<String> extracted from public getQualifier() : Optional<String> in class org.hibernate.search.backend.elasticsearch.ElasticsearchVersion",
        "diffLocations": [
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/ElasticsearchVersion.java",
                "startLine": 91,
                "endLine": 93,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/ElasticsearchVersion.java",
                "startLine": 134,
                "endLine": 141,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/ElasticsearchVersion.java",
                "startLine": 127,
                "endLine": 132,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public Optional<String> getQualifier() {\n\t\treturn Optional.ofNullable( qualifier );\n\t}",
        "filePathBefore": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/ElasticsearchVersion.java",
        "isPureRefactoring": true,
        "commitId": "5512dedaa650a0484cde33dfd5c3f7ff1ac2a849",
        "packageNameBefore": "org.hibernate.search.backend.elasticsearch",
        "classNameBefore": "org.hibernate.search.backend.elasticsearch.ElasticsearchVersion",
        "methodNameBefore": "org.hibernate.search.backend.elasticsearch.ElasticsearchVersion#getQualifier",
        "classSignatureBefore": "public class ElasticsearchVersion ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.ElasticsearchVersion#getQualifier"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.ElasticsearchVersion"
        ],
        "classSignatureBeforeSet": [
            "public class ElasticsearchVersion "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.Locale;\nimport java.util.Optional;\nimport java.util.OptionalInt;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\npublic class ElasticsearchVersion {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate static final Pattern pattern = Pattern.compile( \"(\\\\d+)(?:\\\\.(\\\\d+)(?:\\\\.(\\\\d+)(?:-(\\\\w+))?)?)?\" );\n\n\t/**\n\t * @param versionString A version string following the format {@code x.y.z-qualifier},\n\t * where {@code x}, {@code y} and {@code z} are integers and {@code qualifier} is a string of word characters (alphanumeric or '_').\n\t * Incomplete versions are allowed, for example {@code 7.0} or just {@code 7}.\n\t * @return An {@link ElasticsearchVersion} object representing the given version.\n\t * @throws org.hibernate.search.util.common.SearchException If the input string doesn't follow the required format.\n\t */\n\t// This method conforms to the MicroProfile Config specification. Do not change its signature.\n\tpublic static ElasticsearchVersion of(String versionString) {\n\t\tfinal String normalizedVersion = versionString.trim().toLowerCase( Locale.ROOT );\n\t\tMatcher matcher = pattern.matcher( normalizedVersion );\n\t\tif ( !matcher.matches() ) {\n\t\t\tthrow log.invalidElasticsearchVersion( normalizedVersion );\n\t\t}\n\t\tString major = matcher.group( 1 );\n\t\tString minor = matcher.group( 2 );\n\t\tString micro = matcher.group( 3 );\n\t\tString qualifier = matcher.group( 4 );\n\t\treturn new ElasticsearchVersion(\n\t\t\t\tInteger.parseInt( major ),\n\t\t\t\tminor == null ? null : Integer.parseInt( minor ),\n\t\t\t\tmicro == null ? null : Integer.parseInt( micro ),\n\t\t\t\tqualifier\n\t\t);\n\t}\n\n\tprivate final int major;\n\tprivate final Integer minor;\n\tprivate final Integer micro;\n\tprivate final String qualifier;\n\n\tprivate ElasticsearchVersion(int major, Integer minor, Integer micro, String qualifier) {\n\t\tthis.major = major;\n\t\tthis.minor = minor;\n\t\tthis.micro = micro;\n\t\tthis.qualifier = qualifier;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder builder = new StringBuilder();\n\t\tbuilder.append( major );\n\t\tif ( minor != null ) {\n\t\t\tbuilder.append( \".\" ).append( minor );\n\t\t}\n\t\tif ( micro != null ) {\n\t\t\tbuilder.append( \".\" ).append( micro );\n\t\t}\n\t\tif ( qualifier != null ) {\n\t\t\tbuilder.append( \"-\" ).append( qualifier );\n\t\t}\n\t\treturn builder.toString();\n\t}\n\n\tpublic int getMajor() {\n\t\treturn major;\n\t}\n\n\tpublic OptionalInt getMinor() {\n\t\treturn minor == null ? OptionalInt.empty() : OptionalInt.of( minor );\n\t}\n\n\tpublic OptionalInt getMicro() {\n\t\treturn micro == null ? OptionalInt.empty() : OptionalInt.of( micro );\n\t}\n\n\tpublic Optional<String> getQualifier() {\n\t\treturn Optional.ofNullable( qualifier );\n\t}\n\n\t/**\n\t * @param other A version to be matched against this version.\n\t * @return {@code true} if the other version matches this version,\n\t * i.e. if all the components defined in this version are also defined in the other version with the same value.\n\t * {@code false} otherwise.\n\t * Components that are not defined in this version do not matter.\n\t */\n\tpublic boolean matches(ElasticsearchVersion other) {\n\t\treturn major == other.major\n\t\t\t\t&& ( minor == null || minor.equals( other.minor ) )\n\t\t\t\t&& ( micro == null || micro.equals( other.micro ) )\n\t\t\t\t&& ( qualifier == null || qualifier.equals( other.qualifier ) );\n\t}\n}\n",
        "filePathAfter": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/ElasticsearchVersion.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.Locale;\nimport java.util.Optional;\nimport java.util.OptionalInt;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\npublic class ElasticsearchVersion {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate static final Pattern pattern = Pattern.compile( \"(\\\\d+)(?:\\\\.(\\\\d+)(?:\\\\.(\\\\d+)(?:-(\\\\w+))?)?)?\" );\n\n\t/**\n\t * @param versionString A version string following the format {@code x.y.z-qualifier},\n\t * where {@code x}, {@code y} and {@code z} are integers and {@code qualifier} is a string of word characters (alphanumeric or '_').\n\t * Incomplete versions are allowed, for example {@code 7.0} or just {@code 7}.\n\t * @return An {@link ElasticsearchVersion} object representing the given version.\n\t * @throws org.hibernate.search.util.common.SearchException If the input string doesn't follow the required format.\n\t */\n\t// This method conforms to the MicroProfile Config specification. Do not change its signature.\n\tpublic static ElasticsearchVersion of(String versionString) {\n\t\tfinal String normalizedVersion = versionString.trim().toLowerCase( Locale.ROOT );\n\t\tMatcher matcher = pattern.matcher( normalizedVersion );\n\t\tif ( !matcher.matches() ) {\n\t\t\tthrow log.invalidElasticsearchVersion( normalizedVersion );\n\t\t}\n\t\tString major = matcher.group( 1 );\n\t\tString minor = matcher.group( 2 );\n\t\tString micro = matcher.group( 3 );\n\t\tString qualifier = matcher.group( 4 );\n\t\treturn new ElasticsearchVersion(\n\t\t\t\tInteger.parseInt( major ),\n\t\t\t\tminor == null ? null : Integer.parseInt( minor ),\n\t\t\t\tmicro == null ? null : Integer.parseInt( micro ),\n\t\t\t\tqualifier\n\t\t);\n\t}\n\n\tprivate final int major;\n\tprivate final Integer minor;\n\tprivate final Integer micro;\n\tprivate final String qualifier;\n\n\tprivate ElasticsearchVersion(int major, Integer minor, Integer micro, String qualifier) {\n\t\tthis.major = major;\n\t\tthis.minor = minor;\n\t\tthis.micro = micro;\n\t\tthis.qualifier = qualifier;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\tStringBuilder builder = new StringBuilder();\n\t\tbuilder.append( major );\n\t\tif ( minor != null ) {\n\t\t\tbuilder.append( \".\" ).append( minor );\n\t\t}\n\t\tif ( micro != null ) {\n\t\t\tbuilder.append( \".\" ).append( micro );\n\t\t}\n\t\tif ( qualifier != null ) {\n\t\t\tbuilder.append( \"-\" ).append( qualifier );\n\t\t}\n\t\treturn builder.toString();\n\t}\n\n\t/**\n\t * @return The \"major\" number of this version, i.e. the {@code x} in {@code x.y.z-qualifier}.\n\t */\n\tpublic int major() {\n\t\treturn major;\n\t}\n\n\t/**\n\t * @return The \"major\" number of this version, i.e. the {@code x} in {@code x.y.z-qualifier}.\n\t * @deprecated Use {@link #major()} instead.\n\t */\n\t@Deprecated\n\tpublic int getMajor() {\n\t\treturn major();\n\t}\n\n\t/**\n\t * @return The \"minor\" number of this version, i.e. the {@code y} in {@code x.y.z-qualifier}. May be empty.\n\t */\n\tpublic OptionalInt minor() {\n\t\treturn minor == null ? OptionalInt.empty() : OptionalInt.of( minor );\n\t}\n\n\t/**\n\t * @return The \"minor\" number of this version, i.e. the {@code y} in {@code x.y.z-qualifier}. May be empty.\n\t * @deprecated Use {@link #minor()} instead.\n\t */\n\t@Deprecated\n\tpublic OptionalInt getMinor() {\n\t\treturn minor();\n\t}\n\n\t/**\n\t * @return The \"minor\" number of this version, i.e. the {@code z} in {@code x.y.z-qualifier}. May be empty.\n\t */\n\tpublic OptionalInt micro() {\n\t\treturn micro == null ? OptionalInt.empty() : OptionalInt.of( micro );\n\t}\n\n\t/**\n\t * @return The \"minor\" number of this version, i.e. the {@code z} in {@code x.y.z-qualifier}. May be empty.\n\t * @deprecated Use {@link #micro()} instead.\n\t */\n\t@Deprecated\n\tpublic OptionalInt getMicro() {\n\t\treturn micro();\n\t}\n\n\t/**\n\t * @return The qualifier in this version, i.e. the {@code qualifier} in {@code x.y.z-qualifier}. May be empty.\n\t */\n\tpublic Optional<String> qualifier() {\n\t\treturn Optional.ofNullable( qualifier );\n\t}\n\n\t/**\n\t * @return The qualifier in this version, i.e. the {@code qualifier} in {@code x.y.z-qualifier}. May be empty.\n\t * @deprecated Use {@link #qualifier()} instead.\n\t */\n\t@Deprecated\n\tpublic Optional<String> getQualifier() {\n\t\treturn qualifier();\n\t}\n\n\t/**\n\t * @param other A version to be matched against this version.\n\t * @return {@code true} if the other version matches this version,\n\t * i.e. if all the components defined in this version are also defined in the other version with the same value.\n\t * {@code false} otherwise.\n\t * Components that are not defined in this version do not matter.\n\t */\n\tpublic boolean matches(ElasticsearchVersion other) {\n\t\treturn major == other.major\n\t\t\t\t&& ( minor == null || minor.equals( other.minor ) )\n\t\t\t\t&& ( micro == null || micro.equals( other.micro ) )\n\t\t\t\t&& ( qualifier == null || qualifier.equals( other.qualifier ) );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "/**\n\t * @return The qualifier in this version, i.e. the {@code qualifier} in {@code x.y.z-qualifier}. May be empty.\n\t */\n\tpublic Optional<String> qualifier() {\n\t\treturn Optional.ofNullable( qualifier );\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "/**\n\t * @return The qualifier in this version, i.e. the {@code qualifier} in {@code x.y.z-qualifier}. May be empty.\n\t * @deprecated Use {@link #qualifier()} instead.\n\t */\n\t@Deprecated\n\tpublic Optional<String> getQualifier() {\n\t\treturn qualifier();\n\t}\n/**\n\t * @return The qualifier in this version, i.e. the {@code qualifier} in {@code x.y.z-qualifier}. May be empty.\n\t */\n\tpublic Optional<String> qualifier() {\n\t\treturn Optional.ofNullable( qualifier );\n\t}",
        "diffSourceCode": "-   91: \tpublic Optional<String> getQualifier() {\n-   92: \t\treturn Optional.ofNullable( qualifier );\n+   91: \tpublic int getMajor() {\n+   92: \t\treturn major();\n    93: \t}\n+  127: \t/**\n+  128: \t * @return The qualifier in this version, i.e. the {@code qualifier} in {@code x.y.z-qualifier}. May be empty.\n+  129: \t */\n+  130: \tpublic Optional<String> qualifier() {\n+  131: \t\treturn Optional.ofNullable( qualifier );\n+  132: \t}\n+  134: \t/**\n+  135: \t * @return The qualifier in this version, i.e. the {@code qualifier} in {@code x.y.z-qualifier}. May be empty.\n+  136: \t * @deprecated Use {@link #qualifier()} instead.\n+  137: \t */\n+  138: \t@Deprecated\n+  139: \tpublic Optional<String> getQualifier() {\n+  140: \t\treturn qualifier();\n+  141: \t}\n",
        "uniqueId": "5512dedaa650a0484cde33dfd5c3f7ff1ac2a849_91_93_127_132_134_141",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 4
            },
            "LINE": {
                "missed": 0,
                "covered": 1
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tpublic entityLoading(failureFloodingThreshold Optional<Integer>) : void extracted from public entityLoading() : void in class org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/pojo-base/src/test/java/org/hibernate/search/integrationtest/mapper/pojo/massindexing/AbstractMassIndexingFailureIT.java",
                "startLine": 111,
                "endLine": 148,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/pojo-base/src/test/java/org/hibernate/search/integrationtest/mapper/pojo/massindexing/AbstractMassIndexingFailureIT.java",
                "startLine": 118,
                "endLine": 122,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/pojo-base/src/test/java/org/hibernate/search/integrationtest/mapper/pojo/massindexing/AbstractMassIndexingFailureIT.java",
                "startLine": 130,
                "endLine": 184,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-4236\")\n\tpublic void entityLoading() {\n\t\tString exceptionMessage = \"Entity loading error\";\n\n\t\tSearchMapping mapping = setupWithThrowingEntityLoading( exceptionMessage );\n\t\tString failingOperationAsString = \"Loading and extracting entity data for entity '\"\n\t\t\t\t+ Book.NAME + \"' during mass indexing\";\n\n\t\texpectMassIndexerLoadingOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\tfailingOperationAsString, COUNT\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer()\n\t\t\t\t\t\t.threadsToLoadObjects( 1 ) // Just to simplify the assertions\n\t\t\t\t\t\t.batchSizeToLoadObjects( 1 ), // We need more than 1000 batches in order to reproduce HSEARCH-4236\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertMassIndexerLoadingOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\tfailingOperationAsString, COUNT\n\t\t);\n\t}",
        "filePathBefore": "integrationtest/mapper/pojo-base/src/test/java/org/hibernate/search/integrationtest/mapper/pojo/massindexing/AbstractMassIndexingFailureIT.java",
        "isPureRefactoring": true,
        "commitId": "39e046ac17dc2ad0f012048547215ff7e1906ee4",
        "packageNameBefore": "org.hibernate.search.integrationtest.mapper.pojo.massindexing",
        "classNameBefore": "org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT#entityLoading",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureDefaultBackgroundFailureHandlerIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected void assertMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\n}\nmethodSignature: org.hibernate.search.mapper.orm.massindexing.impl.HibernateOrmMassIndexer#batchSizeToLoadObjects\n methodBody: public HibernateOrmMassIndexer batchSizeToLoadObjects(int batchSize) {\ncontext.objectLoadingBatchSize(batchSize);\nreturn this;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureDefaultBackgroundFailureHandlerIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\nlogged.expectEvent(Level.ERROR,ExceptionMatcherBuilder.isException(exceptionType).withMessage(exceptionMessage).build(),failingOperationAsString).times(count);\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithFailure\n methodBody: private void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable... expectationSetters) {\nBook.failOnBook2GetId.set(ExecutionExpectation.FAIL.equals(book2GetIdExpectation));\nBook.failOnBook2GetTitle.set(ExecutionExpectation.FAIL.equals(book2GetTitleExpectation));\nAssertionError assertionError=null;\ntrymassIndexer.context(StubLoadingContext.class,loadingContext);\nMassIndexingFailureHandler massIndexingFailureHandler=getMassIndexingFailureHandler();\nif(massIndexingFailureHandler != null){massIndexer.failureHandler(massIndexingFailureHandler);\n}for(Runnable expectationSetter: expectationSetters){expectationSetter.run();\n}Runnable runnable=() -> {\n  try {\n    massIndexer.startAndWait();\n  }\n catch (  InterruptedException e) {\n    fail(\"Unexpected InterruptedException: \" + e.getMessage());\n  }\n}\n;\nif(thrownExpectation == null){runnable.run();\n}{assertThatThrownBy(runnable::run).asInstanceOf(InstanceOfAssertFactories.type(Throwable.class)).satisfies(thrownExpectation);\n}backendMock.verifyExpectationsMet();\ncatch(AssertionError e)assertionError=e;\nthrow e;\nfinallyBook.failOnBook2GetId.set(false);\nBook.failOnBook2GetTitle.set(false);\nif(assertionError == null){switch(threadExpectation)case CREATED_AND_TERMINATED:Awaitility.await().untilAsserted(() -> assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isNotEmpty().allSatisfy(t -> assertThat(t).extracting(Thread::getState).isEqualTo(Thread.State.TERMINATED)));\nbreak;\ncase NOT_CREATED:assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isEmpty();\nbreak;\n}}\nmethodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT#expectIndexScaleWork\n methodBody: private Runnable expectIndexScaleWork(StubIndexScaleWork.Type type, ExecutionExpectation executionExpectation) {\nreturn () -> {\nswitch (executionExpectation) {\ncase SUCCEED:    backendMock.expectIndexScaleWorks(Book.NAME).indexScaleWork(type);\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(type.name() + \" failure\"));\nbackendMock.expectIndexScaleWorks(Book.NAME).indexScaleWork(type,failingFuture);\nbreak;\ncase SKIP:break;\n}\n}\n;\n}\nmethodSignature: org.hibernate.search.mapper.pojo.massindexing.spi.PojoMassIndexer#threadsToLoadObjects\n methodBody: PojoMassIndexer threadsToLoadObjects(int numberOfThreads);\nmethodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.MassIndexingFailureDefaultBackgroundFailureHandlerIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected void assertMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\n}\nmethodSignature: org.hibernate.search.mapper.orm.massindexing.MassIndexer#batchSizeToLoadObjects\n methodBody: MassIndexer batchSizeToLoadObjects(int batchSize);\nmethodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.MassIndexingFailureCustomBackgroundFailureHandlerIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected abstract void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count);\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithFailure\n methodBody: private void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\nBook.failOnBook2GetId.set(ExecutionExpectation.FAIL.equals(book2GetIdExpectation));\nBook.failOnBook2GetTitle.set(ExecutionExpectation.FAIL.equals(book2GetTitleExpectation));\ntryMassIndexingFailureHandler massIndexingFailureHandler=getMassIndexingFailureHandler();\nif(massIndexingFailureHandler != null){massIndexer.failureHandler(massIndexingFailureHandler);\n}for(Runnable expectationSetter: expectationSetters){expectationSetter.run();\n}Runnable runnable=() -> {\n  try {\n    massIndexer.startAndWait();\n  }\n catch (  InterruptedException e) {\n    fail(\"Unexpected InterruptedException: \" + e.getMessage());\n  }\n}\n;\nif(thrownExpectation == null){runnable.run();\n}{assertThatThrownBy(runnable::run).asInstanceOf(InstanceOfAssertFactories.type(Throwable.class)).satisfies(thrownExpectation);\n}backendMock.verifyExpectationsMet();\nswitch(threadExpectation)case CREATED_AND_TERMINATED:Awaitility.await().untilAsserted(() -> assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isNotEmpty().allSatisfy(t -> assertThat(t).extracting(Thread::getState).isEqualTo(Thread.State.TERMINATED)));\nbreak;\ncase NOT_CREATED:assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isEmpty();\nbreak;\nfinallyBook.failOnBook2GetId.set(false);\nBook.failOnBook2GetTitle.set(false);\n}\nmethodSignature: org.hibernate.search.mapper.pojo.standalone.massindexing.MassIndexer#threadsToLoadObjects\n methodBody: MassIndexer threadsToLoadObjects(int numberOfThreads);\nmethodSignature: org.hibernate.search.mapper.orm.massindexing.impl.HibernateOrmMassIndexer#threadsToLoadObjects\n methodBody: public MassIndexer threadsToLoadObjects(int numberOfThreads) {\ndelegate.threadsToLoadObjects(numberOfThreads);\nreturn this;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.MassIndexingFailureCustomMassIndexingFailureHandlerIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\n}\nmethodSignature: org.hibernate.search.mapper.pojo.standalone.massindexing.MassIndexer#batchSizeToLoadObjects\n methodBody: MassIndexer batchSizeToLoadObjects(int batchSize);\nmethodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.MassIndexingFailureCustomBackgroundFailureHandlerIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected void assertMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\nverify(failureHandler,times(count)).handle(entityFailureContextCapture.capture());\nEntityIndexingFailureContext context=entityFailureContextCapture.getValue();\nassertThat(context.throwable()).isInstanceOf(SimulatedFailure.class).hasMessageContainingAll(exceptionMessage);\nassertThat(context.failingOperation()).asString().isEqualTo(failingOperationAsString);\nassertThat(context.entityReferences()).hasSize(1);\nverifyNoMoreInteractions(failureHandler);\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomMassIndexingFailureHandlerIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\n}\nmethodSignature: org.hibernate.search.mapper.orm.massindexing.MassIndexer#threadsToLoadObjects\n methodBody: MassIndexer threadsToLoadObjects(int numberOfThreads);\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected abstract void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count);\nmethodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.MassIndexingFailureDefaultBackgroundFailureHandlerIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\nlogged.expectEvent(Level.ERROR,ExceptionMatcherBuilder.isException(exceptionType).withMessage(exceptionMessage).build(),failingOperationAsString).times(count);\n}\nmethodSignature: org.hibernate.search.mapper.pojo.standalone.massindexing.impl.StandalonePojoMassIndexer#batchSizeToLoadObjects\n methodBody: public MassIndexer batchSizeToLoadObjects(int batchSize) {\ncontext.batchSize(batchSize);\nreturn this;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomMassIndexingFailureHandlerIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected void assertMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\nverify(failureHandler,times(count)).handle(entityFailureContextCapture.capture());\nMassIndexingEntityFailureContext context=entityFailureContextCapture.getValue();\nassertThat(context.throwable()).isInstanceOf(SimulatedFailure.class).hasMessageContainingAll(exceptionMessage);\nassertThat(context.failingOperation()).asString().isEqualTo(failingOperationAsString);\nassertThat(context.entityReferences()).hasSize(1);\nverifyNoMoreInteractions(failureHandler);\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT#setupWithThrowingEntityLoading\n methodBody: private SearchMapping setupWithThrowingEntityLoading(String exceptionMessage) {\nreturn setup(new MassLoadingStrategy<Book,Integer>(){\n  @Override public MassIdentifierLoader createIdentifierLoader(  LoadingTypeGroup<Book> includedTypes,  MassIdentifierSink<Integer> sink,  MassLoadingOptions options){\n    return new MassIdentifierLoader(){\n      private int i=0;\n      @Override public void close(){\n      }\n      @Override public long totalCount(){\n        return COUNT;\n      }\n      @Override public void loadNext() throws InterruptedException {\n        sink.accept(Collections.singletonList(i++));\n        if (i >= totalCount()) {\n          sink.complete();\n        }\n      }\n    }\n;\n  }\n  @Override public MassEntityLoader<Integer> createEntityLoader(  LoadingTypeGroup<Book> includedTypes,  MassEntitySink<Book> sink,  MassLoadingOptions options){\n    return new MassEntityLoader<Integer>(){\n      @Override public void close(){\n      }\n      @Override public void load(      List<Integer> identifiers){\n        throw new SimulatedFailure(exceptionMessage);\n      }\n    }\n;\n  }\n}\n);\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.MassIndexingFailureCustomMassIndexingFailureHandlerIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected void assertMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\nverify(failureHandler,times(count)).handle(entityFailureContextCapture.capture());\nMassIndexingEntityFailureContext context=entityFailureContextCapture.getValue();\nassertThat(context.throwable()).isInstanceOf(SimulatedFailure.class).hasMessageContainingAll(exceptionMessage);\nassertThat(context.failingOperation()).asString().isEqualTo(failingOperationAsString);\nassertThat(context.entityReferences()).hasSize(1);\nverifyNoMoreInteractions(failureHandler);\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomBackgroundFailureHandlerIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected void assertMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\nverify(failureHandler,times(count)).handle(entityFailureContextCapture.capture());\nEntityIndexingFailureContext context=entityFailureContextCapture.getValue();\nassertThat(context.throwable()).isInstanceOf(SimulatedFailure.class).hasMessageContainingAll(exceptionMessage);\nassertThat(context.failingOperation()).asString().isEqualTo(failingOperationAsString);\nassertThat(context.entityReferences()).hasSize(1);\nverifyNoMoreInteractions(failureHandler);\n}\nmethodSignature: org.hibernate.search.mapper.pojo.massindexing.impl.PojoDefaultMassIndexer#threadsToLoadObjects\n methodBody: public PojoDefaultMassIndexer threadsToLoadObjects(int numberOfThreads) {\nif(numberOfThreads < 1){throw new IllegalArgumentException(\"numberOfThreads must be at least 1\");\n}this.documentBuilderThreads=numberOfThreads;\nreturn this;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexScaleWork\n methodBody: private Runnable expectIndexScaleWork(StubIndexScaleWork.Type type, ExecutionExpectation executionExpectation) {\nreturn () -> {\nswitch (executionExpectation) {\ncase SUCCEED:    backendMock.expectIndexScaleWorks(Book.NAME).indexScaleWork(type);\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(type.name() + \" failure\"));\nbackendMock.expectIndexScaleWorks(Book.NAME).indexScaleWork(type,failingFuture);\nbreak;\ncase SKIP:break;\n}\n}\n;\n}\nmethodSignature: org.hibernate.search.mapper.pojo.standalone.massindexing.impl.StandalonePojoMassIndexer#threadsToLoadObjects\n methodBody: public MassIndexer threadsToLoadObjects(int numberOfThreads) {\ndelegate.threadsToLoadObjects(numberOfThreads);\nreturn this;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomBackgroundFailureHandlerIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected abstract void assertMassIndexerLoadingOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString, int count);\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected abstract void assertMassIndexerLoadingOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString, int count);",
        "classSignatureBefore": "public abstract class AbstractMassIndexingFailureIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT#entityLoading"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT"
        ],
        "classSignatureBeforeSet": [
            "public abstract class AbstractMassIndexingFailureIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Tolerable Changes in the body \n Severe changes",
                "description": "Extra print lines - with non-mapped leaves",
                "mappingState": 2
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.pojo.massindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.assertj.core.api.Fail.fail;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.function.Consumer;\n\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.cfg.EngineSettings;\nimport org.hibernate.search.engine.cfg.spi.EngineSpiSettings;\nimport org.hibernate.search.engine.reporting.FailureHandler;\nimport org.hibernate.search.integrationtest.mapper.pojo.testsupport.loading.PersistenceTypeKey;\nimport org.hibernate.search.integrationtest.mapper.pojo.testsupport.loading.StubLoadingContext;\nimport org.hibernate.search.integrationtest.mapper.pojo.testsupport.loading.StubMassLoadingStrategy;\nimport org.hibernate.search.util.impl.integrationtest.mapper.pojo.standalone.StandalonePojoMappingSetupHelper;\nimport org.hibernate.search.mapper.pojo.standalone.loading.LoadingTypeGroup;\nimport org.hibernate.search.mapper.pojo.standalone.loading.MassEntityLoader;\nimport org.hibernate.search.mapper.pojo.standalone.loading.MassEntitySink;\nimport org.hibernate.search.mapper.pojo.standalone.loading.MassIdentifierLoader;\nimport org.hibernate.search.mapper.pojo.standalone.loading.MassIdentifierSink;\nimport org.hibernate.search.mapper.pojo.standalone.loading.MassLoadingOptions;\nimport org.hibernate.search.mapper.pojo.standalone.loading.MassLoadingStrategy;\nimport org.hibernate.search.mapper.pojo.standalone.mapping.SearchMapping;\nimport org.hibernate.search.mapper.pojo.standalone.massindexing.MassIndexer;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.DocumentId;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.mapper.pojo.massindexing.MassIndexingFailureHandler;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.reporting.FailureReportUtils;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.ThreadSpy;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubIndexScaleWork;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubSchemaManagementWork;\nimport org.hibernate.search.util.impl.test.annotation.TestForIssue;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.assertj.core.api.InstanceOfAssertFactories;\nimport org.awaitility.Awaitility;\n\npublic abstract class AbstractMassIndexingFailureIT {\n\n\tprivate static final int COUNT = 1500;\n\tpublic static final String TITLE_1 = \"Oliver Twist\";\n\tpublic static final String AUTHOR_1 = \"Charles Dickens\";\n\tpublic static final String TITLE_2 = \"Ulysses\";\n\tpublic static final String AUTHOR_2 = \"James Joyce\";\n\tpublic static final String TITLE_3 = \"Frankenstein\";\n\tpublic static final String AUTHOR_3 = \"Mary Shelley\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock();\n\n\t@Rule\n\tpublic final StandalonePojoMappingSetupHelper setupHelper\n\t\t\t= StandalonePojoMappingSetupHelper.withBackendMock( MethodHandles.lookup(), backendMock );\n\n\t@Rule\n\tpublic ThreadSpy threadSpy = new ThreadSpy();\n\n\tprivate final StubLoadingContext loadingContext = new StubLoadingContext();\n\n\t@Test\n\t@TestForIssue(jiraKey = {\"HSEARCH-4218\", \"HSEARCH-4236\"})\n\tpublic void identifierLoading() {\n\t\tString exceptionMessage = \"ID loading error\";\n\n\t\tSearchMapping mapping = setupWithThrowingIdentifierLoading( exceptionMessage );\n\n\t\texpectMassIndexerOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\t\"Fetching identifiers of entities to index for entity '\" + Book.NAME + \"' during mass indexing\"\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\t\"Fetching identifiers of entities to index for entity '\" + Book.NAME + \"' during mass indexing\"\n\t\t);\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-4236\")\n\tpublic void entityLoading() {\n\t\tString exceptionMessage = \"Entity loading error\";\n\n\t\tSearchMapping mapping = setupWithThrowingEntityLoading( exceptionMessage );\n\t\tString failingOperationAsString = \"Loading and extracting entity data for entity '\"\n\t\t\t\t+ Book.NAME + \"' during mass indexing\";\n\n\t\texpectMassIndexerLoadingOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\tfailingOperationAsString, COUNT\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer()\n\t\t\t\t\t\t.threadsToLoadObjects( 1 ) // Just to simplify the assertions\n\t\t\t\t\t\t.batchSizeToLoadObjects( 1 ), // We need more than 1000 batches in order to reproduce HSEARCH-4236\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertMassIndexerLoadingOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\tfailingOperationAsString, COUNT\n\t\t);\n\t}\n\n\t@Test\n\tpublic void indexing() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"Indexing failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getId() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getId failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getTitle() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getTitle failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityNonIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityNonIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void dropAndCreateSchema_exception() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString exceptionMessage = \"DROP_AND_CREATE failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SearchException.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer()\n\t\t\t\t\t\t.dropAndCreateSchemaOnStart( true ),\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.satisfies( FailureReportUtils.hasFailureReport()\n\t\t\t\t\t\t\t\t.typeContext( Book.class.getName() )\n\t\t\t\t\t\t\t\t.failure( exceptionMessage ) ),\n\t\t\t\texpectSchemaManagementWorkException( StubSchemaManagementWork.Type.DROP_AND_CREATE )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SearchException.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void purge() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString exceptionMessage = \"PURGE failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsBefore() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsAfter() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer()\n\t\t\t\t\t\t.mergeSegmentsOnFinish( true ),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void flush() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString exceptionMessage = \"FLUSH failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void refresh() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString exceptionMessage = \"REFRESH failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void indexingAndFlush() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString failingEntityIndexingExceptionMessage = \"Indexing failure\";\n\t\tString failingEntityIndexingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\t\tString failingMassIndexerOperationExceptionMessage = \"FLUSH failure\";\n\t\tString failingMassIndexerOperationAsString = \"MassIndexer operation\";\n\n\t\texpectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"2 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\tfailingEntityIndexingExceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t// The mass indexer operation failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getCause )\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll( failingMassIndexerOperationExceptionMessage )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void indexingAndRefresh() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString failingEntityIndexingExceptionMessage = \"Indexing failure\";\n\t\tString failingEntityIndexingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\t\tString failingMassIndexerOperationExceptionMessage = \"REFRESH failure\";\n\t\tString failingMassIndexerOperationAsString = \"MassIndexer operation\";\n\n\t\texpectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"2 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\tfailingEntityIndexingExceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t// The mass indexer operation failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getCause )\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll( failingMassIndexerOperationExceptionMessage )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\t}\n\n\tprotected abstract FailureHandler getBackgroundFailureHandlerReference();\n\n\tprotected abstract MassIndexingFailureHandler getMassIndexingFailureHandler();\n\n\tprotected void assertBeforeSetup() {\n\t}\n\n\tprotected void assertAfterSetup() {\n\t}\n\n\tprotected abstract void expectEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityIdGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityIdGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityNonIdGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityNonIdGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectMassIndexerOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString);\n\n\tprotected abstract void assertMassIndexerOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString);\n\n\tprotected abstract void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count);\n\n\tprotected abstract void assertMassIndexerLoadingOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString, int count);\n\n\tprotected abstract void expectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprotected abstract void assertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprivate void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmassIndexer,\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable... expectationSetters) {\n\t\tBook.failOnBook2GetId.set( ExecutionExpectation.FAIL.equals( book2GetIdExpectation ) );\n\t\tBook.failOnBook2GetTitle.set( ExecutionExpectation.FAIL.equals( book2GetTitleExpectation ) );\n\t\tAssertionError assertionError = null;\n\t\ttry {\n\t\t\t// Simulate passing information to connect to a DB, ...\n\t\t\tmassIndexer.context( StubLoadingContext.class, loadingContext );\n\n\t\t\tMassIndexingFailureHandler massIndexingFailureHandler = getMassIndexingFailureHandler();\n\t\t\tif ( massIndexingFailureHandler != null ) {\n\t\t\t\tmassIndexer.failureHandler( massIndexingFailureHandler );\n\t\t\t}\n\n\t\t\tfor ( Runnable expectationSetter : expectationSetters ) {\n\t\t\t\texpectationSetter.run();\n\t\t\t}\n\n\t\t\t// TODO HSEARCH-3728 simplify this when even indexing exceptions are propagated\n\t\t\tRunnable runnable = () -> {\n\t\t\t\ttry {\n\t\t\t\t\tmassIndexer.startAndWait();\n\t\t\t\t}\n\t\t\t\tcatch (InterruptedException e) {\n\t\t\t\t\tfail( \"Unexpected InterruptedException: \" + e.getMessage() );\n\t\t\t\t}\n\t\t\t};\n\t\t\tif ( thrownExpectation == null ) {\n\t\t\t\trunnable.run();\n\t\t\t}\n\t\t\telse {\n\t\t\t\tassertThatThrownBy( runnable::run )\n\t\t\t\t\t\t.asInstanceOf( InstanceOfAssertFactories.type( Throwable.class ) )\n\t\t\t\t\t\t.satisfies( thrownExpectation );\n\t\t\t}\n\t\t\tbackendMock.verifyExpectationsMet();\n\t\t}\n\t\tcatch (AssertionError e) {\n\t\t\tassertionError = e;\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tBook.failOnBook2GetId.set( false );\n\t\t\tBook.failOnBook2GetTitle.set( false );\n\n\t\t\tif ( assertionError == null ) {\n\t\t\t\tswitch ( threadExpectation ) {\n\t\t\t\t\tcase CREATED_AND_TERMINATED:\n\t\t\t\t\t\tAwaitility.await().untilAsserted(\n\t\t\t\t\t\t\t\t() -> assertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t\t\t.isNotEmpty()\n\t\t\t\t\t\t\t\t\t\t.allSatisfy( t -> assertThat( t )\n\t\t\t\t\t\t\t\t\t\t.extracting( Thread::getState )\n\t\t\t\t\t\t\t\t\t\t.isEqualTo( Thread.State.TERMINATED )\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase NOT_CREATED:\n\t\t\t\t\t\tassertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t.isEmpty();\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate Runnable expectSchemaManagementWorkException(StubSchemaManagementWork.Type type) {\n\t\treturn () -> {\n\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\tbackendMock.expectSchemaManagementWorks( Book.NAME )\n\t\t\t\t\t.work( type, failingFuture );\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexScaleWork(StubIndexScaleWork.Type type, ExecutionExpectation executionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( executionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectIndexScaleWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScaleWork( type );\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\t\t\tbackendMock.expectIndexScaleWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScaleWork( type, failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( workTwoExecutionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectWorks(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t);\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( \"Indexing failure\" ) );\n\t\t\t\t\tbackendMock.expectWorks(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.createAndExecuteFollowingWorks( failingFuture )\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t);\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbackendMock.expectWorks(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t);\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate SearchMapping setupWithThrowingIdentifierLoading(String exceptionMessage) {\n\t\treturn setup( new MassLoadingStrategy<Book, Integer>() {\n\t\t\t@Override\n\t\t\tpublic MassIdentifierLoader createIdentifierLoader(LoadingTypeGroup<Book> includedTypes,\n\t\t\t\t\tMassIdentifierSink<Integer> sink, MassLoadingOptions options) {\n\t\t\t\treturn new MassIdentifierLoader() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void close() {\n\t\t\t\t\t\t// Nothing to do\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic long totalCount() {\n\t\t\t\t\t\treturn 100;\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void loadNext() {\n\t\t\t\t\t\tthrow new SimulatedFailure( exceptionMessage );\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic MassEntityLoader<Integer> createEntityLoader(LoadingTypeGroup<Book> includedTypes,\n\t\t\t\t\tMassEntitySink<Book> sink, MassLoadingOptions options) {\n\t\t\t\treturn new MassEntityLoader<Integer>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void close() {\n\t\t\t\t\t\t// Nothing to do\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void load(List<Integer> identifiers) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException( \"Should not be called\" );\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t}\n\t\t} );\n\t}\n\n\tprivate SearchMapping setupWithThrowingEntityLoading(String exceptionMessage) {\n\t\treturn setup( new MassLoadingStrategy<Book, Integer>() {\n\t\t\t@Override\n\t\t\tpublic MassIdentifierLoader createIdentifierLoader(LoadingTypeGroup<Book> includedTypes,\n\t\t\t\t\tMassIdentifierSink<Integer> sink, MassLoadingOptions options) {\n\t\t\t\treturn new MassIdentifierLoader() {\n\t\t\t\t\tprivate int i = 0;\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void close() {\n\t\t\t\t\t\t// Nothing to do\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic long totalCount() {\n\t\t\t\t\t\t// We need more than 1000 batches in order to reproduce HSEARCH-4236.\n\t\t\t\t\t\t// That's because of the size of the queue:\n\t\t\t\t\t\t// see org.hibernate.search.mapper.orm.massindexing.impl.PojoProducerConsumerQueue.DEFAULT_BUFF_LENGTH\n\t\t\t\t\t\treturn COUNT;\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void loadNext() throws InterruptedException {\n\t\t\t\t\t\tsink.accept( Collections.singletonList( i++ ) );\n\t\t\t\t\t\tif ( i >= totalCount() ) {\n\t\t\t\t\t\t\tsink.complete();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic MassEntityLoader<Integer> createEntityLoader(LoadingTypeGroup<Book> includedTypes,\n\t\t\t\t\tMassEntitySink<Book> sink, MassLoadingOptions options) {\n\t\t\t\treturn new MassEntityLoader<Integer>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void close() {\n\t\t\t\t\t\t// Nothing to do\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void load(List<Integer> identifiers) {\n\t\t\t\t\t\tthrow new SimulatedFailure( exceptionMessage );\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t}\n\t\t} );\n\t}\n\n\tprivate SearchMapping setup() {\n\t\treturn setup( new StubMassLoadingStrategy<>( Book.PERSISTENCE_KEY ) );\n\t}\n\n\tprivate SearchMapping setup(MassLoadingStrategy<Book, Integer> loadingStrategy) {\n\t\tassertBeforeSetup();\n\n\t\tbackendMock.expectAnySchema( Book.NAME );\n\n\t\tSearchMapping mapping = setupHelper.start()\n\t\t\t\t.expectCustomBeans()\n\t\t\t\t.withPropertyRadical( EngineSettings.Radicals.BACKGROUND_FAILURE_HANDLER, getBackgroundFailureHandlerReference() )\n\t\t\t\t.withPropertyRadical( EngineSpiSettings.Radicals.THREAD_PROVIDER, threadSpy.getThreadProvider() )\n\t\t\t\t.withConfiguration( b -> {\n\t\t\t\t\tb.addEntityType( Book.class, c -> c .massLoadingStrategy( loadingStrategy ) );\n\t\t\t\t} )\n\t\t\t\t.setup( Book.class );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tpersist( new Book( 1, TITLE_1, AUTHOR_1 ) );\n\t\tpersist( new Book( 2, TITLE_2, AUTHOR_2 ) );\n\t\tpersist( new Book( 3, TITLE_3, AUTHOR_3 ) );\n\n\t\tassertAfterSetup();\n\n\t\treturn mapping;\n\t}\n\n\tprivate void persist(Book book) {\n\t\tloadingContext.persistenceMap( Book.PERSISTENCE_KEY ).put( book.id, book );\n\t}\n\n\tprivate enum ExecutionExpectation {\n\t\tSUCCEED,\n\t\tFAIL,\n\t\tSKIP;\n\t}\n\n\tprivate enum ThreadExpectation {\n\t\tCREATED_AND_TERMINATED,\n\t\tNOT_CREATED;\n\t}\n\n\t@Indexed(index = Book.NAME)\n\tpublic static class Book {\n\n\t\tpublic static final String NAME = \"Book\";\n\t\tpublic static final PersistenceTypeKey<Book, Integer> PERSISTENCE_KEY =\n\t\t\t\tnew PersistenceTypeKey<>( Book.class, Integer.class );\n\n\t\tprivate static final AtomicBoolean failOnBook2GetId = new AtomicBoolean( false );\n\t\tprivate static final AtomicBoolean failOnBook2GetTitle = new AtomicBoolean( false );\n\n\t\tprivate Integer id;\n\n\t\tprivate String title;\n\n\t\tprivate String author;\n\n\t\tpublic Book() {\n\t\t}\n\n\t\tpublic Book(Integer id, String title, String author) {\n\t\t\tthis.id = id;\n\t\t\tthis.title = title;\n\t\t\tthis.author = author;\n\t\t}\n\n\t\t@DocumentId // This must be on the getter, so that Hibernate Search uses getters instead of direct field access\n\t\tpublic Integer getId() {\n\t\t\tif ( id == 2 && failOnBook2GetId.getAndSet( false ) ) {\n\t\t\t\tthrow new SimulatedFailure( \"getId failure\" );\n\t\t\t}\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getTitle() {\n\t\t\tif ( id == 2 && failOnBook2GetTitle.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getTitle failure\" );\n\t\t\t}\n\t\t\treturn title;\n\t\t}\n\n\t\tpublic void setTitle(String title) {\n\t\t\tthis.title = title;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getAuthor() {\n\t\t\treturn author;\n\t\t}\n\n\t\tpublic void setAuthor(String author) {\n\t\t\tthis.author = author;\n\t\t}\n\t}\n\n\tprotected static class SimulatedFailure extends RuntimeException {\n\t\tSimulatedFailure(String message) {\n\t\t\tsuper( message );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/pojo-base/src/test/java/org/hibernate/search/integrationtest/mapper/pojo/massindexing/AbstractMassIndexingFailureIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.pojo.massindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.assertj.core.api.Fail.fail;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Optional;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.function.Consumer;\n\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.cfg.EngineSettings;\nimport org.hibernate.search.engine.cfg.spi.EngineSpiSettings;\nimport org.hibernate.search.engine.reporting.FailureHandler;\nimport org.hibernate.search.integrationtest.mapper.pojo.testsupport.loading.PersistenceTypeKey;\nimport org.hibernate.search.integrationtest.mapper.pojo.testsupport.loading.StubLoadingContext;\nimport org.hibernate.search.integrationtest.mapper.pojo.testsupport.loading.StubMassLoadingStrategy;\nimport org.hibernate.search.util.impl.integrationtest.mapper.pojo.standalone.StandalonePojoMappingSetupHelper;\nimport org.hibernate.search.mapper.pojo.standalone.loading.LoadingTypeGroup;\nimport org.hibernate.search.mapper.pojo.standalone.loading.MassEntityLoader;\nimport org.hibernate.search.mapper.pojo.standalone.loading.MassEntitySink;\nimport org.hibernate.search.mapper.pojo.standalone.loading.MassIdentifierLoader;\nimport org.hibernate.search.mapper.pojo.standalone.loading.MassIdentifierSink;\nimport org.hibernate.search.mapper.pojo.standalone.loading.MassLoadingOptions;\nimport org.hibernate.search.mapper.pojo.standalone.loading.MassLoadingStrategy;\nimport org.hibernate.search.mapper.pojo.standalone.mapping.SearchMapping;\nimport org.hibernate.search.mapper.pojo.standalone.massindexing.MassIndexer;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.DocumentId;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.mapper.pojo.massindexing.MassIndexingFailureHandler;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.reporting.FailureReportUtils;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.ThreadSpy;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubIndexScaleWork;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubSchemaManagementWork;\nimport org.hibernate.search.util.impl.test.annotation.TestForIssue;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.assertj.core.api.InstanceOfAssertFactories;\nimport org.awaitility.Awaitility;\n\npublic abstract class AbstractMassIndexingFailureIT {\n\n\tprivate static final int COUNT = 1500;\n\tprivate static final int FAILURE_FLOODING_THRESHOLD = 45;\n\tprivate static final int DEFAULT_FAILURE_FLOODING_THRESHOLD = 100;\n\tpublic static final String TITLE_1 = \"Oliver Twist\";\n\tpublic static final String AUTHOR_1 = \"Charles Dickens\";\n\tpublic static final String TITLE_2 = \"Ulysses\";\n\tpublic static final String AUTHOR_2 = \"James Joyce\";\n\tpublic static final String TITLE_3 = \"Frankenstein\";\n\tpublic static final String AUTHOR_3 = \"Mary Shelley\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock();\n\n\t@Rule\n\tpublic final StandalonePojoMappingSetupHelper setupHelper\n\t\t\t= StandalonePojoMappingSetupHelper.withBackendMock( MethodHandles.lookup(), backendMock );\n\n\t@Rule\n\tpublic ThreadSpy threadSpy = new ThreadSpy();\n\n\tprivate final StubLoadingContext loadingContext = new StubLoadingContext();\n\n\tpublic int getDefaultFailureFloodingThreshold() {\n\t\treturn DEFAULT_FAILURE_FLOODING_THRESHOLD;\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = {\"HSEARCH-4218\", \"HSEARCH-4236\"})\n\tpublic void identifierLoading() {\n\t\tString exceptionMessage = \"ID loading error\";\n\n\t\tSearchMapping mapping = setupWithThrowingIdentifierLoading( exceptionMessage );\n\n\t\texpectMassIndexerOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\t\"Fetching identifiers of entities to index for entity '\" + Book.NAME + \"' during mass indexing\"\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\t\"Fetching identifiers of entities to index for entity '\" + Book.NAME + \"' during mass indexing\"\n\t\t);\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-4236\")\n\tpublic void entityLoading() {\n\t\tentityLoading( Optional.empty() );\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-4236\")\n\tpublic void entityLoadingWithFailureFloodingThreshold() {\n\t\tentityLoading( Optional.of( FAILURE_FLOODING_THRESHOLD ) );\n\t}\n\n\tpublic void entityLoading(Optional<Integer> failureFloodingThreshold) {\n\t\tString exceptionMessage = \"Entity loading error\";\n\n\t\tSearchMapping mapping = setupWithThrowingEntityLoading( exceptionMessage );\n\t\tString failingOperationAsString = \"Loading and extracting entity data for entity '\"\n\t\t\t\t+ Book.NAME + \"' during mass indexing\";\n\n\t\t// in case of using default handlers we will get a failure flooding threshold of 100 coming from PojoMassIndexingDelegatingFailureHandler\n\t\tInteger actualThreshold = failureFloodingThreshold.orElseGet( this::getDefaultFailureFloodingThreshold );\n\t\texpectMassIndexerLoadingOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\tactualThreshold,\n\t\t\t\tfailingOperationAsString,\n\t\t\t\t\"Entities that could not be indexed correctly\"\n\t\t);\n\t\texpectMassIndexerLoadingOperationFailureHandling(\n\t\t\t\tSearchException.class,\n\t\t\t\t\"\",\n\t\t\t\t1,\n\t\t\t\t( COUNT - actualThreshold ) + \" failures went unreported for this operation to avoid flooding.\"\n\t\t);\n\n\t\tMassIndexer massIndexer = mapping.scope( Object.class ).massIndexer()\n\t\t\t\t.threadsToLoadObjects( 1 ) // Just to simplify the assertions\n\t\t\t\t.batchSizeToLoadObjects( 1 );\n\t\tfailureFloodingThreshold.ifPresent( threshold -> {\n\t\t\tmassIndexer.failureFloodingThreshold( FAILURE_FLOODING_THRESHOLD );\n\t\t} );\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmassIndexer,\n\t\t\t\t// We need more than 1000 batches in order to reproduce HSEARCH-4236\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertMassIndexerLoadingOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\tfailingOperationAsString,\n\t\t\t\tactualThreshold,\n\t\t\t\tSearchException.class,\n\t\t\t\t( COUNT - actualThreshold ) + \" failures went unreported for this operation to avoid flooding.\",\n\t\t\t\tfailingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void indexing() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"Indexing failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getId() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getId failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getTitle() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getTitle failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityNonIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityNonIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void dropAndCreateSchema_exception() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString exceptionMessage = \"DROP_AND_CREATE failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SearchException.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer()\n\t\t\t\t\t\t.dropAndCreateSchemaOnStart( true ),\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.satisfies( FailureReportUtils.hasFailureReport()\n\t\t\t\t\t\t\t\t.typeContext( Book.class.getName() )\n\t\t\t\t\t\t\t\t.failure( exceptionMessage ) ),\n\t\t\t\texpectSchemaManagementWorkException( StubSchemaManagementWork.Type.DROP_AND_CREATE )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SearchException.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void purge() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString exceptionMessage = \"PURGE failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsBefore() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsAfter() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer()\n\t\t\t\t\t\t.mergeSegmentsOnFinish( true ),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void flush() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString exceptionMessage = \"FLUSH failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void refresh() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString exceptionMessage = \"REFRESH failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void indexingAndFlush() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString failingEntityIndexingExceptionMessage = \"Indexing failure\";\n\t\tString failingEntityIndexingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\t\tString failingMassIndexerOperationExceptionMessage = \"FLUSH failure\";\n\t\tString failingMassIndexerOperationAsString = \"MassIndexer operation\";\n\n\t\texpectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"2 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\tfailingEntityIndexingExceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t// The mass indexer operation failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getCause )\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll( failingMassIndexerOperationExceptionMessage )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void indexingAndRefresh() {\n\t\tSearchMapping mapping = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString failingEntityIndexingExceptionMessage = \"Indexing failure\";\n\t\tString failingEntityIndexingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\t\tString failingMassIndexerOperationExceptionMessage = \"REFRESH failure\";\n\t\tString failingMassIndexerOperationAsString = \"MassIndexer operation\";\n\n\t\texpectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmapping.scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"2 failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\tfailingEntityIndexingExceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t// The mass indexer operation failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getCause )\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll( failingMassIndexerOperationExceptionMessage )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\t}\n\n\tprotected abstract FailureHandler getBackgroundFailureHandlerReference();\n\n\tprotected abstract MassIndexingFailureHandler getMassIndexingFailureHandler();\n\n\tprotected void assertBeforeSetup() {\n\t}\n\n\tprotected void assertAfterSetup() {\n\t}\n\n\tprotected abstract void expectEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityIdGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityIdGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityNonIdGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityNonIdGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectMassIndexerOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString);\n\n\tprotected abstract void assertMassIndexerOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString);\n\n\tprotected abstract void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, int count, String failingOperationAsString, String... extraMessages);\n\n\tprotected abstract void assertMassIndexerLoadingOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString,\n\t\t\tint failureFloodingThreshold, Class<? extends Throwable> closingExceptionType,\n\t\t\tString closingExceptionMessage, String closingFailingOperationAsString);\n\n\tprotected abstract void expectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprotected abstract void assertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprivate void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmassIndexer,\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable... expectationSetters) {\n\t\tBook.failOnBook2GetId.set( ExecutionExpectation.FAIL.equals( book2GetIdExpectation ) );\n\t\tBook.failOnBook2GetTitle.set( ExecutionExpectation.FAIL.equals( book2GetTitleExpectation ) );\n\t\tAssertionError assertionError = null;\n\t\ttry {\n\t\t\t// Simulate passing information to connect to a DB, ...\n\t\t\tmassIndexer.context( StubLoadingContext.class, loadingContext );\n\n\t\t\tMassIndexingFailureHandler massIndexingFailureHandler = getMassIndexingFailureHandler();\n\t\t\tif ( massIndexingFailureHandler != null ) {\n\t\t\t\tmassIndexer.failureHandler( massIndexingFailureHandler );\n\t\t\t}\n\n\t\t\tfor ( Runnable expectationSetter : expectationSetters ) {\n\t\t\t\texpectationSetter.run();\n\t\t\t}\n\n\t\t\t// TODO HSEARCH-3728 simplify this when even indexing exceptions are propagated\n\t\t\tRunnable runnable = () -> {\n\t\t\t\ttry {\n\t\t\t\t\tmassIndexer.startAndWait();\n\t\t\t\t}\n\t\t\t\tcatch (InterruptedException e) {\n\t\t\t\t\tfail( \"Unexpected InterruptedException: \" + e.getMessage() );\n\t\t\t\t}\n\t\t\t};\n\t\t\tif ( thrownExpectation == null ) {\n\t\t\t\trunnable.run();\n\t\t\t}\n\t\t\telse {\n\t\t\t\tassertThatThrownBy( runnable::run )\n\t\t\t\t\t\t.asInstanceOf( InstanceOfAssertFactories.type( Throwable.class ) )\n\t\t\t\t\t\t.satisfies( thrownExpectation );\n\t\t\t}\n\t\t\tbackendMock.verifyExpectationsMet();\n\t\t}\n\t\tcatch (AssertionError e) {\n\t\t\tassertionError = e;\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tBook.failOnBook2GetId.set( false );\n\t\t\tBook.failOnBook2GetTitle.set( false );\n\n\t\t\tif ( assertionError == null ) {\n\t\t\t\tswitch ( threadExpectation ) {\n\t\t\t\t\tcase CREATED_AND_TERMINATED:\n\t\t\t\t\t\tAwaitility.await().untilAsserted(\n\t\t\t\t\t\t\t\t() -> assertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t\t\t.isNotEmpty()\n\t\t\t\t\t\t\t\t\t\t.allSatisfy( t -> assertThat( t )\n\t\t\t\t\t\t\t\t\t\t.extracting( Thread::getState )\n\t\t\t\t\t\t\t\t\t\t.isEqualTo( Thread.State.TERMINATED )\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase NOT_CREATED:\n\t\t\t\t\t\tassertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t.isEmpty();\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate Runnable expectSchemaManagementWorkException(StubSchemaManagementWork.Type type) {\n\t\treturn () -> {\n\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\tbackendMock.expectSchemaManagementWorks( Book.NAME )\n\t\t\t\t\t.work( type, failingFuture );\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexScaleWork(StubIndexScaleWork.Type type, ExecutionExpectation executionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( executionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectIndexScaleWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScaleWork( type );\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\t\t\tbackendMock.expectIndexScaleWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScaleWork( type, failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( workTwoExecutionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectWorks(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t);\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( \"Indexing failure\" ) );\n\t\t\t\t\tbackendMock.expectWorks(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.createAndExecuteFollowingWorks( failingFuture )\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t);\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbackendMock.expectWorks(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t);\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate SearchMapping setupWithThrowingIdentifierLoading(String exceptionMessage) {\n\t\treturn setup( new MassLoadingStrategy<Book, Integer>() {\n\t\t\t@Override\n\t\t\tpublic MassIdentifierLoader createIdentifierLoader(LoadingTypeGroup<Book> includedTypes,\n\t\t\t\t\tMassIdentifierSink<Integer> sink, MassLoadingOptions options) {\n\t\t\t\treturn new MassIdentifierLoader() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void close() {\n\t\t\t\t\t\t// Nothing to do\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic long totalCount() {\n\t\t\t\t\t\treturn 100;\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void loadNext() {\n\t\t\t\t\t\tthrow new SimulatedFailure( exceptionMessage );\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic MassEntityLoader<Integer> createEntityLoader(LoadingTypeGroup<Book> includedTypes,\n\t\t\t\t\tMassEntitySink<Book> sink, MassLoadingOptions options) {\n\t\t\t\treturn new MassEntityLoader<Integer>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void close() {\n\t\t\t\t\t\t// Nothing to do\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void load(List<Integer> identifiers) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException( \"Should not be called\" );\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t}\n\t\t} );\n\t}\n\n\tprivate SearchMapping setupWithThrowingEntityLoading(String exceptionMessage) {\n\t\treturn setup( new MassLoadingStrategy<Book, Integer>() {\n\t\t\t@Override\n\t\t\tpublic MassIdentifierLoader createIdentifierLoader(LoadingTypeGroup<Book> includedTypes,\n\t\t\t\t\tMassIdentifierSink<Integer> sink, MassLoadingOptions options) {\n\t\t\t\treturn new MassIdentifierLoader() {\n\t\t\t\t\tprivate int i = 0;\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void close() {\n\t\t\t\t\t\t// Nothing to do\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic long totalCount() {\n\t\t\t\t\t\t// We need more than 1000 batches in order to reproduce HSEARCH-4236.\n\t\t\t\t\t\t// That's because of the size of the queue:\n\t\t\t\t\t\t// see org.hibernate.search.mapper.orm.massindexing.impl.PojoProducerConsumerQueue.DEFAULT_BUFF_LENGTH\n\t\t\t\t\t\treturn COUNT;\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void loadNext() throws InterruptedException {\n\t\t\t\t\t\tsink.accept( Collections.singletonList( i++ ) );\n\t\t\t\t\t\tif ( i >= totalCount() ) {\n\t\t\t\t\t\t\tsink.complete();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic MassEntityLoader<Integer> createEntityLoader(LoadingTypeGroup<Book> includedTypes,\n\t\t\t\t\tMassEntitySink<Book> sink, MassLoadingOptions options) {\n\t\t\t\treturn new MassEntityLoader<Integer>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void close() {\n\t\t\t\t\t\t// Nothing to do\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void load(List<Integer> identifiers) {\n\t\t\t\t\t\tthrow new SimulatedFailure( exceptionMessage );\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t}\n\t\t} );\n\t}\n\n\tprivate SearchMapping setup() {\n\t\treturn setup( new StubMassLoadingStrategy<>( Book.PERSISTENCE_KEY ) );\n\t}\n\n\tprivate SearchMapping setup(MassLoadingStrategy<Book, Integer> loadingStrategy) {\n\t\tassertBeforeSetup();\n\n\t\tbackendMock.expectAnySchema( Book.NAME );\n\n\t\tSearchMapping mapping = setupHelper.start()\n\t\t\t\t.expectCustomBeans()\n\t\t\t\t.withPropertyRadical( EngineSettings.Radicals.BACKGROUND_FAILURE_HANDLER, getBackgroundFailureHandlerReference() )\n\t\t\t\t.withPropertyRadical( EngineSpiSettings.Radicals.THREAD_PROVIDER, threadSpy.getThreadProvider() )\n\t\t\t\t.withConfiguration( b -> {\n\t\t\t\t\tb.addEntityType( Book.class, c -> c .massLoadingStrategy( loadingStrategy ) );\n\t\t\t\t} )\n\t\t\t\t.setup( Book.class );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tpersist( new Book( 1, TITLE_1, AUTHOR_1 ) );\n\t\tpersist( new Book( 2, TITLE_2, AUTHOR_2 ) );\n\t\tpersist( new Book( 3, TITLE_3, AUTHOR_3 ) );\n\n\t\tassertAfterSetup();\n\n\t\treturn mapping;\n\t}\n\n\tprivate void persist(Book book) {\n\t\tloadingContext.persistenceMap( Book.PERSISTENCE_KEY ).put( book.id, book );\n\t}\n\n\tprivate enum ExecutionExpectation {\n\t\tSUCCEED,\n\t\tFAIL,\n\t\tSKIP;\n\t}\n\n\tprivate enum ThreadExpectation {\n\t\tCREATED_AND_TERMINATED,\n\t\tNOT_CREATED;\n\t}\n\n\t@Indexed(index = Book.NAME)\n\tpublic static class Book {\n\n\t\tpublic static final String NAME = \"Book\";\n\t\tpublic static final PersistenceTypeKey<Book, Integer> PERSISTENCE_KEY =\n\t\t\t\tnew PersistenceTypeKey<>( Book.class, Integer.class );\n\n\t\tprivate static final AtomicBoolean failOnBook2GetId = new AtomicBoolean( false );\n\t\tprivate static final AtomicBoolean failOnBook2GetTitle = new AtomicBoolean( false );\n\n\t\tprivate Integer id;\n\n\t\tprivate String title;\n\n\t\tprivate String author;\n\n\t\tpublic Book() {\n\t\t}\n\n\t\tpublic Book(Integer id, String title, String author) {\n\t\t\tthis.id = id;\n\t\t\tthis.title = title;\n\t\t\tthis.author = author;\n\t\t}\n\n\t\t@DocumentId // This must be on the getter, so that Hibernate Search uses getters instead of direct field access\n\t\tpublic Integer getId() {\n\t\t\tif ( id == 2 && failOnBook2GetId.getAndSet( false ) ) {\n\t\t\t\tthrow new SimulatedFailure( \"getId failure\" );\n\t\t\t}\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getTitle() {\n\t\t\tif ( id == 2 && failOnBook2GetTitle.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getTitle failure\" );\n\t\t\t}\n\t\t\treturn title;\n\t\t}\n\n\t\tpublic void setTitle(String title) {\n\t\t\tthis.title = title;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getAuthor() {\n\t\t\treturn author;\n\t\t}\n\n\t\tpublic void setAuthor(String author) {\n\t\t\tthis.author = author;\n\t\t}\n\t}\n\n\tprotected static class SimulatedFailure extends RuntimeException {\n\t\tSimulatedFailure(String message) {\n\t\t\tsuper( message );\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "public void entityLoading(Optional<Integer> failureFloodingThreshold) {\n\t\tString exceptionMessage = \"Entity loading error\";\n\n\t\tSearchMapping mapping = setupWithThrowingEntityLoading( exceptionMessage );\n\t\tString failingOperationAsString = \"Loading and extracting entity data for entity '\"\n\t\t\t\t+ Book.NAME + \"' during mass indexing\";\n\n\t\t// in case of using default handlers we will get a failure flooding threshold of 100 coming from PojoMassIndexingDelegatingFailureHandler\n\t\tInteger actualThreshold = failureFloodingThreshold.orElseGet( this::getDefaultFailureFloodingThreshold );\n\t\texpectMassIndexerLoadingOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\tactualThreshold,\n\t\t\t\tfailingOperationAsString,\n\t\t\t\t\"Entities that could not be indexed correctly\"\n\t\t);\n\t\texpectMassIndexerLoadingOperationFailureHandling(\n\t\t\t\tSearchException.class,\n\t\t\t\t\"\",\n\t\t\t\t1,\n\t\t\t\t( COUNT - actualThreshold ) + \" failures went unreported for this operation to avoid flooding.\"\n\t\t);\n\n\t\tMassIndexer massIndexer = mapping.scope( Object.class ).massIndexer()\n\t\t\t\t.threadsToLoadObjects( 1 ) // Just to simplify the assertions\n\t\t\t\t.batchSizeToLoadObjects( 1 );\n\t\tfailureFloodingThreshold.ifPresent( threshold -> {\n\t\t\tmassIndexer.failureFloodingThreshold( FAILURE_FLOODING_THRESHOLD );\n\t\t} );\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmassIndexer,\n\t\t\t\t// We need more than 1000 batches in order to reproduce HSEARCH-4236\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertMassIndexerLoadingOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\tfailingOperationAsString,\n\t\t\t\tactualThreshold,\n\t\t\t\tSearchException.class,\n\t\t\t\t( COUNT - actualThreshold ) + \" failures went unreported for this operation to avoid flooding.\",\n\t\t\t\tfailingOperationAsString\n\t\t);\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureDefaultBackgroundFailureHandlerIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected void assertMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\n}",
            "methodSignature: org.hibernate.search.mapper.orm.massindexing.impl.HibernateOrmMassIndexer#batchSizeToLoadObjects\n methodBody: public HibernateOrmMassIndexer batchSizeToLoadObjects(int batchSize) {\ncontext.objectLoadingBatchSize(batchSize);\nreturn this;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureDefaultBackgroundFailureHandlerIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\nlogged.expectEvent(Level.ERROR,ExceptionMatcherBuilder.isException(exceptionType).withMessage(exceptionMessage).build(),failingOperationAsString).times(count);\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithFailure\n methodBody: private void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable... expectationSetters) {\nBook.failOnBook2GetId.set(ExecutionExpectation.FAIL.equals(book2GetIdExpectation));\nBook.failOnBook2GetTitle.set(ExecutionExpectation.FAIL.equals(book2GetTitleExpectation));\nAssertionError assertionError=null;\ntrymassIndexer.context(StubLoadingContext.class,loadingContext);\nMassIndexingFailureHandler massIndexingFailureHandler=getMassIndexingFailureHandler();\nif(massIndexingFailureHandler != null){massIndexer.failureHandler(massIndexingFailureHandler);\n}for(Runnable expectationSetter: expectationSetters){expectationSetter.run();\n}Runnable runnable=() -> {\n  try {\n    massIndexer.startAndWait();\n  }\n catch (  InterruptedException e) {\n    fail(\"Unexpected InterruptedException: \" + e.getMessage());\n  }\n}\n;\nif(thrownExpectation == null){runnable.run();\n}{assertThatThrownBy(runnable::run).asInstanceOf(InstanceOfAssertFactories.type(Throwable.class)).satisfies(thrownExpectation);\n}backendMock.verifyExpectationsMet();\ncatch(AssertionError e)assertionError=e;\nthrow e;\nfinallyBook.failOnBook2GetId.set(false);\nBook.failOnBook2GetTitle.set(false);\nif(assertionError == null){switch(threadExpectation)case CREATED_AND_TERMINATED:Awaitility.await().untilAsserted(() -> assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isNotEmpty().allSatisfy(t -> assertThat(t).extracting(Thread::getState).isEqualTo(Thread.State.TERMINATED)));\nbreak;\ncase NOT_CREATED:assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isEmpty();\nbreak;\n}}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT#expectIndexScaleWork\n methodBody: private Runnable expectIndexScaleWork(StubIndexScaleWork.Type type, ExecutionExpectation executionExpectation) {\nreturn () -> {\nswitch (executionExpectation) {\ncase SUCCEED:    backendMock.expectIndexScaleWorks(Book.NAME).indexScaleWork(type);\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(type.name() + \" failure\"));\nbackendMock.expectIndexScaleWorks(Book.NAME).indexScaleWork(type,failingFuture);\nbreak;\ncase SKIP:break;\n}\n}\n;\n}",
            "methodSignature: org.hibernate.search.mapper.pojo.massindexing.spi.PojoMassIndexer#threadsToLoadObjects\n methodBody: PojoMassIndexer threadsToLoadObjects(int numberOfThreads);",
            "methodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.MassIndexingFailureDefaultBackgroundFailureHandlerIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected void assertMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\n}",
            "methodSignature: org.hibernate.search.mapper.orm.massindexing.MassIndexer#batchSizeToLoadObjects\n methodBody: MassIndexer batchSizeToLoadObjects(int batchSize);",
            "methodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.MassIndexingFailureCustomBackgroundFailureHandlerIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected abstract void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count);",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithFailure\n methodBody: private void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\nBook.failOnBook2GetId.set(ExecutionExpectation.FAIL.equals(book2GetIdExpectation));\nBook.failOnBook2GetTitle.set(ExecutionExpectation.FAIL.equals(book2GetTitleExpectation));\ntryMassIndexingFailureHandler massIndexingFailureHandler=getMassIndexingFailureHandler();\nif(massIndexingFailureHandler != null){massIndexer.failureHandler(massIndexingFailureHandler);\n}for(Runnable expectationSetter: expectationSetters){expectationSetter.run();\n}Runnable runnable=() -> {\n  try {\n    massIndexer.startAndWait();\n  }\n catch (  InterruptedException e) {\n    fail(\"Unexpected InterruptedException: \" + e.getMessage());\n  }\n}\n;\nif(thrownExpectation == null){runnable.run();\n}{assertThatThrownBy(runnable::run).asInstanceOf(InstanceOfAssertFactories.type(Throwable.class)).satisfies(thrownExpectation);\n}backendMock.verifyExpectationsMet();\nswitch(threadExpectation)case CREATED_AND_TERMINATED:Awaitility.await().untilAsserted(() -> assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isNotEmpty().allSatisfy(t -> assertThat(t).extracting(Thread::getState).isEqualTo(Thread.State.TERMINATED)));\nbreak;\ncase NOT_CREATED:assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isEmpty();\nbreak;\nfinallyBook.failOnBook2GetId.set(false);\nBook.failOnBook2GetTitle.set(false);\n}",
            "methodSignature: org.hibernate.search.mapper.pojo.standalone.massindexing.MassIndexer#threadsToLoadObjects\n methodBody: MassIndexer threadsToLoadObjects(int numberOfThreads);",
            "methodSignature: org.hibernate.search.mapper.orm.massindexing.impl.HibernateOrmMassIndexer#threadsToLoadObjects\n methodBody: public MassIndexer threadsToLoadObjects(int numberOfThreads) {\ndelegate.threadsToLoadObjects(numberOfThreads);\nreturn this;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.MassIndexingFailureCustomMassIndexingFailureHandlerIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\n}",
            "methodSignature: org.hibernate.search.mapper.pojo.standalone.massindexing.MassIndexer#batchSizeToLoadObjects\n methodBody: MassIndexer batchSizeToLoadObjects(int batchSize);",
            "methodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.MassIndexingFailureCustomBackgroundFailureHandlerIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected void assertMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\nverify(failureHandler,times(count)).handle(entityFailureContextCapture.capture());\nEntityIndexingFailureContext context=entityFailureContextCapture.getValue();\nassertThat(context.throwable()).isInstanceOf(SimulatedFailure.class).hasMessageContainingAll(exceptionMessage);\nassertThat(context.failingOperation()).asString().isEqualTo(failingOperationAsString);\nassertThat(context.entityReferences()).hasSize(1);\nverifyNoMoreInteractions(failureHandler);\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomMassIndexingFailureHandlerIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\n}",
            "methodSignature: org.hibernate.search.mapper.orm.massindexing.MassIndexer#threadsToLoadObjects\n methodBody: MassIndexer threadsToLoadObjects(int numberOfThreads);",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected abstract void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count);",
            "methodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.MassIndexingFailureDefaultBackgroundFailureHandlerIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\nlogged.expectEvent(Level.ERROR,ExceptionMatcherBuilder.isException(exceptionType).withMessage(exceptionMessage).build(),failingOperationAsString).times(count);\n}",
            "methodSignature: org.hibernate.search.mapper.pojo.standalone.massindexing.impl.StandalonePojoMassIndexer#batchSizeToLoadObjects\n methodBody: public MassIndexer batchSizeToLoadObjects(int batchSize) {\ncontext.batchSize(batchSize);\nreturn this;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomMassIndexingFailureHandlerIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected void assertMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\nverify(failureHandler,times(count)).handle(entityFailureContextCapture.capture());\nMassIndexingEntityFailureContext context=entityFailureContextCapture.getValue();\nassertThat(context.throwable()).isInstanceOf(SimulatedFailure.class).hasMessageContainingAll(exceptionMessage);\nassertThat(context.failingOperation()).asString().isEqualTo(failingOperationAsString);\nassertThat(context.entityReferences()).hasSize(1);\nverifyNoMoreInteractions(failureHandler);\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT#setupWithThrowingEntityLoading\n methodBody: private SearchMapping setupWithThrowingEntityLoading(String exceptionMessage) {\nreturn setup(new MassLoadingStrategy<Book,Integer>(){\n  @Override public MassIdentifierLoader createIdentifierLoader(  LoadingTypeGroup<Book> includedTypes,  MassIdentifierSink<Integer> sink,  MassLoadingOptions options){\n    return new MassIdentifierLoader(){\n      private int i=0;\n      @Override public void close(){\n      }\n      @Override public long totalCount(){\n        return COUNT;\n      }\n      @Override public void loadNext() throws InterruptedException {\n        sink.accept(Collections.singletonList(i++));\n        if (i >= totalCount()) {\n          sink.complete();\n        }\n      }\n    }\n;\n  }\n  @Override public MassEntityLoader<Integer> createEntityLoader(  LoadingTypeGroup<Book> includedTypes,  MassEntitySink<Book> sink,  MassLoadingOptions options){\n    return new MassEntityLoader<Integer>(){\n      @Override public void close(){\n      }\n      @Override public void load(      List<Integer> identifiers){\n        throw new SimulatedFailure(exceptionMessage);\n      }\n    }\n;\n  }\n}\n);\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.MassIndexingFailureCustomMassIndexingFailureHandlerIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected void assertMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\nverify(failureHandler,times(count)).handle(entityFailureContextCapture.capture());\nMassIndexingEntityFailureContext context=entityFailureContextCapture.getValue();\nassertThat(context.throwable()).isInstanceOf(SimulatedFailure.class).hasMessageContainingAll(exceptionMessage);\nassertThat(context.failingOperation()).asString().isEqualTo(failingOperationAsString);\nassertThat(context.entityReferences()).hasSize(1);\nverifyNoMoreInteractions(failureHandler);\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomBackgroundFailureHandlerIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected void assertMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\nverify(failureHandler,times(count)).handle(entityFailureContextCapture.capture());\nEntityIndexingFailureContext context=entityFailureContextCapture.getValue();\nassertThat(context.throwable()).isInstanceOf(SimulatedFailure.class).hasMessageContainingAll(exceptionMessage);\nassertThat(context.failingOperation()).asString().isEqualTo(failingOperationAsString);\nassertThat(context.entityReferences()).hasSize(1);\nverifyNoMoreInteractions(failureHandler);\n}",
            "methodSignature: org.hibernate.search.mapper.pojo.massindexing.impl.PojoDefaultMassIndexer#threadsToLoadObjects\n methodBody: public PojoDefaultMassIndexer threadsToLoadObjects(int numberOfThreads) {\nif(numberOfThreads < 1){throw new IllegalArgumentException(\"numberOfThreads must be at least 1\");\n}this.documentBuilderThreads=numberOfThreads;\nreturn this;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexScaleWork\n methodBody: private Runnable expectIndexScaleWork(StubIndexScaleWork.Type type, ExecutionExpectation executionExpectation) {\nreturn () -> {\nswitch (executionExpectation) {\ncase SUCCEED:    backendMock.expectIndexScaleWorks(Book.NAME).indexScaleWork(type);\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(type.name() + \" failure\"));\nbackendMock.expectIndexScaleWorks(Book.NAME).indexScaleWork(type,failingFuture);\nbreak;\ncase SKIP:break;\n}\n}\n;\n}",
            "methodSignature: org.hibernate.search.mapper.pojo.standalone.massindexing.impl.StandalonePojoMassIndexer#threadsToLoadObjects\n methodBody: public MassIndexer threadsToLoadObjects(int numberOfThreads) {\ndelegate.threadsToLoadObjects(numberOfThreads);\nreturn this;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomBackgroundFailureHandlerIT#expectMassIndexerLoadingOperationFailureHandling\n methodBody: protected void expectMassIndexerLoadingOperationFailureHandling(Class<? extends Throwable> exceptionType,\n\t\t\tString exceptionMessage, String failingOperationAsString, int count) {\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.pojo.massindexing.AbstractMassIndexingFailureIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected abstract void assertMassIndexerLoadingOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString, int count);",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#assertMassIndexerLoadingOperationFailureHandling\n methodBody: protected abstract void assertMassIndexerLoadingOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString, int count);"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-4236\")\n\tpublic void entityLoading() {\n\t\tentityLoading( Optional.empty() );\n\t}\npublic void entityLoading(Optional<Integer> failureFloodingThreshold) {\n\t\tString exceptionMessage = \"Entity loading error\";\n\n\t\tSearchMapping mapping = setupWithThrowingEntityLoading( exceptionMessage );\n\t\tString failingOperationAsString = \"Loading and extracting entity data for entity '\"\n\t\t\t\t+ Book.NAME + \"' during mass indexing\";\n\n\t\t// in case of using default handlers we will get a failure flooding threshold of 100 coming from PojoMassIndexingDelegatingFailureHandler\n\t\tInteger actualThreshold = failureFloodingThreshold.orElseGet( this::getDefaultFailureFloodingThreshold );\n\t\texpectMassIndexerLoadingOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\tactualThreshold,\n\t\t\t\tfailingOperationAsString,\n\t\t\t\t\"Entities that could not be indexed correctly\"\n\t\t);\n\t\texpectMassIndexerLoadingOperationFailureHandling(\n\t\t\t\tSearchException.class,\n\t\t\t\t\"\",\n\t\t\t\t1,\n\t\t\t\t( COUNT - actualThreshold ) + \" failures went unreported for this operation to avoid flooding.\"\n\t\t);\n\n\t\tMassIndexer massIndexer = mapping.scope( Object.class ).massIndexer()\n\t\t\t\t.threadsToLoadObjects( 1 ) // Just to simplify the assertions\n\t\t\t\t.batchSizeToLoadObjects( 1 );\n\t\tfailureFloodingThreshold.ifPresent( threshold -> {\n\t\t\tmassIndexer.failureFloodingThreshold( FAILURE_FLOODING_THRESHOLD );\n\t\t} );\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmassIndexer,\n\t\t\t\t// We need more than 1000 batches in order to reproduce HSEARCH-4236\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"failure(s) occurred during mass indexing\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure: \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertMassIndexerLoadingOperationFailureHandling(\n\t\t\t\tSimulatedFailure.class, exceptionMessage,\n\t\t\t\tfailingOperationAsString,\n\t\t\t\tactualThreshold,\n\t\t\t\tSearchException.class,\n\t\t\t\t( COUNT - actualThreshold ) + \" failures went unreported for this operation to avoid flooding.\",\n\t\t\t\tfailingOperationAsString\n\t\t);\n\t}",
        "diffSourceCode": "-  111: \t@Test\n-  112: \t@TestForIssue(jiraKey = \"HSEARCH-4236\")\n-  113: \tpublic void entityLoading() {\n-  114: \t\tString exceptionMessage = \"Entity loading error\";\n-  115: \n-  116: \t\tSearchMapping mapping = setupWithThrowingEntityLoading( exceptionMessage );\n-  117: \t\tString failingOperationAsString = \"Loading and extracting entity data for entity '\"\n-  118: \t\t\t\t+ Book.NAME + \"' during mass indexing\";\n-  119: \n-  120: \t\texpectMassIndexerLoadingOperationFailureHandling(\n-  121: \t\t\t\tSimulatedFailure.class, exceptionMessage,\n-  122: \t\t\t\tfailingOperationAsString, COUNT\n-  123: \t\t);\n-  124: \n-  125: \t\tdoMassIndexingWithFailure(\n-  126: \t\t\t\tmapping.scope( Object.class ).massIndexer()\n-  127: \t\t\t\t\t\t.threadsToLoadObjects( 1 ) // Just to simplify the assertions\n-  128: \t\t\t\t\t\t.batchSizeToLoadObjects( 1 ), // We need more than 1000 batches in order to reproduce HSEARCH-4236\n-  129: \t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n-  130: \t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n-  131: \t\t\t\t\t\t.hasMessageContainingAll(\n-  132: \t\t\t\t\t\t\t\t\"failure(s) occurred during mass indexing\",\n-  133: \t\t\t\t\t\t\t\t\"See the logs for details.\",\n-  134: \t\t\t\t\t\t\t\t\"First failure: \",\n-  135: \t\t\t\t\t\t\t\texceptionMessage\n-  136: \t\t\t\t\t\t)\n-  137: \t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n-  138: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n-  139: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n-  140: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n-  141: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n-  142: \t\t);\n-  143: \n-  144: \t\tassertMassIndexerLoadingOperationFailureHandling(\n-  145: \t\t\t\tSimulatedFailure.class, exceptionMessage,\n-  146: \t\t\t\tfailingOperationAsString, COUNT\n-  147: \t\t);\n-  148: \t}\n-  149: \n-  150: \t@Test\n-  151: \tpublic void indexing() {\n-  152: \t\tSearchMapping mapping = setup();\n-  153: \n-  154: \t\tString entityName = Book.NAME;\n-  155: \t\tString entityReferenceAsString = Book.NAME + \"#2\";\n-  156: \t\tString exceptionMessage = \"Indexing failure\";\n-  157: \t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n-  158: \n-  159: \t\texpectEntityIndexingFailureHandling(\n-  160: \t\t\t\tentityName, entityReferenceAsString,\n-  161: \t\t\t\texceptionMessage, failingOperationAsString\n-  162: \t\t);\n-  163: \n-  164: \t\tdoMassIndexingWithFailure(\n-  165: \t\t\t\tmapping.scope( Object.class ).massIndexer(),\n-  166: \t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n-  167: \t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n-  168: \t\t\t\t\t\t.hasMessageContainingAll(\n-  169: \t\t\t\t\t\t\t\t\"1 failure(s) occurred during mass indexing\",\n-  170: \t\t\t\t\t\t\t\t\"See the logs for details.\",\n-  171: \t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n-  172: \t\t\t\t\t\t\t\texceptionMessage\n-  173: \t\t\t\t\t\t)\n-  174: \t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n-  175: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n-  176: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n-  177: \t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n-  178: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n-  179: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n-  180: \t\t);\n-  181: \n-  182: \t\tassertEntityIndexingFailureHandling(\n-  183: \t\t\t\tentityName, entityReferenceAsString,\n-  184: \t\t\t\texceptionMessage, failingOperationAsString\n+  111: \n+  112: \t\tassertMassIndexerOperationFailureHandling(\n+  113: \t\t\t\tSimulatedFailure.class, exceptionMessage,\n+  114: \t\t\t\t\"Fetching identifiers of entities to index for entity '\" + Book.NAME + \"' during mass indexing\"\n+  115: \t\t);\n+  116: \t}\n+  117: \n+  118: \t@Test\n+  119: \t@TestForIssue(jiraKey = \"HSEARCH-4236\")\n+  120: \tpublic void entityLoading() {\n+  121: \t\tentityLoading( Optional.empty() );\n+  122: \t}\n+  123: \n+  124: \t@Test\n+  125: \t@TestForIssue(jiraKey = \"HSEARCH-4236\")\n+  126: \tpublic void entityLoadingWithFailureFloodingThreshold() {\n+  127: \t\tentityLoading( Optional.of( FAILURE_FLOODING_THRESHOLD ) );\n+  128: \t}\n+  129: \n+  130: \tpublic void entityLoading(Optional<Integer> failureFloodingThreshold) {\n+  131: \t\tString exceptionMessage = \"Entity loading error\";\n+  132: \n+  133: \t\tSearchMapping mapping = setupWithThrowingEntityLoading( exceptionMessage );\n+  134: \t\tString failingOperationAsString = \"Loading and extracting entity data for entity '\"\n+  135: \t\t\t\t+ Book.NAME + \"' during mass indexing\";\n+  136: \n+  137: \t\t// in case of using default handlers we will get a failure flooding threshold of 100 coming from PojoMassIndexingDelegatingFailureHandler\n+  138: \t\tInteger actualThreshold = failureFloodingThreshold.orElseGet( this::getDefaultFailureFloodingThreshold );\n+  139: \t\texpectMassIndexerLoadingOperationFailureHandling(\n+  140: \t\t\t\tSimulatedFailure.class, exceptionMessage,\n+  141: \t\t\t\tactualThreshold,\n+  142: \t\t\t\tfailingOperationAsString,\n+  143: \t\t\t\t\"Entities that could not be indexed correctly\"\n+  144: \t\t);\n+  145: \t\texpectMassIndexerLoadingOperationFailureHandling(\n+  146: \t\t\t\tSearchException.class,\n+  147: \t\t\t\t\"\",\n+  148: \t\t\t\t1,\n+  149: \t\t\t\t( COUNT - actualThreshold ) + \" failures went unreported for this operation to avoid flooding.\"\n+  150: \t\t);\n+  151: \n+  152: \t\tMassIndexer massIndexer = mapping.scope( Object.class ).massIndexer()\n+  153: \t\t\t\t.threadsToLoadObjects( 1 ) // Just to simplify the assertions\n+  154: \t\t\t\t.batchSizeToLoadObjects( 1 );\n+  155: \t\tfailureFloodingThreshold.ifPresent( threshold -> {\n+  156: \t\t\tmassIndexer.failureFloodingThreshold( FAILURE_FLOODING_THRESHOLD );\n+  157: \t\t} );\n+  158: \t\tdoMassIndexingWithFailure(\n+  159: \t\t\t\tmassIndexer,\n+  160: \t\t\t\t// We need more than 1000 batches in order to reproduce HSEARCH-4236\n+  161: \t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n+  162: \t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n+  163: \t\t\t\t\t\t.hasMessageContainingAll(\n+  164: \t\t\t\t\t\t\t\t\"failure(s) occurred during mass indexing\",\n+  165: \t\t\t\t\t\t\t\t\"See the logs for details.\",\n+  166: \t\t\t\t\t\t\t\t\"First failure: \",\n+  167: \t\t\t\t\t\t\t\texceptionMessage\n+  168: \t\t\t\t\t\t)\n+  169: \t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n+  170: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n+  171: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n+  172: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n+  173: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n+  174: \t\t);\n+  175: \n+  176: \t\tassertMassIndexerLoadingOperationFailureHandling(\n+  177: \t\t\t\tSimulatedFailure.class, exceptionMessage,\n+  178: \t\t\t\tfailingOperationAsString,\n+  179: \t\t\t\tactualThreshold,\n+  180: \t\t\t\tSearchException.class,\n+  181: \t\t\t\t( COUNT - actualThreshold ) + \" failures went unreported for this operation to avoid flooding.\",\n+  182: \t\t\t\tfailingOperationAsString\n+  183: \t\t);\n+  184: \t}\n",
        "uniqueId": "39e046ac17dc2ad0f012048547215ff7e1906ee4_111_148_130_184_118_122",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17
    },
    {
        "type": "Move And Rename Method",
        "description": "Move And Rename Method\tpublic of(descriptor ShardAssignmentDescriptor, finderProvider OutboxEventFinderProvider) : ShardAssignment from class org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.ShardAssignment to package create(descriptor ShardAssignmentDescriptor) : ShardAssignment from class org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.ShardAssignment.Provider",
        "diffLocations": [
            {
                "filePath": "mapper/orm-coordination-outbox-polling/src/main/java/org/hibernate/search/mapper/orm/coordination/outboxpolling/event/impl/ShardAssignment.java",
                "startLine": 22,
                "endLine": 34,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/orm-coordination-outbox-polling/src/main/java/org/hibernate/search/mapper/orm/coordination/outboxpolling/event/impl/ShardAssignment.java",
                "startLine": 29,
                "endLine": 40,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public static ShardAssignment of(ShardAssignmentDescriptor descriptor,\n\t\t\tOutboxEventFinderProvider finderProvider) {\n\t\tOptional<OutboxEventPredicate> predicate;\n\t\tif ( descriptor.totalShardCount == 1 ) {\n\t\t\tpredicate = Optional.empty();\n\t\t}\n\t\telse {\n\t\t\tRangeHashTable<Void> hashTable = new RangeHashTable<>( HASH_FUNCTION, descriptor.totalShardCount );\n\t\t\tRange<Integer> entityIdHashRange = hashTable.rangeForBucket( descriptor.assignedShardIndex );\n\t\t\tpredicate = Optional.of( new EntityIdHashRangeOutboxEventPredicate( entityIdHashRange ) );\n\t\t}\n\t\treturn new ShardAssignment( descriptor, finderProvider.create( predicate ) );\n\t}",
        "filePathBefore": "mapper/orm-coordination-outbox-polling/src/main/java/org/hibernate/search/mapper/orm/coordination/outboxpolling/event/impl/ShardAssignment.java",
        "isPureRefactoring": true,
        "commitId": "c1738e3fcb5ac2ded491a9ff1b9aa65e941be327",
        "packageNameBefore": "org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl",
        "classNameBefore": "org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.ShardAssignment",
        "methodNameBefore": "org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.ShardAssignment#of",
        "invokedMethod": "methodSignature: org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.OutboxPollingEventProcessor.Factory#create\n methodBody: public OutboxPollingEventProcessor create(ScheduledExecutorService scheduledExecutor,\n\t\t\t\tOutboxEventFinderProvider finderProvider, AgentRepositoryProvider agentRepositoryProvider,\n\t\t\t\tShardAssignmentDescriptor shardAssignmentOrNull) {\nString agentName=namePrefix(tenantId) + (shardAssignmentOrNull == null ? \"\" : \" - \" + shardAssignmentOrNull.assignedShardIndex);\nOutboxPollingEventProcessorClusterLink clusterLink=new OutboxPollingEventProcessorClusterLink(agentName,mapping.failureHandler(),clock,finderProvider,pollingInterval,pulseInterval,pulseExpiration,shardAssignmentOrNull);\nreturn new OutboxPollingEventProcessor(agentName,this,scheduledExecutor,agentRepositoryProvider,clusterLink);\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.FilteringOutboxEventFinder.Provider#create\n methodBody: public OutboxEventFinder create(Optional<OutboxEventPredicate> predicate) {\nreturn (session,maxResults) -> FilteringOutboxEventFinder.this.findOutboxEvents(session,maxResults,predicate);\n}\nmethodSignature: org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.ShardAssignment#of\n methodBody: public static ShardAssignment of(ShardAssignmentDescriptor descriptor,\n\t\t\tOutboxEventFinderProvider finderProvider) {\nOptional<OutboxEventPredicate> predicate;\nif(descriptor.totalShardCount == 1){predicate=Optional.empty();\n}{RangeHashTable<Void> hashTable=new RangeHashTable<>(HASH_FUNCTION,descriptor.totalShardCount);\nRange<Integer> entityIdHashRange=hashTable.rangeForBucket(descriptor.assignedShardIndex);\npredicate=Optional.of(new EntityIdHashRangeOutboxEventPredicate(entityIdHashRange));\n}return new ShardAssignment(descriptor,finderProvider.create(predicate));\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.DisconnectionSimulatingAgentRepositoryProvider#create\n methodBody: public AgentRepository create(Session session) {\nif(!preventPulse){throw new RuntimeException(\"Simulating a disconnection from the database\");\n}return delegate.create(session);\n}\nmethodSignature: org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.DefaultOutboxEventFinder.Provider#create\n methodBody: public DefaultOutboxEventFinder create(Optional<OutboxEventPredicate> predicate) {\nreturn new DefaultOutboxEventFinder(predicate);\n}",
        "classSignatureBefore": "final class ShardAssignment ",
        "methodNameBeforeSet": [
            "org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.ShardAssignment#of"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.ShardAssignment"
        ],
        "classSignatureBeforeSet": [
            "final class ShardAssignment "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl;\n\nimport java.util.Optional;\n\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.cluster.impl.ShardAssignmentDescriptor;\nimport org.hibernate.search.util.common.data.Range;\nimport org.hibernate.search.util.common.data.impl.Murmur3HashFunction;\nimport org.hibernate.search.util.common.data.impl.RangeCompatibleHashFunction;\nimport org.hibernate.search.util.common.data.impl.RangeHashTable;\n\nfinal class ShardAssignment {\n\t// Note the hash function / table implementations MUST NOT CHANGE,\n\t// otherwise existing indexes will no longer work correctly.\n\tpublic static final RangeCompatibleHashFunction HASH_FUNCTION = Murmur3HashFunction.INSTANCE;\n\n\tpublic static ShardAssignment of(ShardAssignmentDescriptor descriptor,\n\t\t\tOutboxEventFinderProvider finderProvider) {\n\t\tOptional<OutboxEventPredicate> predicate;\n\t\tif ( descriptor.totalShardCount == 1 ) {\n\t\t\tpredicate = Optional.empty();\n\t\t}\n\t\telse {\n\t\t\tRangeHashTable<Void> hashTable = new RangeHashTable<>( HASH_FUNCTION, descriptor.totalShardCount );\n\t\t\tRange<Integer> entityIdHashRange = hashTable.rangeForBucket( descriptor.assignedShardIndex );\n\t\t\tpredicate = Optional.of( new EntityIdHashRangeOutboxEventPredicate( entityIdHashRange ) );\n\t\t}\n\t\treturn new ShardAssignment( descriptor, finderProvider.create( predicate ) );\n\t}\n\n\tfinal ShardAssignmentDescriptor descriptor;\n\tfinal OutboxEventFinder eventFinder;\n\n\t// Exposed for testing purposes only\n\tShardAssignment(ShardAssignmentDescriptor descriptor, OutboxEventFinder eventFinder) {\n\t\tthis.descriptor = descriptor;\n\t\tthis.eventFinder = eventFinder;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn descriptor.toString();\n\t}\n\n}\n",
        "filePathAfter": "mapper/orm-coordination-outbox-polling/src/main/java/org/hibernate/search/mapper/orm/coordination/outboxpolling/event/impl/ShardAssignment.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl;\n\nimport java.util.Optional;\n\nimport org.hibernate.search.mapper.orm.coordination.outboxpolling.cluster.impl.ShardAssignmentDescriptor;\nimport org.hibernate.search.util.common.data.Range;\nimport org.hibernate.search.util.common.data.impl.Murmur3HashFunction;\nimport org.hibernate.search.util.common.data.impl.RangeCompatibleHashFunction;\nimport org.hibernate.search.util.common.data.impl.RangeHashTable;\n\nfinal class ShardAssignment {\n\t// Note the hash function / table implementations MUST NOT CHANGE,\n\t// otherwise existing indexes will no longer work correctly.\n\tpublic static final RangeCompatibleHashFunction HASH_FUNCTION = Murmur3HashFunction.INSTANCE;\n\n\tpublic static class Provider {\n\t\tprivate final OutboxEventFinderProvider finderProvider;\n\n\t\tpublic Provider(OutboxEventFinderProvider finderProvider) {\n\t\t\tthis.finderProvider = finderProvider;\n\t\t}\n\n\t\tShardAssignment create(ShardAssignmentDescriptor descriptor) {\n\t\t\tOptional<OutboxEventPredicate> predicate;\n\t\t\tif ( descriptor.totalShardCount == 1 ) {\n\t\t\t\tpredicate = Optional.empty();\n\t\t\t}\n\t\t\telse {\n\t\t\t\tRangeHashTable<Void> hashTable = new RangeHashTable<>( HASH_FUNCTION, descriptor.totalShardCount );\n\t\t\t\tRange<Integer> entityIdHashRange = hashTable.rangeForBucket( descriptor.assignedShardIndex );\n\t\t\t\tpredicate = Optional.of( new EntityIdHashRangeOutboxEventPredicate( entityIdHashRange ) );\n\t\t\t}\n\t\t\treturn new ShardAssignment( descriptor, finderProvider.create( predicate ) );\n\t\t}\n\n\t}\n\n\tfinal ShardAssignmentDescriptor descriptor;\n\tfinal OutboxEventFinder eventFinder;\n\n\t// Exposed for testing purposes only\n\tShardAssignment(ShardAssignmentDescriptor descriptor, OutboxEventFinder eventFinder) {\n\t\tthis.descriptor = descriptor;\n\t\tthis.eventFinder = eventFinder;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn descriptor.toString();\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.OutboxPollingEventProcessor.Factory#create\n methodBody: public OutboxPollingEventProcessor create(ScheduledExecutorService scheduledExecutor,\n\t\t\t\tOutboxEventFinderProvider finderProvider, AgentRepositoryProvider agentRepositoryProvider,\n\t\t\t\tShardAssignmentDescriptor shardAssignmentOrNull) {\nString agentName=namePrefix(tenantId) + (shardAssignmentOrNull == null ? \"\" : \" - \" + shardAssignmentOrNull.assignedShardIndex);\nOutboxPollingEventProcessorClusterLink clusterLink=new OutboxPollingEventProcessorClusterLink(agentName,mapping.failureHandler(),clock,finderProvider,pollingInterval,pulseInterval,pulseExpiration,shardAssignmentOrNull);\nreturn new OutboxPollingEventProcessor(agentName,this,scheduledExecutor,agentRepositoryProvider,clusterLink);\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.FilteringOutboxEventFinder.Provider#create\n methodBody: public OutboxEventFinder create(Optional<OutboxEventPredicate> predicate) {\nreturn (session,maxResults) -> FilteringOutboxEventFinder.this.findOutboxEvents(session,maxResults,predicate);\n}",
            "methodSignature: org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.ShardAssignment#of\n methodBody: public static ShardAssignment of(ShardAssignmentDescriptor descriptor,\n\t\t\tOutboxEventFinderProvider finderProvider) {\nOptional<OutboxEventPredicate> predicate;\nif(descriptor.totalShardCount == 1){predicate=Optional.empty();\n}{RangeHashTable<Void> hashTable=new RangeHashTable<>(HASH_FUNCTION,descriptor.totalShardCount);\nRange<Integer> entityIdHashRange=hashTable.rangeForBucket(descriptor.assignedShardIndex);\npredicate=Optional.of(new EntityIdHashRangeOutboxEventPredicate(entityIdHashRange));\n}return new ShardAssignment(descriptor,finderProvider.create(predicate));\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.coordination.outboxpolling.automaticindexing.DisconnectionSimulatingAgentRepositoryProvider#create\n methodBody: public AgentRepository create(Session session) {\nif(!preventPulse){throw new RuntimeException(\"Simulating a disconnection from the database\");\n}return delegate.create(session);\n}",
            "methodSignature: org.hibernate.search.mapper.orm.coordination.outboxpolling.event.impl.DefaultOutboxEventFinder.Provider#create\n methodBody: public DefaultOutboxEventFinder create(Optional<OutboxEventPredicate> predicate) {\nreturn new DefaultOutboxEventFinder(predicate);\n}"
        ],
        "sourceCodeAfterRefactoring": "ShardAssignment create(ShardAssignmentDescriptor descriptor) {\n\t\t\tOptional<OutboxEventPredicate> predicate;\n\t\t\tif ( descriptor.totalShardCount == 1 ) {\n\t\t\t\tpredicate = Optional.empty();\n\t\t\t}\n\t\t\telse {\n\t\t\t\tRangeHashTable<Void> hashTable = new RangeHashTable<>( HASH_FUNCTION, descriptor.totalShardCount );\n\t\t\t\tRange<Integer> entityIdHashRange = hashTable.rangeForBucket( descriptor.assignedShardIndex );\n\t\t\t\tpredicate = Optional.of( new EntityIdHashRangeOutboxEventPredicate( entityIdHashRange ) );\n\t\t\t}\n\t\t\treturn new ShardAssignment( descriptor, finderProvider.create( predicate ) );\n\t\t}",
        "diffSourceCode": "-   22: \tpublic static ShardAssignment of(ShardAssignmentDescriptor descriptor,\n-   23: \t\t\tOutboxEventFinderProvider finderProvider) {\n-   24: \t\tOptional<OutboxEventPredicate> predicate;\n-   25: \t\tif ( descriptor.totalShardCount == 1 ) {\n-   26: \t\t\tpredicate = Optional.empty();\n+   22: \tpublic static class Provider {\n+   23: \t\tprivate final OutboxEventFinderProvider finderProvider;\n+   24: \n+   25: \t\tpublic Provider(OutboxEventFinderProvider finderProvider) {\n+   26: \t\t\tthis.finderProvider = finderProvider;\n    27: \t\t}\n-   28: \t\telse {\n-   29: \t\t\tRangeHashTable<Void> hashTable = new RangeHashTable<>( HASH_FUNCTION, descriptor.totalShardCount );\n-   30: \t\t\tRange<Integer> entityIdHashRange = hashTable.rangeForBucket( descriptor.assignedShardIndex );\n-   31: \t\t\tpredicate = Optional.of( new EntityIdHashRangeOutboxEventPredicate( entityIdHashRange ) );\n-   32: \t\t}\n-   33: \t\treturn new ShardAssignment( descriptor, finderProvider.create( predicate ) );\n-   34: \t}\n-   35: \n-   36: \tfinal ShardAssignmentDescriptor descriptor;\n-   37: \tfinal OutboxEventFinder eventFinder;\n-   38: \n-   39: \t// Exposed for testing purposes only\n-   40: \tShardAssignment(ShardAssignmentDescriptor descriptor, OutboxEventFinder eventFinder) {\n+   28: \n+   29: \t\tShardAssignment create(ShardAssignmentDescriptor descriptor) {\n+   30: \t\t\tOptional<OutboxEventPredicate> predicate;\n+   31: \t\t\tif ( descriptor.totalShardCount == 1 ) {\n+   32: \t\t\t\tpredicate = Optional.empty();\n+   33: \t\t\t}\n+   34: \t\t\telse {\n+   35: \t\t\t\tRangeHashTable<Void> hashTable = new RangeHashTable<>( HASH_FUNCTION, descriptor.totalShardCount );\n+   36: \t\t\t\tRange<Integer> entityIdHashRange = hashTable.rangeForBucket( descriptor.assignedShardIndex );\n+   37: \t\t\t\tpredicate = Optional.of( new EntityIdHashRangeOutboxEventPredicate( entityIdHashRange ) );\n+   38: \t\t\t}\n+   39: \t\t\treturn new ShardAssignment( descriptor, finderProvider.create( predicate ) );\n+   40: \t\t}\n",
        "uniqueId": "c1738e3fcb5ac2ded491a9ff1b9aa65e941be327_22_34__29_40",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 33
            },
            "BRANCH": {
                "missed": 0,
                "covered": 2
            },
            "LINE": {
                "missed": 0,
                "covered": 6
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 2
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate createAndStartExecutor(maxTasksPerBatch int, fair boolean, blockingRetryProducer Consumer<? super BatchedWork<? super StubWorkProcessor>>) : void extracted from private createAndStartExecutor(maxTasksPerBatch int, fair boolean) : void in class org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutorTest",
        "diffLocations": [
            {
                "filePath": "engine/src/test/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutorTest.java",
                "startLine": 545,
                "endLine": 565,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/test/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutorTest.java",
                "startLine": 547,
                "endLine": 549,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/test/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutorTest.java",
                "startLine": 550,
                "endLine": 570,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void createAndStartExecutor(int maxTasksPerBatch, boolean fair) {\n\t\tthis.executor = new BatchingExecutor<>(\n\t\t\t\tNAME, processorMock, maxTasksPerBatch, fair, failureHandlerMock\n\t\t);\n\n\t\t// Having multiple threads should not matter:\n\t\t// the batching executor takes care of executing in only one thread at a time.\n\t\tthis.executorService = threadPoolProvider.newScheduledExecutor( 4, \"BatchingExecutorTest\" );\n\n\t\texecutor.start( new DelegatingSimpleScheduledExecutor( executorService ) );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// No calls expected yet\n\t\t} );\n\n\t\tCompletableFuture<?> completion = executor.completion();\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// This should not trigger any call to the mocks\n\t\t} );\n\t\t// Initially, there are no works, so works are considered completed.\n\t\tassertThatFuture( completion ).isSuccessful();\n\t}",
        "filePathBefore": "engine/src/test/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutorTest.java",
        "isPureRefactoring": true,
        "commitId": "56f06536bb2abc1339124800a0673afb8a240112",
        "packageNameBefore": "org.hibernate.search.engine.backend.orchestration.spi",
        "classNameBefore": "org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutorTest",
        "methodNameBefore": "org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutorTest#createAndStartExecutor",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutorTest#verifyAsynchronouslyAndReset\n methodBody: private void verifyAsynchronouslyAndReset(Consumer<InOrder> verify) {\nawait().untilAsserted(() -> {\n  InOrder inOrder=inOrder(mocks.toArray());\n  verify.accept(inOrder);\n}\n);\nverifyNoMoreInteractions(mocks.toArray());\nreset(mocks.toArray());\n}\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.AbstractWorkOrchestrator#completion\n methodBody: protected abstract CompletableFuture<?> completion();\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#completion\n methodBody: public CompletableFuture<?> completion() {\nif(processingTask == null){return CompletableFuture.completedFuture(null);\n}return processingTask.completion();\n}\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.AbstractWorkOrchestrator#start\n methodBody: public final void start(ConfigurationPropertySource propertySource) {\nlifecycleLock.writeLock().lock();\ntryswitch(state)case RUNNING:return;\ncase PRE_STOPPING:throw new IllegalStateException(\"Cannot start an orchestrator while it's stopping\");\ncase STOPPED:state=State.RUNNING;\ndoStart(propertySource);\nbreak;\nfinallylifecycleLock.writeLock().unlock();\n}\nmethodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchBatchingWorkOrchestrator#completion\n methodBody: protected CompletableFuture<?> completion() {\nCompletableFuture<?>[] completions=new CompletableFuture[executors.size()];\nfor(int i=0; i < executors.size(); i++){completions[i]=executors.get(i).completion();\n}return CompletableFuture.allOf(completions);\n}\nmethodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchSimpleWorkOrchestrator#completion\n methodBody: protected CompletableFuture<?> completion() {\nreturn CompletableFuture.completedFuture(null);\n}\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneSerialWorkOrchestratorImpl#completion\n methodBody: protected CompletableFuture<?> completion() {\nCompletableFuture<?>[] completions=new CompletableFuture[executors.size()];\nfor(int i=0; i < executors.size(); i++){completions[i]=executors.get(i).completion();\n}return CompletableFuture.allOf(completions);\n}\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#start\n methodBody: public synchronized void start(SimpleScheduledExecutor executorService) {\nlog.startingExecutor(name);\nprocessingTask=new SingletonTask(name,worker,new BatchScheduler(executorService),failureHandler);\n}\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneParallelWorkOrchestratorImpl#completion\n methodBody: protected CompletableFuture<?> completion() {\nreturn CompletableFuture.completedFuture(null);\n}\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneSyncWorkOrchestratorImpl#completion\n methodBody: protected CompletableFuture<?> completion() {\nreturn CompletableFuture.completedFuture(null);\n}",
        "classSignatureBefore": "public class BatchingExecutorTest ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutorTest#createAndStartExecutor"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutorTest"
        ],
        "classSignatureBeforeSet": [
            "public class BatchingExecutorTest "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Tolerable changes in the body\n",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.backend.orchestration.spi;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.assertj.core.api.Assertions.fail;\nimport static org.awaitility.Awaitility.await;\nimport static org.hibernate.search.util.impl.test.FutureAssert.assertThatFuture;\nimport static org.junit.Assume.assumeFalse;\nimport static org.junit.Assume.assumeTrue;\nimport static org.mockito.ArgumentMatchers.any;\nimport static org.mockito.Mockito.doAnswer;\nimport static org.mockito.Mockito.doThrow;\nimport static org.mockito.Mockito.inOrder;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.reset;\nimport static org.mockito.Mockito.verify;\nimport static org.mockito.Mockito.verifyNoInteractions;\nimport static org.mockito.Mockito.verifyNoMoreInteractions;\nimport static org.mockito.Mockito.when;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ForkJoinPool;\nimport java.util.concurrent.RejectedExecutionException;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.Consumer;\n\nimport org.hibernate.search.engine.backend.work.execution.OperationSubmitter;\nimport org.hibernate.search.engine.common.execution.spi.DelegatingSimpleScheduledExecutor;\nimport org.hibernate.search.engine.environment.bean.BeanHolder;\nimport org.hibernate.search.engine.environment.thread.impl.EmbeddedThreadProvider;\nimport org.hibernate.search.engine.environment.thread.impl.ThreadPoolProviderImpl;\nimport org.hibernate.search.engine.reporting.FailureContext;\nimport org.hibernate.search.engine.reporting.FailureHandler;\n\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.junit.runners.Parameterized;\n\nimport org.awaitility.Awaitility;\nimport org.mockito.ArgumentCaptor;\nimport org.mockito.InOrder;\nimport org.mockito.Mock;\nimport org.mockito.junit.MockitoJUnit;\nimport org.mockito.junit.MockitoRule;\nimport org.mockito.quality.Strictness;\n\n@SuppressWarnings({\"unchecked\", \"rawtypes\"}) // Raw types are the only way to mock parameterized types\n@RunWith(Parameterized.class)\npublic class BatchingExecutorTest {\n\n\tprivate static final String NAME = \"executor-name\";\n\n\t@Parameterized.Parameters(name = \"operation submitter = {0}\")\n\tpublic static Object[][] params() {\n\t\treturn new Object[][] {\n\t\t\t\t{ \"BLOCKING\", OperationSubmitter.blocking() },\n\t\t\t\t{ \"REJECTING\", OperationSubmitter.rejecting() },\n\t\t\t\t{ \"OFFLOADING\", OperationSubmitter.offloading( CompletableFuture::runAsync ) }\n\t\t};\n\t}\n\n\t@Rule\n\tpublic final MockitoRule mockito = MockitoJUnit.rule().strictness( Strictness.STRICT_STUBS );\n\n\t@Mock\n\tprivate StubWorkProcessor processorMock;\n\t@Mock\n\tprivate FailureHandler failureHandlerMock;\n\n\tprivate final List<Object> mocks = new ArrayList<>();\n\n\tprivate final ThreadPoolProviderImpl threadPoolProvider =\n\t\t\tnew ThreadPoolProviderImpl( BeanHolder.of( new EmbeddedThreadProvider() ) );\n\n\t// To execute code asynchronously. Just use more threads than we'll ever need, we don't care about performance.\n\tprivate final ForkJoinPool asyncExecutor = new ForkJoinPool( 12 );\n\n\tprivate ScheduledExecutorService executorService;\n\tprivate BatchingExecutor<StubWorkProcessor> executor;\n\n\tprivate final OperationSubmitter operationSubmitter;\n\n\tpublic BatchingExecutorTest(String name, OperationSubmitter operationSubmitter) {\n\t\tthis.operationSubmitter = operationSubmitter;\n\t}\n\n\t@Before\n\tpublic void setup() {\n\t\tmocks.add( processorMock );\n\t\tmocks.add( failureHandlerMock );\n\t}\n\n\t@After\n\tpublic void cleanup() {\n\t\tif ( executorService != null ) {\n\t\t\texecutorService.shutdownNow();\n\t\t}\n\t\tthreadPoolProvider.close();\n\t\tasyncExecutor.shutdownNow();\n\t\texecutor.stop();\n\t}\n\n\t@Test\n\tpublic void simple_batchEndsImmediately() throws InterruptedException {\n\t\tcreateAndStartExecutor( 2, true );\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\t// The batch is already completed when the endBatch() method returns,\n\t\t// allowing the executor to handle the next batch immediately.\n\t\tCompletableFuture<Object> batch1Future = CompletableFuture.completedFuture( null );\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) batch1Future );\n\t\texecutor.submit( work1Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t} );\n\n\t\t// Submitting other works should start the executor/processor again\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void simple_batchEndsLater_someAdditionalWorkBeforeComplete() throws InterruptedException {\n\t\tcreateAndStartExecutor( 2, true );\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\t// The batch is not yet completed when the endBatch() method returns,\n\t\t// forcing the executor to wait before it handles the next batch.\n\t\tCompletableFuture<Object> batch1Future = new CompletableFuture<>();\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) batch1Future );\n\t\texecutor.submit( work1Mock, operationSubmitter, w -> () -> { } );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since the batch didn't end yet, complete() should not be called right away\n\t\t} );\n\n\t\tStubCompletionListener completionListenerAfterSubmit1 = addPendingCompletionListener();\n\n\t\t// Submit other works before the first batch ends\n\t\tStubWork work2Mock = workMock( 2 );\n\t\tStubWork work3Mock = workMock( 3 );\n\t\texecutor.submit( work2Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\texecutor.submit( work3Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// No calls expected yet\n\t\t} );\n\n\t\tStubCompletionListener completionListenerAfterSubmit2 = addPendingCompletionListener();\n\n\t\t// The batch is not yet completed when the endBatch() method returns,\n\t\t// forcing the executor to wait before it considers works complete.\n\t\tCompletableFuture<Object> batch2Future = new CompletableFuture<>();\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) batch2Future );\n\t\t// End the first batch: the second batch should begin\n\t\tbatch1Future.complete( null );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work2Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work3Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since another work was submitted before the batch ended, complete() should not be called right away\n\t\t} );\n\n\t\t// End the second batch\n\t\tbatch2Future.complete( null );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t\t// The relative order of these two is undefined\n\t\t\tverify( completionListenerAfterSubmit1 ).onComplete();\n\t\t\tverify( completionListenerAfterSubmit2 ).onComplete();\n\t\t} );\n\n\t\t// Submitting other works should start the executor/processor again\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void simple_batchEndsLater_noAdditionalWork() throws InterruptedException {\n\t\tcreateAndStartExecutor( 2, true );\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\t// The batch is not yet completed when the endBatch() method returns,\n\t\t// forcing the executor to wait before it handles the next batch.\n\t\tCompletableFuture<Object> batch1Future = new CompletableFuture<>();\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) batch1Future );\n\t\texecutor.submit( work1Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since the batch didn't end yet, complete() should not be called right away\n\t\t} );\n\n\t\tStubCompletionListener completionListenerAfterSubmit = addPendingCompletionListener();\n\n\t\t// End the first batch\n\t\tbatch1Future.complete( null );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t\tinOrder.verify( completionListenerAfterSubmit ).onComplete();\n\t\t} );\n\n\t\t// Submitting other works should start the executor/processor again\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void beginBatchFailure() throws InterruptedException {\n\t\tcreateAndStartExecutor( 4, true );\n\n\t\tSimulatedFailure simulatedFailure = new SimulatedFailure();\n\n\t\tRunnable unblockExecutorSwitch = blockExecutor();\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\texecutor.submit( work1Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// No calls expected yet\n\t\t} );\n\n\t\tStubCompletionListener completionListenerAfterSubmit = addPendingCompletionListener();\n\n\t\tArgumentCaptor<FailureContext> failureContextCaptor = ArgumentCaptor.forClass( FailureContext.class );\n\t\tdoThrow( simulatedFailure ).when( processorMock ).beginBatch();\n\t\tunblockExecutorSwitch.run();\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( failureHandlerMock ).handle( failureContextCaptor.capture() );\n\t\t\t// The next works should not be submitted to the processor: something is very wrong\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t\tinOrder.verify( completionListenerAfterSubmit ).onComplete();\n\t\t} );\n\n\t\tFailureContext failureContext = failureContextCaptor.getValue();\n\t\tassertThat( failureContext.throwable() )\n\t\t\t\t.isSameAs( simulatedFailure );\n\t\tassertThat( failureContext.failingOperation() ).asString()\n\t\t\t\t.contains( \"Executing task '\" + NAME + \"'\" );\n\n\t\t// The executor should still try to process submitted works, even after a failure\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void submitFailure() throws InterruptedException {\n\t\tcreateAndStartExecutor( 4, true );\n\n\t\tSimulatedFailure simulatedFailure = new SimulatedFailure();\n\n\t\tRunnable unblockExecutorSwitch = blockExecutor();\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\tStubWork work2Mock = workMock( 2 );\n\t\tStubWork work3Mock = workMock( 3 );\n\t\texecutor.submit( work1Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\texecutor.submit( work2Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\texecutor.submit( work3Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// No calls expected yet\n\t\t} );\n\n\t\tStubCompletionListener completionListenerAfterSubmit = addPendingCompletionListener();\n\n\t\tCompletableFuture<Object> batch1Future = CompletableFuture.completedFuture( null );\n\t\tdoThrow( simulatedFailure ).when( work2Mock ).submitTo( processorMock );\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) batch1Future );\n\t\tunblockExecutorSwitch.run();\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work2Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work2Mock ).markAsFailed( simulatedFailure );\n\t\t\t// The next works should still be submitted to the processor\n\t\t\tinOrder.verify( work3Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t\tinOrder.verify( completionListenerAfterSubmit ).onComplete();\n\t\t} );\n\n\t\t// The executor should still try to process submitted works, even after a failure\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void endBatchFailure() throws InterruptedException {\n\t\tcreateAndStartExecutor( 4, true );\n\n\t\tSimulatedFailure simulatedFailure = new SimulatedFailure();\n\n\t\tRunnable unblockExecutorSwitch = blockExecutor();\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\tStubWork work2Mock = workMock( 2 );\n\t\texecutor.submit( work1Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\texecutor.submit( work2Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// No calls expected yet\n\t\t} );\n\n\t\tStubCompletionListener completionListenerAfterSubmit = addPendingCompletionListener();\n\n\t\tArgumentCaptor<FailureContext> failureContextCaptor = ArgumentCaptor.forClass( FailureContext.class );\n\t\tdoThrow( simulatedFailure ).when( processorMock ).endBatch();\n\t\tunblockExecutorSwitch.run();\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work2Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\tinOrder.verify( failureHandlerMock ).handle( failureContextCaptor.capture() );\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t\tinOrder.verify( completionListenerAfterSubmit ).onComplete();\n\t\t} );\n\n\t\tFailureContext failureContext = failureContextCaptor.getValue();\n\t\tassertThat( failureContext.throwable() )\n\t\t\t\t.isSameAs( simulatedFailure );\n\t\tassertThat( failureContext.failingOperation() ).asString()\n\t\t\t\t.contains( \"Executing task '\" + NAME + \"'\" );\n\n\t\t// The executor should still try to process submitted works, even after a failure\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void simple_newTasksBlockedException() throws InterruptedException {\n\t\tcreateAndStartExecutor( 2, true );\n\n\t\tassumeTrue(\n\t\t\t\t\"This test only makes sense for nonblocking submitter\",\n\t\t\t\tOperationSubmitter.rejecting().equals( operationSubmitter )\n\t\t);\n\n\t\tRunnable unblockExecutorSwitch = blockExecutor();\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\tStubWork work2Mock = workMock( 2 );\n\t\tStubWork work3Mock = workMock( 3 );\n\t\texecutor.submit( work1Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\texecutor.submit( work2Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\n\t\tassertThatThrownBy( () -> executor.submit( work3Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } ) )\n\t\t\t\t.isInstanceOf( RejectedExecutionException.class );\n\n\t\twhen( processorMock.endBatch() ).thenReturn( CompletableFuture.completedFuture( null ) );\n\t\tunblockExecutorSwitch.run();\n\n\t\tArgumentCaptor<FailureContext> failureContextCaptor = ArgumentCaptor.forClass( FailureContext.class );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work2Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t} );\n\n\t\t// Submitting other works should start the executor/processor again\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void simple_newTasksBlockedWaitAndCompletes() throws InterruptedException {\n\t\tcreateAndStartExecutor( 2, true );\n\n\t\tassumeTrue(\n\t\t\t\t\"This test only makes sense for blocking submitter\",\n\t\t\t\tOperationSubmitter.blocking().equals( operationSubmitter )\n\t\t);\n\n\t\tRunnable unblockExecutorSwitch = blockExecutor();\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\tStubWork work2Mock = workMock( 2 );\n\t\tStubWork work3Mock = workMock( 3 );\n\n\t\texecutor.submit( work1Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\texecutor.submit( work2Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\n\t\tAtomicReference<Thread> work3SubmitThread = new AtomicReference<>();\n\t\tCompletableFuture<Boolean> future = CompletableFuture.supplyAsync( () -> {\n\t\t\ttry {\n\t\t\t\twork3SubmitThread.set( Thread.currentThread() );\n\t\t\t\texecutor.submit( work3Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\t\t}\n\t\t\tcatch (InterruptedException e) {\n\t\t\t\tThread.currentThread().interrupt();\n\t\t\t}\n\t\t\treturn true;\n\t\t} );\n\n\t\t// wait until the thread submitting work3 is submitting and blocked\n\t\tAwaitility.await().untilAsserted( () -> assertThat( work3SubmitThread )\n\t\t\t\t.hasValueSatisfying( thread -> assertThat( thread.getState() )\n\t\t\t\t\t\t.isIn( Thread.State.BLOCKED, Thread.State.WAITING, Thread.State.TIMED_WAITING ) ) );\n\n\t\t// queue is full so submitting work3 will block indefinitely\n\t\tassertThat( future.isDone() ).isFalse();\n\n\t\twhen( processorMock.endBatch() ).thenReturn( CompletableFuture.completedFuture( null ) );\n\t\tdoAnswer( invocation -> {\n\t\t\t// See https://hibernate.atlassian.net/browse/HSEARCH-4750\n\t\t\t// Just make sure that we don't finish the batch until work3 has actually been submitted.\n\t\t\t// If we don't do this, and the batch finishes before work3 has been submitted,\n\t\t\t// then processorMock.complete() gets called and the call to verifyAsynchronouslyAndReset\n\t\t\t// below fails. Since that can happen randomly, it's very inconvenient and leads to flaky tests.\n\t\t\tAwaitility.await().until( future::isDone );\n\t\t\treturn null;\n\t\t} ).when( work2Mock ).submitTo( any( StubWorkProcessor.class ) );\n\n\t\tunblockExecutorSwitch.run();\n\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work2Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work3Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t} );\n\n\t\t// Submitting other works should start the executor/processor again\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void simple_newTasksBlockedAndOffloadedCompletes() throws InterruptedException {\n\t\tcreateAndStartExecutor( 2, true );\n\n\t\tassumeFalse(\n\t\t\t\t\"This test only makes sense for offloading submitter\",\n\t\t\t\tOperationSubmitter.blocking().equals( operationSubmitter ) ||\n\t\t\t\t\t\tOperationSubmitter.rejecting().equals( operationSubmitter )\n\t\t);\n\n\t\tRunnable unblockExecutorSwitch = blockExecutor();\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\tStubWork work2Mock = workMock( 2 );\n\t\tStubWork work3Mock = workMock( 3 );\n\n\t\texecutor.submit( work1Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\t\texecutor.submit( work2Mock, operationSubmitter, w -> () -> { fail( \"shouldn't be offloaded!\" ); } );\n\n\t\tAtomicReference<Thread> work3SubmitThread = new AtomicReference<>();\n\t\tAtomicBoolean work3Submitted = new AtomicBoolean( false );\n\t\texecutor.submit( work3Mock, operationSubmitter, w -> () -> {\n\t\t\twork3SubmitThread.set( Thread.currentThread() );\n\t\t\ttry {\n\t\t\t\texecutor.submit( work3Mock, OperationSubmitter.blocking(), w2 -> () -> { } );\n\t\t\t\twork3Submitted.set( true );\n\t\t\t}\n\t\t\tcatch (InterruptedException e) {\n\t\t\t\tThread.currentThread().interrupt();\n\t\t\t}\n\t\t} );\n\n\t\t// wait until the thread submitting work3 is submitting and blocked\n\t\tAwaitility.await().untilAsserted( () -> assertThat( work3SubmitThread )\n\t\t\t\t.hasValueSatisfying( thread -> assertThat( thread.getState() )\n\t\t\t\t\t\t.isIn( Thread.State.BLOCKED, Thread.State.WAITING, Thread.State.TIMED_WAITING ) ) );\n\n\t\twhen( processorMock.endBatch() ).thenReturn( CompletableFuture.completedFuture( null ) );\n\n\t\tdoAnswer( invocation -> {\n\t\t\t// See https://hibernate.atlassian.net/browse/HSEARCH-4750\n\t\t\t// Just make sure that we don't finish the batch until work3 has actually been submitted.\n\t\t\t// If we don't do this, and the batch finishes before work3 has been submitted,\n\t\t\t// then processorMock.complete() gets called and the call to verifyAsynchronouslyAndReset\n\t\t\t// below fails. Since that can happen randomly, it's very inconvenient and leads to flaky tests.\n\t\t\tAwaitility.await().until( work3Submitted::get );\n\t\t\treturn null;\n\t\t} ).when( work2Mock ).submitTo( any( StubWorkProcessor.class ) );\n\n\t\tunblockExecutorSwitch.run();\n\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work2Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work3Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t} );\n\n\t\t// Submitting other works should start the executor/processor again\n\t\tcheckPostExecution();\n\t}\n\n\tprivate void verifyAsynchronouslyAndReset(Consumer<InOrder> verify) {\n\t\tawait().untilAsserted( () -> {\n\t\t\tInOrder inOrder = inOrder( mocks.toArray() );\n\t\t\tverify.accept( inOrder );\n\t\t} );\n\t\tverifyNoMoreInteractions( mocks.toArray() );\n\t\treset( mocks.toArray() );\n\t}\n\n\t/*\n\t * Block the executor by submitting a batch that will only complete when the returned runnable is executed.\n\t * Used to give us the time to carefully craft the next batch with a specific sequence of works.\n\t */\n\tprivate Runnable blockExecutor()\n\t\t\tthrows InterruptedException {\n\t\tStubWork blockingWorkMock = workMock( 0 );\n\t\tCompletableFuture<Object> blockingBatchFuture = new CompletableFuture<>();\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) blockingBatchFuture );\n\t\texecutor.submit( blockingWorkMock, operationSubmitter, w -> () -> { } );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( blockingWorkMock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since the batch didn't end yet, complete() should not be called right away\n\t\t} );\n\t\t// Return a runnable that will unblock the executor\n\t\treturn () -> blockingBatchFuture.complete( null );\n\t}\n\n\tprivate void createAndStartExecutor(int maxTasksPerBatch, boolean fair) {\n\t\tthis.executor = new BatchingExecutor<>(\n\t\t\t\tNAME, processorMock, maxTasksPerBatch, fair, failureHandlerMock\n\t\t);\n\n\t\t// Having multiple threads should not matter:\n\t\t// the batching executor takes care of executing in only one thread at a time.\n\t\tthis.executorService = threadPoolProvider.newScheduledExecutor( 4, \"BatchingExecutorTest\" );\n\n\t\texecutor.start( new DelegatingSimpleScheduledExecutor( executorService ) );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// No calls expected yet\n\t\t} );\n\n\t\tCompletableFuture<?> completion = executor.completion();\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// This should not trigger any call to the mocks\n\t\t} );\n\t\t// Initially, there are no works, so works are considered completed.\n\t\tassertThatFuture( completion ).isSuccessful();\n\t}\n\n\tprivate StubCompletionListener addPendingCompletionListener() {\n\t\tStubCompletionListener listener = mock( StubCompletionListener.class );\n\t\tmocks.add( listener );\n\n\t\texecutor.completion()\n\t\t\t\t.whenComplete( (result, throwable) -> {\n\t\t\t\t\tassertThat( result ).isNull();\n\t\t\t\t\tassertThat( throwable ).isNull();\n\t\t\t\t\tlistener.onComplete();\n\t\t\t\t} );\n\t\t// We should be pending completion, so the listener shouldn't have been called yet.\n\t\tverifyNoInteractions( listener );\n\n\t\treturn listener;\n\t}\n\n\tprivate void checkPostExecution() throws InterruptedException {\n\t\tCompletableFuture<?> completion = executor.completion();\n\t\t// This should not trigger any call to the mocks\n\t\tverifyNoInteractions( mocks.toArray() );\n\t\t// The queue is empty, so works are considered completed.\n\t\tassertThatFuture( completion ).isSuccessful();\n\n\t\t// The executor still accepts and processes new works.\n\t\tStubWork workMock = workMock( 42 );\n\t\tCompletableFuture<Object> batchFuture = CompletableFuture.completedFuture( null );\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) batchFuture );\n\t\texecutor.submit( workMock, operationSubmitter, w -> () -> { } );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( workMock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t} );\n\t}\n\n\tprivate StubWork workMock(int id) {\n\t\tStubWork mock = mock( StubWork.class, \"work #\" + id );\n\t\tmocks.add( mock );\n\t\treturn mock;\n\t}\n\n\tprivate interface StubWork extends BatchedWork<StubWorkProcessor> {\n\t}\n\n\tprivate interface StubWorkProcessor extends BatchedWorkProcessor {\n\t}\n\n\tprivate interface StubCompletionListener {\n\t\tvoid onComplete();\n\t}\n\n\tprivate static class SimulatedFailure extends RuntimeException {\n\t}\n}\n",
        "filePathAfter": "engine/src/test/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutorTest.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.backend.orchestration.spi;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.assertj.core.api.Assertions.fail;\nimport static org.awaitility.Awaitility.await;\nimport static org.hibernate.search.util.impl.test.FutureAssert.assertThatFuture;\nimport static org.junit.Assume.assumeFalse;\nimport static org.junit.Assume.assumeTrue;\nimport static org.mockito.ArgumentMatchers.any;\nimport static org.mockito.Mockito.doAnswer;\nimport static org.mockito.Mockito.doThrow;\nimport static org.mockito.Mockito.inOrder;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.reset;\nimport static org.mockito.Mockito.verify;\nimport static org.mockito.Mockito.verifyNoInteractions;\nimport static org.mockito.Mockito.verifyNoMoreInteractions;\nimport static org.mockito.Mockito.when;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ForkJoinPool;\nimport java.util.concurrent.RejectedExecutionException;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.concurrent.atomic.AtomicReference;\nimport java.util.function.Consumer;\n\nimport org.hibernate.search.engine.backend.work.execution.OperationSubmitter;\nimport org.hibernate.search.engine.common.execution.spi.DelegatingSimpleScheduledExecutor;\nimport org.hibernate.search.engine.environment.bean.BeanHolder;\nimport org.hibernate.search.engine.environment.thread.impl.EmbeddedThreadProvider;\nimport org.hibernate.search.engine.environment.thread.impl.ThreadPoolProviderImpl;\nimport org.hibernate.search.engine.reporting.FailureContext;\nimport org.hibernate.search.engine.reporting.FailureHandler;\n\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.junit.runners.Parameterized;\n\nimport org.awaitility.Awaitility;\nimport org.mockito.ArgumentCaptor;\nimport org.mockito.InOrder;\nimport org.mockito.Mock;\nimport org.mockito.junit.MockitoJUnit;\nimport org.mockito.junit.MockitoRule;\nimport org.mockito.quality.Strictness;\n\n@SuppressWarnings({\"unchecked\", \"rawtypes\"}) // Raw types are the only way to mock parameterized types\n@RunWith(Parameterized.class)\npublic class BatchingExecutorTest {\n\n\tprivate static final String NAME = \"executor-name\";\n\n\t@Parameterized.Parameters(name = \"operation submitter = {0}\")\n\tpublic static Object[][] params() {\n\t\treturn new Object[][] {\n\t\t\t\t{ \"BLOCKING\", OperationSubmitter.blocking() },\n\t\t\t\t{ \"REJECTING\", OperationSubmitter.rejecting() },\n\t\t\t\t{ \"OFFLOADING\", OperationSubmitter.offloading( CompletableFuture::runAsync ) }\n\t\t};\n\t}\n\n\t@Rule\n\tpublic final MockitoRule mockito = MockitoJUnit.rule().strictness( Strictness.STRICT_STUBS );\n\n\t@Mock\n\tprivate StubWorkProcessor processorMock;\n\t@Mock\n\tprivate FailureHandler failureHandlerMock;\n\n\tprivate final List<Object> mocks = new ArrayList<>();\n\n\tprivate final ThreadPoolProviderImpl threadPoolProvider =\n\t\t\tnew ThreadPoolProviderImpl( BeanHolder.of( new EmbeddedThreadProvider() ) );\n\n\t// To execute code asynchronously. Just use more threads than we'll ever need, we don't care about performance.\n\tprivate final ForkJoinPool asyncExecutor = new ForkJoinPool( 12 );\n\n\tprivate ScheduledExecutorService executorService;\n\tprivate BatchingExecutor<StubWorkProcessor> executor;\n\n\tprivate final OperationSubmitter operationSubmitter;\n\n\tpublic BatchingExecutorTest(String name, OperationSubmitter operationSubmitter) {\n\t\tthis.operationSubmitter = operationSubmitter;\n\t}\n\n\t@Before\n\tpublic void setup() {\n\t\tmocks.add( processorMock );\n\t\tmocks.add( failureHandlerMock );\n\t}\n\n\t@After\n\tpublic void cleanup() {\n\t\tif ( executorService != null ) {\n\t\t\texecutorService.shutdownNow();\n\t\t}\n\t\tthreadPoolProvider.close();\n\t\tasyncExecutor.shutdownNow();\n\t\texecutor.stop();\n\t}\n\n\t@Test\n\tpublic void simple_batchEndsImmediately() throws InterruptedException {\n\t\tcreateAndStartExecutor( 2, true );\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\t// The batch is already completed when the endBatch() method returns,\n\t\t// allowing the executor to handle the next batch immediately.\n\t\tCompletableFuture<Object> batch1Future = CompletableFuture.completedFuture( null );\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) batch1Future );\n\t\texecutor.submit( work1Mock, operationSubmitter );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t} );\n\n\t\t// Submitting other works should start the executor/processor again\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void simple_batchEndsLater_someAdditionalWorkBeforeComplete() throws InterruptedException {\n\t\tcreateAndStartExecutor( 2, true );\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\t// The batch is not yet completed when the endBatch() method returns,\n\t\t// forcing the executor to wait before it handles the next batch.\n\t\tCompletableFuture<Object> batch1Future = new CompletableFuture<>();\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) batch1Future );\n\t\texecutor.submit( work1Mock, operationSubmitter );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since the batch didn't end yet, complete() should not be called right away\n\t\t} );\n\n\t\tStubCompletionListener completionListenerAfterSubmit1 = addPendingCompletionListener();\n\n\t\t// Submit other works before the first batch ends\n\t\tStubWork work2Mock = workMock( 2 );\n\t\tStubWork work3Mock = workMock( 3 );\n\t\texecutor.submit( work2Mock, operationSubmitter );\n\t\texecutor.submit( work3Mock, operationSubmitter );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// No calls expected yet\n\t\t} );\n\n\t\tStubCompletionListener completionListenerAfterSubmit2 = addPendingCompletionListener();\n\n\t\t// The batch is not yet completed when the endBatch() method returns,\n\t\t// forcing the executor to wait before it considers works complete.\n\t\tCompletableFuture<Object> batch2Future = new CompletableFuture<>();\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) batch2Future );\n\t\t// End the first batch: the second batch should begin\n\t\tbatch1Future.complete( null );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work2Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work3Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since another work was submitted before the batch ended, complete() should not be called right away\n\t\t} );\n\n\t\t// End the second batch\n\t\tbatch2Future.complete( null );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t\t// The relative order of these two is undefined\n\t\t\tverify( completionListenerAfterSubmit1 ).onComplete();\n\t\t\tverify( completionListenerAfterSubmit2 ).onComplete();\n\t\t} );\n\n\t\t// Submitting other works should start the executor/processor again\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void simple_batchEndsLater_noAdditionalWork() throws InterruptedException {\n\t\tcreateAndStartExecutor( 2, true );\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\t// The batch is not yet completed when the endBatch() method returns,\n\t\t// forcing the executor to wait before it handles the next batch.\n\t\tCompletableFuture<Object> batch1Future = new CompletableFuture<>();\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) batch1Future );\n\t\texecutor.submit( work1Mock, operationSubmitter );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since the batch didn't end yet, complete() should not be called right away\n\t\t} );\n\n\t\tStubCompletionListener completionListenerAfterSubmit = addPendingCompletionListener();\n\n\t\t// End the first batch\n\t\tbatch1Future.complete( null );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t\tinOrder.verify( completionListenerAfterSubmit ).onComplete();\n\t\t} );\n\n\t\t// Submitting other works should start the executor/processor again\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void beginBatchFailure() throws InterruptedException {\n\t\tcreateAndStartExecutor( 4, true );\n\n\t\tSimulatedFailure simulatedFailure = new SimulatedFailure();\n\n\t\tRunnable unblockExecutorSwitch = blockExecutor();\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\texecutor.submit( work1Mock, operationSubmitter );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// No calls expected yet\n\t\t} );\n\n\t\tStubCompletionListener completionListenerAfterSubmit = addPendingCompletionListener();\n\n\t\tArgumentCaptor<FailureContext> failureContextCaptor = ArgumentCaptor.forClass( FailureContext.class );\n\t\tdoThrow( simulatedFailure ).when( processorMock ).beginBatch();\n\t\tunblockExecutorSwitch.run();\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( failureHandlerMock ).handle( failureContextCaptor.capture() );\n\t\t\t// The next works should not be submitted to the processor: something is very wrong\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t\tinOrder.verify( completionListenerAfterSubmit ).onComplete();\n\t\t} );\n\n\t\tFailureContext failureContext = failureContextCaptor.getValue();\n\t\tassertThat( failureContext.throwable() )\n\t\t\t\t.isSameAs( simulatedFailure );\n\t\tassertThat( failureContext.failingOperation() ).asString()\n\t\t\t\t.contains( \"Executing task '\" + NAME + \"'\" );\n\n\t\t// The executor should still try to process submitted works, even after a failure\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void submitFailure() throws InterruptedException {\n\t\tcreateAndStartExecutor( 4, true );\n\n\t\tSimulatedFailure simulatedFailure = new SimulatedFailure();\n\n\t\tRunnable unblockExecutorSwitch = blockExecutor();\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\tStubWork work2Mock = workMock( 2 );\n\t\tStubWork work3Mock = workMock( 3 );\n\t\texecutor.submit( work1Mock, operationSubmitter );\n\t\texecutor.submit( work2Mock, operationSubmitter );\n\t\texecutor.submit( work3Mock, operationSubmitter );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// No calls expected yet\n\t\t} );\n\n\t\tStubCompletionListener completionListenerAfterSubmit = addPendingCompletionListener();\n\n\t\tCompletableFuture<Object> batch1Future = CompletableFuture.completedFuture( null );\n\t\tdoThrow( simulatedFailure ).when( work2Mock ).submitTo( processorMock );\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) batch1Future );\n\t\tunblockExecutorSwitch.run();\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work2Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work2Mock ).markAsFailed( simulatedFailure );\n\t\t\t// The next works should still be submitted to the processor\n\t\t\tinOrder.verify( work3Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t\tinOrder.verify( completionListenerAfterSubmit ).onComplete();\n\t\t} );\n\n\t\t// The executor should still try to process submitted works, even after a failure\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void endBatchFailure() throws InterruptedException {\n\t\tcreateAndStartExecutor( 4, true );\n\n\t\tSimulatedFailure simulatedFailure = new SimulatedFailure();\n\n\t\tRunnable unblockExecutorSwitch = blockExecutor();\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\tStubWork work2Mock = workMock( 2 );\n\t\texecutor.submit( work1Mock, operationSubmitter );\n\t\texecutor.submit( work2Mock, operationSubmitter );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// No calls expected yet\n\t\t} );\n\n\t\tStubCompletionListener completionListenerAfterSubmit = addPendingCompletionListener();\n\n\t\tArgumentCaptor<FailureContext> failureContextCaptor = ArgumentCaptor.forClass( FailureContext.class );\n\t\tdoThrow( simulatedFailure ).when( processorMock ).endBatch();\n\t\tunblockExecutorSwitch.run();\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work2Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\tinOrder.verify( failureHandlerMock ).handle( failureContextCaptor.capture() );\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t\tinOrder.verify( completionListenerAfterSubmit ).onComplete();\n\t\t} );\n\n\t\tFailureContext failureContext = failureContextCaptor.getValue();\n\t\tassertThat( failureContext.throwable() )\n\t\t\t\t.isSameAs( simulatedFailure );\n\t\tassertThat( failureContext.failingOperation() ).asString()\n\t\t\t\t.contains( \"Executing task '\" + NAME + \"'\" );\n\n\t\t// The executor should still try to process submitted works, even after a failure\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void simple_newTasksBlockedException() throws InterruptedException {\n\t\tcreateAndStartExecutor( 2, true );\n\n\t\tassumeTrue(\n\t\t\t\t\"This test only makes sense for nonblocking submitter\",\n\t\t\t\tOperationSubmitter.rejecting().equals( operationSubmitter )\n\t\t);\n\n\t\tRunnable unblockExecutorSwitch = blockExecutor();\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\tStubWork work2Mock = workMock( 2 );\n\t\tStubWork work3Mock = workMock( 3 );\n\t\texecutor.submit( work1Mock, operationSubmitter );\n\t\texecutor.submit( work2Mock, operationSubmitter );\n\n\t\tassertThatThrownBy( () -> executor.submit( work3Mock, operationSubmitter ) )\n\t\t\t\t.isInstanceOf( RejectedExecutionException.class );\n\n\t\twhen( processorMock.endBatch() ).thenReturn( CompletableFuture.completedFuture( null ) );\n\t\tunblockExecutorSwitch.run();\n\n\t\tArgumentCaptor<FailureContext> failureContextCaptor = ArgumentCaptor.forClass( FailureContext.class );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work2Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t} );\n\n\t\t// Submitting other works should start the executor/processor again\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void simple_newTasksBlockedWaitAndCompletes() throws InterruptedException {\n\t\tcreateAndStartExecutor( 2, true );\n\n\t\tassumeTrue(\n\t\t\t\t\"This test only makes sense for blocking submitter\",\n\t\t\t\tOperationSubmitter.blocking().equals( operationSubmitter )\n\t\t);\n\n\t\tRunnable unblockExecutorSwitch = blockExecutor();\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\tStubWork work2Mock = workMock( 2 );\n\t\tStubWork work3Mock = workMock( 3 );\n\n\t\texecutor.submit( work1Mock, operationSubmitter );\n\t\texecutor.submit( work2Mock, operationSubmitter );\n\n\t\tAtomicReference<Thread> work3SubmitThread = new AtomicReference<>();\n\t\tCompletableFuture<Boolean> future = CompletableFuture.supplyAsync( () -> {\n\t\t\ttry {\n\t\t\t\twork3SubmitThread.set( Thread.currentThread() );\n\t\t\t\texecutor.submit( work3Mock, operationSubmitter );\n\t\t\t}\n\t\t\tcatch (InterruptedException e) {\n\t\t\t\tThread.currentThread().interrupt();\n\t\t\t}\n\t\t\treturn true;\n\t\t} );\n\n\t\t// wait until the thread submitting work3 is submitting and blocked\n\t\tAwaitility.await().untilAsserted( () -> assertThat( work3SubmitThread )\n\t\t\t\t.hasValueSatisfying( thread -> assertThat( thread.getState() )\n\t\t\t\t\t\t.isIn( Thread.State.BLOCKED, Thread.State.WAITING, Thread.State.TIMED_WAITING ) ) );\n\n\t\t// queue is full so submitting work3 will block indefinitely\n\t\tassertThat( future.isDone() ).isFalse();\n\n\t\twhen( processorMock.endBatch() ).thenReturn( CompletableFuture.completedFuture( null ) );\n\t\tdoAnswer( invocation -> {\n\t\t\t// See https://hibernate.atlassian.net/browse/HSEARCH-4750\n\t\t\t// Just make sure that we don't finish the batch until work3 has actually been submitted.\n\t\t\t// If we don't do this, and the batch finishes before work3 has been submitted,\n\t\t\t// then processorMock.complete() gets called and the call to verifyAsynchronouslyAndReset\n\t\t\t// below fails. Since that can happen randomly, it's very inconvenient and leads to flaky tests.\n\t\t\tAwaitility.await().until( future::isDone );\n\t\t\treturn null;\n\t\t} ).when( work2Mock ).submitTo( any( StubWorkProcessor.class ) );\n\n\t\tunblockExecutorSwitch.run();\n\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work2Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work3Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t} );\n\n\t\t// Submitting other works should start the executor/processor again\n\t\tcheckPostExecution();\n\t}\n\n\t@Test\n\tpublic void simple_newTasksBlockedAndOffloadedCompletes() throws InterruptedException {\n\t\tAtomicReference<Runnable> offloadAction = new AtomicReference<>( () -> { } );\n\t\tcreateAndStartExecutor( 2, true, w -> offloadAction.get().run() );\n\n\t\tassumeFalse(\n\t\t\t\t\"This test only makes sense for offloading submitter\",\n\t\t\t\tOperationSubmitter.blocking().equals( operationSubmitter ) ||\n\t\t\t\t\t\tOperationSubmitter.rejecting().equals( operationSubmitter )\n\t\t);\n\n\t\tRunnable unblockExecutorSwitch = blockExecutor();\n\n\t\tStubWork work1Mock = workMock( 1 );\n\t\tStubWork work2Mock = workMock( 2 );\n\t\tStubWork work3Mock = workMock( 3 );\n\n\t\texecutor.submit( work1Mock, operationSubmitter );\n\t\texecutor.submit( work2Mock, operationSubmitter );\n\n\t\tAtomicReference<Thread> work3SubmitThread = new AtomicReference<>();\n\t\tAtomicBoolean work3Submitted = new AtomicBoolean( false );\n\t\toffloadAction.set( () -> {\n\t\t\twork3SubmitThread.set( Thread.currentThread() );\n\t\t\ttry {\n\t\t\t\texecutor.submit( work3Mock, OperationSubmitter.blocking() );\n\t\t\t\twork3Submitted.set( true );\n\t\t\t}\n\t\t\tcatch (InterruptedException e) {\n\t\t\t\tThread.currentThread().interrupt();\n\t\t\t}\n\t\t} );\n\t\texecutor.submit( work3Mock, operationSubmitter );\n\n\t\t// wait until the thread submitting work3 is submitting and blocked\n\t\tAwaitility.await().untilAsserted( () -> assertThat( work3SubmitThread )\n\t\t\t\t.hasValueSatisfying( thread -> assertThat( thread.getState() )\n\t\t\t\t\t\t.isIn( Thread.State.BLOCKED, Thread.State.WAITING, Thread.State.TIMED_WAITING ) ) );\n\n\t\twhen( processorMock.endBatch() ).thenReturn( CompletableFuture.completedFuture( null ) );\n\n\t\tdoAnswer( invocation -> {\n\t\t\t// See https://hibernate.atlassian.net/browse/HSEARCH-4750\n\t\t\t// Just make sure that we don't finish the batch until work3 has actually been submitted.\n\t\t\t// If we don't do this, and the batch finishes before work3 has been submitted,\n\t\t\t// then processorMock.complete() gets called and the call to verifyAsynchronouslyAndReset\n\t\t\t// below fails. Since that can happen randomly, it's very inconvenient and leads to flaky tests.\n\t\t\tAwaitility.await().until( work3Submitted::get );\n\t\t\treturn null;\n\t\t} ).when( work2Mock ).submitTo( any( StubWorkProcessor.class ) );\n\n\t\tunblockExecutorSwitch.run();\n\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work1Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( work2Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( work3Mock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t} );\n\n\t\t// Submitting other works should start the executor/processor again\n\t\tcheckPostExecution();\n\t}\n\n\tprivate void verifyAsynchronouslyAndReset(Consumer<InOrder> verify) {\n\t\tawait().untilAsserted( () -> {\n\t\t\tInOrder inOrder = inOrder( mocks.toArray() );\n\t\t\tverify.accept( inOrder );\n\t\t} );\n\t\tverifyNoMoreInteractions( mocks.toArray() );\n\t\treset( mocks.toArray() );\n\t}\n\n\t/*\n\t * Block the executor by submitting a batch that will only complete when the returned runnable is executed.\n\t * Used to give us the time to carefully craft the next batch with a specific sequence of works.\n\t */\n\tprivate Runnable blockExecutor()\n\t\t\tthrows InterruptedException {\n\t\tStubWork blockingWorkMock = workMock( 0 );\n\t\tCompletableFuture<Object> blockingBatchFuture = new CompletableFuture<>();\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) blockingBatchFuture );\n\t\texecutor.submit( blockingWorkMock, operationSubmitter );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( blockingWorkMock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since the batch didn't end yet, complete() should not be called right away\n\t\t} );\n\t\t// Return a runnable that will unblock the executor\n\t\treturn () -> blockingBatchFuture.complete( null );\n\t}\n\n\tprivate void createAndStartExecutor(int maxTasksPerBatch, boolean fair) {\n\t\tcreateAndStartExecutor( maxTasksPerBatch, fair, w -> fail( \"Work shouldn't be offloaded.\" ) );\n\t}\n\tprivate void createAndStartExecutor(int maxTasksPerBatch, boolean fair, Consumer<? super BatchedWork<? super StubWorkProcessor>> blockingRetryProducer) {\n\t\tthis.executor = new BatchingExecutor<>(\n\t\t\t\tNAME, processorMock, maxTasksPerBatch, fair, failureHandlerMock, blockingRetryProducer\n\t\t);\n\n\t\t// Having multiple threads should not matter:\n\t\t// the batching executor takes care of executing in only one thread at a time.\n\t\tthis.executorService = threadPoolProvider.newScheduledExecutor( 4, \"BatchingExecutorTest\" );\n\n\t\texecutor.start( new DelegatingSimpleScheduledExecutor( executorService ) );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// No calls expected yet\n\t\t} );\n\n\t\tCompletableFuture<?> completion = executor.completion();\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// This should not trigger any call to the mocks\n\t\t} );\n\t\t// Initially, there are no works, so works are considered completed.\n\t\tassertThatFuture( completion ).isSuccessful();\n\t}\n\n\tprivate StubCompletionListener addPendingCompletionListener() {\n\t\tStubCompletionListener listener = mock( StubCompletionListener.class );\n\t\tmocks.add( listener );\n\n\t\texecutor.completion()\n\t\t\t\t.whenComplete( (result, throwable) -> {\n\t\t\t\t\tassertThat( result ).isNull();\n\t\t\t\t\tassertThat( throwable ).isNull();\n\t\t\t\t\tlistener.onComplete();\n\t\t\t\t} );\n\t\t// We should be pending completion, so the listener shouldn't have been called yet.\n\t\tverifyNoInteractions( listener );\n\n\t\treturn listener;\n\t}\n\n\tprivate void checkPostExecution() throws InterruptedException {\n\t\tCompletableFuture<?> completion = executor.completion();\n\t\t// This should not trigger any call to the mocks\n\t\tverifyNoInteractions( mocks.toArray() );\n\t\t// The queue is empty, so works are considered completed.\n\t\tassertThatFuture( completion ).isSuccessful();\n\n\t\t// The executor still accepts and processes new works.\n\t\tStubWork workMock = workMock( 42 );\n\t\tCompletableFuture<Object> batchFuture = CompletableFuture.completedFuture( null );\n\t\twhen( processorMock.endBatch() ).thenReturn( (CompletableFuture) batchFuture );\n\t\texecutor.submit( workMock, operationSubmitter );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\tinOrder.verify( processorMock ).beginBatch();\n\t\t\tinOrder.verify( workMock ).submitTo( processorMock );\n\t\t\tinOrder.verify( processorMock ).endBatch();\n\t\t\t// Since the queue is empty, works should be considered complete.\n\t\t\tinOrder.verify( processorMock ).complete();\n\t\t} );\n\t}\n\n\tprivate StubWork workMock(int id) {\n\t\tStubWork mock = mock( StubWork.class, \"work #\" + id );\n\t\tmocks.add( mock );\n\t\treturn mock;\n\t}\n\n\tprivate interface StubWork extends BatchedWork<StubWorkProcessor> {\n\t}\n\n\tprivate interface StubWorkProcessor extends BatchedWorkProcessor {\n\t}\n\n\tprivate interface StubCompletionListener {\n\t\tvoid onComplete();\n\t}\n\n\tprivate static class SimulatedFailure extends RuntimeException {\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private void createAndStartExecutor(int maxTasksPerBatch, boolean fair, Consumer<? super BatchedWork<? super StubWorkProcessor>> blockingRetryProducer) {\n\t\tthis.executor = new BatchingExecutor<>(\n\t\t\t\tNAME, processorMock, maxTasksPerBatch, fair, failureHandlerMock, blockingRetryProducer\n\t\t);\n\n\t\t// Having multiple threads should not matter:\n\t\t// the batching executor takes care of executing in only one thread at a time.\n\t\tthis.executorService = threadPoolProvider.newScheduledExecutor( 4, \"BatchingExecutorTest\" );\n\n\t\texecutor.start( new DelegatingSimpleScheduledExecutor( executorService ) );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// No calls expected yet\n\t\t} );\n\n\t\tCompletableFuture<?> completion = executor.completion();\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// This should not trigger any call to the mocks\n\t\t} );\n\t\t// Initially, there are no works, so works are considered completed.\n\t\tassertThatFuture( completion ).isSuccessful();\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutorTest#verifyAsynchronouslyAndReset\n methodBody: private void verifyAsynchronouslyAndReset(Consumer<InOrder> verify) {\nawait().untilAsserted(() -> {\n  InOrder inOrder=inOrder(mocks.toArray());\n  verify.accept(inOrder);\n}\n);\nverifyNoMoreInteractions(mocks.toArray());\nreset(mocks.toArray());\n}",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.AbstractWorkOrchestrator#completion\n methodBody: protected abstract CompletableFuture<?> completion();",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#completion\n methodBody: public CompletableFuture<?> completion() {\nif(processingTask == null){return CompletableFuture.completedFuture(null);\n}return processingTask.completion();\n}",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.AbstractWorkOrchestrator#start\n methodBody: public final void start(ConfigurationPropertySource propertySource) {\nlifecycleLock.writeLock().lock();\ntryswitch(state)case RUNNING:return;\ncase PRE_STOPPING:throw new IllegalStateException(\"Cannot start an orchestrator while it's stopping\");\ncase STOPPED:state=State.RUNNING;\ndoStart(propertySource);\nbreak;\nfinallylifecycleLock.writeLock().unlock();\n}",
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchBatchingWorkOrchestrator#completion\n methodBody: protected CompletableFuture<?> completion() {\nCompletableFuture<?>[] completions=new CompletableFuture[executors.size()];\nfor(int i=0; i < executors.size(); i++){completions[i]=executors.get(i).completion();\n}return CompletableFuture.allOf(completions);\n}",
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchSimpleWorkOrchestrator#completion\n methodBody: protected CompletableFuture<?> completion() {\nreturn CompletableFuture.completedFuture(null);\n}",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneSerialWorkOrchestratorImpl#completion\n methodBody: protected CompletableFuture<?> completion() {\nCompletableFuture<?>[] completions=new CompletableFuture[executors.size()];\nfor(int i=0; i < executors.size(); i++){completions[i]=executors.get(i).completion();\n}return CompletableFuture.allOf(completions);\n}",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#start\n methodBody: public synchronized void start(SimpleScheduledExecutor executorService) {\nlog.startingExecutor(name);\nprocessingTask=new SingletonTask(name,worker,new BatchScheduler(executorService),failureHandler);\n}",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneParallelWorkOrchestratorImpl#completion\n methodBody: protected CompletableFuture<?> completion() {\nreturn CompletableFuture.completedFuture(null);\n}",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneSyncWorkOrchestratorImpl#completion\n methodBody: protected CompletableFuture<?> completion() {\nreturn CompletableFuture.completedFuture(null);\n}"
        ],
        "sourceCodeAfterRefactoring": "private void createAndStartExecutor(int maxTasksPerBatch, boolean fair) {\n\t\tcreateAndStartExecutor( maxTasksPerBatch, fair, w -> fail( \"Work shouldn't be offloaded.\" ) );\n\t}\nprivate void createAndStartExecutor(int maxTasksPerBatch, boolean fair, Consumer<? super BatchedWork<? super StubWorkProcessor>> blockingRetryProducer) {\n\t\tthis.executor = new BatchingExecutor<>(\n\t\t\t\tNAME, processorMock, maxTasksPerBatch, fair, failureHandlerMock, blockingRetryProducer\n\t\t);\n\n\t\t// Having multiple threads should not matter:\n\t\t// the batching executor takes care of executing in only one thread at a time.\n\t\tthis.executorService = threadPoolProvider.newScheduledExecutor( 4, \"BatchingExecutorTest\" );\n\n\t\texecutor.start( new DelegatingSimpleScheduledExecutor( executorService ) );\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// No calls expected yet\n\t\t} );\n\n\t\tCompletableFuture<?> completion = executor.completion();\n\t\tverifyAsynchronouslyAndReset( inOrder -> {\n\t\t\t// This should not trigger any call to the mocks\n\t\t} );\n\t\t// Initially, there are no works, so works are considered completed.\n\t\tassertThatFuture( completion ).isSuccessful();\n\t}",
        "diffSourceCode": "-  545: \tprivate void createAndStartExecutor(int maxTasksPerBatch, boolean fair) {\n-  546: \t\tthis.executor = new BatchingExecutor<>(\n-  547: \t\t\t\tNAME, processorMock, maxTasksPerBatch, fair, failureHandlerMock\n-  548: \t\t);\n-  549: \n-  550: \t\t// Having multiple threads should not matter:\n-  551: \t\t// the batching executor takes care of executing in only one thread at a time.\n-  552: \t\tthis.executorService = threadPoolProvider.newScheduledExecutor( 4, \"BatchingExecutorTest\" );\n-  553: \n-  554: \t\texecutor.start( new DelegatingSimpleScheduledExecutor( executorService ) );\n-  555: \t\tverifyAsynchronouslyAndReset( inOrder -> {\n-  556: \t\t\t// No calls expected yet\n-  557: \t\t} );\n+  545: \t}\n+  546: \n+  547: \tprivate void createAndStartExecutor(int maxTasksPerBatch, boolean fair) {\n+  548: \t\tcreateAndStartExecutor( maxTasksPerBatch, fair, w -> fail( \"Work shouldn't be offloaded.\" ) );\n+  549: \t}\n+  550: \tprivate void createAndStartExecutor(int maxTasksPerBatch, boolean fair, Consumer<? super BatchedWork<? super StubWorkProcessor>> blockingRetryProducer) {\n+  551: \t\tthis.executor = new BatchingExecutor<>(\n+  552: \t\t\t\tNAME, processorMock, maxTasksPerBatch, fair, failureHandlerMock, blockingRetryProducer\n+  553: \t\t);\n+  554: \n+  555: \t\t// Having multiple threads should not matter:\n+  556: \t\t// the batching executor takes care of executing in only one thread at a time.\n+  557: \t\tthis.executorService = threadPoolProvider.newScheduledExecutor( 4, \"BatchingExecutorTest\" );\n   558: \n-  559: \t\tCompletableFuture<?> completion = executor.completion();\n+  559: \t\texecutor.start( new DelegatingSimpleScheduledExecutor( executorService ) );\n   560: \t\tverifyAsynchronouslyAndReset( inOrder -> {\n-  561: \t\t\t// This should not trigger any call to the mocks\n+  561: \t\t\t// No calls expected yet\n   562: \t\t} );\n-  563: \t\t// Initially, there are no works, so works are considered completed.\n-  564: \t\tassertThatFuture( completion ).isSuccessful();\n-  565: \t}\n-  566: \n-  567: \tprivate StubCompletionListener addPendingCompletionListener() {\n-  568: \t\tStubCompletionListener listener = mock( StubCompletionListener.class );\n-  569: \t\tmocks.add( listener );\n-  570: \n+  563: \n+  564: \t\tCompletableFuture<?> completion = executor.completion();\n+  565: \t\tverifyAsynchronouslyAndReset( inOrder -> {\n+  566: \t\t\t// This should not trigger any call to the mocks\n+  567: \t\t} );\n+  568: \t\t// Initially, there are no works, so works are considered completed.\n+  569: \t\tassertThatFuture( completion ).isSuccessful();\n+  570: \t}\n",
        "uniqueId": "56f06536bb2abc1339124800a0673afb8a240112_545_565_550_570_547_549",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17
    },
    {
        "type": "Move Method",
        "description": "Move Method\tprivate compare(a ElasticsearchVersion, b ElasticsearchVersion, defaultInt int) : int from class org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.dialect.ElasticsearchVersionUtils to private compare(a ElasticsearchVersion, b ElasticsearchVersion, defaultInt int) : int from class org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.dialect.ElasticsearchTestDialect.ElasticsearchVersionCondition",
        "diffLocations": [
            {
                "filePath": "util/internal/integrationtest/backend/elasticsearch/src/main/java/org/hibernate/search/util/impl/integrationtest/backend/elasticsearch/dialect/ElasticsearchVersionUtils.java",
                "startLine": 19,
                "endLine": 32,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/integrationtest/backend/elasticsearch/src/main/java/org/hibernate/search/util/impl/integrationtest/backend/elasticsearch/dialect/ElasticsearchTestDialect.java",
                "startLine": 158,
                "endLine": 171,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private static int compare(ElasticsearchVersion a, ElasticsearchVersion b, int defaultInt) {\n\t\tif ( !a.distribution().equals( b.distribution() ) ) {\n\t\t\tthrow new IllegalArgumentException( \"Cannot compare different distributions\" );\n\t\t}\n\n\t\tif ( a.qualifier().isPresent() || b.qualifier().isPresent() ) {\n\t\t\tthrow new IllegalArgumentException( \"Qualifiers are ignored for version ranges.\" );\n\t\t}\n\n\t\treturn Comparator.comparing( ElasticsearchVersion::major )\n\t\t\t\t.thenComparing( version -> version.minor().orElse( defaultInt ) )\n\t\t\t\t.thenComparing( version -> version.micro().orElse( defaultInt ) )\n\t\t\t\t.compare( a, b );\n\t}",
        "filePathBefore": "util/internal/integrationtest/backend/elasticsearch/src/main/java/org/hibernate/search/util/impl/integrationtest/backend/elasticsearch/dialect/ElasticsearchVersionUtils.java",
        "isPureRefactoring": true,
        "commitId": "b272e400c87646314130cf34e94679be94a61a38",
        "packageNameBefore": "org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.dialect",
        "classNameBefore": "org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.dialect.ElasticsearchVersionUtils",
        "methodNameBefore": "org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.dialect.ElasticsearchVersionUtils#compare",
        "invokedMethod": "methodSignature: org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.dialect.ElasticsearchVersionUtils#compare\n methodBody: private static int compare(ElasticsearchVersion a, ElasticsearchVersion b, int defaultInt) {\nif(!a.distribution().equals(b.distribution())){throw new IllegalArgumentException(\"Cannot compare different distributions\");\n}if(a.qualifier().isPresent() || b.qualifier().isPresent()){throw new IllegalArgumentException(\"Qualifiers are ignored for version ranges.\");\n}return Comparator.comparing(ElasticsearchVersion::major).thenComparing(version -> version.minor().orElse(defaultInt)).thenComparing(version -> version.micro().orElse(defaultInt)).compare(a,b);\n}",
        "classSignatureBefore": "public final class ElasticsearchVersionUtils ",
        "methodNameBeforeSet": [
            "org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.dialect.ElasticsearchVersionUtils#compare"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.dialect.ElasticsearchVersionUtils"
        ],
        "classSignatureBeforeSet": [
            "public final class ElasticsearchVersionUtils "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.dialect;\n\nimport java.util.Comparator;\nimport java.util.function.BooleanSupplier;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchDistributionName;\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchVersion;\n\npublic final class ElasticsearchVersionUtils {\n\tprivate ElasticsearchVersionUtils() {\n\t}\n\n\tprivate static int compare(ElasticsearchVersion a, ElasticsearchVersion b, int defaultInt) {\n\t\tif ( !a.distribution().equals( b.distribution() ) ) {\n\t\t\tthrow new IllegalArgumentException( \"Cannot compare different distributions\" );\n\t\t}\n\n\t\tif ( a.qualifier().isPresent() || b.qualifier().isPresent() ) {\n\t\t\tthrow new IllegalArgumentException( \"Qualifiers are ignored for version ranges.\" );\n\t\t}\n\n\t\treturn Comparator.comparing( ElasticsearchVersion::major )\n\t\t\t\t.thenComparing( version -> version.minor().orElse( defaultInt ) )\n\t\t\t\t.thenComparing( version -> version.micro().orElse( defaultInt ) )\n\t\t\t\t.compare( a, b );\n\t}\n\n\tpublic static boolean isOpenSearch(ElasticsearchVersion actual) {\n\n\t\treturn isDistribution( actual, ElasticsearchDistributionName.OPENSEARCH );\n\t}\n\n\tprivate static boolean isDistribution(\n\t\t\tElasticsearchVersion actual,\n\t\t\tElasticsearchDistributionName distribution\n\t) {\n\n\t\treturn actual.distribution().equals( distribution );\n\t}\n\n\tpublic static boolean isMatching(ElasticsearchVersion actual, String version) {\n\t\tElasticsearchVersion v = ElasticsearchVersion.of( version );\n\n\t\treturn v.matches( actual );\n\t}\n\n\tpublic static boolean isAtMost(ElasticsearchVersion actual, String version) {\n\t\tElasticsearchVersion v = ElasticsearchVersion.of( version );\n\n\t\treturn tryOrFalse(\n\t\t\t\t() -> compare( actual, v, Integer.MAX_VALUE ) <= 0\n\t\t);\n\t}\n\n\tpublic static boolean isLessThan(ElasticsearchVersion actual, String version) {\n\t\tElasticsearchVersion v = ElasticsearchVersion.of( version );\n\n\t\treturn tryOrFalse(\n\t\t\t\t() -> compare( actual, v, Integer.MAX_VALUE ) < 0\n\t\t);\n\t}\n\n\tpublic static boolean isBetween(ElasticsearchVersion actual, String minVersion, String maxVersion) {\n\t\tElasticsearchVersion min = ElasticsearchVersion.of( minVersion );\n\t\tElasticsearchVersion max = ElasticsearchVersion.of( maxVersion );\n\n\t\treturn tryOrFalse(\n\t\t\t\t() -> !( compare( max, actual, Integer.MAX_VALUE ) < 0 || compare( min, actual, Integer.MIN_VALUE ) > 0 )\n\t\t);\n\t}\n\n\tprivate static boolean tryOrFalse(BooleanSupplier test) {\n\t\ttry {\n\t\t\treturn test.getAsBoolean();\n\t\t}\n\t\tcatch (IllegalArgumentException e) {\n\t\t\treturn false;\n\t\t}\n\t}\n\n}\n",
        "filePathAfter": "util/internal/integrationtest/backend/elasticsearch/src/main/java/org/hibernate/search/util/impl/integrationtest/backend/elasticsearch/dialect/ElasticsearchTestDialect.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.dialect;\n\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.Comparator;\nimport java.util.List;\nimport java.util.Optional;\nimport java.util.function.Predicate;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchVersion;\nimport org.hibernate.search.backend.elasticsearch.client.impl.Paths;\nimport org.hibernate.search.backend.elasticsearch.util.spi.URLEncodedString;\nimport org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.ElasticsearchTestHostConnectionConfiguration;\n\npublic class ElasticsearchTestDialect {\n\n\tprivate static final ElasticsearchVersion ACTUAL_VERSION = ElasticsearchVersion.of(\n\t\t\tSystem.getProperty( \"org.hibernate.search.integrationtest.backend.elasticsearch.version\" )\n\t);\n\n\tprivate static final ElasticsearchTestDialect INSTANCE = new ElasticsearchTestDialect();\n\n\tpublic static ElasticsearchTestDialect get() {\n\t\treturn INSTANCE;\n\t}\n\n\tpublic static ElasticsearchVersion getActualVersion() {\n\t\treturn ACTUAL_VERSION;\n\t}\n\n\tpublic boolean isEmptyMappingPossible() {\n\t\treturn isActualVersion(\n\t\t\t\tesVersion -> esVersion.isAtMost( \"6.8\" ),\n\t\t\t\tosVersion -> false\n\t\t);\n\t}\n\n\t@SuppressWarnings(\"deprecation\")\n\tpublic Optional<URLEncodedString> getTypeNameForMappingAndBulkApi() {\n\t\tif ( isActualVersion(\n\t\t\t\tesVersion -> esVersion.isAtMost( \"6.8\" ),\n\t\t\t\tosVersion -> false\n\t\t) ) {\n\t\t\treturn Optional.of( Paths.DOC );\n\t\t}\n\t\treturn Optional.empty();\n\t}\n\n\tpublic Boolean getIncludeTypeNameParameterForMappingApi() {\n\t\tif ( isActualVersion(\n\t\t\t\tesVersion -> esVersion.isBetween( \"6.7\", \"6.8\" ),\n\t\t\t\tosVersion -> false\n\t\t) ) {\n\t\t\treturn Boolean.TRUE;\n\t\t}\n\t\treturn null;\n\t}\n\n\tpublic List<String> getAllLocalDateDefaultMappingFormats() {\n\t\tif ( isActualVersion(\n\t\t\t\tesVersion -> esVersion.isAtMost( \"6.8\" ),\n\t\t\t\tosVersion -> false\n\t\t) ) {\n\t\t\treturn Arrays.asList( \"yyyy-MM-dd\", \"yyyyyyyyy-MM-dd\" );\n\t\t}\n\t\treturn Collections.singletonList( \"uuuu-MM-dd\" );\n\t}\n\n\tpublic boolean supportsIsWriteIndex() {\n\t\treturn isActualVersion(\n\t\t\t\tesVersion -> !esVersion.isLessThan( \"6.4.0\" ),\n\t\t\t\tosVersion -> true\n\t\t);\n\t}\n\n\tpublic String getFirstLocalDateDefaultMappingFormat() {\n\t\treturn getAllLocalDateDefaultMappingFormats().get( 0 );\n\t}\n\n\tpublic String getConcatenatedLocalDateDefaultMappingFormats() {\n\t\treturn String.join( \"||\", getAllLocalDateDefaultMappingFormats() );\n\t}\n\n\tpublic static boolean isActualVersion(\n\t\t\tPredicate<ElasticsearchVersionCondition> elasticsearchPredicate,\n\t\t\tPredicate<ElasticsearchVersionCondition> opensearchPredicate\n\t) {\n\t\treturn isVersion(\n\t\t\t\tACTUAL_VERSION,\n\t\t\t\telasticsearchPredicate,\n\t\t\t\topensearchPredicate\n\t\t);\n\t}\n\n\tstatic boolean isVersion(\n\t\t\tElasticsearchVersion version,\n\t\t\tPredicate<ElasticsearchVersionCondition> elasticsearchPredicate,\n\t\t\tPredicate<ElasticsearchVersionCondition> opensearchPredicate\n\t) {\n\t\tElasticsearchVersionCondition condition = new ElasticsearchVersionCondition( version );\n\n\t\tswitch ( version.distribution() ) {\n\t\t\tcase ELASTIC:\n\t\t\t\treturn elasticsearchPredicate.test( condition );\n\t\t\tcase OPENSEARCH:\n\t\t\t\treturn opensearchPredicate.test( condition );\n\t\t\tdefault:\n\t\t\t\tthrow new IllegalStateException( \"Unknown distribution\" );\n\t\t}\n\t}\n\n\tpublic static class ElasticsearchVersionCondition {\n\t\tprivate final ElasticsearchVersion actual;\n\n\t\tprivate ElasticsearchVersionCondition(ElasticsearchVersion actual) {\n\t\t\tthis.actual = actual;\n\t\t}\n\n\t\tpublic boolean isAws() {\n\t\t\treturn ElasticsearchTestHostConnectionConfiguration.get().isAws();\n\t\t}\n\n\t\tpublic boolean isMatching(String version) {\n\t\t\tElasticsearchVersion v = ElasticsearchVersion.of( actual.distribution(), version );\n\n\t\t\treturn v.matches( actual );\n\t\t}\n\n\t\tpublic boolean isAtMost(String version) {\n\t\t\tElasticsearchVersion v = ElasticsearchVersion.of( actual.distribution(), version );\n\n\t\t\treturn compare( actual, v, Integer.MAX_VALUE ) <= 0;\n\t\t}\n\n\t\tpublic boolean isLessThan(String version) {\n\t\t\tElasticsearchVersion v = ElasticsearchVersion.of( actual.distribution(), version );\n\n\t\t\treturn compare( actual, v, Integer.MAX_VALUE ) < 0;\n\t\t}\n\n\t\tpublic boolean isBetween(String minVersion, String maxVersion) {\n\t\t\tElasticsearchVersion min = ElasticsearchVersion.of( actual.distribution(), minVersion );\n\t\t\tElasticsearchVersion max = ElasticsearchVersion.of( actual.distribution(), maxVersion );\n\n\t\t\treturn !( compare( max, actual, Integer.MAX_VALUE ) < 0 || compare( min, actual, Integer.MIN_VALUE ) > 0 );\n\t\t}\n\n\t\tpublic ElasticsearchVersion actual() {\n\t\t\treturn actual;\n\t\t}\n\n\t\tprivate static int compare(ElasticsearchVersion a, ElasticsearchVersion b, int defaultInt) {\n\t\t\tif ( !a.distribution().equals( b.distribution() ) ) {\n\t\t\t\tthrow new IllegalArgumentException( \"Cannot compare different distributions\" );\n\t\t\t}\n\n\t\t\tif ( a.qualifier().isPresent() || b.qualifier().isPresent() ) {\n\t\t\t\tthrow new IllegalArgumentException( \"Qualifiers are ignored for version ranges.\" );\n\t\t\t}\n\n\t\t\treturn Comparator.comparing( ElasticsearchVersion::major )\n\t\t\t\t\t.thenComparing( version -> version.minor().orElse( defaultInt ) )\n\t\t\t\t\t.thenComparing( version -> version.micro().orElse( defaultInt ) )\n\t\t\t\t\t.compare( a, b );\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.dialect.ElasticsearchVersionUtils#compare\n methodBody: private static int compare(ElasticsearchVersion a, ElasticsearchVersion b, int defaultInt) {\nif(!a.distribution().equals(b.distribution())){throw new IllegalArgumentException(\"Cannot compare different distributions\");\n}if(a.qualifier().isPresent() || b.qualifier().isPresent()){throw new IllegalArgumentException(\"Qualifiers are ignored for version ranges.\");\n}return Comparator.comparing(ElasticsearchVersion::major).thenComparing(version -> version.minor().orElse(defaultInt)).thenComparing(version -> version.micro().orElse(defaultInt)).compare(a,b);\n}"
        ],
        "sourceCodeAfterRefactoring": "private static int compare(ElasticsearchVersion a, ElasticsearchVersion b, int defaultInt) {\n\t\t\tif ( !a.distribution().equals( b.distribution() ) ) {\n\t\t\t\tthrow new IllegalArgumentException( \"Cannot compare different distributions\" );\n\t\t\t}\n\n\t\t\tif ( a.qualifier().isPresent() || b.qualifier().isPresent() ) {\n\t\t\t\tthrow new IllegalArgumentException( \"Qualifiers are ignored for version ranges.\" );\n\t\t\t}\n\n\t\t\treturn Comparator.comparing( ElasticsearchVersion::major )\n\t\t\t\t\t.thenComparing( version -> version.minor().orElse( defaultInt ) )\n\t\t\t\t\t.thenComparing( version -> version.micro().orElse( defaultInt ) )\n\t\t\t\t\t.compare( a, b );\n\t\t}",
        "diffSourceCode": "-   19: \tprivate static int compare(ElasticsearchVersion a, ElasticsearchVersion b, int defaultInt) {\n-   20: \t\tif ( !a.distribution().equals( b.distribution() ) ) {\n-   21: \t\t\tthrow new IllegalArgumentException( \"Cannot compare different distributions\" );\n-   22: \t\t}\n-   23: \n-   24: \t\tif ( a.qualifier().isPresent() || b.qualifier().isPresent() ) {\n-   25: \t\t\tthrow new IllegalArgumentException( \"Qualifiers are ignored for version ranges.\" );\n-   26: \t\t}\n-   27: \n-   28: \t\treturn Comparator.comparing( ElasticsearchVersion::major )\n-   29: \t\t\t\t.thenComparing( version -> version.minor().orElse( defaultInt ) )\n-   30: \t\t\t\t.thenComparing( version -> version.micro().orElse( defaultInt ) )\n-   31: \t\t\t\t.compare( a, b );\n-   32: \t}\n+   19: import org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.ElasticsearchTestHostConnectionConfiguration;\n+   20: \n+   21: public class ElasticsearchTestDialect {\n+   22: \n+   23: \tprivate static final ElasticsearchVersion ACTUAL_VERSION = ElasticsearchVersion.of(\n+   24: \t\t\tSystem.getProperty( \"org.hibernate.search.integrationtest.backend.elasticsearch.version\" )\n+   25: \t);\n+   26: \n+   27: \tprivate static final ElasticsearchTestDialect INSTANCE = new ElasticsearchTestDialect();\n+   28: \n+   29: \tpublic static ElasticsearchTestDialect get() {\n+   30: \t\treturn INSTANCE;\n+   31: \t}\n+   32: \n+  158: \t\tprivate static int compare(ElasticsearchVersion a, ElasticsearchVersion b, int defaultInt) {\n+  159: \t\t\tif ( !a.distribution().equals( b.distribution() ) ) {\n+  160: \t\t\t\tthrow new IllegalArgumentException( \"Cannot compare different distributions\" );\n+  161: \t\t\t}\n+  162: \n+  163: \t\t\tif ( a.qualifier().isPresent() || b.qualifier().isPresent() ) {\n+  164: \t\t\t\tthrow new IllegalArgumentException( \"Qualifiers are ignored for version ranges.\" );\n+  165: \t\t\t}\n+  166: \n+  167: \t\t\treturn Comparator.comparing( ElasticsearchVersion::major )\n+  168: \t\t\t\t\t.thenComparing( version -> version.minor().orElse( defaultInt ) )\n+  169: \t\t\t\t\t.thenComparing( version -> version.micro().orElse( defaultInt ) )\n+  170: \t\t\t\t\t.compare( a, b );\n+  171: \t\t}\n",
        "uniqueId": "b272e400c87646314130cf34e94679be94a61a38_19_32__158_171",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 5,
                "covered": 31
            },
            "BRANCH": {
                "missed": 2,
                "covered": 4
            },
            "LINE": {
                "missed": 1,
                "covered": 7
            },
            "COMPLEXITY": {
                "missed": 2,
                "covered": 2
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tpublic of(latitude double, longitude double) : GeoPoint extracted from public searchAroundMe_spatial() : void in class org.hibernate.search.integrationtest.showcase.OrmElasticsearchLibraryShowcaseIT & moved to class org.hibernate.search.engine.spatial.GeoPoint",
        "diffLocations": [
            {
                "filePath": "integrationtest/showcase/library/src/test/java/org/hibernate/search/integrationtest/showcase/OrmElasticsearchLibraryShowcaseIT.java",
                "startLine": 321,
                "endLine": 385,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/showcase/library/src/test/java/org/hibernate/search/integrationtest/showcase/OrmElasticsearchLibraryShowcaseIT.java",
                "startLine": 320,
                "endLine": 384,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/showcase/library/src/test/java/org/hibernate/search/integrationtest/showcase/OrmElasticsearchLibraryShowcaseIT.java",
                "startLine": 23,
                "endLine": 32,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void searchAroundMe_spatial() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n\n\t\t\tGeoPoint myLocation = new ImmutableGeoPoint( 42.0, 0.5 );\n\n\t\t\tList<Document<?>> documents = dao.searchAroundMe(\n\t\t\t\t\tnull, null,\n\t\t\t\t\tmyLocation, 20.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from university\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n\t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\tnull, null,\n\t\t\t\t\tmyLocation, 40.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from suburb1 or university\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, CALLIGRAPHY_ID ),\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n\t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\t\"calligraphy\", null,\n\t\t\t\t\tmyLocation, 40.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from suburb1 or university with \"calligraphy\" in it\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, CALLIGRAPHY_ID )\n\t\t\t);\n\n\t\t\tmyLocation = new ImmutableGeoPoint( 42.0, 0.75 );\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\tnull, null,\n\t\t\t\t\tmyLocation, 40.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from university\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n\t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n\t\t\t);\n\t\t} );\n\t}",
        "filePathBefore": "integrationtest/showcase/library/src/test/java/org/hibernate/search/integrationtest/showcase/OrmElasticsearchLibraryShowcaseIT.java",
        "isPureRefactoring": true,
        "commitId": "520908200e1e83c994b39a93131741bc114ee85d",
        "packageNameBefore": "org.hibernate.search.integrationtest.showcase",
        "classNameBefore": "org.hibernate.search.integrationtest.showcase.OrmElasticsearchLibraryShowcaseIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.showcase.OrmElasticsearchLibraryShowcaseIT#searchAroundMe_spatial",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.backend.tck.search.projection.SearchProjectionIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(DOCUMENT_1),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.supportedFieldWithProjectionConverterModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.string1Field.document1Value.write(document);\n  indexMapping.string2Field.document1Value.write(document);\n  DocumentElement flattenedObject=indexMapping.flattenedObject.self.add(document);\n  indexMapping.flattenedObject.supportedFieldModels.forEach(f -> f.document1Value.write(flattenedObject));\n  DocumentElement nestedObject=indexMapping.nestedObject.self.add(document);\n  indexMapping.nestedObject.supportedFieldModels.forEach(f -> f.document1Value.write(nestedObject));\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_2),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.supportedFieldWithProjectionConverterModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.string1Field.document2Value.write(document);\n  indexMapping.string2Field.document2Value.write(document);\n  DocumentElement flattenedObject=indexMapping.flattenedObject.self.add(document);\n  indexMapping.flattenedObject.supportedFieldModels.forEach(f -> f.document2Value.write(flattenedObject));\n  DocumentElement nestedObject=indexMapping.nestedObject.self.add(document);\n  indexMapping.nestedObject.supportedFieldModels.forEach(f -> f.document2Value.write(nestedObject));\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_3),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.supportedFieldWithProjectionConverterModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.string1Field.document3Value.write(document);\n  indexMapping.string2Field.document3Value.write(document);\n  DocumentElement flattenedObject=indexMapping.flattenedObject.self.add(document);\n  indexMapping.flattenedObject.supportedFieldModels.forEach(f -> f.document3Value.write(flattenedObject));\n  DocumentElement nestedObject=indexMapping.nestedObject.self.add(document);\n  indexMapping.nestedObject.supportedFieldModels.forEach(f -> f.document3Value.write(nestedObject));\n}\n);\nworkPlan.add(referenceProvider(EMPTY),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nDocumentReferencesSearchResultAssert.assertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,DOCUMENT_1,DOCUMENT_2,DOCUMENT_3,EMPTY);\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.tck.search.predicate.RangeSearchPredicateIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(DOCUMENT_1),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.string1Field.document1Value.write(document);\n  indexMapping.string2Field.document1Value.write(document);\n  indexMapping.string3Field.document1Value.write(document);\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_2),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.string1Field.document2Value.write(document);\n  indexMapping.string2Field.document2Value.write(document);\n  indexMapping.string3Field.document2Value.write(document);\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_3),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.string1Field.document3Value.write(document);\n  indexMapping.string2Field.document3Value.write(document);\n  indexMapping.string3Field.document3Value.write(document);\n}\n);\nworkPlan.add(referenceProvider(EMPTY_ID),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,DOCUMENT_1,DOCUMENT_2,DOCUMENT_3,EMPTY_ID);\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.tck.SmokeIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(\"1\"),document -> {\n  indexAccessors.string.write(document,\"text 1\");\n  indexAccessors.string_analyzed.write(document,\"text 1\");\n  indexAccessors.integer.write(document,1);\n  indexAccessors.localDate.write(document,LocalDate.of(2018,1,1));\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(0,1));\n  DocumentElement flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"text 1_1\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,101);\n  flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"text 1_2\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,102);\n  DocumentElement nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"text 1_1\");\n  indexAccessors.nestedObject.integer.write(nestedObject,101);\n  nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"text 1_2\");\n  indexAccessors.nestedObject.integer.write(nestedObject,102);\n}\n);\nworkPlan.add(referenceProvider(\"2\"),document -> {\n  indexAccessors.string.write(document,\"text 2\");\n  indexAccessors.string_analyzed.write(document,\"text 2\");\n  indexAccessors.integer.write(document,2);\n  indexAccessors.localDate.write(document,LocalDate.of(2018,1,2));\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(0,2));\n  DocumentElement flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"text 2_1\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,201);\n  flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"text 2_2\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,202);\n  DocumentElement nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"text 2_1\");\n  indexAccessors.nestedObject.integer.write(nestedObject,201);\n  nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"text 2_2\");\n  indexAccessors.nestedObject.integer.write(nestedObject,202);\n}\n);\nworkPlan.add(referenceProvider(\"3\"),document -> {\n  indexAccessors.string.write(document,\"text 3\");\n  indexAccessors.string_analyzed.write(document,\"text 3\");\n  indexAccessors.integer.write(document,3);\n}\n);\nworkPlan.add(referenceProvider(\"neverMatching\"),document -> {\n  indexAccessors.string.write(document,\"never matching\");\n  indexAccessors.string_analyzed.write(document,\"never matching\");\n  indexAccessors.integer.write(document,9484);\n}\n);\nworkPlan.add(referenceProvider(\"empty\"),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,\"1\",\"2\",\"3\",\"neverMatching\",\"empty\");\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.tck.search.sort.SearchSortIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(SECOND_ID),document -> {\n  indexAccessors.string.write(document,\"george\");\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.7705687,4.835233));\n  indexAccessors.string_analyzed_forScore.write(document,\"Hooray Hooray\");\n  indexAccessors.unsortable.write(document,\"george\");\n  DocumentElement flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"george\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,2);\n  DocumentElement nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"george\");\n  indexAccessors.nestedObject.integer.write(nestedObject,2);\n}\n);\nworkPlan.add(referenceProvider(FIRST_ID),document -> {\n  indexAccessors.string.write(document,\"aaron\");\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.7541719,4.8386221));\n  indexAccessors.string_analyzed_forScore.write(document,\"Hooray Hooray Hooray\");\n  indexAccessors.unsortable.write(document,\"aaron\");\n  DocumentElement flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"aaron\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,1);\n  DocumentElement nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"aaron\");\n  indexAccessors.nestedObject.integer.write(nestedObject,1);\n}\n);\nworkPlan.add(referenceProvider(THIRD_ID),document -> {\n  indexAccessors.string.write(document,\"zach\");\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.7530374,4.8510299));\n  indexAccessors.string_analyzed_forScore.write(document,\"Hooray\");\n  indexAccessors.unsortable.write(document,\"zach\");\n  DocumentElement flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"zach\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,3);\n  DocumentElement nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"zach\");\n  indexAccessors.nestedObject.integer.write(nestedObject,3);\n}\n);\nworkPlan.add(referenceProvider(EMPTY_ID),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,FIRST_ID,SECOND_ID,THIRD_ID,EMPTY_ID);\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.tck.search.predicate.MatchSearchPredicateIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(DOCUMENT_1),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.string1Field.document1Value.write(document);\n  indexMapping.string2Field.document1Value.write(document);\n  indexMapping.string3Field.document1Value.write(document);\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_2),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.string1Field.document2Value.write(document);\n  indexMapping.string2Field.document2Value.write(document);\n  indexMapping.string3Field.document2Value.write(document);\n}\n);\nworkPlan.add(referenceProvider(EMPTY),document -> {\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_3),document -> {\n  indexMapping.string1Field.document3Value.write(document);\n  indexMapping.string2Field.document3Value.write(document);\n  indexMapping.string3Field.document3Value.write(document);\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,DOCUMENT_1,DOCUMENT_2,EMPTY,DOCUMENT_3);\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.tck.search.predicate.spatial.AbstractSpatialWithinSearchPredicateIT#initData\n methodBody: protected void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(OURSON_QUI_BOIT_ID),document -> {\n  indexAccessors.string.write(document,OURSON_QUI_BOIT_STRING);\n  indexAccessors.geoPoint.write(document,OURSON_QUI_BOIT_GEO_POINT);\n  indexAccessors.geoPoint_1.write(document,new ImmutableGeoPoint(OURSON_QUI_BOIT_GEO_POINT.getLatitude() - 1,OURSON_QUI_BOIT_GEO_POINT.getLongitude() - 1));\n  indexAccessors.geoPoint_2.write(document,new ImmutableGeoPoint(OURSON_QUI_BOIT_GEO_POINT.getLatitude() - 2,OURSON_QUI_BOIT_GEO_POINT.getLongitude() - 2));\n}\n);\nworkPlan.add(referenceProvider(IMOUTO_ID),document -> {\n  indexAccessors.string.write(document,IMOUTO_STRING);\n  indexAccessors.geoPoint.write(document,IMOUTO_GEO_POINT);\n  indexAccessors.geoPoint_1.write(document,new ImmutableGeoPoint(IMOUTO_GEO_POINT.getLatitude() - 1,IMOUTO_GEO_POINT.getLongitude() - 1));\n  indexAccessors.geoPoint_2.write(document,new ImmutableGeoPoint(IMOUTO_GEO_POINT.getLatitude() - 2,IMOUTO_GEO_POINT.getLongitude() - 2));\n}\n);\nworkPlan.add(referenceProvider(CHEZ_MARGOTTE_ID),document -> {\n  indexAccessors.string.write(document,CHEZ_MARGOTTE_STRING);\n  indexAccessors.geoPoint.write(document,CHEZ_MARGOTTE_GEO_POINT);\n  indexAccessors.geoPoint_1.write(document,new ImmutableGeoPoint(CHEZ_MARGOTTE_GEO_POINT.getLatitude() - 1,CHEZ_MARGOTTE_GEO_POINT.getLongitude() - 1));\n  indexAccessors.geoPoint_2.write(document,new ImmutableGeoPoint(CHEZ_MARGOTTE_GEO_POINT.getLatitude() - 2,CHEZ_MARGOTTE_GEO_POINT.getLongitude() - 2));\n}\n);\nworkPlan.add(referenceProvider(EMPTY_ID),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,OURSON_QUI_BOIT_ID,IMOUTO_ID,CHEZ_MARGOTTE_ID,EMPTY_ID);\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.tck.search.sort.SearchSortByFieldIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(DOCUMENT_2),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.identicalForFirstTwo.document2Value.write(document);\n  indexMapping.identicalForLastTwo.document2Value.write(document);\n  DocumentElement flattenedObject=indexMapping.flattenedObject.self.add(document);\n  indexMapping.flattenedObject.supportedFieldModels.forEach(f -> f.document2Value.write(flattenedObject));\n  indexMapping.flattenedObject.unsupportedFieldModels.forEach(f -> f.document2Value.write(flattenedObject));\n  DocumentElement nestedObject=indexMapping.nestedObject.self.add(document);\n  indexMapping.nestedObject.supportedFieldModels.forEach(f -> f.document2Value.write(nestedObject));\n  indexMapping.nestedObject.unsupportedFieldModels.forEach(f -> f.document2Value.write(nestedObject));\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_1),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.identicalForFirstTwo.document1Value.write(document);\n  indexMapping.identicalForLastTwo.document1Value.write(document);\n  DocumentElement flattenedObject=indexMapping.flattenedObject.self.add(document);\n  indexMapping.flattenedObject.supportedFieldModels.forEach(f -> f.document1Value.write(flattenedObject));\n  indexMapping.flattenedObject.unsupportedFieldModels.forEach(f -> f.document1Value.write(flattenedObject));\n  DocumentElement nestedObject=indexMapping.nestedObject.self.add(document);\n  indexMapping.nestedObject.supportedFieldModels.forEach(f -> f.document1Value.write(nestedObject));\n  indexMapping.nestedObject.unsupportedFieldModels.forEach(f -> f.document1Value.write(nestedObject));\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_3),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.identicalForFirstTwo.document3Value.write(document);\n  indexMapping.identicalForLastTwo.document3Value.write(document);\n  DocumentElement flattenedObject=indexMapping.flattenedObject.self.add(document);\n  indexMapping.flattenedObject.supportedFieldModels.forEach(f -> f.document3Value.write(flattenedObject));\n  indexMapping.flattenedObject.unsupportedFieldModels.forEach(f -> f.document3Value.write(flattenedObject));\n  DocumentElement nestedObject=indexMapping.nestedObject.self.add(document);\n  indexMapping.nestedObject.supportedFieldModels.forEach(f -> f.document3Value.write(nestedObject));\n  indexMapping.nestedObject.unsupportedFieldModels.forEach(f -> f.document3Value.write(nestedObject));\n}\n);\nworkPlan.add(referenceProvider(EMPTY),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,DOCUMENT_1,DOCUMENT_2,DOCUMENT_3,EMPTY);\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.lucene.search.LuceneSearchSortIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(FIRST_ID),document -> {\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.7705687,4.835233));\n}\n);\nworkPlan.add(referenceProvider(SECOND_ID),document -> {\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.7541719,4.8386221));\n}\n);\nworkPlan.add(referenceProvider(THIRD_ID),document -> {\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.7530374,4.8510299));\n}\n);\nworkPlan.add(referenceProvider(EMPTY_ID),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,FIRST_ID,SECOND_ID,THIRD_ID,EMPTY_ID);\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.lucene.ExtensionIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(FIRST_ID),document -> {\n  indexAccessors.string.write(document,\"text 1\");\n  indexAccessors.nativeField.write(document,37);\n  indexAccessors.nativeField_unsupportedProjection.write(document,37);\n  indexAccessors.sort1.write(document,\"a\");\n  indexAccessors.sort2.write(document,\"z\");\n  indexAccessors.sort3.write(document,\"z\");\n}\n);\nworkPlan.add(referenceProvider(SECOND_ID),document -> {\n  indexAccessors.integer.write(document,2);\n  indexAccessors.nativeField.write(document,78);\n  indexAccessors.nativeField_unsupportedProjection.write(document,78);\n  indexAccessors.sort1.write(document,\"z\");\n  indexAccessors.sort2.write(document,\"a\");\n  indexAccessors.sort3.write(document,\"z\");\n}\n);\nworkPlan.add(referenceProvider(THIRD_ID),document -> {\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(40.12,-71.34));\n  indexAccessors.nativeField.write(document,13);\n  indexAccessors.nativeField_unsupportedProjection.write(document,13);\n  indexAccessors.sort1.write(document,\"z\");\n  indexAccessors.sort2.write(document,\"z\");\n  indexAccessors.sort3.write(document,\"a\");\n}\n);\nworkPlan.add(referenceProvider(FOURTH_ID),document -> {\n  indexAccessors.nativeField.write(document,89);\n  indexAccessors.nativeField_unsupportedProjection.write(document,89);\n  indexAccessors.sort1.write(document,\"z\");\n  indexAccessors.sort2.write(document,\"z\");\n  indexAccessors.sort3.write(document,\"z\");\n}\n);\nworkPlan.add(referenceProvider(FIFTH_ID),document -> {\n  indexAccessors.string.write(document,\"text 2\");\n  indexAccessors.integer.write(document,1);\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.12,-75.34));\n  indexAccessors.nativeField.write(document,53);\n  indexAccessors.nativeField_unsupportedProjection.write(document,53);\n  indexAccessors.sort1.write(document,\"zz\");\n  indexAccessors.sort2.write(document,\"zz\");\n  indexAccessors.sort3.write(document,\"zz\");\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,FIRST_ID,SECOND_ID,THIRD_ID,FOURTH_ID,FIFTH_ID);\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.tck.search.SearchResultLoadingOrTransformingIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(MAIN_ID),document -> {\n  indexAccessors.string.write(document,STRING_VALUE);\n  indexAccessors.string_analyzed.write(document,STRING_ANALYZED_VALUE);\n  indexAccessors.integer.write(document,INTEGER_VALUE);\n  indexAccessors.localDate.write(document,LOCAL_DATE_VALUE);\n  indexAccessors.geoPoint.write(document,GEO_POINT_VALUE);\n  DocumentElement flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,FLATTENED_OBJECT_STRING_VALUE);\n  indexAccessors.flattenedObject.integer.write(flattenedObject,FLATTENED_OBJECT_INTEGER_VALUE);\n  DocumentElement nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,NESTED_OBJECT_STRING_VALUE);\n  indexAccessors.nestedObject.integer.write(nestedObject,NESTED_OBJECT_INTEGER_VALUE);\n}\n);\nworkPlan.add(referenceProvider(EMPTY_ID),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,MAIN_ID,EMPTY_ID);\n}\nmethodSignature: org.hibernate.search.integrationtest.showcase.OrmElasticsearchLibraryShowcaseIT#initData\n methodBody: private void initData(Session session) {\nLibraryDao libraryDao=daoFactory.createLibraryDao(session);\nDocumentDao documentDao=daoFactory.createDocumentDao(session);\nPersonDao personDao=daoFactory.createPersonDao(session);\nBook calligraphy=documentDao.createBook(CALLIGRAPHY_ID,new ISBN(\"978-0-00-000001-1\"),\"Calligraphy for Dummies\",\"Learn to write artfully in ten lessons\",\"calligraphy,art\");\nVideo javaDancing=documentDao.createVideo(JAVA_DANCING_ID,\"Java le dire \u00e0 tout le monde\",\"A brief history of Java dancing in Paris during the early 20th century\",\"java,dancing,history\");\nBook indonesianEconomy=documentDao.createBook(INDONESIAN_ECONOMY_ID,new ISBN(\"978-0-00-000003-3\"),\"Comparative Study of the Economy of Java and other Indonesian Islands\",\"Comparative study of the late 20th century economy of the main islands of Indonesia\" + \" with accurate projections over the next ten centuries\",\"geography,economy,java,sumatra,borneo,sulawesi\");\nBook javaForDummies=documentDao.createBook(JAVA_FOR_DUMMIES_ID,new ISBN(\"978-0-00-000004-4\"),\"java for Dummies\",\"Learning the Java programming language in ten lessons\",\"programming,language,java\");\nBook artOfComputerProg=documentDao.createBook(ART_OF_COMPUTER_PROG_ID,new ISBN(\"978-0-00-000005-5\"),\"The Art of Computer Programming\",\"Quick review of basic computer programming principles in 965 chapters\",\"programming\");\nBook thesaurusOfLanguages=documentDao.createBook(THESAURUS_OF_LANGUAGES_ID,new ISBN(\"978-0-00-000006-6\"),\"Thesaurus of Indo-European Languages\",\"An entertaining list of about three thousand languages, most of which are long dead\",\"geography,language\");\nVideo livingOnIsland=documentDao.createVideo(LIVING_ON_ISLAND_ID,\"Living in an Island, Episode 3: Indonesia\",\"A journey across Indonesia's smallest islands depicting how island way of life differs from mainland living\",\"geography,java,sumatra,borneo,sulawesi\");\nLibrary cityCenterLibrary=libraryDao.create(CITY_CENTER_ID,\"City Center Library\",12400,42.0,0.0,LibraryService.READING_ROOMS,LibraryService.HARDCOPY_LOAN);\ndocumentDao.createCopy(cityCenterLibrary,calligraphy,BookMedium.HARDCOPY);\ndocumentDao.createCopy(cityCenterLibrary,javaDancing,VideoMedium.DVD);\ndocumentDao.createCopy(cityCenterLibrary,indonesianEconomy,BookMedium.HARDCOPY);\ndocumentDao.createCopy(cityCenterLibrary,javaForDummies,BookMedium.HARDCOPY);\ndocumentDao.createCopy(cityCenterLibrary,artOfComputerProg,BookMedium.HARDCOPY);\ndocumentDao.createCopy(cityCenterLibrary,thesaurusOfLanguages,BookMedium.HARDCOPY);\ndocumentDao.createCopy(cityCenterLibrary,livingOnIsland,VideoMedium.BLURAY);\nLibrary suburbanLibrary1=libraryDao.create(SUBURBAN_1_ID,\"suburban Library 1\",800,42.0,0.25,LibraryService.DISABLED_ACCESS,LibraryService.HARDCOPY_LOAN);\ndocumentDao.createCopy(suburbanLibrary1,calligraphy,BookMedium.HARDCOPY);\ndocumentDao.createCopy(suburbanLibrary1,indonesianEconomy,BookMedium.HARDCOPY);\ndocumentDao.createCopy(suburbanLibrary1,javaForDummies,BookMedium.HARDCOPY);\ndocumentDao.createCopy(suburbanLibrary1,artOfComputerProg,BookMedium.HARDCOPY);\ndocumentDao.createCopy(suburbanLibrary1,thesaurusOfLanguages,BookMedium.HARDCOPY);\nLibrary suburbanLibrary2=libraryDao.create(SUBURBAN_2_ID,\"Suburban Library 2\",800,42.0,-0.25,LibraryService.DISABLED_ACCESS,LibraryService.READING_ROOMS,LibraryService.HARDCOPY_LOAN);\ndocumentDao.createCopy(suburbanLibrary2,calligraphy,BookMedium.HARDCOPY);\ndocumentDao.createCopy(suburbanLibrary2,calligraphy,BookMedium.DEMATERIALIZED);\ndocumentDao.createCopy(suburbanLibrary2,javaDancing,VideoMedium.DVD);\ndocumentDao.createCopy(suburbanLibrary2,javaDancing,VideoMedium.DEMATERIALIZED);\ndocumentDao.createCopy(suburbanLibrary2,javaForDummies,BookMedium.HARDCOPY);\ndocumentDao.createCopy(suburbanLibrary2,javaForDummies,BookMedium.DEMATERIALIZED);\ndocumentDao.createCopy(suburbanLibrary2,livingOnIsland,VideoMedium.BLURAY);\ndocumentDao.createCopy(suburbanLibrary2,livingOnIsland,VideoMedium.DEMATERIALIZED);\nLibrary universityLibrary=libraryDao.create(UNIVERSITY_ID,\"University Library\",9000,42.0,0.5,LibraryService.READING_ROOMS,LibraryService.HARDCOPY_LOAN,LibraryService.DEMATERIALIZED_LOAN);\ndocumentDao.createCopy(universityLibrary,indonesianEconomy,BookMedium.HARDCOPY);\ndocumentDao.createCopy(universityLibrary,javaForDummies,BookMedium.HARDCOPY);\ndocumentDao.createCopy(universityLibrary,artOfComputerProg,BookMedium.HARDCOPY);\ndocumentDao.createCopy(universityLibrary,thesaurusOfLanguages,BookMedium.HARDCOPY);\nPerson janeSmith=personDao.create(JANE_SMITH_ID,\"Jane\",\"Smith\");\npersonDao.createAccount(janeSmith);\ncreateBorrowal(personDao,janeSmith,cityCenterLibrary,indonesianEconomy,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,janeSmith,cityCenterLibrary,artOfComputerProg,BorrowalType.LONG_TERM);\ncreateBorrowal(personDao,janeSmith,cityCenterLibrary,javaForDummies,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,janeSmith,cityCenterLibrary,calligraphy,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,janeSmith,cityCenterLibrary,javaDancing,BorrowalType.LONG_TERM);\nPerson janeFonda=personDao.create(JANE_FONDA_ID,\"Jane\",\"Fonda\");\npersonDao.createAccount(janeFonda);\ncreateBorrowal(personDao,janeFonda,universityLibrary,javaForDummies,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,janeFonda,universityLibrary,artOfComputerProg,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,janeFonda,universityLibrary,thesaurusOfLanguages,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,janeFonda,universityLibrary,indonesianEconomy,BorrowalType.SHORT_TERM);\nPerson janePorter=personDao.create(JANE_PORTER_ID,\"Jane\",\"porter\");\npersonDao.createAccount(janePorter);\ncreateBorrowal(personDao,janePorter,suburbanLibrary1,indonesianEconomy,BorrowalType.LONG_TERM);\ncreateBorrowal(personDao,janePorter,suburbanLibrary2,livingOnIsland,1,BorrowalType.LONG_TERM);\ncreateBorrowal(personDao,janePorter,universityLibrary,thesaurusOfLanguages,BorrowalType.LONG_TERM);\nPerson johnLennon=personDao.create(JOHN_LENNON_ID,\"john\",\"Lennon\");\npersonDao.createAccount(johnLennon);\ncreateBorrowal(personDao,johnLennon,suburbanLibrary2,javaDancing,1,BorrowalType.SHORT_TERM);\nPerson eltonJohn=personDao.create(ELTON_JOHN_ID,\"elton\",\"john\");\npersonDao.createAccount(eltonJohn);\ncreateBorrowal(personDao,eltonJohn,suburbanLibrary2,javaDancing,1,BorrowalType.SHORT_TERM);\nPerson pattySmith=personDao.create(PATTY_SMITH_ID,\"Patty\",\"Smith\");\npersonDao.createAccount(pattySmith);\ncreateBorrowal(personDao,pattySmith,suburbanLibrary2,javaDancing,1,BorrowalType.SHORT_TERM);\nPerson johnSmith=personDao.create(JOHN_SMITH_ID,\"John\",\"Smith\");\npersonDao.createAccount(johnSmith);\ncreateBorrowal(personDao,johnSmith,suburbanLibrary1,indonesianEconomy,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,johnSmith,suburbanLibrary1,artOfComputerProg,BorrowalType.LONG_TERM);\nPerson johnPaulSmith=personDao.create(JOHN_PAUL_SMITH_ID,\"John Paul\",\"Smith\");\nPerson johnPaul=personDao.create(JOHN_PAUL_ID,\"John\",\"Paul\");\npersonDao.createAccount(johnPaul);\nPerson paulJohn=personDao.create(PAUL_JOHN_ID,\"Paul\",\"John\");\npersonDao.createAccount(paulJohn);\ncreateBorrowal(personDao,paulJohn,cityCenterLibrary,javaForDummies,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,paulJohn,cityCenterLibrary,artOfComputerProg,BorrowalType.SHORT_TERM);\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.tck.search.predicate.spatial.SpatialWithinBoundingBoxSearchPredicateIT#initData\n methodBody: protected void initData() {\nsuper.initData();\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(ADDITIONAL_POINT_1_ID),document -> {\n  indexAccessors.geoPoint.write(document,ADDITIONAL_POINT_1_GEO_POINT);\n}\n);\nworkPlan.add(referenceProvider(ADDITIONAL_POINT_2_ID),document -> {\n  indexAccessors.geoPoint.write(document,ADDITIONAL_POINT_2_GEO_POINT);\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nDocumentReferencesSearchResultAssert.assertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,OURSON_QUI_BOIT_ID,IMOUTO_ID,CHEZ_MARGOTTE_ID,EMPTY_ID,ADDITIONAL_POINT_1_ID,ADDITIONAL_POINT_2_ID);\n}",
        "classSignatureBefore": "public class OrmElasticsearchLibraryShowcaseIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.showcase.OrmElasticsearchLibraryShowcaseIT#searchAroundMe_spatial"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.showcase.OrmElasticsearchLibraryShowcaseIT"
        ],
        "classSignatureBeforeSet": [
            "public class OrmElasticsearchLibraryShowcaseIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.showcase;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.orm.OrmUtils.withinSession;\nimport static org.hibernate.search.util.impl.integrationtest.orm.OrmUtils.withinTransaction;\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\nimport static org.junit.Assume.assumeTrue;\n\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Optional;\n\nimport org.hibernate.Session;\nimport org.hibernate.SessionFactory;\nimport org.hibernate.boot.Metadata;\nimport org.hibernate.boot.MetadataSources;\nimport org.hibernate.boot.SessionFactoryBuilder;\nimport org.hibernate.boot.registry.StandardServiceRegistryBuilder;\nimport org.hibernate.search.backend.elasticsearch.cfg.SearchBackendElasticsearchSettings;\nimport org.hibernate.search.backend.elasticsearch.impl.ElasticsearchBackendFactory;\nimport org.hibernate.search.integrationtest.showcase.library.analysis.LibraryAnalysisConfigurer;\nimport org.hibernate.search.mapper.orm.cfg.SearchOrmSettings;\nimport org.hibernate.search.integrationtest.showcase.library.bridge.AccountBorrowalSummaryBridge;\nimport org.hibernate.search.integrationtest.showcase.library.dao.DaoFactory;\nimport org.hibernate.search.integrationtest.showcase.library.dao.DocumentDao;\nimport org.hibernate.search.integrationtest.showcase.library.dao.LibraryDao;\nimport org.hibernate.search.integrationtest.showcase.library.dao.PersonDao;\nimport org.hibernate.search.integrationtest.showcase.library.dao.syntax.fluidandlambda.FluidAndLambdaSyntaxDaoFactory;\nimport org.hibernate.search.integrationtest.showcase.library.dao.syntax.fluidandobject.FluidAndObjectSyntaxDaoFactory;\nimport org.hibernate.search.integrationtest.showcase.library.dao.syntax.lambda.LambdaSyntaxDaoFactory;\nimport org.hibernate.search.integrationtest.showcase.library.dao.syntax.object.ObjectSyntaxDaoFactory;\nimport org.hibernate.search.integrationtest.showcase.library.model.Account;\nimport org.hibernate.search.integrationtest.showcase.library.model.Book;\nimport org.hibernate.search.integrationtest.showcase.library.model.BookCopy;\nimport org.hibernate.search.integrationtest.showcase.library.model.BookMedium;\nimport org.hibernate.search.integrationtest.showcase.library.model.Borrowal;\nimport org.hibernate.search.integrationtest.showcase.library.model.BorrowalType;\nimport org.hibernate.search.integrationtest.showcase.library.model.Document;\nimport org.hibernate.search.integrationtest.showcase.library.model.DocumentCopy;\nimport org.hibernate.search.integrationtest.showcase.library.model.ISBN;\nimport org.hibernate.search.integrationtest.showcase.library.model.Library;\nimport org.hibernate.search.integrationtest.showcase.library.model.LibraryService;\nimport org.hibernate.search.integrationtest.showcase.library.model.Person;\nimport org.hibernate.search.integrationtest.showcase.library.model.ProgrammaticMappingConfigurer;\nimport org.hibernate.search.integrationtest.showcase.library.model.Video;\nimport org.hibernate.search.integrationtest.showcase.library.model.VideoCopy;\nimport org.hibernate.search.integrationtest.showcase.library.model.VideoMedium;\nimport org.hibernate.search.engine.spatial.GeoPoint;\nimport org.hibernate.search.engine.spatial.ImmutableGeoPoint;\nimport org.hibernate.service.ServiceRegistry;\nimport org.hibernate.tool.schema.Action;\n\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.junit.runners.Parameterized;\n\n@RunWith(Parameterized.class)\npublic class OrmElasticsearchLibraryShowcaseIT {\n\n\tprivate enum MappingMode {\n\t\tANNOTATION_MAPPING,\n\t\tPROGRAMMATIC_MAPPING;\n\t}\n\n\t@Parameterized.Parameters(name = \"{0} - {1}\")\n\tpublic static Object[][] parameters() {\n\t\tMappingMode[] mappingModes = new MappingMode[] {\n\t\t\t\tMappingMode.ANNOTATION_MAPPING,\n\t\t\t\tMappingMode.PROGRAMMATIC_MAPPING\n\t\t};\n\t\tDaoFactory[] daoFactories = new DaoFactory[] {\n\t\t\t\tnew FluidAndLambdaSyntaxDaoFactory(),\n\t\t\t\tnew FluidAndObjectSyntaxDaoFactory(),\n\t\t\t\tnew LambdaSyntaxDaoFactory(),\n\t\t\t\tnew ObjectSyntaxDaoFactory()\n\t\t};\n\t\t// Compute the cross product\n\t\tObject[][] parameters = new Object[mappingModes.length * daoFactories.length][2];\n\t\tint i = 0;\n\t\tfor ( MappingMode mappingMode : mappingModes ) {\n\t\t\tfor ( DaoFactory daoFactory : daoFactories ) {\n\t\t\t\tparameters[i][0] = mappingMode;\n\t\t\t\tparameters[i][1] = daoFactory;\n\t\t\t\t++i;\n\t\t\t}\n\t\t}\n\t\treturn parameters;\n\t}\n\n\tprivate static final String PREFIX = SearchOrmSettings.PREFIX;\n\n\t// Document IDs\n\tprivate static final int CALLIGRAPHY_ID = 1;\n\tprivate static final int JAVA_DANCING_ID = 2;\n\tprivate static final int INDONESIAN_ECONOMY_ID = 3;\n\tprivate static final int JAVA_FOR_DUMMIES_ID = 4;\n\tprivate static final int ART_OF_COMPUTER_PROG_ID = 5;\n\tprivate static final int THESAURUS_OF_LANGUAGES_ID = 6;\n\tprivate static final int LIVING_ON_ISLAND_ID = 7;\n\n\t// Library IDs\n\tprivate static final int CITY_CENTER_ID = 1;\n\tprivate static final int SUBURBAN_1_ID = 2;\n\tprivate static final int SUBURBAN_2_ID = 3;\n\tprivate static final int UNIVERSITY_ID = 4;\n\n\t// Person IDs\n\tprivate static final int JANE_SMITH_ID = 1;\n\tprivate static final int JANE_FONDA_ID = 2;\n\tprivate static final int JANE_PORTER_ID = 3;\n\tprivate static final int JOHN_LENNON_ID = 4;\n\tprivate static final int ELTON_JOHN_ID = 5;\n\tprivate static final int PATTY_SMITH_ID = 6;\n\tprivate static final int JOHN_SMITH_ID = 7;\n\tprivate static final int JOHN_PAUL_SMITH_ID = 8;\n\tprivate static final int JOHN_PAUL_ID = 9;\n\tprivate static final int PAUL_JOHN_ID = 10;\n\n\tprivate final MappingMode mappingMode;\n\tprivate final DaoFactory daoFactory;\n\n\tprivate SessionFactory sessionFactory;\n\n\tpublic OrmElasticsearchLibraryShowcaseIT(MappingMode mappingMode, DaoFactory daoFactory) {\n\t\tthis.mappingMode = mappingMode;\n\t\tthis.daoFactory = daoFactory;\n\t}\n\n\t@Before\n\tpublic void setup() {\n\t\tStandardServiceRegistryBuilder registryBuilder = new StandardServiceRegistryBuilder()\n\t\t\t\t.applySetting( PREFIX + \"backend.elasticsearchBackend_1.type\", ElasticsearchBackendFactory.class.getName() )\n\t\t\t\t.applySetting( PREFIX + \"index.default.backend\", \"elasticsearchBackend_1\" )\n\t\t\t\t.applySetting( PREFIX + \"backend.elasticsearchBackend_1.log.json_pretty_printing\", true )\n\t\t\t\t.applySetting(\n\t\t\t\t\t\tPREFIX + \"backend.elasticsearchBackend_1.\" + SearchBackendElasticsearchSettings.ANALYSIS_CONFIGURER,\n\t\t\t\t\t\tnew LibraryAnalysisConfigurer()\n\t\t\t\t)\n\t\t\t\t.applySetting( org.hibernate.cfg.AvailableSettings.HBM2DDL_AUTO, Action.CREATE_DROP );\n\n\t\tif ( MappingMode.PROGRAMMATIC_MAPPING.equals( mappingMode ) ) {\n\t\t\tregistryBuilder.applySetting( SearchOrmSettings.ENABLE_ANNOTATION_MAPPING, \"false\" );\n\t\t\tregistryBuilder.applySetting( SearchOrmSettings.MAPPING_CONFIGURER,\n\t\t\t\t\tnew ProgrammaticMappingConfigurer() );\n\t\t}\n\n\t\tServiceRegistry serviceRegistry = registryBuilder.build();\n\n\t\tMetadataSources ms = new MetadataSources( serviceRegistry )\n\t\t\t\t.addAnnotatedClass( Document.class )\n\t\t\t\t.addAnnotatedClass( Book.class )\n\t\t\t\t.addAnnotatedClass( Video.class )\n\t\t\t\t.addAnnotatedClass( Library.class )\n\t\t\t\t.addAnnotatedClass( DocumentCopy.class )\n\t\t\t\t.addAnnotatedClass( BookCopy.class )\n\t\t\t\t.addAnnotatedClass( VideoCopy.class )\n\t\t\t\t.addAnnotatedClass( Person.class )\n\t\t\t\t.addAnnotatedClass( Account.class )\n\t\t\t\t.addAnnotatedClass( Borrowal.class );\n\n\t\tMetadata metadata = ms.buildMetadata();\n\n\t\tfinal SessionFactoryBuilder sfb = metadata.getSessionFactoryBuilder();\n\t\tthis.sessionFactory = sfb.build();\n\t}\n\n\t@After\n\tpublic void cleanup() {\n\t\tif ( sessionFactory != null ) {\n\t\t\tsessionFactory.close();\n\t\t}\n\t}\n\n\t@Test\n\tpublic void search_library() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tLibraryDao dao = daoFactory.createLibraryDao( session );\n\n\t\t\tList<Library> libraries = dao.search( \"library\", 0, 10 );\n\t\t\tassertThat( libraries ).containsExactly(\n\t\t\t\t\tsession.get( Library.class, CITY_CENTER_ID ),\n\t\t\t\t\tsession.get( Library.class, UNIVERSITY_ID ), // Bumped to this position because of its collection size\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_1_ID ),\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_2_ID )\n\t\t\t);\n\t\t\tlibraries = dao.search( \"library\", 1, 2 );\n\t\t\tassertThat( libraries ).containsExactly(\n\t\t\t\t\tsession.get( Library.class, UNIVERSITY_ID ),\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_1_ID )\n\t\t\t);\n\t\t\tlibraries = dao.search( \"sUburban\", 0, 10 );\n\t\t\tassertThat( libraries ).containsExactly(\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_1_ID ),\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_2_ID )\n\t\t\t);\n\t\t\t// TODO introduce an AND operator in the match query to make this match SUBURBAN_1_ID only\n\t\t\tlibraries = dao.search( \"Suburban 1\", 0, 10 );\n\t\t\tassertThat( libraries ).containsExactly(\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_1_ID ),\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_2_ID )\n\t\t\t);\n\t\t\tlibraries = dao.search( \"city center\", 0, 10 );\n\t\t\tassertThat( libraries ).containsExactly(\n\t\t\t\t\tsession.get( Library.class, CITY_CENTER_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void search_person() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tPersonDao dao = daoFactory.createPersonDao( sessionFactory.createEntityManager() );\n\n\t\t\tList<Person> results = dao.search(\n\t\t\t\t\t\"smith\", 0, 10\n\t\t\t);\n\t\t\tassertThat( results ).containsExactly(\n\t\t\t\t\tsession.get( Person.class, JANE_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_PAUL_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, PATTY_SMITH_ID )\n\t\t\t);\n\n\t\t\tresults = dao.search(\n\t\t\t\t\t\"john\", 0, 10\n\t\t\t);\n\t\t\tassertThat( results ).containsExactly(\n\t\t\t\t\tsession.get( Person.class, ELTON_JOHN_ID ),\n\t\t\t\t\tsession.get( Person.class, PAUL_JOHN_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_LENNON_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_PAUL_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_PAUL_SMITH_ID )\n\t\t\t);\n\n\t\t\t// TODO introduce an AND operator in the match query to make this match JOHN_SMITH_ID and JOHN_PAUL_SMITH_ID only\n\t\t\tresults = dao.search(\n\t\t\t\t\t\"john smith\", 0, 10\n\t\t\t);\n\t\t\tassertThat( results ).containsExactly(\n\t\t\t\t\tsession.get( Person.class, ELTON_JOHN_ID ),\n\t\t\t\t\tsession.get( Person.class, PAUL_JOHN_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_LENNON_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_PAUL_ID ),\n\t\t\t\t\tsession.get( Person.class, JANE_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_PAUL_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, PATTY_SMITH_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void search_single() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n\n\t\t\tOptional<Book> book = dao.getByIsbn( \"978-0-00-000001-1\" );\n\t\t\tassertTrue( book.isPresent() );\n\t\t\tassertThat( book.get() ).isEqualTo( session.get( Book.class, CALLIGRAPHY_ID ) );\n\n\t\t\tbook = dao.getByIsbn( \"978-0-00-000005-5\" );\n\t\t\tassertTrue( book.isPresent() );\n\t\t\tassertThat( book.get() ).isEqualTo( session.get( Book.class, ART_OF_COMPUTER_PROG_ID ) );\n\n\t\t\tbook = dao.getByIsbn( \"978-0-00-000005-1\" );\n\t\t\tassertFalse( book.isPresent() );\n\n\t\t\t// Test the normalizer\n\t\t\tbook = dao.getByIsbn( \"9780000000055\" );\n\t\t\tassertTrue( book.isPresent() );\n\t\t\tassertThat( book.get() ).isEqualTo( session.get( Book.class, ART_OF_COMPUTER_PROG_ID ) );\n\t\t} );\n\t}\n\n\t/**\n\t * This demonstrates generics are resolved properly, since the field \"medium\" doesn't appear in {@link DocumentCopy}\n\t * and could only exist in the index if the \"copies\" property in class {@link Book}\n\t * was successfully resolved to {@code List<BookCopy>}.\n\t */\n\t@Test\n\tpublic void searchByMedium() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n\n\t\t\tList<Book> books = dao.searchByMedium(\n\t\t\t\t\t\"java\", BookMedium.DEMATERIALIZED, 0, 10\n\t\t\t);\n\t\t\tassertThat( books ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID )\n\t\t\t);\n\n\t\t\tbooks = dao.searchByMedium(\n\t\t\t\t\t\"java\", BookMedium.HARDCOPY, 0, 10\n\t\t\t);\n\t\t\tassertThat( books ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void searchAroundMe_spatial() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n\n\t\t\tGeoPoint myLocation = new ImmutableGeoPoint( 42.0, 0.5 );\n\n\t\t\tList<Document<?>> documents = dao.searchAroundMe(\n\t\t\t\t\tnull, null,\n\t\t\t\t\tmyLocation, 20.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from university\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n\t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\tnull, null,\n\t\t\t\t\tmyLocation, 40.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from suburb1 or university\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, CALLIGRAPHY_ID ),\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n\t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\t\"calligraphy\", null,\n\t\t\t\t\tmyLocation, 40.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from suburb1 or university with \"calligraphy\" in it\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, CALLIGRAPHY_ID )\n\t\t\t);\n\n\t\t\tmyLocation = new ImmutableGeoPoint( 42.0, 0.75 );\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\tnull, null,\n\t\t\t\t\tmyLocation, 40.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from university\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n\t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void searchAroundMe_nested() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n\n\t\t\tList<Document<?>> documents = dao.searchAroundMe(\n\t\t\t\t\t\"java\", null,\n\t\t\t\t\tnull, null,\n\t\t\t\t\tCollections.singletonList( LibraryService.DISABLED_ACCESS ),\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\tassertThat( documents ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Video.class, JAVA_DANCING_ID ),\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\t\"java\", null,\n\t\t\t\t\tnull, null,\n\t\t\t\t\tCollections.singletonList( LibraryService.READING_ROOMS ),\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\tassertThat( documents ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Video.class, JAVA_DANCING_ID ),\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\t\"java\", null,\n\t\t\t\t\tnull, null,\n\t\t\t\t\tArrays.asList( LibraryService.DISABLED_ACCESS, LibraryService.READING_ROOMS ),\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t/*\n\t\t\t * In particular, should not match the document \"indonesianEconomy\",\n\t\t\t * which is present in a library with disabled access and in a library with reading rooms,\n\t\t\t * but not in a library with both.\n\t\t\t */\n\t\t\tassertThat( documents ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Video.class, JAVA_DANCING_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void searchAroundMe_searchBridge() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n\n\t\t\tList<Document<?>> documents = dao.searchAroundMe(\n\t\t\t\t\tnull, \"java\",\n\t\t\t\t\tnull, null,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\tassertThat( documents ).containsOnly(\n\t\t\t\t\tsession.get( Video.class, JAVA_DANCING_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Video.class, LIVING_ON_ISLAND_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\tnull, \"programming\",\n\t\t\t\t\tnull, null,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\tassertThat( documents ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\tnull, \"java,programming\",\n\t\t\t\t\tnull, null,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\tassertThat( documents ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t/**\n\t * This demonstrates how a non-trivial bridge ({@link AccountBorrowalSummaryBridge})\n\t * can be used to index data derived from the main model,\n\t * and how this indexed data can then be queried.\n\t */\n\t@Test\n\tpublic void listTopBorrowers() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tPersonDao dao = daoFactory.createPersonDao( session );\n\n\t\t\tList<Person> results = dao.listTopBorrowers( 0, 3 );\n\t\t\tassertThat( results ).containsExactly(\n\t\t\t\t\tsession.get( Person.class, JANE_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JANE_FONDA_ID ),\n\t\t\t\t\tsession.get( Person.class, JANE_PORTER_ID )\n\t\t\t);\n\n\t\t\tresults = dao.listTopShortTermBorrowers( 0, 3 );\n\t\t\tassertThat( results ).containsExactly(\n\t\t\t\t\tsession.get( Person.class, JANE_FONDA_ID ),\n\t\t\t\t\tsession.get( Person.class, JANE_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, PAUL_JOHN_ID )\n\t\t\t);\n\n\t\t\tresults = dao.listTopLongTermBorrowers( 0, 3 );\n\t\t\tassertThat( results ).containsExactly(\n\t\t\t\t\tsession.get( Person.class, JANE_PORTER_ID ),\n\t\t\t\t\tsession.get( Person.class, JANE_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_SMITH_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void aggregation() {\n\t\t// TODO aggregation\n\t\tassumeTrue( \"Aggregation not implemented yet\", false );\n\t}\n\n\tprivate void initData(Session session) {\n\t\tLibraryDao libraryDao = daoFactory.createLibraryDao( session );\n\t\tDocumentDao documentDao = daoFactory.createDocumentDao( session );\n\t\tPersonDao personDao = daoFactory.createPersonDao( session );\n\n\t\tBook calligraphy = documentDao.createBook(\n\t\t\t\tCALLIGRAPHY_ID,\n\t\t\t\tnew ISBN( \"978-0-00-000001-1\" ),\n\t\t\t\t\"Calligraphy for Dummies\",\n\t\t\t\t\"Learn to write artfully in ten lessons\",\n\t\t\t\t\"calligraphy,art\"\n\t\t);\n\n\t\tVideo javaDancing = documentDao.createVideo(\n\t\t\t\tJAVA_DANCING_ID,\n\t\t\t\t\"Java le dire \u00e0 tout le monde\",\n\t\t\t\t\"A brief history of Java dancing in Paris during the early 20th century\",\n\t\t\t\t\"java,dancing,history\"\n\t\t);\n\n\t\tBook indonesianEconomy = documentDao.createBook(\n\t\t\t\tINDONESIAN_ECONOMY_ID,\n\t\t\t\tnew ISBN( \"978-0-00-000003-3\" ),\n\t\t\t\t\"Comparative Study of the Economy of Java and other Indonesian Islands\",\n\t\t\t\t\"Comparative study of the late 20th century economy of the main islands of Indonesia\"\n\t\t\t\t\t\t+ \" with accurate projections over the next ten centuries\",\n\t\t\t\t\"geography,economy,java,sumatra,borneo,sulawesi\"\n\t\t);\n\n\t\tBook javaForDummies = documentDao.createBook(\n\t\t\t\tJAVA_FOR_DUMMIES_ID,\n\t\t\t\tnew ISBN( \"978-0-00-000004-4\" ),\n\t\t\t\t// Use varying case on purpose\n\t\t\t\t\"java for Dummies\",\n\t\t\t\t\"Learning the Java programming language in ten lessons\",\n\t\t\t\t\"programming,language,java\"\n\t\t);\n\n\t\tBook artOfComputerProg = documentDao.createBook(\n\t\t\t\tART_OF_COMPUTER_PROG_ID,\n\t\t\t\tnew ISBN( \"978-0-00-000005-5\" ),\n\t\t\t\t\"The Art of Computer Programming\",\n\t\t\t\t\"Quick review of basic computer programming principles in 965 chapters\",\n\t\t\t\t\"programming\"\n\t\t);\n\n\t\tBook thesaurusOfLanguages = documentDao.createBook(\n\t\t\t\tTHESAURUS_OF_LANGUAGES_ID,\n\t\t\t\tnew ISBN( \"978-0-00-000006-6\" ),\n\t\t\t\t\"Thesaurus of Indo-European Languages\",\n\t\t\t\t\"An entertaining list of about three thousand languages, most of which are long dead\",\n\t\t\t\t\"geography,language\"\n\t\t);\n\n\t\tVideo livingOnIsland = documentDao.createVideo(\n\t\t\t\tLIVING_ON_ISLAND_ID,\n\t\t\t\t\"Living in an Island, Episode 3: Indonesia\",\n\t\t\t\t\"A journey across Indonesia's smallest islands depicting how island way of life differs from mainland living\",\n\t\t\t\t\"geography,java,sumatra,borneo,sulawesi\"\n\t\t);\n\n\t\t// City center library\n\t\tLibrary cityCenterLibrary = libraryDao.create(\n\t\t\t\tCITY_CENTER_ID,\n\t\t\t\t\"City Center Library\",\n\t\t\t\t12400,\n\t\t\t\t42.0, 0.0,\n\t\t\t\tLibraryService.READING_ROOMS,\n\t\t\t\tLibraryService.HARDCOPY_LOAN\n\t\t);\n\t\t// Content: every document, but no dematerialized copy\n\t\tdocumentDao.createCopy( cityCenterLibrary, calligraphy, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( cityCenterLibrary, javaDancing, VideoMedium.DVD );\n\t\tdocumentDao.createCopy( cityCenterLibrary, indonesianEconomy, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( cityCenterLibrary, javaForDummies, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( cityCenterLibrary, artOfComputerProg, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( cityCenterLibrary, thesaurusOfLanguages, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( cityCenterLibrary, livingOnIsland, VideoMedium.BLURAY );\n\n\t\t// Suburban library 1\n\t\tLibrary suburbanLibrary1 = libraryDao.create(\n\t\t\t\tSUBURBAN_1_ID,\n\t\t\t\t// Use varying case on purpose\n\t\t\t\t\"suburban Library 1\",\n\t\t\t\t800,\n\t\t\t\t42.0, 0.25,\n\t\t\t\tLibraryService.DISABLED_ACCESS,\n\t\t\t\tLibraryService.HARDCOPY_LOAN\n\t\t);\n\t\t// Content: no video document\n\t\tdocumentDao.createCopy( suburbanLibrary1, calligraphy, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( suburbanLibrary1, indonesianEconomy, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( suburbanLibrary1, javaForDummies, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( suburbanLibrary1, artOfComputerProg, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( suburbanLibrary1, thesaurusOfLanguages, BookMedium.HARDCOPY );\n\n\t\t// Suburban library 2\n\t\tLibrary suburbanLibrary2 = libraryDao.create(\n\t\t\t\tSUBURBAN_2_ID,\n\t\t\t\t\"Suburban Library 2\",\n\t\t\t\t800, // Same as the other suburban library\n\t\t\t\t42.0, -0.25,\n\t\t\t\tLibraryService.DISABLED_ACCESS, LibraryService.READING_ROOMS,\n\t\t\t\tLibraryService.HARDCOPY_LOAN\n\t\t);\n\t\t// Content: no academic document, offers dematerialized copies\n\t\tdocumentDao.createCopy( suburbanLibrary2, calligraphy, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( suburbanLibrary2, calligraphy, BookMedium.DEMATERIALIZED );\n\t\tdocumentDao.createCopy( suburbanLibrary2, javaDancing, VideoMedium.DVD );\n\t\tdocumentDao.createCopy( suburbanLibrary2, javaDancing, VideoMedium.DEMATERIALIZED );\n\t\tdocumentDao.createCopy( suburbanLibrary2, javaForDummies, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( suburbanLibrary2, javaForDummies, BookMedium.DEMATERIALIZED );\n\t\tdocumentDao.createCopy( suburbanLibrary2, livingOnIsland, VideoMedium.BLURAY );\n\t\tdocumentDao.createCopy( suburbanLibrary2, livingOnIsland, VideoMedium.DEMATERIALIZED );\n\n\t\t// University library\n\t\tLibrary universityLibrary = libraryDao.create(\n\t\t\t\tUNIVERSITY_ID,\n\t\t\t\t\"University Library\",\n\t\t\t\t9000,\n\t\t\t\t42.0, 0.5,\n\t\t\t\tLibraryService.READING_ROOMS,\n\t\t\t\tLibraryService.HARDCOPY_LOAN, LibraryService.DEMATERIALIZED_LOAN\n\t\t);\n\t\t// Content: only academic and learning documents\n\t\tdocumentDao.createCopy( universityLibrary, indonesianEconomy, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( universityLibrary, javaForDummies, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( universityLibrary, artOfComputerProg, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( universityLibrary, thesaurusOfLanguages, BookMedium.HARDCOPY );\n\n\t\tPerson janeSmith = personDao.create( JANE_SMITH_ID, \"Jane\", \"Smith\" );\n\t\tpersonDao.createAccount( janeSmith );\n\t\tcreateBorrowal( personDao, janeSmith, cityCenterLibrary, indonesianEconomy, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, janeSmith, cityCenterLibrary, artOfComputerProg, BorrowalType.LONG_TERM );\n\t\tcreateBorrowal( personDao, janeSmith, cityCenterLibrary, javaForDummies, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, janeSmith, cityCenterLibrary, calligraphy, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, janeSmith, cityCenterLibrary, javaDancing, BorrowalType.LONG_TERM );\n\n\t\tPerson janeFonda = personDao.create( JANE_FONDA_ID, \"Jane\", \"Fonda\" );\n\t\tpersonDao.createAccount( janeFonda );\n\t\tcreateBorrowal( personDao, janeFonda, universityLibrary, javaForDummies, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, janeFonda, universityLibrary, artOfComputerProg, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, janeFonda, universityLibrary, thesaurusOfLanguages, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, janeFonda, universityLibrary, indonesianEconomy, BorrowalType.SHORT_TERM );\n\n\t\t// Use varying case on purpose\n\t\tPerson janePorter = personDao.create( JANE_PORTER_ID, \"Jane\", \"porter\" );\n\t\tpersonDao.createAccount( janePorter );\n\t\tcreateBorrowal( personDao, janePorter, suburbanLibrary1, indonesianEconomy, BorrowalType.LONG_TERM );\n\t\tcreateBorrowal( personDao, janePorter, suburbanLibrary2, livingOnIsland, 1, BorrowalType.LONG_TERM );\n\t\tcreateBorrowal( personDao, janePorter, universityLibrary, thesaurusOfLanguages, BorrowalType.LONG_TERM );\n\n\t\t// Use varying case on purpose\n\t\tPerson johnLennon = personDao.create( JOHN_LENNON_ID, \"john\", \"Lennon\" );\n\t\tpersonDao.createAccount( johnLennon );\n\t\tcreateBorrowal( personDao, johnLennon, suburbanLibrary2, javaDancing, 1, BorrowalType.SHORT_TERM );\n\n\t\t// Use varying case on purpose\n\t\tPerson eltonJohn = personDao.create( ELTON_JOHN_ID, \"elton\", \"john\" );\n\t\tpersonDao.createAccount( eltonJohn );\n\t\tcreateBorrowal( personDao, eltonJohn, suburbanLibrary2, javaDancing, 1, BorrowalType.SHORT_TERM );\n\n\t\tPerson pattySmith = personDao.create( PATTY_SMITH_ID, \"Patty\", \"Smith\" );\n\t\tpersonDao.createAccount( pattySmith );\n\t\tcreateBorrowal( personDao, pattySmith, suburbanLibrary2, javaDancing, 1, BorrowalType.SHORT_TERM );\n\n\t\tPerson johnSmith = personDao.create( JOHN_SMITH_ID, \"John\", \"Smith\" );\n\t\tpersonDao.createAccount( johnSmith );\n\t\tcreateBorrowal( personDao, johnSmith, suburbanLibrary1, indonesianEconomy, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, johnSmith, suburbanLibrary1, artOfComputerProg, BorrowalType.LONG_TERM );\n\n\t\tPerson johnPaulSmith = personDao.create( JOHN_PAUL_SMITH_ID, \"John Paul\", \"Smith\" );\n\t\t// No account for this one\n\n\t\tPerson johnPaul = personDao.create( JOHN_PAUL_ID, \"John\", \"Paul\" );\n\t\tpersonDao.createAccount( johnPaul );\n\t\t// This one has an account, but no borrowal\n\n\t\tPerson paulJohn = personDao.create( PAUL_JOHN_ID, \"Paul\", \"John\" );\n\t\tpersonDao.createAccount( paulJohn );\n\t\tcreateBorrowal( personDao, paulJohn, cityCenterLibrary, javaForDummies, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, paulJohn, cityCenterLibrary, artOfComputerProg, BorrowalType.SHORT_TERM );\n\t}\n\n\t// Helper methods\n\n\tprivate <D extends Document<C>, C extends DocumentCopy<D>> Borrowal createBorrowal(\n\t\t\tPersonDao personDao, Person person, Library library, D document, BorrowalType borrowalType) {\n\t\treturn createBorrowal( personDao, person, library, document, 0, borrowalType );\n\t}\n\n\tprivate <D extends Document<C>, C extends DocumentCopy<D>> Borrowal createBorrowal(\n\t\t\tPersonDao personDao, Person person, Library library, D document, int copyIndex, BorrowalType borrowalType) {\n\t\treturn personDao.createBorrowal(\n\t\t\t\tperson.getAccount(),\n\t\t\t\tgetCopy( library, document, copyIndex ),\n\t\t\t\tborrowalType\n\t\t);\n\t}\n\n\tprivate <D extends Document<C>, C extends DocumentCopy<D>> C getCopy(Library library, D document, int copyIndex) {\n\t\treturn document.getCopies().stream()\n\t\t\t\t.filter( c -> c.getLibrary().equals( library ) )\n\t\t\t\t.skip( copyIndex )\n\t\t\t\t.findFirst()\n\t\t\t\t.orElseThrow( () -> new IllegalStateException(\n\t\t\t\t\t\t\"The test setup is incorrect; could not find copy #\" + copyIndex\n\t\t\t\t\t\t\t\t+ \" of document \" + document\n\t\t\t\t\t\t\t\t+ \" for library \" + library\n\t\t\t\t) );\n\t}\n}\n",
        "filePathAfter": "integrationtest/showcase/library/src/test/java/org/hibernate/search/integrationtest/showcase/OrmElasticsearchLibraryShowcaseIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.showcase;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.orm.OrmUtils.withinSession;\nimport static org.hibernate.search.util.impl.integrationtest.orm.OrmUtils.withinTransaction;\nimport static org.junit.Assert.assertFalse;\nimport static org.junit.Assert.assertTrue;\nimport static org.junit.Assume.assumeTrue;\n\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Optional;\n\nimport org.hibernate.Session;\nimport org.hibernate.SessionFactory;\nimport org.hibernate.boot.Metadata;\nimport org.hibernate.boot.MetadataSources;\nimport org.hibernate.boot.SessionFactoryBuilder;\nimport org.hibernate.boot.registry.StandardServiceRegistryBuilder;\nimport org.hibernate.search.backend.elasticsearch.cfg.SearchBackendElasticsearchSettings;\nimport org.hibernate.search.backend.elasticsearch.impl.ElasticsearchBackendFactory;\nimport org.hibernate.search.integrationtest.showcase.library.analysis.LibraryAnalysisConfigurer;\nimport org.hibernate.search.mapper.orm.cfg.SearchOrmSettings;\nimport org.hibernate.search.integrationtest.showcase.library.bridge.AccountBorrowalSummaryBridge;\nimport org.hibernate.search.integrationtest.showcase.library.dao.DaoFactory;\nimport org.hibernate.search.integrationtest.showcase.library.dao.DocumentDao;\nimport org.hibernate.search.integrationtest.showcase.library.dao.LibraryDao;\nimport org.hibernate.search.integrationtest.showcase.library.dao.PersonDao;\nimport org.hibernate.search.integrationtest.showcase.library.dao.syntax.fluidandlambda.FluidAndLambdaSyntaxDaoFactory;\nimport org.hibernate.search.integrationtest.showcase.library.dao.syntax.fluidandobject.FluidAndObjectSyntaxDaoFactory;\nimport org.hibernate.search.integrationtest.showcase.library.dao.syntax.lambda.LambdaSyntaxDaoFactory;\nimport org.hibernate.search.integrationtest.showcase.library.dao.syntax.object.ObjectSyntaxDaoFactory;\nimport org.hibernate.search.integrationtest.showcase.library.model.Account;\nimport org.hibernate.search.integrationtest.showcase.library.model.Book;\nimport org.hibernate.search.integrationtest.showcase.library.model.BookCopy;\nimport org.hibernate.search.integrationtest.showcase.library.model.BookMedium;\nimport org.hibernate.search.integrationtest.showcase.library.model.Borrowal;\nimport org.hibernate.search.integrationtest.showcase.library.model.BorrowalType;\nimport org.hibernate.search.integrationtest.showcase.library.model.Document;\nimport org.hibernate.search.integrationtest.showcase.library.model.DocumentCopy;\nimport org.hibernate.search.integrationtest.showcase.library.model.ISBN;\nimport org.hibernate.search.integrationtest.showcase.library.model.Library;\nimport org.hibernate.search.integrationtest.showcase.library.model.LibraryService;\nimport org.hibernate.search.integrationtest.showcase.library.model.Person;\nimport org.hibernate.search.integrationtest.showcase.library.model.ProgrammaticMappingConfigurer;\nimport org.hibernate.search.integrationtest.showcase.library.model.Video;\nimport org.hibernate.search.integrationtest.showcase.library.model.VideoCopy;\nimport org.hibernate.search.integrationtest.showcase.library.model.VideoMedium;\nimport org.hibernate.search.engine.spatial.GeoPoint;\nimport org.hibernate.service.ServiceRegistry;\nimport org.hibernate.tool.schema.Action;\n\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.junit.runners.Parameterized;\n\n@RunWith(Parameterized.class)\npublic class OrmElasticsearchLibraryShowcaseIT {\n\n\tprivate enum MappingMode {\n\t\tANNOTATION_MAPPING,\n\t\tPROGRAMMATIC_MAPPING;\n\t}\n\n\t@Parameterized.Parameters(name = \"{0} - {1}\")\n\tpublic static Object[][] parameters() {\n\t\tMappingMode[] mappingModes = new MappingMode[] {\n\t\t\t\tMappingMode.ANNOTATION_MAPPING,\n\t\t\t\tMappingMode.PROGRAMMATIC_MAPPING\n\t\t};\n\t\tDaoFactory[] daoFactories = new DaoFactory[] {\n\t\t\t\tnew FluidAndLambdaSyntaxDaoFactory(),\n\t\t\t\tnew FluidAndObjectSyntaxDaoFactory(),\n\t\t\t\tnew LambdaSyntaxDaoFactory(),\n\t\t\t\tnew ObjectSyntaxDaoFactory()\n\t\t};\n\t\t// Compute the cross product\n\t\tObject[][] parameters = new Object[mappingModes.length * daoFactories.length][2];\n\t\tint i = 0;\n\t\tfor ( MappingMode mappingMode : mappingModes ) {\n\t\t\tfor ( DaoFactory daoFactory : daoFactories ) {\n\t\t\t\tparameters[i][0] = mappingMode;\n\t\t\t\tparameters[i][1] = daoFactory;\n\t\t\t\t++i;\n\t\t\t}\n\t\t}\n\t\treturn parameters;\n\t}\n\n\tprivate static final String PREFIX = SearchOrmSettings.PREFIX;\n\n\t// Document IDs\n\tprivate static final int CALLIGRAPHY_ID = 1;\n\tprivate static final int JAVA_DANCING_ID = 2;\n\tprivate static final int INDONESIAN_ECONOMY_ID = 3;\n\tprivate static final int JAVA_FOR_DUMMIES_ID = 4;\n\tprivate static final int ART_OF_COMPUTER_PROG_ID = 5;\n\tprivate static final int THESAURUS_OF_LANGUAGES_ID = 6;\n\tprivate static final int LIVING_ON_ISLAND_ID = 7;\n\n\t// Library IDs\n\tprivate static final int CITY_CENTER_ID = 1;\n\tprivate static final int SUBURBAN_1_ID = 2;\n\tprivate static final int SUBURBAN_2_ID = 3;\n\tprivate static final int UNIVERSITY_ID = 4;\n\n\t// Person IDs\n\tprivate static final int JANE_SMITH_ID = 1;\n\tprivate static final int JANE_FONDA_ID = 2;\n\tprivate static final int JANE_PORTER_ID = 3;\n\tprivate static final int JOHN_LENNON_ID = 4;\n\tprivate static final int ELTON_JOHN_ID = 5;\n\tprivate static final int PATTY_SMITH_ID = 6;\n\tprivate static final int JOHN_SMITH_ID = 7;\n\tprivate static final int JOHN_PAUL_SMITH_ID = 8;\n\tprivate static final int JOHN_PAUL_ID = 9;\n\tprivate static final int PAUL_JOHN_ID = 10;\n\n\tprivate final MappingMode mappingMode;\n\tprivate final DaoFactory daoFactory;\n\n\tprivate SessionFactory sessionFactory;\n\n\tpublic OrmElasticsearchLibraryShowcaseIT(MappingMode mappingMode, DaoFactory daoFactory) {\n\t\tthis.mappingMode = mappingMode;\n\t\tthis.daoFactory = daoFactory;\n\t}\n\n\t@Before\n\tpublic void setup() {\n\t\tStandardServiceRegistryBuilder registryBuilder = new StandardServiceRegistryBuilder()\n\t\t\t\t.applySetting( PREFIX + \"backend.elasticsearchBackend_1.type\", ElasticsearchBackendFactory.class.getName() )\n\t\t\t\t.applySetting( PREFIX + \"index.default.backend\", \"elasticsearchBackend_1\" )\n\t\t\t\t.applySetting( PREFIX + \"backend.elasticsearchBackend_1.log.json_pretty_printing\", true )\n\t\t\t\t.applySetting(\n\t\t\t\t\t\tPREFIX + \"backend.elasticsearchBackend_1.\" + SearchBackendElasticsearchSettings.ANALYSIS_CONFIGURER,\n\t\t\t\t\t\tnew LibraryAnalysisConfigurer()\n\t\t\t\t)\n\t\t\t\t.applySetting( org.hibernate.cfg.AvailableSettings.HBM2DDL_AUTO, Action.CREATE_DROP );\n\n\t\tif ( MappingMode.PROGRAMMATIC_MAPPING.equals( mappingMode ) ) {\n\t\t\tregistryBuilder.applySetting( SearchOrmSettings.ENABLE_ANNOTATION_MAPPING, \"false\" );\n\t\t\tregistryBuilder.applySetting( SearchOrmSettings.MAPPING_CONFIGURER,\n\t\t\t\t\tnew ProgrammaticMappingConfigurer() );\n\t\t}\n\n\t\tServiceRegistry serviceRegistry = registryBuilder.build();\n\n\t\tMetadataSources ms = new MetadataSources( serviceRegistry )\n\t\t\t\t.addAnnotatedClass( Document.class )\n\t\t\t\t.addAnnotatedClass( Book.class )\n\t\t\t\t.addAnnotatedClass( Video.class )\n\t\t\t\t.addAnnotatedClass( Library.class )\n\t\t\t\t.addAnnotatedClass( DocumentCopy.class )\n\t\t\t\t.addAnnotatedClass( BookCopy.class )\n\t\t\t\t.addAnnotatedClass( VideoCopy.class )\n\t\t\t\t.addAnnotatedClass( Person.class )\n\t\t\t\t.addAnnotatedClass( Account.class )\n\t\t\t\t.addAnnotatedClass( Borrowal.class );\n\n\t\tMetadata metadata = ms.buildMetadata();\n\n\t\tfinal SessionFactoryBuilder sfb = metadata.getSessionFactoryBuilder();\n\t\tthis.sessionFactory = sfb.build();\n\t}\n\n\t@After\n\tpublic void cleanup() {\n\t\tif ( sessionFactory != null ) {\n\t\t\tsessionFactory.close();\n\t\t}\n\t}\n\n\t@Test\n\tpublic void search_library() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tLibraryDao dao = daoFactory.createLibraryDao( session );\n\n\t\t\tList<Library> libraries = dao.search( \"library\", 0, 10 );\n\t\t\tassertThat( libraries ).containsExactly(\n\t\t\t\t\tsession.get( Library.class, CITY_CENTER_ID ),\n\t\t\t\t\tsession.get( Library.class, UNIVERSITY_ID ), // Bumped to this position because of its collection size\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_1_ID ),\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_2_ID )\n\t\t\t);\n\t\t\tlibraries = dao.search( \"library\", 1, 2 );\n\t\t\tassertThat( libraries ).containsExactly(\n\t\t\t\t\tsession.get( Library.class, UNIVERSITY_ID ),\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_1_ID )\n\t\t\t);\n\t\t\tlibraries = dao.search( \"sUburban\", 0, 10 );\n\t\t\tassertThat( libraries ).containsExactly(\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_1_ID ),\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_2_ID )\n\t\t\t);\n\t\t\t// TODO introduce an AND operator in the match query to make this match SUBURBAN_1_ID only\n\t\t\tlibraries = dao.search( \"Suburban 1\", 0, 10 );\n\t\t\tassertThat( libraries ).containsExactly(\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_1_ID ),\n\t\t\t\t\tsession.get( Library.class, SUBURBAN_2_ID )\n\t\t\t);\n\t\t\tlibraries = dao.search( \"city center\", 0, 10 );\n\t\t\tassertThat( libraries ).containsExactly(\n\t\t\t\t\tsession.get( Library.class, CITY_CENTER_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void search_person() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tPersonDao dao = daoFactory.createPersonDao( sessionFactory.createEntityManager() );\n\n\t\t\tList<Person> results = dao.search(\n\t\t\t\t\t\"smith\", 0, 10\n\t\t\t);\n\t\t\tassertThat( results ).containsExactly(\n\t\t\t\t\tsession.get( Person.class, JANE_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_PAUL_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, PATTY_SMITH_ID )\n\t\t\t);\n\n\t\t\tresults = dao.search(\n\t\t\t\t\t\"john\", 0, 10\n\t\t\t);\n\t\t\tassertThat( results ).containsExactly(\n\t\t\t\t\tsession.get( Person.class, ELTON_JOHN_ID ),\n\t\t\t\t\tsession.get( Person.class, PAUL_JOHN_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_LENNON_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_PAUL_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_PAUL_SMITH_ID )\n\t\t\t);\n\n\t\t\t// TODO introduce an AND operator in the match query to make this match JOHN_SMITH_ID and JOHN_PAUL_SMITH_ID only\n\t\t\tresults = dao.search(\n\t\t\t\t\t\"john smith\", 0, 10\n\t\t\t);\n\t\t\tassertThat( results ).containsExactly(\n\t\t\t\t\tsession.get( Person.class, ELTON_JOHN_ID ),\n\t\t\t\t\tsession.get( Person.class, PAUL_JOHN_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_LENNON_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_PAUL_ID ),\n\t\t\t\t\tsession.get( Person.class, JANE_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_PAUL_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, PATTY_SMITH_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void search_single() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n\n\t\t\tOptional<Book> book = dao.getByIsbn( \"978-0-00-000001-1\" );\n\t\t\tassertTrue( book.isPresent() );\n\t\t\tassertThat( book.get() ).isEqualTo( session.get( Book.class, CALLIGRAPHY_ID ) );\n\n\t\t\tbook = dao.getByIsbn( \"978-0-00-000005-5\" );\n\t\t\tassertTrue( book.isPresent() );\n\t\t\tassertThat( book.get() ).isEqualTo( session.get( Book.class, ART_OF_COMPUTER_PROG_ID ) );\n\n\t\t\tbook = dao.getByIsbn( \"978-0-00-000005-1\" );\n\t\t\tassertFalse( book.isPresent() );\n\n\t\t\t// Test the normalizer\n\t\t\tbook = dao.getByIsbn( \"9780000000055\" );\n\t\t\tassertTrue( book.isPresent() );\n\t\t\tassertThat( book.get() ).isEqualTo( session.get( Book.class, ART_OF_COMPUTER_PROG_ID ) );\n\t\t} );\n\t}\n\n\t/**\n\t * This demonstrates generics are resolved properly, since the field \"medium\" doesn't appear in {@link DocumentCopy}\n\t * and could only exist in the index if the \"copies\" property in class {@link Book}\n\t * was successfully resolved to {@code List<BookCopy>}.\n\t */\n\t@Test\n\tpublic void searchByMedium() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n\n\t\t\tList<Book> books = dao.searchByMedium(\n\t\t\t\t\t\"java\", BookMedium.DEMATERIALIZED, 0, 10\n\t\t\t);\n\t\t\tassertThat( books ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID )\n\t\t\t);\n\n\t\t\tbooks = dao.searchByMedium(\n\t\t\t\t\t\"java\", BookMedium.HARDCOPY, 0, 10\n\t\t\t);\n\t\t\tassertThat( books ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void searchAroundMe_spatial() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n\n\t\t\tGeoPoint myLocation = GeoPoint.of( 42.0, 0.5 );\n\n\t\t\tList<Document<?>> documents = dao.searchAroundMe(\n\t\t\t\t\tnull, null,\n\t\t\t\t\tmyLocation, 20.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from university\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n\t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\tnull, null,\n\t\t\t\t\tmyLocation, 40.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from suburb1 or university\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, CALLIGRAPHY_ID ),\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n\t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\t\"calligraphy\", null,\n\t\t\t\t\tmyLocation, 40.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from suburb1 or university with \"calligraphy\" in it\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, CALLIGRAPHY_ID )\n\t\t\t);\n\n\t\t\tmyLocation = GeoPoint.of( 42.0, 0.75 );\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\tnull, null,\n\t\t\t\t\tmyLocation, 40.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from university\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n\t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void searchAroundMe_nested() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n\n\t\t\tList<Document<?>> documents = dao.searchAroundMe(\n\t\t\t\t\t\"java\", null,\n\t\t\t\t\tnull, null,\n\t\t\t\t\tCollections.singletonList( LibraryService.DISABLED_ACCESS ),\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\tassertThat( documents ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Video.class, JAVA_DANCING_ID ),\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\t\"java\", null,\n\t\t\t\t\tnull, null,\n\t\t\t\t\tCollections.singletonList( LibraryService.READING_ROOMS ),\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\tassertThat( documents ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Video.class, JAVA_DANCING_ID ),\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\t\"java\", null,\n\t\t\t\t\tnull, null,\n\t\t\t\t\tArrays.asList( LibraryService.DISABLED_ACCESS, LibraryService.READING_ROOMS ),\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t/*\n\t\t\t * In particular, should not match the document \"indonesianEconomy\",\n\t\t\t * which is present in a library with disabled access and in a library with reading rooms,\n\t\t\t * but not in a library with both.\n\t\t\t */\n\t\t\tassertThat( documents ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Video.class, JAVA_DANCING_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void searchAroundMe_searchBridge() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n\n\t\t\tList<Document<?>> documents = dao.searchAroundMe(\n\t\t\t\t\tnull, \"java\",\n\t\t\t\t\tnull, null,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\tassertThat( documents ).containsOnly(\n\t\t\t\t\tsession.get( Video.class, JAVA_DANCING_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Video.class, LIVING_ON_ISLAND_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\tnull, \"programming\",\n\t\t\t\t\tnull, null,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\tassertThat( documents ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\tnull, \"java,programming\",\n\t\t\t\t\tnull, null,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\tassertThat( documents ).containsOnly(\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t/**\n\t * This demonstrates how a non-trivial bridge ({@link AccountBorrowalSummaryBridge})\n\t * can be used to index data derived from the main model,\n\t * and how this indexed data can then be queried.\n\t */\n\t@Test\n\tpublic void listTopBorrowers() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tPersonDao dao = daoFactory.createPersonDao( session );\n\n\t\t\tList<Person> results = dao.listTopBorrowers( 0, 3 );\n\t\t\tassertThat( results ).containsExactly(\n\t\t\t\t\tsession.get( Person.class, JANE_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JANE_FONDA_ID ),\n\t\t\t\t\tsession.get( Person.class, JANE_PORTER_ID )\n\t\t\t);\n\n\t\t\tresults = dao.listTopShortTermBorrowers( 0, 3 );\n\t\t\tassertThat( results ).containsExactly(\n\t\t\t\t\tsession.get( Person.class, JANE_FONDA_ID ),\n\t\t\t\t\tsession.get( Person.class, JANE_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, PAUL_JOHN_ID )\n\t\t\t);\n\n\t\t\tresults = dao.listTopLongTermBorrowers( 0, 3 );\n\t\t\tassertThat( results ).containsExactly(\n\t\t\t\t\tsession.get( Person.class, JANE_PORTER_ID ),\n\t\t\t\t\tsession.get( Person.class, JANE_SMITH_ID ),\n\t\t\t\t\tsession.get( Person.class, JOHN_SMITH_ID )\n\t\t\t);\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void aggregation() {\n\t\t// TODO aggregation\n\t\tassumeTrue( \"Aggregation not implemented yet\", false );\n\t}\n\n\tprivate void initData(Session session) {\n\t\tLibraryDao libraryDao = daoFactory.createLibraryDao( session );\n\t\tDocumentDao documentDao = daoFactory.createDocumentDao( session );\n\t\tPersonDao personDao = daoFactory.createPersonDao( session );\n\n\t\tBook calligraphy = documentDao.createBook(\n\t\t\t\tCALLIGRAPHY_ID,\n\t\t\t\tnew ISBN( \"978-0-00-000001-1\" ),\n\t\t\t\t\"Calligraphy for Dummies\",\n\t\t\t\t\"Learn to write artfully in ten lessons\",\n\t\t\t\t\"calligraphy,art\"\n\t\t);\n\n\t\tVideo javaDancing = documentDao.createVideo(\n\t\t\t\tJAVA_DANCING_ID,\n\t\t\t\t\"Java le dire \u00e0 tout le monde\",\n\t\t\t\t\"A brief history of Java dancing in Paris during the early 20th century\",\n\t\t\t\t\"java,dancing,history\"\n\t\t);\n\n\t\tBook indonesianEconomy = documentDao.createBook(\n\t\t\t\tINDONESIAN_ECONOMY_ID,\n\t\t\t\tnew ISBN( \"978-0-00-000003-3\" ),\n\t\t\t\t\"Comparative Study of the Economy of Java and other Indonesian Islands\",\n\t\t\t\t\"Comparative study of the late 20th century economy of the main islands of Indonesia\"\n\t\t\t\t\t\t+ \" with accurate projections over the next ten centuries\",\n\t\t\t\t\"geography,economy,java,sumatra,borneo,sulawesi\"\n\t\t);\n\n\t\tBook javaForDummies = documentDao.createBook(\n\t\t\t\tJAVA_FOR_DUMMIES_ID,\n\t\t\t\tnew ISBN( \"978-0-00-000004-4\" ),\n\t\t\t\t// Use varying case on purpose\n\t\t\t\t\"java for Dummies\",\n\t\t\t\t\"Learning the Java programming language in ten lessons\",\n\t\t\t\t\"programming,language,java\"\n\t\t);\n\n\t\tBook artOfComputerProg = documentDao.createBook(\n\t\t\t\tART_OF_COMPUTER_PROG_ID,\n\t\t\t\tnew ISBN( \"978-0-00-000005-5\" ),\n\t\t\t\t\"The Art of Computer Programming\",\n\t\t\t\t\"Quick review of basic computer programming principles in 965 chapters\",\n\t\t\t\t\"programming\"\n\t\t);\n\n\t\tBook thesaurusOfLanguages = documentDao.createBook(\n\t\t\t\tTHESAURUS_OF_LANGUAGES_ID,\n\t\t\t\tnew ISBN( \"978-0-00-000006-6\" ),\n\t\t\t\t\"Thesaurus of Indo-European Languages\",\n\t\t\t\t\"An entertaining list of about three thousand languages, most of which are long dead\",\n\t\t\t\t\"geography,language\"\n\t\t);\n\n\t\tVideo livingOnIsland = documentDao.createVideo(\n\t\t\t\tLIVING_ON_ISLAND_ID,\n\t\t\t\t\"Living in an Island, Episode 3: Indonesia\",\n\t\t\t\t\"A journey across Indonesia's smallest islands depicting how island way of life differs from mainland living\",\n\t\t\t\t\"geography,java,sumatra,borneo,sulawesi\"\n\t\t);\n\n\t\t// City center library\n\t\tLibrary cityCenterLibrary = libraryDao.create(\n\t\t\t\tCITY_CENTER_ID,\n\t\t\t\t\"City Center Library\",\n\t\t\t\t12400,\n\t\t\t\t42.0, 0.0,\n\t\t\t\tLibraryService.READING_ROOMS,\n\t\t\t\tLibraryService.HARDCOPY_LOAN\n\t\t);\n\t\t// Content: every document, but no dematerialized copy\n\t\tdocumentDao.createCopy( cityCenterLibrary, calligraphy, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( cityCenterLibrary, javaDancing, VideoMedium.DVD );\n\t\tdocumentDao.createCopy( cityCenterLibrary, indonesianEconomy, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( cityCenterLibrary, javaForDummies, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( cityCenterLibrary, artOfComputerProg, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( cityCenterLibrary, thesaurusOfLanguages, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( cityCenterLibrary, livingOnIsland, VideoMedium.BLURAY );\n\n\t\t// Suburban library 1\n\t\tLibrary suburbanLibrary1 = libraryDao.create(\n\t\t\t\tSUBURBAN_1_ID,\n\t\t\t\t// Use varying case on purpose\n\t\t\t\t\"suburban Library 1\",\n\t\t\t\t800,\n\t\t\t\t42.0, 0.25,\n\t\t\t\tLibraryService.DISABLED_ACCESS,\n\t\t\t\tLibraryService.HARDCOPY_LOAN\n\t\t);\n\t\t// Content: no video document\n\t\tdocumentDao.createCopy( suburbanLibrary1, calligraphy, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( suburbanLibrary1, indonesianEconomy, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( suburbanLibrary1, javaForDummies, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( suburbanLibrary1, artOfComputerProg, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( suburbanLibrary1, thesaurusOfLanguages, BookMedium.HARDCOPY );\n\n\t\t// Suburban library 2\n\t\tLibrary suburbanLibrary2 = libraryDao.create(\n\t\t\t\tSUBURBAN_2_ID,\n\t\t\t\t\"Suburban Library 2\",\n\t\t\t\t800, // Same as the other suburban library\n\t\t\t\t42.0, -0.25,\n\t\t\t\tLibraryService.DISABLED_ACCESS, LibraryService.READING_ROOMS,\n\t\t\t\tLibraryService.HARDCOPY_LOAN\n\t\t);\n\t\t// Content: no academic document, offers dematerialized copies\n\t\tdocumentDao.createCopy( suburbanLibrary2, calligraphy, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( suburbanLibrary2, calligraphy, BookMedium.DEMATERIALIZED );\n\t\tdocumentDao.createCopy( suburbanLibrary2, javaDancing, VideoMedium.DVD );\n\t\tdocumentDao.createCopy( suburbanLibrary2, javaDancing, VideoMedium.DEMATERIALIZED );\n\t\tdocumentDao.createCopy( suburbanLibrary2, javaForDummies, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( suburbanLibrary2, javaForDummies, BookMedium.DEMATERIALIZED );\n\t\tdocumentDao.createCopy( suburbanLibrary2, livingOnIsland, VideoMedium.BLURAY );\n\t\tdocumentDao.createCopy( suburbanLibrary2, livingOnIsland, VideoMedium.DEMATERIALIZED );\n\n\t\t// University library\n\t\tLibrary universityLibrary = libraryDao.create(\n\t\t\t\tUNIVERSITY_ID,\n\t\t\t\t\"University Library\",\n\t\t\t\t9000,\n\t\t\t\t42.0, 0.5,\n\t\t\t\tLibraryService.READING_ROOMS,\n\t\t\t\tLibraryService.HARDCOPY_LOAN, LibraryService.DEMATERIALIZED_LOAN\n\t\t);\n\t\t// Content: only academic and learning documents\n\t\tdocumentDao.createCopy( universityLibrary, indonesianEconomy, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( universityLibrary, javaForDummies, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( universityLibrary, artOfComputerProg, BookMedium.HARDCOPY );\n\t\tdocumentDao.createCopy( universityLibrary, thesaurusOfLanguages, BookMedium.HARDCOPY );\n\n\t\tPerson janeSmith = personDao.create( JANE_SMITH_ID, \"Jane\", \"Smith\" );\n\t\tpersonDao.createAccount( janeSmith );\n\t\tcreateBorrowal( personDao, janeSmith, cityCenterLibrary, indonesianEconomy, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, janeSmith, cityCenterLibrary, artOfComputerProg, BorrowalType.LONG_TERM );\n\t\tcreateBorrowal( personDao, janeSmith, cityCenterLibrary, javaForDummies, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, janeSmith, cityCenterLibrary, calligraphy, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, janeSmith, cityCenterLibrary, javaDancing, BorrowalType.LONG_TERM );\n\n\t\tPerson janeFonda = personDao.create( JANE_FONDA_ID, \"Jane\", \"Fonda\" );\n\t\tpersonDao.createAccount( janeFonda );\n\t\tcreateBorrowal( personDao, janeFonda, universityLibrary, javaForDummies, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, janeFonda, universityLibrary, artOfComputerProg, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, janeFonda, universityLibrary, thesaurusOfLanguages, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, janeFonda, universityLibrary, indonesianEconomy, BorrowalType.SHORT_TERM );\n\n\t\t// Use varying case on purpose\n\t\tPerson janePorter = personDao.create( JANE_PORTER_ID, \"Jane\", \"porter\" );\n\t\tpersonDao.createAccount( janePorter );\n\t\tcreateBorrowal( personDao, janePorter, suburbanLibrary1, indonesianEconomy, BorrowalType.LONG_TERM );\n\t\tcreateBorrowal( personDao, janePorter, suburbanLibrary2, livingOnIsland, 1, BorrowalType.LONG_TERM );\n\t\tcreateBorrowal( personDao, janePorter, universityLibrary, thesaurusOfLanguages, BorrowalType.LONG_TERM );\n\n\t\t// Use varying case on purpose\n\t\tPerson johnLennon = personDao.create( JOHN_LENNON_ID, \"john\", \"Lennon\" );\n\t\tpersonDao.createAccount( johnLennon );\n\t\tcreateBorrowal( personDao, johnLennon, suburbanLibrary2, javaDancing, 1, BorrowalType.SHORT_TERM );\n\n\t\t// Use varying case on purpose\n\t\tPerson eltonJohn = personDao.create( ELTON_JOHN_ID, \"elton\", \"john\" );\n\t\tpersonDao.createAccount( eltonJohn );\n\t\tcreateBorrowal( personDao, eltonJohn, suburbanLibrary2, javaDancing, 1, BorrowalType.SHORT_TERM );\n\n\t\tPerson pattySmith = personDao.create( PATTY_SMITH_ID, \"Patty\", \"Smith\" );\n\t\tpersonDao.createAccount( pattySmith );\n\t\tcreateBorrowal( personDao, pattySmith, suburbanLibrary2, javaDancing, 1, BorrowalType.SHORT_TERM );\n\n\t\tPerson johnSmith = personDao.create( JOHN_SMITH_ID, \"John\", \"Smith\" );\n\t\tpersonDao.createAccount( johnSmith );\n\t\tcreateBorrowal( personDao, johnSmith, suburbanLibrary1, indonesianEconomy, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, johnSmith, suburbanLibrary1, artOfComputerProg, BorrowalType.LONG_TERM );\n\n\t\tPerson johnPaulSmith = personDao.create( JOHN_PAUL_SMITH_ID, \"John Paul\", \"Smith\" );\n\t\t// No account for this one\n\n\t\tPerson johnPaul = personDao.create( JOHN_PAUL_ID, \"John\", \"Paul\" );\n\t\tpersonDao.createAccount( johnPaul );\n\t\t// This one has an account, but no borrowal\n\n\t\tPerson paulJohn = personDao.create( PAUL_JOHN_ID, \"Paul\", \"John\" );\n\t\tpersonDao.createAccount( paulJohn );\n\t\tcreateBorrowal( personDao, paulJohn, cityCenterLibrary, javaForDummies, BorrowalType.SHORT_TERM );\n\t\tcreateBorrowal( personDao, paulJohn, cityCenterLibrary, artOfComputerProg, BorrowalType.SHORT_TERM );\n\t}\n\n\t// Helper methods\n\n\tprivate <D extends Document<C>, C extends DocumentCopy<D>> Borrowal createBorrowal(\n\t\t\tPersonDao personDao, Person person, Library library, D document, BorrowalType borrowalType) {\n\t\treturn createBorrowal( personDao, person, library, document, 0, borrowalType );\n\t}\n\n\tprivate <D extends Document<C>, C extends DocumentCopy<D>> Borrowal createBorrowal(\n\t\t\tPersonDao personDao, Person person, Library library, D document, int copyIndex, BorrowalType borrowalType) {\n\t\treturn personDao.createBorrowal(\n\t\t\t\tperson.getAccount(),\n\t\t\t\tgetCopy( library, document, copyIndex ),\n\t\t\t\tborrowalType\n\t\t);\n\t}\n\n\tprivate <D extends Document<C>, C extends DocumentCopy<D>> C getCopy(Library library, D document, int copyIndex) {\n\t\treturn document.getCopies().stream()\n\t\t\t\t.filter( c -> c.getLibrary().equals( library ) )\n\t\t\t\t.skip( copyIndex )\n\t\t\t\t.findFirst()\n\t\t\t\t.orElseThrow( () -> new IllegalStateException(\n\t\t\t\t\t\t\"The test setup is incorrect; could not find copy #\" + copyIndex\n\t\t\t\t\t\t\t\t+ \" of document \" + document\n\t\t\t\t\t\t\t\t+ \" for library \" + library\n\t\t\t\t) );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "import org.hibernate.boot.Metadata;\nimport org.hibernate.boot.MetadataSources;\nimport org.hibernate.boot.SessionFactoryBuilder;\nimport org.hibernate.boot.registry.StandardServiceRegistryBuilder;\nimport org.hibernate.search.backend.elasticsearch.cfg.SearchBackendElasticsearchSettings;\nimport org.hibernate.search.backend.elasticsearch.impl.ElasticsearchBackendFactory;\nimport org.hibernate.search.integrationtest.showcase.library.analysis.LibraryAnalysisConfigurer;\nimport org.hibernate.search.mapper.orm.cfg.SearchOrmSettings;\nimport org.hibernate.search.integrationtest.showcase.library.bridge.AccountBorrowalSummaryBridge;\nimport org.hibernate.search.integrationtest.showcase.library.dao.DaoFactory;"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.backend.tck.search.projection.SearchProjectionIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(DOCUMENT_1),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.supportedFieldWithProjectionConverterModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.string1Field.document1Value.write(document);\n  indexMapping.string2Field.document1Value.write(document);\n  DocumentElement flattenedObject=indexMapping.flattenedObject.self.add(document);\n  indexMapping.flattenedObject.supportedFieldModels.forEach(f -> f.document1Value.write(flattenedObject));\n  DocumentElement nestedObject=indexMapping.nestedObject.self.add(document);\n  indexMapping.nestedObject.supportedFieldModels.forEach(f -> f.document1Value.write(nestedObject));\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_2),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.supportedFieldWithProjectionConverterModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.string1Field.document2Value.write(document);\n  indexMapping.string2Field.document2Value.write(document);\n  DocumentElement flattenedObject=indexMapping.flattenedObject.self.add(document);\n  indexMapping.flattenedObject.supportedFieldModels.forEach(f -> f.document2Value.write(flattenedObject));\n  DocumentElement nestedObject=indexMapping.nestedObject.self.add(document);\n  indexMapping.nestedObject.supportedFieldModels.forEach(f -> f.document2Value.write(nestedObject));\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_3),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.supportedFieldWithProjectionConverterModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.string1Field.document3Value.write(document);\n  indexMapping.string2Field.document3Value.write(document);\n  DocumentElement flattenedObject=indexMapping.flattenedObject.self.add(document);\n  indexMapping.flattenedObject.supportedFieldModels.forEach(f -> f.document3Value.write(flattenedObject));\n  DocumentElement nestedObject=indexMapping.nestedObject.self.add(document);\n  indexMapping.nestedObject.supportedFieldModels.forEach(f -> f.document3Value.write(nestedObject));\n}\n);\nworkPlan.add(referenceProvider(EMPTY),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nDocumentReferencesSearchResultAssert.assertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,DOCUMENT_1,DOCUMENT_2,DOCUMENT_3,EMPTY);\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.tck.search.predicate.RangeSearchPredicateIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(DOCUMENT_1),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.string1Field.document1Value.write(document);\n  indexMapping.string2Field.document1Value.write(document);\n  indexMapping.string3Field.document1Value.write(document);\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_2),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.string1Field.document2Value.write(document);\n  indexMapping.string2Field.document2Value.write(document);\n  indexMapping.string3Field.document2Value.write(document);\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_3),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.string1Field.document3Value.write(document);\n  indexMapping.string2Field.document3Value.write(document);\n  indexMapping.string3Field.document3Value.write(document);\n}\n);\nworkPlan.add(referenceProvider(EMPTY_ID),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,DOCUMENT_1,DOCUMENT_2,DOCUMENT_3,EMPTY_ID);\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.tck.SmokeIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(\"1\"),document -> {\n  indexAccessors.string.write(document,\"text 1\");\n  indexAccessors.string_analyzed.write(document,\"text 1\");\n  indexAccessors.integer.write(document,1);\n  indexAccessors.localDate.write(document,LocalDate.of(2018,1,1));\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(0,1));\n  DocumentElement flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"text 1_1\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,101);\n  flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"text 1_2\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,102);\n  DocumentElement nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"text 1_1\");\n  indexAccessors.nestedObject.integer.write(nestedObject,101);\n  nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"text 1_2\");\n  indexAccessors.nestedObject.integer.write(nestedObject,102);\n}\n);\nworkPlan.add(referenceProvider(\"2\"),document -> {\n  indexAccessors.string.write(document,\"text 2\");\n  indexAccessors.string_analyzed.write(document,\"text 2\");\n  indexAccessors.integer.write(document,2);\n  indexAccessors.localDate.write(document,LocalDate.of(2018,1,2));\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(0,2));\n  DocumentElement flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"text 2_1\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,201);\n  flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"text 2_2\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,202);\n  DocumentElement nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"text 2_1\");\n  indexAccessors.nestedObject.integer.write(nestedObject,201);\n  nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"text 2_2\");\n  indexAccessors.nestedObject.integer.write(nestedObject,202);\n}\n);\nworkPlan.add(referenceProvider(\"3\"),document -> {\n  indexAccessors.string.write(document,\"text 3\");\n  indexAccessors.string_analyzed.write(document,\"text 3\");\n  indexAccessors.integer.write(document,3);\n}\n);\nworkPlan.add(referenceProvider(\"neverMatching\"),document -> {\n  indexAccessors.string.write(document,\"never matching\");\n  indexAccessors.string_analyzed.write(document,\"never matching\");\n  indexAccessors.integer.write(document,9484);\n}\n);\nworkPlan.add(referenceProvider(\"empty\"),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,\"1\",\"2\",\"3\",\"neverMatching\",\"empty\");\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.tck.search.sort.SearchSortIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(SECOND_ID),document -> {\n  indexAccessors.string.write(document,\"george\");\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.7705687,4.835233));\n  indexAccessors.string_analyzed_forScore.write(document,\"Hooray Hooray\");\n  indexAccessors.unsortable.write(document,\"george\");\n  DocumentElement flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"george\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,2);\n  DocumentElement nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"george\");\n  indexAccessors.nestedObject.integer.write(nestedObject,2);\n}\n);\nworkPlan.add(referenceProvider(FIRST_ID),document -> {\n  indexAccessors.string.write(document,\"aaron\");\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.7541719,4.8386221));\n  indexAccessors.string_analyzed_forScore.write(document,\"Hooray Hooray Hooray\");\n  indexAccessors.unsortable.write(document,\"aaron\");\n  DocumentElement flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"aaron\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,1);\n  DocumentElement nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"aaron\");\n  indexAccessors.nestedObject.integer.write(nestedObject,1);\n}\n);\nworkPlan.add(referenceProvider(THIRD_ID),document -> {\n  indexAccessors.string.write(document,\"zach\");\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.7530374,4.8510299));\n  indexAccessors.string_analyzed_forScore.write(document,\"Hooray\");\n  indexAccessors.unsortable.write(document,\"zach\");\n  DocumentElement flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,\"zach\");\n  indexAccessors.flattenedObject.integer.write(flattenedObject,3);\n  DocumentElement nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,\"zach\");\n  indexAccessors.nestedObject.integer.write(nestedObject,3);\n}\n);\nworkPlan.add(referenceProvider(EMPTY_ID),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,FIRST_ID,SECOND_ID,THIRD_ID,EMPTY_ID);\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.tck.search.predicate.MatchSearchPredicateIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(DOCUMENT_1),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.string1Field.document1Value.write(document);\n  indexMapping.string2Field.document1Value.write(document);\n  indexMapping.string3Field.document1Value.write(document);\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_2),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.string1Field.document2Value.write(document);\n  indexMapping.string2Field.document2Value.write(document);\n  indexMapping.string3Field.document2Value.write(document);\n}\n);\nworkPlan.add(referenceProvider(EMPTY),document -> {\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_3),document -> {\n  indexMapping.string1Field.document3Value.write(document);\n  indexMapping.string2Field.document3Value.write(document);\n  indexMapping.string3Field.document3Value.write(document);\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,DOCUMENT_1,DOCUMENT_2,EMPTY,DOCUMENT_3);\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.tck.search.predicate.spatial.AbstractSpatialWithinSearchPredicateIT#initData\n methodBody: protected void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(OURSON_QUI_BOIT_ID),document -> {\n  indexAccessors.string.write(document,OURSON_QUI_BOIT_STRING);\n  indexAccessors.geoPoint.write(document,OURSON_QUI_BOIT_GEO_POINT);\n  indexAccessors.geoPoint_1.write(document,new ImmutableGeoPoint(OURSON_QUI_BOIT_GEO_POINT.getLatitude() - 1,OURSON_QUI_BOIT_GEO_POINT.getLongitude() - 1));\n  indexAccessors.geoPoint_2.write(document,new ImmutableGeoPoint(OURSON_QUI_BOIT_GEO_POINT.getLatitude() - 2,OURSON_QUI_BOIT_GEO_POINT.getLongitude() - 2));\n}\n);\nworkPlan.add(referenceProvider(IMOUTO_ID),document -> {\n  indexAccessors.string.write(document,IMOUTO_STRING);\n  indexAccessors.geoPoint.write(document,IMOUTO_GEO_POINT);\n  indexAccessors.geoPoint_1.write(document,new ImmutableGeoPoint(IMOUTO_GEO_POINT.getLatitude() - 1,IMOUTO_GEO_POINT.getLongitude() - 1));\n  indexAccessors.geoPoint_2.write(document,new ImmutableGeoPoint(IMOUTO_GEO_POINT.getLatitude() - 2,IMOUTO_GEO_POINT.getLongitude() - 2));\n}\n);\nworkPlan.add(referenceProvider(CHEZ_MARGOTTE_ID),document -> {\n  indexAccessors.string.write(document,CHEZ_MARGOTTE_STRING);\n  indexAccessors.geoPoint.write(document,CHEZ_MARGOTTE_GEO_POINT);\n  indexAccessors.geoPoint_1.write(document,new ImmutableGeoPoint(CHEZ_MARGOTTE_GEO_POINT.getLatitude() - 1,CHEZ_MARGOTTE_GEO_POINT.getLongitude() - 1));\n  indexAccessors.geoPoint_2.write(document,new ImmutableGeoPoint(CHEZ_MARGOTTE_GEO_POINT.getLatitude() - 2,CHEZ_MARGOTTE_GEO_POINT.getLongitude() - 2));\n}\n);\nworkPlan.add(referenceProvider(EMPTY_ID),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,OURSON_QUI_BOIT_ID,IMOUTO_ID,CHEZ_MARGOTTE_ID,EMPTY_ID);\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.tck.search.sort.SearchSortByFieldIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(DOCUMENT_2),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document2Value.write(document));\n  indexMapping.identicalForFirstTwo.document2Value.write(document);\n  indexMapping.identicalForLastTwo.document2Value.write(document);\n  DocumentElement flattenedObject=indexMapping.flattenedObject.self.add(document);\n  indexMapping.flattenedObject.supportedFieldModels.forEach(f -> f.document2Value.write(flattenedObject));\n  indexMapping.flattenedObject.unsupportedFieldModels.forEach(f -> f.document2Value.write(flattenedObject));\n  DocumentElement nestedObject=indexMapping.nestedObject.self.add(document);\n  indexMapping.nestedObject.supportedFieldModels.forEach(f -> f.document2Value.write(nestedObject));\n  indexMapping.nestedObject.unsupportedFieldModels.forEach(f -> f.document2Value.write(nestedObject));\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_1),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document1Value.write(document));\n  indexMapping.identicalForFirstTwo.document1Value.write(document);\n  indexMapping.identicalForLastTwo.document1Value.write(document);\n  DocumentElement flattenedObject=indexMapping.flattenedObject.self.add(document);\n  indexMapping.flattenedObject.supportedFieldModels.forEach(f -> f.document1Value.write(flattenedObject));\n  indexMapping.flattenedObject.unsupportedFieldModels.forEach(f -> f.document1Value.write(flattenedObject));\n  DocumentElement nestedObject=indexMapping.nestedObject.self.add(document);\n  indexMapping.nestedObject.supportedFieldModels.forEach(f -> f.document1Value.write(nestedObject));\n  indexMapping.nestedObject.unsupportedFieldModels.forEach(f -> f.document1Value.write(nestedObject));\n}\n);\nworkPlan.add(referenceProvider(DOCUMENT_3),document -> {\n  indexMapping.supportedFieldModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.supportedFieldWithDslConverterModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.unsupportedFieldModels.forEach(f -> f.document3Value.write(document));\n  indexMapping.identicalForFirstTwo.document3Value.write(document);\n  indexMapping.identicalForLastTwo.document3Value.write(document);\n  DocumentElement flattenedObject=indexMapping.flattenedObject.self.add(document);\n  indexMapping.flattenedObject.supportedFieldModels.forEach(f -> f.document3Value.write(flattenedObject));\n  indexMapping.flattenedObject.unsupportedFieldModels.forEach(f -> f.document3Value.write(flattenedObject));\n  DocumentElement nestedObject=indexMapping.nestedObject.self.add(document);\n  indexMapping.nestedObject.supportedFieldModels.forEach(f -> f.document3Value.write(nestedObject));\n  indexMapping.nestedObject.unsupportedFieldModels.forEach(f -> f.document3Value.write(nestedObject));\n}\n);\nworkPlan.add(referenceProvider(EMPTY),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,DOCUMENT_1,DOCUMENT_2,DOCUMENT_3,EMPTY);\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.lucene.search.LuceneSearchSortIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(FIRST_ID),document -> {\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.7705687,4.835233));\n}\n);\nworkPlan.add(referenceProvider(SECOND_ID),document -> {\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.7541719,4.8386221));\n}\n);\nworkPlan.add(referenceProvider(THIRD_ID),document -> {\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.7530374,4.8510299));\n}\n);\nworkPlan.add(referenceProvider(EMPTY_ID),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,FIRST_ID,SECOND_ID,THIRD_ID,EMPTY_ID);\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.lucene.ExtensionIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(FIRST_ID),document -> {\n  indexAccessors.string.write(document,\"text 1\");\n  indexAccessors.nativeField.write(document,37);\n  indexAccessors.nativeField_unsupportedProjection.write(document,37);\n  indexAccessors.sort1.write(document,\"a\");\n  indexAccessors.sort2.write(document,\"z\");\n  indexAccessors.sort3.write(document,\"z\");\n}\n);\nworkPlan.add(referenceProvider(SECOND_ID),document -> {\n  indexAccessors.integer.write(document,2);\n  indexAccessors.nativeField.write(document,78);\n  indexAccessors.nativeField_unsupportedProjection.write(document,78);\n  indexAccessors.sort1.write(document,\"z\");\n  indexAccessors.sort2.write(document,\"a\");\n  indexAccessors.sort3.write(document,\"z\");\n}\n);\nworkPlan.add(referenceProvider(THIRD_ID),document -> {\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(40.12,-71.34));\n  indexAccessors.nativeField.write(document,13);\n  indexAccessors.nativeField_unsupportedProjection.write(document,13);\n  indexAccessors.sort1.write(document,\"z\");\n  indexAccessors.sort2.write(document,\"z\");\n  indexAccessors.sort3.write(document,\"a\");\n}\n);\nworkPlan.add(referenceProvider(FOURTH_ID),document -> {\n  indexAccessors.nativeField.write(document,89);\n  indexAccessors.nativeField_unsupportedProjection.write(document,89);\n  indexAccessors.sort1.write(document,\"z\");\n  indexAccessors.sort2.write(document,\"z\");\n  indexAccessors.sort3.write(document,\"z\");\n}\n);\nworkPlan.add(referenceProvider(FIFTH_ID),document -> {\n  indexAccessors.string.write(document,\"text 2\");\n  indexAccessors.integer.write(document,1);\n  indexAccessors.geoPoint.write(document,new ImmutableGeoPoint(45.12,-75.34));\n  indexAccessors.nativeField.write(document,53);\n  indexAccessors.nativeField_unsupportedProjection.write(document,53);\n  indexAccessors.sort1.write(document,\"zz\");\n  indexAccessors.sort2.write(document,\"zz\");\n  indexAccessors.sort3.write(document,\"zz\");\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,FIRST_ID,SECOND_ID,THIRD_ID,FOURTH_ID,FIFTH_ID);\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.tck.search.SearchResultLoadingOrTransformingIT#initData\n methodBody: private void initData() {\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(MAIN_ID),document -> {\n  indexAccessors.string.write(document,STRING_VALUE);\n  indexAccessors.string_analyzed.write(document,STRING_ANALYZED_VALUE);\n  indexAccessors.integer.write(document,INTEGER_VALUE);\n  indexAccessors.localDate.write(document,LOCAL_DATE_VALUE);\n  indexAccessors.geoPoint.write(document,GEO_POINT_VALUE);\n  DocumentElement flattenedObject=indexAccessors.flattenedObject.self.add(document);\n  indexAccessors.flattenedObject.string.write(flattenedObject,FLATTENED_OBJECT_STRING_VALUE);\n  indexAccessors.flattenedObject.integer.write(flattenedObject,FLATTENED_OBJECT_INTEGER_VALUE);\n  DocumentElement nestedObject=indexAccessors.nestedObject.self.add(document);\n  indexAccessors.nestedObject.string.write(nestedObject,NESTED_OBJECT_STRING_VALUE);\n  indexAccessors.nestedObject.integer.write(nestedObject,NESTED_OBJECT_INTEGER_VALUE);\n}\n);\nworkPlan.add(referenceProvider(EMPTY_ID),document -> {\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nassertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,MAIN_ID,EMPTY_ID);\n}",
            "methodSignature: org.hibernate.search.integrationtest.showcase.OrmElasticsearchLibraryShowcaseIT#initData\n methodBody: private void initData(Session session) {\nLibraryDao libraryDao=daoFactory.createLibraryDao(session);\nDocumentDao documentDao=daoFactory.createDocumentDao(session);\nPersonDao personDao=daoFactory.createPersonDao(session);\nBook calligraphy=documentDao.createBook(CALLIGRAPHY_ID,new ISBN(\"978-0-00-000001-1\"),\"Calligraphy for Dummies\",\"Learn to write artfully in ten lessons\",\"calligraphy,art\");\nVideo javaDancing=documentDao.createVideo(JAVA_DANCING_ID,\"Java le dire \u00e0 tout le monde\",\"A brief history of Java dancing in Paris during the early 20th century\",\"java,dancing,history\");\nBook indonesianEconomy=documentDao.createBook(INDONESIAN_ECONOMY_ID,new ISBN(\"978-0-00-000003-3\"),\"Comparative Study of the Economy of Java and other Indonesian Islands\",\"Comparative study of the late 20th century economy of the main islands of Indonesia\" + \" with accurate projections over the next ten centuries\",\"geography,economy,java,sumatra,borneo,sulawesi\");\nBook javaForDummies=documentDao.createBook(JAVA_FOR_DUMMIES_ID,new ISBN(\"978-0-00-000004-4\"),\"java for Dummies\",\"Learning the Java programming language in ten lessons\",\"programming,language,java\");\nBook artOfComputerProg=documentDao.createBook(ART_OF_COMPUTER_PROG_ID,new ISBN(\"978-0-00-000005-5\"),\"The Art of Computer Programming\",\"Quick review of basic computer programming principles in 965 chapters\",\"programming\");\nBook thesaurusOfLanguages=documentDao.createBook(THESAURUS_OF_LANGUAGES_ID,new ISBN(\"978-0-00-000006-6\"),\"Thesaurus of Indo-European Languages\",\"An entertaining list of about three thousand languages, most of which are long dead\",\"geography,language\");\nVideo livingOnIsland=documentDao.createVideo(LIVING_ON_ISLAND_ID,\"Living in an Island, Episode 3: Indonesia\",\"A journey across Indonesia's smallest islands depicting how island way of life differs from mainland living\",\"geography,java,sumatra,borneo,sulawesi\");\nLibrary cityCenterLibrary=libraryDao.create(CITY_CENTER_ID,\"City Center Library\",12400,42.0,0.0,LibraryService.READING_ROOMS,LibraryService.HARDCOPY_LOAN);\ndocumentDao.createCopy(cityCenterLibrary,calligraphy,BookMedium.HARDCOPY);\ndocumentDao.createCopy(cityCenterLibrary,javaDancing,VideoMedium.DVD);\ndocumentDao.createCopy(cityCenterLibrary,indonesianEconomy,BookMedium.HARDCOPY);\ndocumentDao.createCopy(cityCenterLibrary,javaForDummies,BookMedium.HARDCOPY);\ndocumentDao.createCopy(cityCenterLibrary,artOfComputerProg,BookMedium.HARDCOPY);\ndocumentDao.createCopy(cityCenterLibrary,thesaurusOfLanguages,BookMedium.HARDCOPY);\ndocumentDao.createCopy(cityCenterLibrary,livingOnIsland,VideoMedium.BLURAY);\nLibrary suburbanLibrary1=libraryDao.create(SUBURBAN_1_ID,\"suburban Library 1\",800,42.0,0.25,LibraryService.DISABLED_ACCESS,LibraryService.HARDCOPY_LOAN);\ndocumentDao.createCopy(suburbanLibrary1,calligraphy,BookMedium.HARDCOPY);\ndocumentDao.createCopy(suburbanLibrary1,indonesianEconomy,BookMedium.HARDCOPY);\ndocumentDao.createCopy(suburbanLibrary1,javaForDummies,BookMedium.HARDCOPY);\ndocumentDao.createCopy(suburbanLibrary1,artOfComputerProg,BookMedium.HARDCOPY);\ndocumentDao.createCopy(suburbanLibrary1,thesaurusOfLanguages,BookMedium.HARDCOPY);\nLibrary suburbanLibrary2=libraryDao.create(SUBURBAN_2_ID,\"Suburban Library 2\",800,42.0,-0.25,LibraryService.DISABLED_ACCESS,LibraryService.READING_ROOMS,LibraryService.HARDCOPY_LOAN);\ndocumentDao.createCopy(suburbanLibrary2,calligraphy,BookMedium.HARDCOPY);\ndocumentDao.createCopy(suburbanLibrary2,calligraphy,BookMedium.DEMATERIALIZED);\ndocumentDao.createCopy(suburbanLibrary2,javaDancing,VideoMedium.DVD);\ndocumentDao.createCopy(suburbanLibrary2,javaDancing,VideoMedium.DEMATERIALIZED);\ndocumentDao.createCopy(suburbanLibrary2,javaForDummies,BookMedium.HARDCOPY);\ndocumentDao.createCopy(suburbanLibrary2,javaForDummies,BookMedium.DEMATERIALIZED);\ndocumentDao.createCopy(suburbanLibrary2,livingOnIsland,VideoMedium.BLURAY);\ndocumentDao.createCopy(suburbanLibrary2,livingOnIsland,VideoMedium.DEMATERIALIZED);\nLibrary universityLibrary=libraryDao.create(UNIVERSITY_ID,\"University Library\",9000,42.0,0.5,LibraryService.READING_ROOMS,LibraryService.HARDCOPY_LOAN,LibraryService.DEMATERIALIZED_LOAN);\ndocumentDao.createCopy(universityLibrary,indonesianEconomy,BookMedium.HARDCOPY);\ndocumentDao.createCopy(universityLibrary,javaForDummies,BookMedium.HARDCOPY);\ndocumentDao.createCopy(universityLibrary,artOfComputerProg,BookMedium.HARDCOPY);\ndocumentDao.createCopy(universityLibrary,thesaurusOfLanguages,BookMedium.HARDCOPY);\nPerson janeSmith=personDao.create(JANE_SMITH_ID,\"Jane\",\"Smith\");\npersonDao.createAccount(janeSmith);\ncreateBorrowal(personDao,janeSmith,cityCenterLibrary,indonesianEconomy,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,janeSmith,cityCenterLibrary,artOfComputerProg,BorrowalType.LONG_TERM);\ncreateBorrowal(personDao,janeSmith,cityCenterLibrary,javaForDummies,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,janeSmith,cityCenterLibrary,calligraphy,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,janeSmith,cityCenterLibrary,javaDancing,BorrowalType.LONG_TERM);\nPerson janeFonda=personDao.create(JANE_FONDA_ID,\"Jane\",\"Fonda\");\npersonDao.createAccount(janeFonda);\ncreateBorrowal(personDao,janeFonda,universityLibrary,javaForDummies,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,janeFonda,universityLibrary,artOfComputerProg,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,janeFonda,universityLibrary,thesaurusOfLanguages,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,janeFonda,universityLibrary,indonesianEconomy,BorrowalType.SHORT_TERM);\nPerson janePorter=personDao.create(JANE_PORTER_ID,\"Jane\",\"porter\");\npersonDao.createAccount(janePorter);\ncreateBorrowal(personDao,janePorter,suburbanLibrary1,indonesianEconomy,BorrowalType.LONG_TERM);\ncreateBorrowal(personDao,janePorter,suburbanLibrary2,livingOnIsland,1,BorrowalType.LONG_TERM);\ncreateBorrowal(personDao,janePorter,universityLibrary,thesaurusOfLanguages,BorrowalType.LONG_TERM);\nPerson johnLennon=personDao.create(JOHN_LENNON_ID,\"john\",\"Lennon\");\npersonDao.createAccount(johnLennon);\ncreateBorrowal(personDao,johnLennon,suburbanLibrary2,javaDancing,1,BorrowalType.SHORT_TERM);\nPerson eltonJohn=personDao.create(ELTON_JOHN_ID,\"elton\",\"john\");\npersonDao.createAccount(eltonJohn);\ncreateBorrowal(personDao,eltonJohn,suburbanLibrary2,javaDancing,1,BorrowalType.SHORT_TERM);\nPerson pattySmith=personDao.create(PATTY_SMITH_ID,\"Patty\",\"Smith\");\npersonDao.createAccount(pattySmith);\ncreateBorrowal(personDao,pattySmith,suburbanLibrary2,javaDancing,1,BorrowalType.SHORT_TERM);\nPerson johnSmith=personDao.create(JOHN_SMITH_ID,\"John\",\"Smith\");\npersonDao.createAccount(johnSmith);\ncreateBorrowal(personDao,johnSmith,suburbanLibrary1,indonesianEconomy,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,johnSmith,suburbanLibrary1,artOfComputerProg,BorrowalType.LONG_TERM);\nPerson johnPaulSmith=personDao.create(JOHN_PAUL_SMITH_ID,\"John Paul\",\"Smith\");\nPerson johnPaul=personDao.create(JOHN_PAUL_ID,\"John\",\"Paul\");\npersonDao.createAccount(johnPaul);\nPerson paulJohn=personDao.create(PAUL_JOHN_ID,\"Paul\",\"John\");\npersonDao.createAccount(paulJohn);\ncreateBorrowal(personDao,paulJohn,cityCenterLibrary,javaForDummies,BorrowalType.SHORT_TERM);\ncreateBorrowal(personDao,paulJohn,cityCenterLibrary,artOfComputerProg,BorrowalType.SHORT_TERM);\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.tck.search.predicate.spatial.SpatialWithinBoundingBoxSearchPredicateIT#initData\n methodBody: protected void initData() {\nsuper.initData();\nIndexWorkPlan<? extends DocumentElement> workPlan=indexManager.createWorkPlan(sessionContext);\nworkPlan.add(referenceProvider(ADDITIONAL_POINT_1_ID),document -> {\n  indexAccessors.geoPoint.write(document,ADDITIONAL_POINT_1_GEO_POINT);\n}\n);\nworkPlan.add(referenceProvider(ADDITIONAL_POINT_2_ID),document -> {\n  indexAccessors.geoPoint.write(document,ADDITIONAL_POINT_2_GEO_POINT);\n}\n);\nworkPlan.execute().join();\nIndexSearchTarget searchTarget=indexManager.createSearchTarget().build();\nSearchQuery<DocumentReference> query=searchTarget.query(sessionContext).asReferences().predicate().matchAll().end().build();\nDocumentReferencesSearchResultAssert.assertThat(query).hasReferencesHitsAnyOrder(INDEX_NAME,OURSON_QUI_BOIT_ID,IMOUTO_ID,CHEZ_MARGOTTE_ID,EMPTY_ID,ADDITIONAL_POINT_1_ID,ADDITIONAL_POINT_2_ID);\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void searchAroundMe_spatial() {\n\t\twithinTransaction( sessionFactory, this::initData );\n\n\t\twithinSession( sessionFactory, session -> {\n\t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n\n\t\t\tGeoPoint myLocation = GeoPoint.of( 42.0, 0.5 );\n\n\t\t\tList<Document<?>> documents = dao.searchAroundMe(\n\t\t\t\t\tnull, null,\n\t\t\t\t\tmyLocation, 20.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from university\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n\t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\tnull, null,\n\t\t\t\t\tmyLocation, 40.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from suburb1 or university\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, CALLIGRAPHY_ID ),\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n\t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n\t\t\t);\n\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\t\"calligraphy\", null,\n\t\t\t\t\tmyLocation, 40.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from suburb1 or university with \"calligraphy\" in it\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, CALLIGRAPHY_ID )\n\t\t\t);\n\n\t\t\tmyLocation = GeoPoint.of( 42.0, 0.75 );\n\t\t\tdocuments = dao.searchAroundMe(\n\t\t\t\t\tnull, null,\n\t\t\t\t\tmyLocation, 40.0,\n\t\t\t\t\tnull,\n\t\t\t\t\t0, 10\n\t\t\t);\n\t\t\t// Should only include content from university\n\t\t\tassertThat( documents ).containsExactly(\n\t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n\t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n\t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n\t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n\t\t\t);\n\t\t} );\n\t}\nimport org.hibernate.boot.Metadata;\nimport org.hibernate.boot.MetadataSources;\nimport org.hibernate.boot.SessionFactoryBuilder;\nimport org.hibernate.boot.registry.StandardServiceRegistryBuilder;\nimport org.hibernate.search.backend.elasticsearch.cfg.SearchBackendElasticsearchSettings;\nimport org.hibernate.search.backend.elasticsearch.impl.ElasticsearchBackendFactory;\nimport org.hibernate.search.integrationtest.showcase.library.analysis.LibraryAnalysisConfigurer;\nimport org.hibernate.search.mapper.orm.cfg.SearchOrmSettings;\nimport org.hibernate.search.integrationtest.showcase.library.bridge.AccountBorrowalSummaryBridge;\nimport org.hibernate.search.integrationtest.showcase.library.dao.DaoFactory;",
        "diffSourceCode": "    23: import org.hibernate.boot.Metadata;\n    24: import org.hibernate.boot.MetadataSources;\n    25: import org.hibernate.boot.SessionFactoryBuilder;\n    26: import org.hibernate.boot.registry.StandardServiceRegistryBuilder;\n    27: import org.hibernate.search.backend.elasticsearch.cfg.SearchBackendElasticsearchSettings;\n    28: import org.hibernate.search.backend.elasticsearch.impl.ElasticsearchBackendFactory;\n    29: import org.hibernate.search.integrationtest.showcase.library.analysis.LibraryAnalysisConfigurer;\n    30: import org.hibernate.search.mapper.orm.cfg.SearchOrmSettings;\n    31: import org.hibernate.search.integrationtest.showcase.library.bridge.AccountBorrowalSummaryBridge;\n    32: import org.hibernate.search.integrationtest.showcase.library.dao.DaoFactory;\n-  320: \n-  321: \t@Test\n-  322: \tpublic void searchAroundMe_spatial() {\n-  323: \t\twithinTransaction( sessionFactory, this::initData );\n-  324: \n-  325: \t\twithinSession( sessionFactory, session -> {\n-  326: \t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n-  327: \n-  328: \t\t\tGeoPoint myLocation = new ImmutableGeoPoint( 42.0, 0.5 );\n-  329: \n-  330: \t\t\tList<Document<?>> documents = dao.searchAroundMe(\n-  331: \t\t\t\t\tnull, null,\n-  332: \t\t\t\t\tmyLocation, 20.0,\n-  333: \t\t\t\t\tnull,\n-  334: \t\t\t\t\t0, 10\n-  335: \t\t\t);\n-  336: \t\t\t// Should only include content from university\n-  337: \t\t\tassertThat( documents ).containsExactly(\n-  338: \t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n-  339: \t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n-  340: \t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n-  341: \t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n-  342: \t\t\t);\n-  343: \n-  344: \t\t\tdocuments = dao.searchAroundMe(\n-  345: \t\t\t\t\tnull, null,\n-  346: \t\t\t\t\tmyLocation, 40.0,\n-  347: \t\t\t\t\tnull,\n-  348: \t\t\t\t\t0, 10\n-  349: \t\t\t);\n-  350: \t\t\t// Should only include content from suburb1 or university\n-  351: \t\t\tassertThat( documents ).containsExactly(\n-  352: \t\t\t\t\tsession.get( Book.class, CALLIGRAPHY_ID ),\n-  353: \t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n-  354: \t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n-  355: \t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n-  356: \t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n-  357: \t\t\t);\n-  358: \n-  359: \t\t\tdocuments = dao.searchAroundMe(\n-  360: \t\t\t\t\t\"calligraphy\", null,\n-  361: \t\t\t\t\tmyLocation, 40.0,\n-  362: \t\t\t\t\tnull,\n-  363: \t\t\t\t\t0, 10\n-  364: \t\t\t);\n-  365: \t\t\t// Should only include content from suburb1 or university with \"calligraphy\" in it\n-  366: \t\t\tassertThat( documents ).containsExactly(\n-  367: \t\t\t\t\tsession.get( Book.class, CALLIGRAPHY_ID )\n-  368: \t\t\t);\n-  369: \n-  370: \t\t\tmyLocation = new ImmutableGeoPoint( 42.0, 0.75 );\n-  371: \t\t\tdocuments = dao.searchAroundMe(\n-  372: \t\t\t\t\tnull, null,\n-  373: \t\t\t\t\tmyLocation, 40.0,\n-  374: \t\t\t\t\tnull,\n-  375: \t\t\t\t\t0, 10\n-  376: \t\t\t);\n-  377: \t\t\t// Should only include content from university\n-  378: \t\t\tassertThat( documents ).containsExactly(\n-  379: \t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n-  380: \t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n-  381: \t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n-  382: \t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n-  383: \t\t\t);\n-  384: \t\t} );\n-  385: \t}\n+  320: \t@Test\n+  321: \tpublic void searchAroundMe_spatial() {\n+  322: \t\twithinTransaction( sessionFactory, this::initData );\n+  323: \n+  324: \t\twithinSession( sessionFactory, session -> {\n+  325: \t\t\tDocumentDao dao = daoFactory.createDocumentDao( session );\n+  326: \n+  327: \t\t\tGeoPoint myLocation = GeoPoint.of( 42.0, 0.5 );\n+  328: \n+  329: \t\t\tList<Document<?>> documents = dao.searchAroundMe(\n+  330: \t\t\t\t\tnull, null,\n+  331: \t\t\t\t\tmyLocation, 20.0,\n+  332: \t\t\t\t\tnull,\n+  333: \t\t\t\t\t0, 10\n+  334: \t\t\t);\n+  335: \t\t\t// Should only include content from university\n+  336: \t\t\tassertThat( documents ).containsExactly(\n+  337: \t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n+  338: \t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n+  339: \t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n+  340: \t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n+  341: \t\t\t);\n+  342: \n+  343: \t\t\tdocuments = dao.searchAroundMe(\n+  344: \t\t\t\t\tnull, null,\n+  345: \t\t\t\t\tmyLocation, 40.0,\n+  346: \t\t\t\t\tnull,\n+  347: \t\t\t\t\t0, 10\n+  348: \t\t\t);\n+  349: \t\t\t// Should only include content from suburb1 or university\n+  350: \t\t\tassertThat( documents ).containsExactly(\n+  351: \t\t\t\t\tsession.get( Book.class, CALLIGRAPHY_ID ),\n+  352: \t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n+  353: \t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n+  354: \t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n+  355: \t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n+  356: \t\t\t);\n+  357: \n+  358: \t\t\tdocuments = dao.searchAroundMe(\n+  359: \t\t\t\t\t\"calligraphy\", null,\n+  360: \t\t\t\t\tmyLocation, 40.0,\n+  361: \t\t\t\t\tnull,\n+  362: \t\t\t\t\t0, 10\n+  363: \t\t\t);\n+  364: \t\t\t// Should only include content from suburb1 or university with \"calligraphy\" in it\n+  365: \t\t\tassertThat( documents ).containsExactly(\n+  366: \t\t\t\t\tsession.get( Book.class, CALLIGRAPHY_ID )\n+  367: \t\t\t);\n+  368: \n+  369: \t\t\tmyLocation = GeoPoint.of( 42.0, 0.75 );\n+  370: \t\t\tdocuments = dao.searchAroundMe(\n+  371: \t\t\t\t\tnull, null,\n+  372: \t\t\t\t\tmyLocation, 40.0,\n+  373: \t\t\t\t\tnull,\n+  374: \t\t\t\t\t0, 10\n+  375: \t\t\t);\n+  376: \t\t\t// Should only include content from university\n+  377: \t\t\tassertThat( documents ).containsExactly(\n+  378: \t\t\t\t\tsession.get( Book.class, INDONESIAN_ECONOMY_ID ),\n+  379: \t\t\t\t\tsession.get( Book.class, JAVA_FOR_DUMMIES_ID ),\n+  380: \t\t\t\t\tsession.get( Book.class, ART_OF_COMPUTER_PROG_ID ),\n+  381: \t\t\t\t\tsession.get( Book.class, THESAURUS_OF_LANGUAGES_ID )\n+  382: \t\t\t);\n+  383: \t\t} );\n+  384: \t}\n+  385: \n",
        "uniqueId": "520908200e1e83c994b39a93131741bc114ee85d_321_385_23_32_320_384",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate eventLevelMatcher(level Level) : Matcher<LoggingEvent> extracted from public expectLevelMissing(level Level) : void in class org.hibernate.search.util.impl.test.rule.ExpectedLog4jLog",
        "diffLocations": [
            {
                "filePath": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/rule/ExpectedLog4jLog.java",
                "startLine": 69,
                "endLine": 83,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/rule/ExpectedLog4jLog.java",
                "startLine": 84,
                "endLine": 89,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/rule/ExpectedLog4jLog.java",
                "startLine": 142,
                "endLine": 153,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "/**\n\t * Verify that your code <strong>doesn't</strong> produce a log event matching the given level or higher.\n\t */\n\tpublic void expectLevelMissing(Level level) {\n\t\texpectEventMissing( new TypeSafeMatcher<LoggingEvent>() {\n\t\t\t@Override\n\t\t\tpublic void describeTo(Description description) {\n\t\t\t\tdescription.appendText( \"a LoggingEvent with \" ).appendValue( level ).appendText( \" level or higher\" );\n\t\t\t}\n\t\t\t@Override\n\t\t\tprotected boolean matchesSafely(LoggingEvent item) {\n\t\t\t\treturn item.getLevel().isGreaterOrEqual( level );\n\t\t\t}\n\t\t} );\n\t}",
        "filePathBefore": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/rule/ExpectedLog4jLog.java",
        "isPureRefactoring": true,
        "commitId": "bd746f6ff6562a850ee95384fb393c2ca9154c47",
        "packageNameBefore": "org.hibernate.search.util.impl.test.rule",
        "classNameBefore": "org.hibernate.search.util.impl.test.rule.ExpectedLog4jLog",
        "methodNameBefore": "org.hibernate.search.util.impl.test.rule.ExpectedLog4jLog#expectLevelMissing",
        "invokedMethod": "methodSignature: org.hibernate.search.util.impl.test.rule.ExpectedLog4jLog#expectEventMissing\n methodBody: public void expectEventMissing(Matcher<? extends LoggingEvent> matcher) {\nabsenceExpectations.add(matcher);\n}",
        "classSignatureBefore": "public class ExpectedLog4jLog implements TestRule ",
        "methodNameBeforeSet": [
            "org.hibernate.search.util.impl.test.rule.ExpectedLog4jLog#expectLevelMissing"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.util.impl.test.rule.ExpectedLog4jLog"
        ],
        "classSignatureBeforeSet": [
            "public class ExpectedLog4jLog implements TestRule "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.test.rule;\n\nimport static org.junit.Assert.fail;\n\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Set;\n\nimport org.junit.rules.TestRule;\nimport org.junit.runners.model.Statement;\n\nimport org.apache.log4j.AppenderSkeleton;\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.log4j.spi.LoggingEvent;\nimport org.hamcrest.CoreMatchers;\nimport org.hamcrest.Description;\nimport org.hamcrest.Matcher;\nimport org.hamcrest.StringDescription;\nimport org.hamcrest.TypeSafeMatcher;\n\n/**\n * @author Yoann Rodiere\n */\npublic class ExpectedLog4jLog implements TestRule {\n\n\t/**\n\t * Returns a {@linkplain TestRule rule} that does not mandate any particular log to be produced (identical to\n\t * behavior without this rule).\n\t */\n\tpublic static ExpectedLog4jLog create() {\n\t\treturn new ExpectedLog4jLog();\n\t}\n\n\tprivate List<Matcher<?>> expectations = new ArrayList<>();\n\n\tprivate List<Matcher<?>> absenceExpectations = new ArrayList<>();\n\n\tprivate ExpectedLog4jLog() {\n\t}\n\n\t@Override\n\tpublic Statement apply(Statement base, org.junit.runner.Description description) {\n\t\treturn new ExpectedLogStatement( base );\n\t}\n\n\t/**\n\t * Verify that your code produces a log event matching the given matcher.\n\t */\n\tpublic void expectEvent(Matcher<? extends LoggingEvent> matcher) {\n\t\texpectations.add( matcher );\n\t}\n\n\t/**\n\t * Verify that your code <strong>doesn't</strong> produce a log event matching the given matcher.\n\t */\n\tpublic void expectEventMissing(Matcher<? extends LoggingEvent> matcher) {\n\t\tabsenceExpectations.add( matcher );\n\t}\n\n\t/**\n\t * Verify that your code <strong>doesn't</strong> produce a log event matching the given level or higher.\n\t */\n\tpublic void expectLevelMissing(Level level) {\n\t\texpectEventMissing( new TypeSafeMatcher<LoggingEvent>() {\n\t\t\t@Override\n\t\t\tpublic void describeTo(Description description) {\n\t\t\t\tdescription.appendText( \"a LoggingEvent with \" ).appendValue( level ).appendText( \" level or higher\" );\n\t\t\t}\n\t\t\t@Override\n\t\t\tprotected boolean matchesSafely(LoggingEvent item) {\n\t\t\t\treturn item.getLevel().isGreaterOrEqual( level );\n\t\t\t}\n\t\t} );\n\t}\n\n\t/**\n\t * Verify that your code produces a log message containing the given string.\n\t */\n\tpublic void expectMessage(String containedString) {\n\t\texpectMessage( CoreMatchers.containsString( containedString ) );\n\t}\n\n\t/**\n\t * Verify that your code <strong>doesn't</strong> produce a log message containing the given string.\n\t */\n\tpublic void expectMessageMissing(String containedString) {\n\t\texpectMessageMissing( CoreMatchers.containsString( containedString ) );\n\t}\n\n\t/**\n\t * Verify that your code produces a log message containing all of the given string.\n\t */\n\tpublic void expectMessage(String containedString, String... otherContainedStrings) {\n\t\texpectMessage( containsAllStrings( containedString, otherContainedStrings ) );\n\t}\n\n\t/**\n\t * Verify that your code <strong>doesn't</strong> produce a log message containing all of the given string.\n\t */\n\tpublic void expectMessageMissing(String containedString, String... otherContainedStrings) {\n\t\texpectMessageMissing( containsAllStrings( containedString, otherContainedStrings ) );\n\t}\n\n\t/**\n\t * Verify that your code produces a log message matches the given Hamcrest matcher.\n\t */\n\tpublic void expectMessage(Matcher<String> matcher) {\n\t\texpectEvent( eventMessageMatcher( matcher ) );\n\t}\n\n\t/**\n\t * Verify that your code <strong>doesn't</strong> produce a log message matches the given Hamcrest matcher.\n\t */\n\tpublic void expectMessageMissing(Matcher<String> matcher) {\n\t\texpectEventMissing( eventMessageMatcher( matcher ) );\n\t}\n\n\tprivate Matcher<String> containsAllStrings(String containedString, String... otherContainedStrings) {\n\t\tCollection<Matcher<? super String>> matchers = new ArrayList<>();\n\t\tmatchers.add( CoreMatchers.containsString( containedString ) );\n\t\tfor ( String otherContainedString : otherContainedStrings ) {\n\t\t\tmatchers.add( CoreMatchers.containsString( otherContainedString ) );\n\t\t}\n\t\treturn CoreMatchers.<String>allOf( matchers );\n\t}\n\n\tprivate Matcher<LoggingEvent> eventMessageMatcher(final Matcher<String> messageMatcher) {\n\t\treturn new TypeSafeMatcher<LoggingEvent>() {\n\n\t\t\t@Override\n\t\t\tpublic void describeTo(Description description) {\n\t\t\t\tdescription.appendText( \"a LoggingEvent with message matching \" );\n\t\t\t\tmessageMatcher.describeTo( description );\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tprotected boolean matchesSafely(LoggingEvent item) {\n\t\t\t\treturn messageMatcher.matches( item.getMessage() );\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate class TestAppender extends AppenderSkeleton {\n\t\tprivate final Set<Matcher<?>> expectationsMet = new HashSet<>();\n\t\tprivate final Set<LoggingEvent> unexpectedEvents = new HashSet<>();\n\n\t\t@Override\n\t\tpublic void close() {\n\t\t\t// Nothing to clean up\n\t\t}\n\n\t\t@Override\n\t\tpublic boolean requiresLayout() {\n\t\t\treturn false;\n\t\t}\n\n\t\t@Override\n\t\tprotected void append(LoggingEvent event) {\n\t\t\tfor ( Matcher<?> expectation : ExpectedLog4jLog.this.expectations ) {\n\t\t\t\tif ( !expectationsMet.contains( expectation ) && expectation.matches( event ) ) {\n\t\t\t\t\texpectationsMet.add( expectation );\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor ( Matcher<?> absenceExpectation : ExpectedLog4jLog.this.absenceExpectations ) {\n\t\t\t\tif ( absenceExpectation.matches( event ) ) {\n\t\t\t\t\tunexpectedEvents.add( event );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tpublic Set<Matcher<?>> getExpectationsNotMet() {\n\t\t\tSet<Matcher<?>> expectationsNotMet = new HashSet<>();\n\t\t\texpectationsNotMet.addAll( expectations );\n\t\t\texpectationsNotMet.removeAll( expectationsMet );\n\t\t\treturn expectationsNotMet;\n\t\t}\n\n\t\tpublic Set<LoggingEvent> getUnexpectedEvents() {\n\t\t\treturn unexpectedEvents;\n\t\t}\n\n\t}\n\n\tprivate class ExpectedLogStatement extends Statement {\n\n\t\tprivate final Statement next;\n\n\t\tpublic ExpectedLogStatement(Statement base) {\n\t\t\tnext = base;\n\t\t}\n\n\t\t@Override\n\t\tpublic void evaluate() throws Throwable {\n\t\t\tfinal Logger logger = Logger.getRootLogger();\n\t\t\tTestAppender appender = new TestAppender();\n\t\t\tlogger.addAppender( appender );\n\t\t\ttry {\n\t\t\t\tnext.evaluate();\n\t\t\t}\n\t\t\tfinally {\n\t\t\t\tlogger.removeAppender( appender );\n\t\t\t}\n\t\t\tSet<Matcher<?>> expectationsNotMet = appender.getExpectationsNotMet();\n\t\t\tSet<LoggingEvent> unexpectedEvents = appender.getUnexpectedEvents();\n\t\t\tif ( !expectationsNotMet.isEmpty() || !unexpectedEvents.isEmpty() ) {\n\t\t\t\tfail( buildFailureMessage( expectationsNotMet, unexpectedEvents ) );\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate static String buildFailureMessage(Set<Matcher<?>> missingSet, Set<LoggingEvent> unexpectedEvents) {\n\t\tDescription description = new StringDescription();\n\t\tdescription.appendText( \"Produced logs did not meet the expectations.\" );\n\t\tif ( !missingSet.isEmpty() ) {\n\t\t\tdescription.appendText( \"\\nMissing logs:\" );\n\t\t\tfor ( Matcher<?> missing : missingSet ) {\n\t\t\t\tdescription.appendText( \"\\n\\t\" );\n\t\t\t\tmissing.describeTo( description );\n\t\t\t}\n\t\t}\n\t\tif ( !unexpectedEvents.isEmpty() ) {\n\t\t\tdescription.appendText( \"\\nUnexpected logs:\" );\n\t\t\tfor ( LoggingEvent unexpected : unexpectedEvents ) {\n\t\t\t\tdescription.appendText( \"\\n\\t\" );\n\t\t\t\tdescription.appendText( unexpected.getRenderedMessage() );\n\t\t\t}\n\t\t}\n\t\treturn description.toString();\n\t}\n\n}\n",
        "filePathAfter": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/rule/ExpectedLog4jLog.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.test.rule;\n\nimport static org.junit.Assert.fail;\n\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.HashSet;\nimport java.util.List;\nimport java.util.Set;\n\nimport org.junit.rules.TestRule;\nimport org.junit.runners.model.Statement;\n\nimport org.apache.log4j.AppenderSkeleton;\nimport org.apache.log4j.Level;\nimport org.apache.log4j.Logger;\nimport org.apache.log4j.spi.LoggingEvent;\nimport org.apache.log4j.spi.ThrowableInformation;\nimport org.hamcrest.CoreMatchers;\nimport org.hamcrest.Description;\nimport org.hamcrest.Matcher;\nimport org.hamcrest.StringDescription;\nimport org.hamcrest.TypeSafeMatcher;\n\n/**\n * @author Yoann Rodiere\n */\npublic class ExpectedLog4jLog implements TestRule {\n\n\t/**\n\t * Returns a {@linkplain TestRule rule} that does not mandate any particular log to be produced (identical to\n\t * behavior without this rule).\n\t */\n\tpublic static ExpectedLog4jLog create() {\n\t\treturn new ExpectedLog4jLog();\n\t}\n\n\tprivate List<Matcher<?>> expectations = new ArrayList<>();\n\n\tprivate List<Matcher<?>> absenceExpectations = new ArrayList<>();\n\n\tprivate ExpectedLog4jLog() {\n\t}\n\n\t@Override\n\tpublic Statement apply(Statement base, org.junit.runner.Description description) {\n\t\treturn new ExpectedLogStatement( base );\n\t}\n\n\t/**\n\t * Verify that your code produces a log event matching the given matcher.\n\t */\n\tpublic void expectEvent(Matcher<? extends LoggingEvent> matcher) {\n\t\texpectations.add( matcher );\n\t}\n\n\t/**\n\t * Verify that your code produces a log event matching the given level or higher,\n\t * with a throwable matching the given matcher, and a message containing the given strings.\n\t */\n\tpublic void expectEvent(Level level,\n\t\t\tMatcher<? super Throwable> throwableMatcher,\n\t\t\tString containedString, String... otherContainedStrings) {\n\t\texpectEvent( CoreMatchers.allOf(\n\t\t\t\teventLevelMatcher( level ),\n\t\t\t\teventThrowableMatcher( throwableMatcher ),\n\t\t\t\teventMessageMatcher( containsAllStrings( containedString, otherContainedStrings ) )\n\t\t) );\n\t}\n\n\t/**\n\t * Verify that your code <strong>doesn't</strong> produce a log event matching the given matcher.\n\t */\n\tpublic void expectEventMissing(Matcher<? extends LoggingEvent> matcher) {\n\t\tabsenceExpectations.add( matcher );\n\t}\n\n\t/**\n\t * Verify that your code <strong>doesn't</strong> produce a log event matching the given level or higher.\n\t */\n\tpublic void expectLevelMissing(Level level) {\n\t\texpectEventMissing( eventLevelMatcher( level ) );\n\t}\n\n\t/**\n\t * Verify that your code produces a log message containing the given string.\n\t */\n\tpublic void expectMessage(String containedString) {\n\t\texpectMessage( CoreMatchers.containsString( containedString ) );\n\t}\n\n\t/**\n\t * Verify that your code <strong>doesn't</strong> produce a log message containing the given string.\n\t */\n\tpublic void expectMessageMissing(String containedString) {\n\t\texpectMessageMissing( CoreMatchers.containsString( containedString ) );\n\t}\n\n\t/**\n\t * Verify that your code produces a log message containing all of the given string.\n\t */\n\tpublic void expectMessage(String containedString, String... otherContainedStrings) {\n\t\texpectMessage( containsAllStrings( containedString, otherContainedStrings ) );\n\t}\n\n\t/**\n\t * Verify that your code <strong>doesn't</strong> produce a log message containing all of the given string.\n\t */\n\tpublic void expectMessageMissing(String containedString, String... otherContainedStrings) {\n\t\texpectMessageMissing( containsAllStrings( containedString, otherContainedStrings ) );\n\t}\n\n\t/**\n\t * Verify that your code produces a log message matches the given Hamcrest matcher.\n\t */\n\tpublic void expectMessage(Matcher<String> matcher) {\n\t\texpectEvent( eventMessageMatcher( matcher ) );\n\t}\n\n\t/**\n\t * Verify that your code <strong>doesn't</strong> produce a log message matches the given Hamcrest matcher.\n\t */\n\tpublic void expectMessageMissing(Matcher<String> matcher) {\n\t\texpectEventMissing( eventMessageMatcher( matcher ) );\n\t}\n\n\tprivate Matcher<String> containsAllStrings(String containedString, String... otherContainedStrings) {\n\t\tCollection<Matcher<? super String>> matchers = new ArrayList<>();\n\t\tmatchers.add( CoreMatchers.containsString( containedString ) );\n\t\tfor ( String otherContainedString : otherContainedStrings ) {\n\t\t\tmatchers.add( CoreMatchers.containsString( otherContainedString ) );\n\t\t}\n\t\treturn CoreMatchers.<String>allOf( matchers );\n\t}\n\n\tprivate Matcher<LoggingEvent> eventLevelMatcher(Level level) {\n\t\treturn new TypeSafeMatcher<LoggingEvent>() {\n\t\t\t@Override\n\t\t\tpublic void describeTo(Description description) {\n\t\t\t\tdescription.appendText( \"a LoggingEvent with \" ).appendValue( level ).appendText( \" level or higher\" );\n\t\t\t}\n\t\t\t@Override\n\t\t\tprotected boolean matchesSafely(LoggingEvent item) {\n\t\t\t\treturn item.getLevel().isGreaterOrEqual( level );\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate Matcher<LoggingEvent> eventThrowableMatcher(Matcher<? super Throwable> throwableMatcher) {\n\t\treturn new TypeSafeMatcher<LoggingEvent>() {\n\t\t\t@Override\n\t\t\tpublic void describeTo(Description description) {\n\t\t\t\tdescription.appendText( \"a LoggingEvent with throwable \" ).appendValue( throwableMatcher );\n\t\t\t}\n\t\t\t@Override\n\t\t\tprotected boolean matchesSafely(LoggingEvent item) {\n\t\t\t\tThrowableInformation throwableInfo = item.getThrowableInformation();\n\t\t\t\treturn throwableMatcher.matches( throwableInfo == null ? null : throwableInfo.getThrowable() );\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate Matcher<LoggingEvent> eventMessageMatcher(final Matcher<String> messageMatcher) {\n\t\treturn new TypeSafeMatcher<LoggingEvent>() {\n\n\t\t\t@Override\n\t\t\tpublic void describeTo(Description description) {\n\t\t\t\tdescription.appendText( \"a LoggingEvent with message matching \" );\n\t\t\t\tmessageMatcher.describeTo( description );\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tprotected boolean matchesSafely(LoggingEvent item) {\n\t\t\t\treturn messageMatcher.matches( item.getMessage() );\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate class TestAppender extends AppenderSkeleton {\n\t\tprivate final Set<Matcher<?>> expectationsMet = new HashSet<>();\n\t\tprivate final Set<LoggingEvent> unexpectedEvents = new HashSet<>();\n\n\t\t@Override\n\t\tpublic void close() {\n\t\t\t// Nothing to clean up\n\t\t}\n\n\t\t@Override\n\t\tpublic boolean requiresLayout() {\n\t\t\treturn false;\n\t\t}\n\n\t\t@Override\n\t\tprotected void append(LoggingEvent event) {\n\t\t\tfor ( Matcher<?> expectation : ExpectedLog4jLog.this.expectations ) {\n\t\t\t\tif ( !expectationsMet.contains( expectation ) && expectation.matches( event ) ) {\n\t\t\t\t\texpectationsMet.add( expectation );\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor ( Matcher<?> absenceExpectation : ExpectedLog4jLog.this.absenceExpectations ) {\n\t\t\t\tif ( absenceExpectation.matches( event ) ) {\n\t\t\t\t\tunexpectedEvents.add( event );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tpublic Set<Matcher<?>> getExpectationsNotMet() {\n\t\t\tSet<Matcher<?>> expectationsNotMet = new HashSet<>();\n\t\t\texpectationsNotMet.addAll( expectations );\n\t\t\texpectationsNotMet.removeAll( expectationsMet );\n\t\t\treturn expectationsNotMet;\n\t\t}\n\n\t\tpublic Set<LoggingEvent> getUnexpectedEvents() {\n\t\t\treturn unexpectedEvents;\n\t\t}\n\n\t}\n\n\tprivate class ExpectedLogStatement extends Statement {\n\n\t\tprivate final Statement next;\n\n\t\tpublic ExpectedLogStatement(Statement base) {\n\t\t\tnext = base;\n\t\t}\n\n\t\t@Override\n\t\tpublic void evaluate() throws Throwable {\n\t\t\tfinal Logger logger = Logger.getRootLogger();\n\t\t\tTestAppender appender = new TestAppender();\n\t\t\tlogger.addAppender( appender );\n\t\t\ttry {\n\t\t\t\tnext.evaluate();\n\t\t\t}\n\t\t\tfinally {\n\t\t\t\tlogger.removeAppender( appender );\n\t\t\t}\n\t\t\tSet<Matcher<?>> expectationsNotMet = appender.getExpectationsNotMet();\n\t\t\tSet<LoggingEvent> unexpectedEvents = appender.getUnexpectedEvents();\n\t\t\tif ( !expectationsNotMet.isEmpty() || !unexpectedEvents.isEmpty() ) {\n\t\t\t\tfail( buildFailureMessage( expectationsNotMet, unexpectedEvents ) );\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate static String buildFailureMessage(Set<Matcher<?>> missingSet, Set<LoggingEvent> unexpectedEvents) {\n\t\tDescription description = new StringDescription();\n\t\tdescription.appendText( \"Produced logs did not meet the expectations.\" );\n\t\tif ( !missingSet.isEmpty() ) {\n\t\t\tdescription.appendText( \"\\nMissing logs:\" );\n\t\t\tfor ( Matcher<?> missing : missingSet ) {\n\t\t\t\tdescription.appendText( \"\\n\\t\" );\n\t\t\t\tmissing.describeTo( description );\n\t\t\t}\n\t\t}\n\t\tif ( !unexpectedEvents.isEmpty() ) {\n\t\t\tdescription.appendText( \"\\nUnexpected logs:\" );\n\t\t\tfor ( LoggingEvent unexpected : unexpectedEvents ) {\n\t\t\t\tdescription.appendText( \"\\n\\t\" );\n\t\t\t\tdescription.appendText( unexpected.getRenderedMessage() );\n\t\t\t}\n\t\t}\n\t\treturn description.toString();\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "private Matcher<LoggingEvent> eventLevelMatcher(Level level) {\n\t\treturn new TypeSafeMatcher<LoggingEvent>() {\n\t\t\t@Override\n\t\t\tpublic void describeTo(Description description) {\n\t\t\t\tdescription.appendText( \"a LoggingEvent with \" ).appendValue( level ).appendText( \" level or higher\" );\n\t\t\t}\n\t\t\t@Override\n\t\t\tprotected boolean matchesSafely(LoggingEvent item) {\n\t\t\t\treturn item.getLevel().isGreaterOrEqual( level );\n\t\t\t}\n\t\t};\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.util.impl.test.rule.ExpectedLog4jLog#expectEventMissing\n methodBody: public void expectEventMissing(Matcher<? extends LoggingEvent> matcher) {\nabsenceExpectations.add(matcher);\n}"
        ],
        "sourceCodeAfterRefactoring": "/**\n\t * Verify that your code <strong>doesn't</strong> produce a log event matching the given level or higher.\n\t */\n\tpublic void expectLevelMissing(Level level) {\n\t\texpectEventMissing( eventLevelMatcher( level ) );\n\t}\nprivate Matcher<LoggingEvent> eventLevelMatcher(Level level) {\n\t\treturn new TypeSafeMatcher<LoggingEvent>() {\n\t\t\t@Override\n\t\t\tpublic void describeTo(Description description) {\n\t\t\t\tdescription.appendText( \"a LoggingEvent with \" ).appendValue( level ).appendText( \" level or higher\" );\n\t\t\t}\n\t\t\t@Override\n\t\t\tprotected boolean matchesSafely(LoggingEvent item) {\n\t\t\t\treturn item.getLevel().isGreaterOrEqual( level );\n\t\t\t}\n\t\t};\n\t}",
        "diffSourceCode": "-   69: \t/**\n-   70: \t * Verify that your code <strong>doesn't</strong> produce a log event matching the given level or higher.\n-   71: \t */\n-   72: \tpublic void expectLevelMissing(Level level) {\n-   73: \t\texpectEventMissing( new TypeSafeMatcher<LoggingEvent>() {\n-   74: \t\t\t@Override\n-   75: \t\t\tpublic void describeTo(Description description) {\n-   76: \t\t\t\tdescription.appendText( \"a LoggingEvent with \" ).appendValue( level ).appendText( \" level or higher\" );\n-   77: \t\t\t}\n-   78: \t\t\t@Override\n-   79: \t\t\tprotected boolean matchesSafely(LoggingEvent item) {\n-   80: \t\t\t\treturn item.getLevel().isGreaterOrEqual( level );\n-   81: \t\t\t}\n-   82: \t\t} );\n-   83: \t}\n-   84: \n-   85: \t/**\n-   86: \t * Verify that your code produces a log message containing the given string.\n-   87: \t */\n-   88: \tpublic void expectMessage(String containedString) {\n-   89: \t\texpectMessage( CoreMatchers.containsString( containedString ) );\n-  142: \t\t\t\tmessageMatcher.describeTo( description );\n-  143: \t\t\t}\n-  144: \n-  145: \t\t\t@Override\n-  146: \t\t\tprotected boolean matchesSafely(LoggingEvent item) {\n-  147: \t\t\t\treturn messageMatcher.matches( item.getMessage() );\n-  148: \t\t\t}\n-  149: \t\t};\n-  150: \t}\n-  151: \n-  152: \tprivate class TestAppender extends AppenderSkeleton {\n-  153: \t\tprivate final Set<Matcher<?>> expectationsMet = new HashSet<>();\n+   69: \t\t\tString containedString, String... otherContainedStrings) {\n+   70: \t\texpectEvent( CoreMatchers.allOf(\n+   71: \t\t\t\teventLevelMatcher( level ),\n+   72: \t\t\t\teventThrowableMatcher( throwableMatcher ),\n+   73: \t\t\t\teventMessageMatcher( containsAllStrings( containedString, otherContainedStrings ) )\n+   74: \t\t) );\n+   75: \t}\n+   76: \n+   77: \t/**\n+   78: \t * Verify that your code <strong>doesn't</strong> produce a log event matching the given matcher.\n+   79: \t */\n+   80: \tpublic void expectEventMissing(Matcher<? extends LoggingEvent> matcher) {\n+   81: \t\tabsenceExpectations.add( matcher );\n+   82: \t}\n+   83: \n+   84: \t/**\n+   85: \t * Verify that your code <strong>doesn't</strong> produce a log event matching the given level or higher.\n+   86: \t */\n+   87: \tpublic void expectLevelMissing(Level level) {\n+   88: \t\texpectEventMissing( eventLevelMatcher( level ) );\n+   89: \t}\n+  142: \tprivate Matcher<LoggingEvent> eventLevelMatcher(Level level) {\n+  143: \t\treturn new TypeSafeMatcher<LoggingEvent>() {\n+  144: \t\t\t@Override\n+  145: \t\t\tpublic void describeTo(Description description) {\n+  146: \t\t\t\tdescription.appendText( \"a LoggingEvent with \" ).appendValue( level ).appendText( \" level or higher\" );\n+  147: \t\t\t}\n+  148: \t\t\t@Override\n+  149: \t\t\tprotected boolean matchesSafely(LoggingEvent item) {\n+  150: \t\t\t\treturn item.getLevel().isGreaterOrEqual( level );\n+  151: \t\t\t}\n+  152: \t\t};\n+  153: \t}\n",
        "uniqueId": "bd746f6ff6562a850ee95384fb393c2ca9154c47_69_83_142_153_84_89",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8
    },
    {
        "type": "Inline Method",
        "description": "Inline Method\tprivate name(name String) : void inlined to private startEntry(name String, containedStructureType StructureType) : void in class org.hibernate.search.util.impl.common.ToStringTreeBuilder",
        "diffLocations": [
            {
                "filePath": "util/internal/common/src/main/java/org/hibernate/search/util/impl/common/ToStringTreeBuilder.java",
                "startLine": 92,
                "endLine": 100,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/common/src/main/java/org/hibernate/search/util/impl/common/ToStringTreeBuilder.java",
                "startLine": 89,
                "endLine": 125,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/common/src/main/java/org/hibernate/search/util/impl/common/ToStringTreeBuilder.java",
                "startLine": 128,
                "endLine": 133,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void name(String name) {\n\t\tif ( name != null && !name.isEmpty() ) {\n\t\t\tbuilder.append( name );\n\t\t\tbuilder.append( style.nameValueSeparator );\n\t\t}\n\t}",
        "filePathBefore": "util/internal/common/src/main/java/org/hibernate/search/util/impl/common/ToStringTreeBuilder.java",
        "isPureRefactoring": true,
        "commitId": "7983b2a0b33ed0624055c006c73a07ae489a7667",
        "packageNameBefore": "org.hibernate.search.util.impl.common",
        "classNameBefore": "org.hibernate.search.util.impl.common.ToStringTreeBuilder",
        "methodNameBefore": "org.hibernate.search.util.impl.common.ToStringTreeBuilder#name",
        "classSignatureBefore": "public class ToStringTreeBuilder ",
        "methodNameBeforeSet": [
            "org.hibernate.search.util.impl.common.ToStringTreeBuilder#name"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.util.impl.common.ToStringTreeBuilder"
        ],
        "classSignatureBeforeSet": [
            "public class ToStringTreeBuilder "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.common;\n\nimport org.hibernate.search.util.AssertionFailure;\n\npublic class ToStringTreeBuilder {\n\n\tprivate final StringBuilder builder = new StringBuilder();\n\tprivate final ToStringStyle style;\n\n\tprivate boolean first = true;\n\tprivate int depth = 0;\n\n\tpublic ToStringTreeBuilder() {\n\t\tthis( ToStringStyle.inline() );\n\t}\n\n\tpublic ToStringTreeBuilder(ToStringStyle style) {\n\t\tthis.style = style;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn builder.toString();\n\t}\n\n\tpublic ToStringTreeBuilder attribute(String name, Object value) {\n\t\tif ( value instanceof ToStringTreeAppendable ) {\n\t\t\tattribute( name, (ToStringTreeAppendable) value );\n\t\t}\n\t\telse {\n\t\t\tbeforeElement();\n\t\t\tname( name );\n\t\t\tbuilder.append( value );\n\t\t}\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder attribute(String name, ToStringTreeAppendable value) {\n\t\tif ( value != null ) {\n\t\t\tstartObject( name );\n\t\t\tvalue.appendTo( this );\n\t\t\tendObject();\n\t\t}\n\t\telse {\n\t\t\tattribute( name, (Object) null );\n\t\t}\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder value(Object value) {\n\t\treturn attribute( null, value );\n\t}\n\n\tpublic ToStringTreeBuilder startObject() {\n\t\treturn startObject( null );\n\t}\n\n\tpublic ToStringTreeBuilder startObject(String name) {\n\t\tbeforeElement();\n\t\tname( name );\n\t\tbuilder.append( style.startObject );\n\t\t++depth;\n\t\tfirst = true;\n\t\tappendNewline();\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder endObject() {\n\t\tif ( depth == 0 ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, already at root\" );\n\t\t}\n\t\t--depth;\n\t\tif ( !first ) {\n\t\t\tappendNewline();\n\t\t}\n\t\tfirst = false;\n\t\tappendIndent();\n\t\tbuilder.append( style.endObject );\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder startList() {\n\t\treturn startList( null );\n\t}\n\n\tpublic ToStringTreeBuilder startList(String name) {\n\t\tbeforeElement();\n\t\tname( name );\n\t\tbuilder.append( style.startList );\n\t\t++depth;\n\t\tfirst = true;\n\t\tappendNewline();\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder endList() {\n\t\tif ( depth == 0 ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, already at root\" );\n\t\t}\n\t\t--depth;\n\t\tif ( !first ) {\n\t\t\tappendNewline();\n\t\t}\n\t\tfirst = false;\n\t\tappendIndent();\n\t\tbuilder.append( style.endList );\n\t\treturn this;\n\t}\n\n\tprivate void beforeElement() {\n\t\tif ( first ) {\n\t\t\tfirst = false;\n\t\t\tappendIndent();\n\t\t}\n\t\telse {\n\t\t\tbuilder.append( style.entrySeparator );\n\t\t\tappendNewline();\n\t\t\tappendIndent();\n\t\t}\n\t}\n\n\tprivate void name(String name) {\n\t\tif ( name != null && !name.isEmpty() ) {\n\t\t\tbuilder.append( name );\n\t\t\tbuilder.append( style.nameValueSeparator );\n\t\t}\n\t}\n\n\tprivate void appendNewline() {\n\t\tbuilder.append( style.newline );\n\t}\n\n\tprivate void appendIndent() {\n\t\tif ( !style.indent.isEmpty() ) {\n\t\t\tfor ( int i = 0; i < depth; ++i ) {\n\t\t\t\tbuilder.append( style.indent );\n\t\t\t}\n\t\t}\n\t}\n\n}\n",
        "filePathAfter": "util/internal/common/src/main/java/org/hibernate/search/util/impl/common/ToStringTreeBuilder.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.common;\n\nimport java.util.ArrayDeque;\nimport java.util.Deque;\nimport java.util.Iterator;\n\nimport org.hibernate.search.util.AssertionFailure;\n\npublic class ToStringTreeBuilder {\n\n\tprivate final ToStringStyle style;\n\tprivate final StringBuilder builder = new StringBuilder();\n\n\tprivate final Deque<StructureType> structureTypeStack = new ArrayDeque<>();\n\tprivate boolean first = true;\n\n\tpublic ToStringTreeBuilder() {\n\t\tthis( ToStringStyle.inlineDelimiterStructure() );\n\t}\n\n\tpublic ToStringTreeBuilder(ToStringStyle style) {\n\t\tthis.style = style;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn builder.toString();\n\t}\n\n\tpublic ToStringTreeBuilder attribute(String name, Object value) {\n\t\tif ( value instanceof ToStringTreeAppendable ) {\n\t\t\tToStringTreeAppendable appendable = ( (ToStringTreeAppendable) value );\n\t\t\tstartEntry( name, StructureType.OBJECT );\n\t\t\tstartStructure( StructureType.OBJECT, style.startObject );\n\t\t\tappendable.appendTo( this );\n\t\t\tendStructure( StructureType.OBJECT, style.endObject );\n\t\t\tendEntry();\n\t\t}\n\t\telse {\n\t\t\tstartEntry( name, null );\n\t\t\tbuilder.append( value );\n\t\t\tendEntry();\n\t\t}\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder value(Object value) {\n\t\treturn attribute( null, value );\n\t}\n\n\tpublic ToStringTreeBuilder startObject() {\n\t\treturn startObject( null );\n\t}\n\n\tpublic ToStringTreeBuilder startObject(String name) {\n\t\tstartEntry( name, StructureType.OBJECT );\n\t\tstartStructure( StructureType.OBJECT, style.startObject );\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder endObject() {\n\t\tendStructure( StructureType.OBJECT, style.endObject );\n\t\tendEntry();\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder startList() {\n\t\treturn startList( null );\n\t}\n\n\tpublic ToStringTreeBuilder startList(String name) {\n\t\tstartEntry( name, StructureType.LIST );\n\t\tstartStructure( StructureType.LIST, style.startList );\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder endList() {\n\t\tendStructure( StructureType.LIST, style.endList );\n\t\tendEntry();\n\t\treturn this;\n\t}\n\n\tprivate void startEntry(String name, StructureType containedStructureType) {\n\t\tif ( !first ) {\n\t\t\tbuilder.append( style.entrySeparator );\n\t\t}\n\n\t\tStructureType entryType =\n\t\t\t\tStringHelper.isEmpty( name ) ? StructureType.UNNAMED_ENTRY : StructureType.NAMED_ENTRY;\n\n\t\t// Add a new line\n\t\tif (\n\t\t\t\t// ... except for the very first element at the root\n\t\t\t\t!( first && structureTypeStack.isEmpty() )\n\t\t\t\t// ... or for entries containing a squeezed structure\n\t\t\t\t&& !shouldSqueeze( containedStructureType, entryType, structureTypeStack.peek() )\n\t\t\t\t// ... or for structures without a name nor a start delimiter\n\t\t\t\t&& !(\n\t\t\t\t\t\tStructureType.UNNAMED_ENTRY.equals( entryType )\n\t\t\t\t\t\t&& StructureType.OBJECT.equals( containedStructureType )\n\t\t\t\t\t\t&& StringHelper.isEmpty( style.startObject )\n\t\t\t\t)\n\t\t\t\t&& !(\n\t\t\t\t\t\tStructureType.UNNAMED_ENTRY.equals( entryType )\n\t\t\t\t\t\t&& StructureType.LIST.equals( containedStructureType )\n\t\t\t\t\t\t&& StringHelper.isEmpty( style.startList )\n\t\t\t\t)\n\t\t) {\n\t\t\tappendNewline();\n\t\t\tappendIndent();\n\t\t}\n\n\t\tif ( StringHelper.isNotEmpty( name ) ) {\n\t\t\tbuilder.append( name );\n\t\t\tbuilder.append( style.nameValueSeparator );\n\t\t}\n\n\t\tstructureTypeStack.push( entryType );\n\t}\n\n\tprivate void endEntry() {\n\t\tStructureType lastType = structureTypeStack.peek();\n\t\tif ( lastType == null ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, already at root\" );\n\t\t}\n\t\telse if ( !StructureType.UNNAMED_ENTRY.equals( lastType )\n\t\t\t\t&& !StructureType.NAMED_ENTRY.equals( lastType ) ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, not inside an entry\" );\n\t\t}\n\t\tstructureTypeStack.pop();\n\t\tfirst = false;\n\t}\n\n\tprivate void startStructure(StructureType structureType, String startDelimiter) {\n\t\tif ( StringHelper.isNotEmpty( startDelimiter ) ) {\n\t\t\tbuilder.append( startDelimiter );\n\t\t}\n\n\t\tstructureTypeStack.push( structureType );\n\t\tfirst = true;\n\t}\n\n\tprivate void endStructure(StructureType structureType, String endDelimiter) {\n\t\tStructureType lastType = structureTypeStack.peek();\n\t\tif ( lastType == null ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, already at root\" );\n\t\t}\n\t\telse if ( lastType != structureType ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, not inside a \" + structureType );\n\t\t}\n\t\tstructureTypeStack.pop();\n\n\t\tif ( StringHelper.isNotEmpty( endDelimiter ) ) {\n\t\t\tappendNewline();\n\t\t\tappendIndent();\n\t\t\tbuilder.append( endDelimiter );\n\t\t}\n\t\tfirst = false;\n\t}\n\n\tprivate void appendNewline() {\n\t\tbuilder.append( style.newline );\n\t}\n\n\tprivate void appendIndent() {\n\t\tif ( structureTypeStack.isEmpty() ) {\n\t\t\treturn;\n\t\t}\n\n\t\tIterator<StructureType> iterator = structureTypeStack.descendingIterator();\n\t\tStructureType grandParent = null;\n\t\tStructureType parent = null;\n\t\tStructureType current = iterator.next();\n\t\tStructureType child = iterator.hasNext() ? iterator.next() : null;\n\t\tStructureType grandChild;\n\t\twhile ( current != null ) {\n\t\t\tgrandChild = iterator.hasNext() ? iterator.next() : null;\n\t\t\tif ( !shouldSqueeze( current, parent, grandParent ) ) {\n\t\t\t\tswitch ( current ) {\n\t\t\t\t\tcase OBJECT:\n\t\t\t\t\t\tbuilder.append( style.indentInObject );\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase LIST:\n\t\t\t\t\t\t// Display a bullet point if:\n\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t\t// We are adding a element directly to the list\n\t\t\t\t\t\t\t\tchild == null\n\t\t\t\t\t\t\t\t// OR we are adding the first element to a squeezed element in the list\n\t\t\t\t\t\t\t\t|| shouldSqueeze( grandChild, child, current ) && !iterator.hasNext() && first\n\t\t\t\t\t\t) {\n\t\t\t\t\t\t\tbuilder.append( style.indentInListBulletPoint );\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\tbuilder.append( style.indentInListNoBulletPoint );\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tgrandParent = parent;\n\t\t\tparent = current;\n\t\t\tcurrent = child;\n\t\t\tchild = grandChild;\n\t\t}\n\t}\n\n\t/**\n\t * @param structureType The type of the potentially squeezed structure\n\t * @param parentStructureType The type of the closest containing structure\n\t * @param grandParentStructureType The type of the second closest containing structure\n\t * @return {@code true} if the child structure should be squeezed,\n\t * i.e. displayed on the same line as its parent if it's the first element,\n\t * and have its indenting ignored.\n\t */\n\tprivate boolean shouldSqueeze(StructureType structureType, StructureType parentStructureType,\n\t\t\tStructureType grandParentStructureType) {\n\t\treturn style.squeezeObjectsInList\n\t\t\t\t&& StructureType.LIST.equals( grandParentStructureType )\n\t\t\t\t&& StructureType.UNNAMED_ENTRY.equals( parentStructureType )\n\t\t\t\t&& StructureType.OBJECT.equals( structureType );\n\t}\n\n\tprivate enum StructureType {\n\t\tOBJECT,\n\t\tLIST,\n\t\tNAMED_ENTRY,\n\t\tUNNAMED_ENTRY\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "private void startEntry(String name, StructureType containedStructureType) {\n\t\tif ( !first ) {\n\t\t\tbuilder.append( style.entrySeparator );\n\t\t}\n\n\t\tStructureType entryType =\n\t\t\t\tStringHelper.isEmpty( name ) ? StructureType.UNNAMED_ENTRY : StructureType.NAMED_ENTRY;\n\n\t\t// Add a new line\n\t\tif (\n\t\t\t\t// ... except for the very first element at the root\n\t\t\t\t!( first && structureTypeStack.isEmpty() )\n\t\t\t\t// ... or for entries containing a squeezed structure\n\t\t\t\t&& !shouldSqueeze( containedStructureType, entryType, structureTypeStack.peek() )\n\t\t\t\t// ... or for structures without a name nor a start delimiter\n\t\t\t\t&& !(\n\t\t\t\t\t\tStructureType.UNNAMED_ENTRY.equals( entryType )\n\t\t\t\t\t\t&& StructureType.OBJECT.equals( containedStructureType )\n\t\t\t\t\t\t&& StringHelper.isEmpty( style.startObject )\n\t\t\t\t)\n\t\t\t\t&& !(\n\t\t\t\t\t\tStructureType.UNNAMED_ENTRY.equals( entryType )\n\t\t\t\t\t\t&& StructureType.LIST.equals( containedStructureType )\n\t\t\t\t\t\t&& StringHelper.isEmpty( style.startList )\n\t\t\t\t)\n\t\t) {\n\t\t\tappendNewline();\n\t\t\tappendIndent();\n\t\t}\n\n\t\tif ( StringHelper.isNotEmpty( name ) ) {\n\t\t\tbuilder.append( name );\n\t\t\tbuilder.append( style.nameValueSeparator );\n\t\t}\n\n\t\tstructureTypeStack.push( entryType );\n\t}",
        "diffSourceCode": "-   89: \t\treturn startList( null );\n-   90: \t}\n-   91: \n-   92: \tpublic ToStringTreeBuilder startList(String name) {\n-   93: \t\tbeforeElement();\n-   94: \t\tname( name );\n-   95: \t\tbuilder.append( style.startList );\n-   96: \t\t++depth;\n-   97: \t\tfirst = true;\n-   98: \t\tappendNewline();\n-   99: \t\treturn this;\n-  100: \t}\n-  101: \n-  102: \tpublic ToStringTreeBuilder endList() {\n-  103: \t\tif ( depth == 0 ) {\n-  104: \t\t\tthrow new AssertionFailure( \"Cannot pop, already at root\" );\n-  105: \t\t}\n-  106: \t\t--depth;\n-  107: \t\tif ( !first ) {\n-  108: \t\t\tappendNewline();\n-  109: \t\t}\n-  110: \t\tfirst = false;\n-  111: \t\tappendIndent();\n-  112: \t\tbuilder.append( style.endList );\n-  113: \t\treturn this;\n-  114: \t}\n-  115: \n-  116: \tprivate void beforeElement() {\n-  117: \t\tif ( first ) {\n-  118: \t\t\tfirst = false;\n-  119: \t\t\tappendIndent();\n-  120: \t\t}\n-  121: \t\telse {\n-  122: \t\t\tbuilder.append( style.entrySeparator );\n-  123: \t\t\tappendNewline();\n-  124: \t\t\tappendIndent();\n-  125: \t\t}\n-  128: \tprivate void name(String name) {\n-  129: \t\tif ( name != null && !name.isEmpty() ) {\n-  130: \t\t\tbuilder.append( name );\n-  131: \t\t\tbuilder.append( style.nameValueSeparator );\n-  132: \t\t}\n-  133: \t}\n+   89: \tprivate void startEntry(String name, StructureType containedStructureType) {\n+   90: \t\tif ( !first ) {\n+   91: \t\t\tbuilder.append( style.entrySeparator );\n+   92: \t\t}\n+   93: \n+   94: \t\tStructureType entryType =\n+   95: \t\t\t\tStringHelper.isEmpty( name ) ? StructureType.UNNAMED_ENTRY : StructureType.NAMED_ENTRY;\n+   96: \n+   97: \t\t// Add a new line\n+   98: \t\tif (\n+   99: \t\t\t\t// ... except for the very first element at the root\n+  100: \t\t\t\t!( first && structureTypeStack.isEmpty() )\n+  101: \t\t\t\t// ... or for entries containing a squeezed structure\n+  102: \t\t\t\t&& !shouldSqueeze( containedStructureType, entryType, structureTypeStack.peek() )\n+  103: \t\t\t\t// ... or for structures without a name nor a start delimiter\n+  104: \t\t\t\t&& !(\n+  105: \t\t\t\t\t\tStructureType.UNNAMED_ENTRY.equals( entryType )\n+  106: \t\t\t\t\t\t&& StructureType.OBJECT.equals( containedStructureType )\n+  107: \t\t\t\t\t\t&& StringHelper.isEmpty( style.startObject )\n+  108: \t\t\t\t)\n+  109: \t\t\t\t&& !(\n+  110: \t\t\t\t\t\tStructureType.UNNAMED_ENTRY.equals( entryType )\n+  111: \t\t\t\t\t\t&& StructureType.LIST.equals( containedStructureType )\n+  112: \t\t\t\t\t\t&& StringHelper.isEmpty( style.startList )\n+  113: \t\t\t\t)\n+  114: \t\t) {\n+  115: \t\t\tappendNewline();\n+  116: \t\t\tappendIndent();\n+  117: \t\t}\n+  118: \n+  119: \t\tif ( StringHelper.isNotEmpty( name ) ) {\n+  120: \t\t\tbuilder.append( name );\n+  121: \t\t\tbuilder.append( style.nameValueSeparator );\n+  122: \t\t}\n+  123: \n+  124: \t\tstructureTypeStack.push( entryType );\n+  125: \t}\n+  128: \t\tStructureType lastType = structureTypeStack.peek();\n+  129: \t\tif ( lastType == null ) {\n+  130: \t\t\tthrow new AssertionFailure( \"Cannot pop, already at root\" );\n+  131: \t\t}\n+  132: \t\telse if ( !StructureType.UNNAMED_ENTRY.equals( lastType )\n+  133: \t\t\t\t&& !StructureType.NAMED_ENTRY.equals( lastType ) ) {\n",
        "uniqueId": "7983b2a0b33ed0624055c006c73a07ae489a7667_92_100__89_125_128_133",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 18
            },
            "BRANCH": {
                "missed": 0,
                "covered": 4
            },
            "LINE": {
                "missed": 0,
                "covered": 4
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 3
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tpublic isDefaultExtractorPath(introspector PojoBootstrapIntrospector, sourceType PojoGenericTypeModel<?>, extractorPath ContainerValueExtractorPath) : boolean extracted from private isDefaultExtractorPath(propertyModel PojoPropertyModel<?>, originalSideBoundExtractorPath BoundContainerValueExtractorPath<?,?>) : boolean in class org.hibernate.search.mapper.pojo.dirtiness.building.impl.PojoAssociationPathInverter & moved to class org.hibernate.search.mapper.pojo.extractor.impl.ContainerValueExtractorBinder",
        "diffLocations": [
            {
                "filePath": "mapper/pojo/src/main/java/org/hibernate/search/mapper/pojo/dirtiness/building/impl/PojoAssociationPathInverter.java",
                "startLine": 132,
                "endLine": 142,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/pojo/src/main/java/org/hibernate/search/mapper/pojo/dirtiness/building/impl/PojoAssociationPathInverter.java",
                "startLine": 132,
                "endLine": 139,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/pojo/src/main/java/org/hibernate/search/mapper/pojo/dirtiness/building/impl/PojoAssociationPathInverter.java",
                "startLine": 214,
                "endLine": 224,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private boolean isDefaultExtractorPath(PojoPropertyModel<?> propertyModel,\n\t\t\tBoundContainerValueExtractorPath<?, ?> originalSideBoundExtractorPath) {\n\t\tOptional<? extends BoundContainerValueExtractorPath<?, ?>> boundDefaultExtractorPathOptional = extractorBinder\n\t\t\t\t.tryBindPath(\n\t\t\t\t\t\tintrospector, propertyModel.getTypeModel(),\n\t\t\t\t\t\tContainerValueExtractorPath.defaultExtractors()\n\t\t\t\t);\n\t\treturn boundDefaultExtractorPathOptional.isPresent() && originalSideBoundExtractorPath.getExtractorPath().equals(\n\t\t\t\tboundDefaultExtractorPathOptional.get().getExtractorPath()\n\t\t);\n\t}",
        "filePathBefore": "mapper/pojo/src/main/java/org/hibernate/search/mapper/pojo/dirtiness/building/impl/PojoAssociationPathInverter.java",
        "isPureRefactoring": true,
        "commitId": "b369ab192fd1c80ab666c63e65224bf90f658292",
        "packageNameBefore": "org.hibernate.search.mapper.pojo.dirtiness.building.impl",
        "classNameBefore": "org.hibernate.search.mapper.pojo.dirtiness.building.impl.PojoAssociationPathInverter",
        "methodNameBefore": "org.hibernate.search.mapper.pojo.dirtiness.building.impl.PojoAssociationPathInverter#isDefaultExtractorPath",
        "invokedMethod": "methodSignature: org.hibernate.search.mapper.pojo.dirtiness.building.impl.PojoIndexingDependencyCollectorTypeNode#getTypeModel\n methodBody: PojoTypeModel<T> getTypeModel() {\nreturn modelPathFromRootEntityNode.getTypeModel();\n}\nmethodSignature: org.hibernate.search.mapper.pojo.extractor.impl.ContainerValueExtractorBinder#tryBindPath\n methodBody: public <C> Optional<BoundContainerValueExtractorPath<C, ?>> tryBindPath(\n\t\t\tPojoBootstrapIntrospector introspector, PojoGenericTypeModel<C> sourceType,\n\t\t\tContainerValueExtractorPath extractorPath) {\nExtractorResolutionState<C> state=new ExtractorResolutionState<>(introspector,sourceType);\nif(extractorPath.isDefault()){firstMatchingExtractorContributor.tryAppend(state);\n}{for(Class<? extends ContainerValueExtractor> extractorClass: extractorPath.getExplicitExtractorClasses()){ExtractorContributor extractorContributor=getExtractorContributorForClass(extractorClass);\nif(!extractorContributor.tryAppend(state)){return Optional.empty();\n}}}return Optional.of(state.build());\n}\nmethodSignature: org.hibernate.search.mapper.pojo.mapping.definition.annotation.impl.AnnotationPojoTypeMetadataContributorImpl#getExtractorPath\n methodBody: private ContainerValueExtractorPath getExtractorPath(\n\t\t\tContainerValueExtractorBeanReference[] extractors, Class<?> defaultExtractorsClass) {\nif(extractors.length == 0){return ContainerValueExtractorPath.noExtractors();\n}if(extractors.length == 1 && defaultExtractorsClass.equals(extractors[0].type())){return ContainerValueExtractorPath.defaultExtractors();\n}{return ContainerValueExtractorPath.explicitExtractors(Arrays.stream(extractors).map(ContainerValueExtractorBeanReference::type).collect(Collectors.toList()));\n}}",
        "classSignatureBefore": "public final class PojoAssociationPathInverter ",
        "methodNameBeforeSet": [
            "org.hibernate.search.mapper.pojo.dirtiness.building.impl.PojoAssociationPathInverter#isDefaultExtractorPath"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.mapper.pojo.dirtiness.building.impl.PojoAssociationPathInverter"
        ],
        "classSignatureBeforeSet": [
            "public final class PojoAssociationPathInverter "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.pojo.dirtiness.building.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.HashSet;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.util.ListIterator;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.Set;\n\nimport org.hibernate.search.mapper.pojo.extractor.ContainerValueExtractorPath;\nimport org.hibernate.search.mapper.pojo.extractor.impl.BoundContainerValueExtractorPath;\nimport org.hibernate.search.mapper.pojo.extractor.impl.ContainerValueExtractorBinder;\nimport org.hibernate.search.mapper.pojo.logging.impl.Log;\nimport org.hibernate.search.mapper.pojo.model.additionalmetadata.building.impl.PojoTypeAdditionalMetadataProvider;\nimport org.hibernate.search.mapper.pojo.model.additionalmetadata.impl.PojoPropertyAdditionalMetadata;\nimport org.hibernate.search.mapper.pojo.model.additionalmetadata.impl.PojoTypeAdditionalMetadata;\nimport org.hibernate.search.mapper.pojo.model.additionalmetadata.impl.PojoValueAdditionalMetadata;\nimport org.hibernate.search.mapper.pojo.model.path.PojoModelPath;\nimport org.hibernate.search.mapper.pojo.model.path.PojoModelPathPropertyNode;\nimport org.hibernate.search.mapper.pojo.model.path.PojoModelPathValueNode;\nimport org.hibernate.search.mapper.pojo.model.path.impl.BoundPojoModelPathPropertyNode;\nimport org.hibernate.search.mapper.pojo.model.path.impl.BoundPojoModelPathTypeNode;\nimport org.hibernate.search.mapper.pojo.model.path.impl.BoundPojoModelPathValueNode;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoBootstrapIntrospector;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoPropertyModel;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeModel;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoTypeModel;\nimport org.hibernate.search.mapper.pojo.model.spi.PropertyHandle;\nimport org.hibernate.search.util.impl.common.LoggerFactory;\n\n/**\n * An object responsible for inverting an association path,\n * i.e. a chain of properties and container value extractors going from one entity to another.\n */\npublic final class PojoAssociationPathInverter {\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate final PojoTypeAdditionalMetadataProvider typeAdditionalMetadataProvider;\n\tprivate final PojoBootstrapIntrospector introspector;\n\tprivate final ContainerValueExtractorBinder extractorBinder;\n\n\tpublic PojoAssociationPathInverter(PojoTypeAdditionalMetadataProvider typeAdditionalMetadataProvider,\n\t\t\tPojoBootstrapIntrospector introspector,\n\t\t\tContainerValueExtractorBinder extractorBinder) {\n\t\tthis.typeAdditionalMetadataProvider = typeAdditionalMetadataProvider;\n\t\tthis.introspector = introspector;\n\t\tthis.extractorBinder = extractorBinder;\n\t}\n\n\tpublic Optional<PojoModelPathValueNode> invertPath(PojoTypeModel<?> inverseSideEntityType,\n\t\t\tBoundPojoModelPathValueNode<?, ?, ?> pathToInvert) {\n\t\tPojoRawTypeModel<?> originalSideEntityType = pathToInvert.getRootType().getRawType();\n\n\t\t// Try to find inverse side information hosted on the side to inverse\n\t\tOptional<PojoModelPathValueNode> inverseSidePathOptional =\n\t\t\t\tfindInverseSidePathFromOriginalSide( pathToInvert );\n\n\t\tif ( !inverseSidePathOptional.isPresent() ) {\n\t\t\tList<PojoModelPathValueNode> associationPathsToMatch = computeAssociationPathsToMatch( pathToInvert );\n\t\t\t// Try to find inverse side information hosted on the other side\n\t\t\tinverseSidePathOptional = findInverseSidePathFromInverseSide(\n\t\t\t\t\tinverseSideEntityType, originalSideEntityType, associationPathsToMatch\n\t\t\t);\n\t\t}\n\n\t\treturn inverseSidePathOptional;\n\t}\n\n\t/*\n\t * One might refer to the extractor path of an association in multiple ways:\n\t * - By intension, e.g. ContainerValueExtractorPath.default()\n\t * - By extension, e.g. ContainerValueExtractorPath.noExtractors()\n\t *   or ContainerValueExtractorPath.explicitExtractors( ... )\n\t * We want to match any type of reference, so we have to determine whether this association\n\t * uses the default extractor path, and if so, add it to the list of paths to match.\n\t */\n\tprivate List<PojoModelPathValueNode> computeAssociationPathsToMatch(\n\t\t\tBoundPojoModelPathValueNode<?, ?, ?> boundPathToInvert) {\n\t\t// We're potentially performing lots of insertions, so let's use a LinkedList\n\t\tList<PojoModelPathValueNode> associationPathsToMatch = new LinkedList<>();\n\t\tcollectAssociationPathsToMatch( associationPathsToMatch, boundPathToInvert );\n\t\treturn associationPathsToMatch;\n\t}\n\n\tprivate void collectAssociationPathsToMatch(\n\t\t\tList<PojoModelPathValueNode> associationPathsToMatch,\n\t\t\tBoundPojoModelPathValueNode<?, ?, ?> boundPathToInvert) {\n\t\tBoundPojoModelPathPropertyNode<?, ?> parentPath = boundPathToInvert.getParent();\n\t\tBoundPojoModelPathValueNode<?, ?, ?> parentValuePath = parentPath.getParent().getParent();\n\t\tString propertyName = parentPath.getPropertyHandle().getName();\n\t\tContainerValueExtractorPath extractorPath = boundPathToInvert.getExtractorPath();\n\t\tboolean isDefaultExtractorPath = isDefaultExtractorPath(\n\t\t\t\tparentPath.getPropertyModel(), boundPathToInvert.getBoundExtractorPath()\n\t\t);\n\t\tif ( parentValuePath != null ) {\n\t\t\tcollectAssociationPathsToMatch( associationPathsToMatch, parentValuePath );\n\t\t\tListIterator<PojoModelPathValueNode> iterator = associationPathsToMatch.listIterator();\n\t\t\twhile ( iterator.hasNext() ) {\n\t\t\t\tPojoModelPathValueNode baseValuePath = iterator.next();\n\t\t\t\tPojoModelPathPropertyNode basePropertyPath = baseValuePath.property( propertyName );\n\t\t\t\t// Append the property and extractor path to the already-collected paths\n\t\t\t\titerator.set( basePropertyPath.value( extractorPath ) );\n\t\t\t\tif ( isDefaultExtractorPath ) {\n\t\t\t\t\t/*\n\t\t\t\t\t * If the current extractor path (which is explicit) represents the default path,\n\t\t\t\t\t * then for each already collected path, add one version using the explicit representation,\n\t\t\t\t\t * and one version using the implicit representation.\n\t\t\t\t\t */\n\t\t\t\t\titerator.add( basePropertyPath.value( ContainerValueExtractorPath.defaultExtractors() ) );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t// We reached the root: collect the first paths\n\t\t\tPojoModelPathPropertyNode basePropertyPath = PojoModelPath.fromRoot( propertyName );\n\t\t\tassociationPathsToMatch.add( basePropertyPath.value( extractorPath ) );\n\t\t\tif ( isDefaultExtractorPath ) {\n\t\t\t\t// The may be two versions of this path, similarly to what we do above\n\t\t\t\tassociationPathsToMatch.add( basePropertyPath.value( ContainerValueExtractorPath.defaultExtractors() ) );\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate boolean isDefaultExtractorPath(PojoPropertyModel<?> propertyModel,\n\t\t\tBoundContainerValueExtractorPath<?, ?> originalSideBoundExtractorPath) {\n\t\tOptional<? extends BoundContainerValueExtractorPath<?, ?>> boundDefaultExtractorPathOptional = extractorBinder\n\t\t\t\t.tryBindPath(\n\t\t\t\t\t\tintrospector, propertyModel.getTypeModel(),\n\t\t\t\t\t\tContainerValueExtractorPath.defaultExtractors()\n\t\t\t\t);\n\t\treturn boundDefaultExtractorPathOptional.isPresent() && originalSideBoundExtractorPath.getExtractorPath().equals(\n\t\t\t\tboundDefaultExtractorPathOptional.get().getExtractorPath()\n\t\t);\n\t}\n\n\tprivate Optional<PojoModelPathValueNode> findInverseSidePathFromOriginalSide(\n\t\t\tBoundPojoModelPathValueNode<?, ?, ?> pathToInvert) {\n\t\tBoundPojoModelPathPropertyNode<?, ?> lastPropertyNode = pathToInvert.getParent();\n\t\tBoundPojoModelPathTypeNode<?> lastTypeNode = lastPropertyNode.getParent();\n\t\tPojoPropertyModel<?> lastPropertyModel = lastPropertyNode.getPropertyModel();\n\t\tPojoTypeModel<?> lastTypeModel = lastTypeNode.getTypeModel();\n\n\t\tPojoTypeAdditionalMetadata typeAdditionalMetadata =\n\t\t\t\ttypeAdditionalMetadataProvider.get( lastTypeModel.getRawType() );\n\t\tPojoPropertyAdditionalMetadata propertyAdditionalMetadata =\n\t\t\t\ttypeAdditionalMetadata.getPropertyAdditionalMetadata( lastPropertyNode.getPropertyModel().getName() );\n\n\t\t// First try to query the additional metadata with the explicit extractor path\n\t\tOptional<PojoModelPathValueNode> result = propertyAdditionalMetadata.getValueAdditionalMetadata( pathToInvert.getExtractorPath() )\n\t\t\t\t.getInverseSidePath();\n\t\tif ( result.isPresent() ) {\n\t\t\treturn result;\n\t\t}\n\n\t\tif ( isDefaultExtractorPath( lastPropertyModel, pathToInvert.getBoundExtractorPath() ) ) {\n\t\t\t/*\n\t\t\t * Since the extractor path was the default one, try to query the additional metadata\n\t\t\t * with the implicit default extractor path.\n\t\t\t */\n\t\t\tresult = propertyAdditionalMetadata.getValueAdditionalMetadata( ContainerValueExtractorPath.defaultExtractors() )\n\t\t\t\t\t.getInverseSidePath();\n\t\t}\n\n\t\treturn result;\n\t}\n\n\tprivate Optional<PojoModelPathValueNode> findInverseSidePathFromInverseSide(\n\t\t\tPojoTypeModel<?> inverseSideTypeModel,\n\t\t\tPojoRawTypeModel<?> originalSideEntityType,\n\t\t\tList<PojoModelPathValueNode> associationPathsToMatch) {\n\t\tBoundPojoModelPathTypeNode<?> inverseSidePathTypeNode = BoundPojoModelPathTypeNode.root( inverseSideTypeModel );\n\t\tSet<PojoRawTypeModel<?>> encounteredAssociationHoldingTypes = new HashSet<>();\n\t\treturn findInverseSidePathFromInverseSideRecursive(\n\t\t\t\tinverseSidePathTypeNode, originalSideEntityType, associationPathsToMatch,\n\t\t\t\tencounteredAssociationHoldingTypes\n\t\t);\n\t}\n\n\tprivate Optional<PojoModelPathValueNode> findInverseSidePathFromInverseSideRecursive(\n\t\t\tBoundPojoModelPathTypeNode<?> inverseSidePathTypeNode,\n\t\t\tPojoRawTypeModel<?> originalSideEntityType,\n\t\t\tList<PojoModelPathValueNode> associationPathsToMatch,\n\t\t\tSet<PojoRawTypeModel<?>> encounteredAssociationHoldingTypes) {\n\t\tPojoTypeModel<?> inverseSideTypeModel = inverseSidePathTypeNode.getTypeModel();\n\t\tPojoTypeAdditionalMetadata inverseSideTypeAdditionalMetadata =\n\t\t\t\ttypeAdditionalMetadataProvider.get( inverseSideTypeModel.getRawType() );\n\n\t\tfor ( Map.Entry<String, PojoPropertyAdditionalMetadata> propertyEntry :\n\t\t\t\tinverseSideTypeAdditionalMetadata.getPropertiesAdditionalMetadata().entrySet() ) {\n\t\t\tString inverseSidePropertyName = propertyEntry.getKey();\n\t\t\tPojoPropertyModel<?> inverseSidePropertyModel = inverseSideTypeModel.getProperty( inverseSidePropertyName );\n\t\t\tPropertyHandle propertyHandle = inverseSidePropertyModel.getHandle();\n\t\t\tBoundPojoModelPathPropertyNode<?, ?> inverseSidePathPropertyNode =\n\t\t\t\t\tinverseSidePathTypeNode.property( propertyHandle );\n\t\t\tPojoPropertyAdditionalMetadata inverseSidePropertyAdditionalMetadata = propertyEntry.getValue();\n\n\t\t\tfor ( Map.Entry<ContainerValueExtractorPath, PojoValueAdditionalMetadata> valueEntry :\n\t\t\t\t\tinverseSidePropertyAdditionalMetadata.getValuesAdditionalMetadata().entrySet() ) {\n\t\t\t\tContainerValueExtractorPath inverseSideExtractorPath = valueEntry.getKey();\n\t\t\t\tBoundPojoModelPathValueNode<?, ?, ?> inverseSidePathValueNode =\n\t\t\t\t\t\tbindExtractors( inverseSidePathPropertyNode, inverseSideExtractorPath );\n\t\t\t\tPojoValueAdditionalMetadata inverseSideValueAdditionalMetadata = valueEntry.getValue();\n\n\t\t\t\tOptional<PojoModelPathValueNode> inverseSidePathOptional =\n\t\t\t\t\t\tfindInverseSidePathFromInverseSideValueRecursive(\n\t\t\t\t\t\t\t\toriginalSideEntityType, associationPathsToMatch,\n\t\t\t\t\t\t\t\tinverseSidePathValueNode, inverseSideValueAdditionalMetadata,\n\t\t\t\t\t\t\t\tencounteredAssociationHoldingTypes\n\t\t\t\t\t\t);\n\n\t\t\t\tif ( inverseSidePathOptional.isPresent() ) {\n\t\t\t\t\treturn inverseSidePathOptional;\n\t\t\t\t}\n\t\t\t\t// else: continue the loop, maybe we'll find the inverse path elsewhere.\n\t\t\t}\n\t\t}\n\n\t\treturn Optional.empty();\n\t}\n\n\tprivate Optional<PojoModelPathValueNode> findInverseSidePathFromInverseSideValueRecursive(\n\t\t\tPojoRawTypeModel<?> originalSideEntityType,\n\t\t\tList<PojoModelPathValueNode> associationPathsToMatch,\n\t\t\tBoundPojoModelPathValueNode<?, ?, ?> inverseSidePathValueNode,\n\t\t\tPojoValueAdditionalMetadata inverseSideValueAdditionalMetadata,\n\t\t\tSet<PojoRawTypeModel<?>> encounteredAssociationHoldingTypes) {\n\t\tOptional<PojoModelPathValueNode> candidatePathOptional =\n\t\t\t\tinverseSideValueAdditionalMetadata.getInverseSidePath();\n\n\t\tPojoRawTypeModel<?> rawExtractedTypeModel =\n\t\t\t\tinverseSidePathValueNode.type().getTypeModel().getRawType();\n\n\t\tif ( candidatePathOptional.isPresent()\n\t\t\t\t&& associationPathsToMatch.contains( candidatePathOptional.get() ) ) {\n\t\t\tPojoModelPathValueNode inverseAssociationPath = inverseSidePathValueNode.toUnboundPath();\n\t\t\t/*\n\t\t\t * In order to match, the inverse path, when applied to the inverse entity type,\n\t\t\t * must also result in a supertype of the entity type holding the association to invert.\n\t\t\t * This is to handle cases where an entity holds inverse associations of multiple associations\n\t\t\t * from multiple different entities: in that case, the \"original\" associations may have\n\t\t\t * the same name and extractors.\n\t\t\t */\n\t\t\tif ( originalSideEntityType.isSubTypeOf( rawExtractedTypeModel ) ) {\n\t\t\t\treturn Optional.of( inverseAssociationPath );\n\t\t\t}\n\t\t}\n\n\t\tif ( inverseSideValueAdditionalMetadata.isAssociationEmbedded() ) {\n\t\t\tif ( encounteredAssociationHoldingTypes.contains( rawExtractedTypeModel ) ) {\n\t\t\t\tthrow log.infiniteRecursionForAssociationEmbeddeds(\n\t\t\t\t\t\tinverseSidePathValueNode.getRootType().getRawType(),\n\t\t\t\t\t\tinverseSidePathValueNode.toUnboundPath()\n\t\t\t\t);\n\t\t\t}\n\n\t\t\tencounteredAssociationHoldingTypes.add( rawExtractedTypeModel );\n\t\t\tcandidatePathOptional = findInverseSidePathFromInverseSideRecursive(\n\t\t\t\t\tinverseSidePathValueNode.type(), originalSideEntityType, associationPathsToMatch,\n\t\t\t\t\tencounteredAssociationHoldingTypes\n\t\t\t);\n\t\t\tencounteredAssociationHoldingTypes.remove( rawExtractedTypeModel );\n\t\t\tif ( candidatePathOptional.isPresent() ) {\n\t\t\t\treturn candidatePathOptional;\n\t\t\t}\n\t\t}\n\n\t\treturn Optional.empty();\n\t}\n\n\tprivate <P> BoundPojoModelPathValueNode<?, P, ?> bindExtractors(\n\t\t\tBoundPojoModelPathPropertyNode<?, P> inverseSidePathPropertyNode,\n\t\t\tContainerValueExtractorPath extractorPath) {\n\t\tBoundContainerValueExtractorPath<P, ?> resolvedExtractorPath =\n\t\t\t\textractorBinder.bindPath(\n\t\t\t\t\t\tintrospector, inverseSidePathPropertyNode.getPropertyModel().getTypeModel(),\n\t\t\t\t\t\textractorPath\n\t\t\t\t);\n\t\treturn inverseSidePathPropertyNode.value( resolvedExtractorPath );\n\t}\n}\n",
        "filePathAfter": "mapper/pojo/src/main/java/org/hibernate/search/mapper/pojo/dirtiness/building/impl/PojoAssociationPathInverter.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.pojo.dirtiness.building.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.HashSet;\nimport java.util.LinkedList;\nimport java.util.List;\nimport java.util.ListIterator;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.Set;\n\nimport org.hibernate.search.mapper.pojo.extractor.ContainerValueExtractorPath;\nimport org.hibernate.search.mapper.pojo.extractor.impl.BoundContainerValueExtractorPath;\nimport org.hibernate.search.mapper.pojo.extractor.impl.ContainerValueExtractorBinder;\nimport org.hibernate.search.mapper.pojo.logging.impl.Log;\nimport org.hibernate.search.mapper.pojo.model.additionalmetadata.building.impl.PojoTypeAdditionalMetadataProvider;\nimport org.hibernate.search.mapper.pojo.model.additionalmetadata.impl.PojoPropertyAdditionalMetadata;\nimport org.hibernate.search.mapper.pojo.model.additionalmetadata.impl.PojoTypeAdditionalMetadata;\nimport org.hibernate.search.mapper.pojo.model.additionalmetadata.impl.PojoValueAdditionalMetadata;\nimport org.hibernate.search.mapper.pojo.model.path.PojoModelPath;\nimport org.hibernate.search.mapper.pojo.model.path.PojoModelPathPropertyNode;\nimport org.hibernate.search.mapper.pojo.model.path.PojoModelPathValueNode;\nimport org.hibernate.search.mapper.pojo.model.path.impl.BoundPojoModelPathPropertyNode;\nimport org.hibernate.search.mapper.pojo.model.path.impl.BoundPojoModelPathTypeNode;\nimport org.hibernate.search.mapper.pojo.model.path.impl.BoundPojoModelPathValueNode;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoBootstrapIntrospector;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoPropertyModel;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeModel;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoTypeModel;\nimport org.hibernate.search.mapper.pojo.model.spi.PropertyHandle;\nimport org.hibernate.search.util.impl.common.LoggerFactory;\n\n/**\n * An object responsible for inverting an association path,\n * i.e. a chain of properties and container value extractors going from one entity to another.\n */\npublic final class PojoAssociationPathInverter {\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate final PojoTypeAdditionalMetadataProvider typeAdditionalMetadataProvider;\n\tprivate final PojoBootstrapIntrospector introspector;\n\tprivate final ContainerValueExtractorBinder extractorBinder;\n\n\tpublic PojoAssociationPathInverter(PojoTypeAdditionalMetadataProvider typeAdditionalMetadataProvider,\n\t\t\tPojoBootstrapIntrospector introspector,\n\t\t\tContainerValueExtractorBinder extractorBinder) {\n\t\tthis.typeAdditionalMetadataProvider = typeAdditionalMetadataProvider;\n\t\tthis.introspector = introspector;\n\t\tthis.extractorBinder = extractorBinder;\n\t}\n\n\tpublic Optional<PojoModelPathValueNode> invertPath(PojoTypeModel<?> inverseSideEntityType,\n\t\t\tBoundPojoModelPathValueNode<?, ?, ?> pathToInvert) {\n\t\tPojoRawTypeModel<?> originalSideEntityType = pathToInvert.getRootType().getRawType();\n\n\t\t// Try to find inverse side information hosted on the side to inverse\n\t\tOptional<PojoModelPathValueNode> inverseSidePathOptional =\n\t\t\t\tfindInverseSidePathFromOriginalSide( pathToInvert );\n\n\t\tif ( !inverseSidePathOptional.isPresent() ) {\n\t\t\tList<PojoModelPathValueNode> associationPathsToMatch = computeAssociationPathsToMatch( pathToInvert );\n\t\t\t// Try to find inverse side information hosted on the other side\n\t\t\tinverseSidePathOptional = findInverseSidePathFromInverseSide(\n\t\t\t\t\tinverseSideEntityType, originalSideEntityType, associationPathsToMatch\n\t\t\t);\n\t\t}\n\n\t\treturn inverseSidePathOptional;\n\t}\n\n\t/*\n\t * One might refer to the extractor path of an association in multiple ways:\n\t * - By intension, e.g. ContainerValueExtractorPath.default()\n\t * - By extension, e.g. ContainerValueExtractorPath.noExtractors()\n\t *   or ContainerValueExtractorPath.explicitExtractors( ... )\n\t * We want to match any type of reference, so we have to determine whether this association\n\t * uses the default extractor path, and if so, add it to the list of paths to match.\n\t */\n\tprivate List<PojoModelPathValueNode> computeAssociationPathsToMatch(\n\t\t\tBoundPojoModelPathValueNode<?, ?, ?> boundPathToInvert) {\n\t\t// We're potentially performing lots of insertions, so let's use a LinkedList\n\t\tList<PojoModelPathValueNode> associationPathsToMatch = new LinkedList<>();\n\t\tcollectAssociationPathsToMatch( associationPathsToMatch, boundPathToInvert );\n\t\treturn associationPathsToMatch;\n\t}\n\n\tprivate void collectAssociationPathsToMatch(\n\t\t\tList<PojoModelPathValueNode> associationPathsToMatch,\n\t\t\tBoundPojoModelPathValueNode<?, ?, ?> boundPathToInvert) {\n\t\tBoundPojoModelPathPropertyNode<?, ?> parentPath = boundPathToInvert.getParent();\n\t\tBoundPojoModelPathValueNode<?, ?, ?> parentValuePath = parentPath.getParent().getParent();\n\t\tString propertyName = parentPath.getPropertyHandle().getName();\n\t\tContainerValueExtractorPath extractorPath = boundPathToInvert.getExtractorPath();\n\t\tboolean isDefaultExtractorPath = isDefaultExtractorPath(\n\t\t\t\tparentPath.getPropertyModel(), boundPathToInvert.getBoundExtractorPath()\n\t\t);\n\t\tif ( parentValuePath != null ) {\n\t\t\tcollectAssociationPathsToMatch( associationPathsToMatch, parentValuePath );\n\t\t\tListIterator<PojoModelPathValueNode> iterator = associationPathsToMatch.listIterator();\n\t\t\twhile ( iterator.hasNext() ) {\n\t\t\t\tPojoModelPathValueNode baseValuePath = iterator.next();\n\t\t\t\tPojoModelPathPropertyNode basePropertyPath = baseValuePath.property( propertyName );\n\t\t\t\t// Append the property and extractor path to the already-collected paths\n\t\t\t\titerator.set( basePropertyPath.value( extractorPath ) );\n\t\t\t\tif ( isDefaultExtractorPath ) {\n\t\t\t\t\t/*\n\t\t\t\t\t * If the current extractor path (which is explicit) represents the default path,\n\t\t\t\t\t * then for each already collected path, add one version using the explicit representation,\n\t\t\t\t\t * and one version using the implicit representation.\n\t\t\t\t\t */\n\t\t\t\t\titerator.add( basePropertyPath.value( ContainerValueExtractorPath.defaultExtractors() ) );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t// We reached the root: collect the first paths\n\t\t\tPojoModelPathPropertyNode basePropertyPath = PojoModelPath.fromRoot( propertyName );\n\t\t\tassociationPathsToMatch.add( basePropertyPath.value( extractorPath ) );\n\t\t\tif ( isDefaultExtractorPath ) {\n\t\t\t\t// The may be two versions of this path, similarly to what we do above\n\t\t\t\tassociationPathsToMatch.add( basePropertyPath.value( ContainerValueExtractorPath.defaultExtractors() ) );\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate boolean isDefaultExtractorPath(PojoPropertyModel<?> propertyModel,\n\t\t\tBoundContainerValueExtractorPath<?, ?> originalSideBoundExtractorPath) {\n\t\treturn extractorBinder.isDefaultExtractorPath(\n\t\t\t\tintrospector,\n\t\t\t\tpropertyModel.getTypeModel(),\n\t\t\t\toriginalSideBoundExtractorPath.getExtractorPath()\n\t\t);\n\t}\n\n\tprivate Optional<PojoModelPathValueNode> findInverseSidePathFromOriginalSide(\n\t\t\tBoundPojoModelPathValueNode<?, ?, ?> pathToInvert) {\n\t\tBoundPojoModelPathPropertyNode<?, ?> lastPropertyNode = pathToInvert.getParent();\n\t\tBoundPojoModelPathTypeNode<?> lastTypeNode = lastPropertyNode.getParent();\n\t\tPojoPropertyModel<?> lastPropertyModel = lastPropertyNode.getPropertyModel();\n\t\tPojoTypeModel<?> lastTypeModel = lastTypeNode.getTypeModel();\n\n\t\tPojoTypeAdditionalMetadata typeAdditionalMetadata =\n\t\t\t\ttypeAdditionalMetadataProvider.get( lastTypeModel.getRawType() );\n\t\tPojoPropertyAdditionalMetadata propertyAdditionalMetadata =\n\t\t\t\ttypeAdditionalMetadata.getPropertyAdditionalMetadata( lastPropertyNode.getPropertyModel().getName() );\n\n\t\t// First try to query the additional metadata with the explicit extractor path\n\t\tOptional<PojoModelPathValueNode> result = propertyAdditionalMetadata.getValueAdditionalMetadata( pathToInvert.getExtractorPath() )\n\t\t\t\t.getInverseSidePath();\n\t\tif ( result.isPresent() ) {\n\t\t\treturn result;\n\t\t}\n\n\t\tif ( isDefaultExtractorPath( lastPropertyModel, pathToInvert.getBoundExtractorPath() ) ) {\n\t\t\t/*\n\t\t\t * Since the extractor path was the default one, try to query the additional metadata\n\t\t\t * with the implicit default extractor path.\n\t\t\t */\n\t\t\tresult = propertyAdditionalMetadata.getValueAdditionalMetadata( ContainerValueExtractorPath.defaultExtractors() )\n\t\t\t\t\t.getInverseSidePath();\n\t\t}\n\n\t\treturn result;\n\t}\n\n\tprivate Optional<PojoModelPathValueNode> findInverseSidePathFromInverseSide(\n\t\t\tPojoTypeModel<?> inverseSideTypeModel,\n\t\t\tPojoRawTypeModel<?> originalSideEntityType,\n\t\t\tList<PojoModelPathValueNode> associationPathsToMatch) {\n\t\tBoundPojoModelPathTypeNode<?> inverseSidePathTypeNode = BoundPojoModelPathTypeNode.root( inverseSideTypeModel );\n\t\tSet<PojoRawTypeModel<?>> encounteredAssociationHoldingTypes = new HashSet<>();\n\t\treturn findInverseSidePathFromInverseSideRecursive(\n\t\t\t\tinverseSidePathTypeNode, originalSideEntityType, associationPathsToMatch,\n\t\t\t\tencounteredAssociationHoldingTypes\n\t\t);\n\t}\n\n\tprivate Optional<PojoModelPathValueNode> findInverseSidePathFromInverseSideRecursive(\n\t\t\tBoundPojoModelPathTypeNode<?> inverseSidePathTypeNode,\n\t\t\tPojoRawTypeModel<?> originalSideEntityType,\n\t\t\tList<PojoModelPathValueNode> associationPathsToMatch,\n\t\t\tSet<PojoRawTypeModel<?>> encounteredAssociationHoldingTypes) {\n\t\tPojoTypeModel<?> inverseSideTypeModel = inverseSidePathTypeNode.getTypeModel();\n\t\tPojoTypeAdditionalMetadata inverseSideTypeAdditionalMetadata =\n\t\t\t\ttypeAdditionalMetadataProvider.get( inverseSideTypeModel.getRawType() );\n\n\t\tfor ( Map.Entry<String, PojoPropertyAdditionalMetadata> propertyEntry :\n\t\t\t\tinverseSideTypeAdditionalMetadata.getPropertiesAdditionalMetadata().entrySet() ) {\n\t\t\tString inverseSidePropertyName = propertyEntry.getKey();\n\t\t\tPojoPropertyModel<?> inverseSidePropertyModel = inverseSideTypeModel.getProperty( inverseSidePropertyName );\n\t\t\tPropertyHandle propertyHandle = inverseSidePropertyModel.getHandle();\n\t\t\tBoundPojoModelPathPropertyNode<?, ?> inverseSidePathPropertyNode =\n\t\t\t\t\tinverseSidePathTypeNode.property( propertyHandle );\n\t\t\tPojoPropertyAdditionalMetadata inverseSidePropertyAdditionalMetadata = propertyEntry.getValue();\n\n\t\t\tfor ( Map.Entry<ContainerValueExtractorPath, PojoValueAdditionalMetadata> valueEntry :\n\t\t\t\t\tinverseSidePropertyAdditionalMetadata.getValuesAdditionalMetadata().entrySet() ) {\n\t\t\t\tContainerValueExtractorPath inverseSideExtractorPath = valueEntry.getKey();\n\t\t\t\tBoundPojoModelPathValueNode<?, ?, ?> inverseSidePathValueNode =\n\t\t\t\t\t\tbindExtractors( inverseSidePathPropertyNode, inverseSideExtractorPath );\n\t\t\t\tPojoValueAdditionalMetadata inverseSideValueAdditionalMetadata = valueEntry.getValue();\n\n\t\t\t\tOptional<PojoModelPathValueNode> inverseSidePathOptional =\n\t\t\t\t\t\tfindInverseSidePathFromInverseSideValueRecursive(\n\t\t\t\t\t\t\t\toriginalSideEntityType, associationPathsToMatch,\n\t\t\t\t\t\t\t\tinverseSidePathValueNode, inverseSideValueAdditionalMetadata,\n\t\t\t\t\t\t\t\tencounteredAssociationHoldingTypes\n\t\t\t\t\t\t);\n\n\t\t\t\tif ( inverseSidePathOptional.isPresent() ) {\n\t\t\t\t\treturn inverseSidePathOptional;\n\t\t\t\t}\n\t\t\t\t// else: continue the loop, maybe we'll find the inverse path elsewhere.\n\t\t\t}\n\t\t}\n\n\t\treturn Optional.empty();\n\t}\n\n\tprivate Optional<PojoModelPathValueNode> findInverseSidePathFromInverseSideValueRecursive(\n\t\t\tPojoRawTypeModel<?> originalSideEntityType,\n\t\t\tList<PojoModelPathValueNode> associationPathsToMatch,\n\t\t\tBoundPojoModelPathValueNode<?, ?, ?> inverseSidePathValueNode,\n\t\t\tPojoValueAdditionalMetadata inverseSideValueAdditionalMetadata,\n\t\t\tSet<PojoRawTypeModel<?>> encounteredAssociationHoldingTypes) {\n\t\tOptional<PojoModelPathValueNode> candidatePathOptional =\n\t\t\t\tinverseSideValueAdditionalMetadata.getInverseSidePath();\n\n\t\tPojoRawTypeModel<?> rawExtractedTypeModel =\n\t\t\t\tinverseSidePathValueNode.type().getTypeModel().getRawType();\n\n\t\tif ( candidatePathOptional.isPresent()\n\t\t\t\t&& associationPathsToMatch.contains( candidatePathOptional.get() ) ) {\n\t\t\tPojoModelPathValueNode inverseAssociationPath = inverseSidePathValueNode.toUnboundPath();\n\t\t\t/*\n\t\t\t * In order to match, the inverse path, when applied to the inverse entity type,\n\t\t\t * must also result in a supertype of the entity type holding the association to invert.\n\t\t\t * This is to handle cases where an entity holds inverse associations of multiple associations\n\t\t\t * from multiple different entities: in that case, the \"original\" associations may have\n\t\t\t * the same name and extractors.\n\t\t\t */\n\t\t\tif ( originalSideEntityType.isSubTypeOf( rawExtractedTypeModel ) ) {\n\t\t\t\treturn Optional.of( inverseAssociationPath );\n\t\t\t}\n\t\t}\n\n\t\tif ( inverseSideValueAdditionalMetadata.isAssociationEmbedded() ) {\n\t\t\tif ( encounteredAssociationHoldingTypes.contains( rawExtractedTypeModel ) ) {\n\t\t\t\tthrow log.infiniteRecursionForAssociationEmbeddeds(\n\t\t\t\t\t\tinverseSidePathValueNode.getRootType().getRawType(),\n\t\t\t\t\t\tinverseSidePathValueNode.toUnboundPath()\n\t\t\t\t);\n\t\t\t}\n\n\t\t\tencounteredAssociationHoldingTypes.add( rawExtractedTypeModel );\n\t\t\tcandidatePathOptional = findInverseSidePathFromInverseSideRecursive(\n\t\t\t\t\tinverseSidePathValueNode.type(), originalSideEntityType, associationPathsToMatch,\n\t\t\t\t\tencounteredAssociationHoldingTypes\n\t\t\t);\n\t\t\tencounteredAssociationHoldingTypes.remove( rawExtractedTypeModel );\n\t\t\tif ( candidatePathOptional.isPresent() ) {\n\t\t\t\treturn candidatePathOptional;\n\t\t\t}\n\t\t}\n\n\t\treturn Optional.empty();\n\t}\n\n\tprivate <P> BoundPojoModelPathValueNode<?, P, ?> bindExtractors(\n\t\t\tBoundPojoModelPathPropertyNode<?, P> inverseSidePathPropertyNode,\n\t\t\tContainerValueExtractorPath extractorPath) {\n\t\tBoundContainerValueExtractorPath<P, ?> resolvedExtractorPath =\n\t\t\t\textractorBinder.bindPath(\n\t\t\t\t\t\tintrospector, inverseSidePathPropertyNode.getPropertyModel().getTypeModel(),\n\t\t\t\t\t\textractorPath\n\t\t\t\t);\n\t\treturn inverseSidePathPropertyNode.value( resolvedExtractorPath );\n\t}\n}\n",
        "diffSourceCodeSet": [
            ");\n\n\t\t\t\tif ( inverseSidePathOptional.isPresent() ) {\n\t\t\t\t\treturn inverseSidePathOptional;\n\t\t\t\t}\n\t\t\t\t// else: continue the loop, maybe we'll find the inverse path elsewhere.\n\t\t\t}\n\t\t}\n\n\t\treturn Optional.empty();\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.mapper.pojo.dirtiness.building.impl.PojoIndexingDependencyCollectorTypeNode#getTypeModel\n methodBody: PojoTypeModel<T> getTypeModel() {\nreturn modelPathFromRootEntityNode.getTypeModel();\n}",
            "methodSignature: org.hibernate.search.mapper.pojo.extractor.impl.ContainerValueExtractorBinder#tryBindPath\n methodBody: public <C> Optional<BoundContainerValueExtractorPath<C, ?>> tryBindPath(\n\t\t\tPojoBootstrapIntrospector introspector, PojoGenericTypeModel<C> sourceType,\n\t\t\tContainerValueExtractorPath extractorPath) {\nExtractorResolutionState<C> state=new ExtractorResolutionState<>(introspector,sourceType);\nif(extractorPath.isDefault()){firstMatchingExtractorContributor.tryAppend(state);\n}{for(Class<? extends ContainerValueExtractor> extractorClass: extractorPath.getExplicitExtractorClasses()){ExtractorContributor extractorContributor=getExtractorContributorForClass(extractorClass);\nif(!extractorContributor.tryAppend(state)){return Optional.empty();\n}}}return Optional.of(state.build());\n}",
            "methodSignature: org.hibernate.search.mapper.pojo.mapping.definition.annotation.impl.AnnotationPojoTypeMetadataContributorImpl#getExtractorPath\n methodBody: private ContainerValueExtractorPath getExtractorPath(\n\t\t\tContainerValueExtractorBeanReference[] extractors, Class<?> defaultExtractorsClass) {\nif(extractors.length == 0){return ContainerValueExtractorPath.noExtractors();\n}if(extractors.length == 1 && defaultExtractorsClass.equals(extractors[0].type())){return ContainerValueExtractorPath.defaultExtractors();\n}{return ContainerValueExtractorPath.explicitExtractors(Arrays.stream(extractors).map(ContainerValueExtractorBeanReference::type).collect(Collectors.toList()));\n}}"
        ],
        "sourceCodeAfterRefactoring": "private boolean isDefaultExtractorPath(PojoPropertyModel<?> propertyModel,\n\t\t\tBoundContainerValueExtractorPath<?, ?> originalSideBoundExtractorPath) {\n\t\treturn extractorBinder.isDefaultExtractorPath(\n\t\t\t\tintrospector,\n\t\t\t\tpropertyModel.getTypeModel(),\n\t\t\t\toriginalSideBoundExtractorPath.getExtractorPath()\n\t\t);\n\t}\n);\n\n\t\t\t\tif ( inverseSidePathOptional.isPresent() ) {\n\t\t\t\t\treturn inverseSidePathOptional;\n\t\t\t\t}\n\t\t\t\t// else: continue the loop, maybe we'll find the inverse path elsewhere.\n\t\t\t}\n\t\t}\n\n\t\treturn Optional.empty();\n\t}",
        "diffSourceCode": "   132: \tprivate boolean isDefaultExtractorPath(PojoPropertyModel<?> propertyModel,\n   133: \t\t\tBoundContainerValueExtractorPath<?, ?> originalSideBoundExtractorPath) {\n-  134: \t\tOptional<? extends BoundContainerValueExtractorPath<?, ?>> boundDefaultExtractorPathOptional = extractorBinder\n-  135: \t\t\t\t.tryBindPath(\n-  136: \t\t\t\t\t\tintrospector, propertyModel.getTypeModel(),\n-  137: \t\t\t\t\t\tContainerValueExtractorPath.defaultExtractors()\n-  138: \t\t\t\t);\n-  139: \t\treturn boundDefaultExtractorPathOptional.isPresent() && originalSideBoundExtractorPath.getExtractorPath().equals(\n-  140: \t\t\t\tboundDefaultExtractorPathOptional.get().getExtractorPath()\n-  141: \t\t);\n-  142: \t}\n-  214: \t\t\t\t\t\t\t\toriginalSideEntityType, associationPathsToMatch,\n-  215: \t\t\t\t\t\t\t\tinverseSidePathValueNode, inverseSideValueAdditionalMetadata,\n-  216: \t\t\t\t\t\t\t\tencounteredAssociationHoldingTypes\n-  217: \t\t\t\t\t\t);\n-  218: \n-  219: \t\t\t\tif ( inverseSidePathOptional.isPresent() ) {\n-  220: \t\t\t\t\treturn inverseSidePathOptional;\n-  221: \t\t\t\t}\n-  222: \t\t\t\t// else: continue the loop, maybe we'll find the inverse path elsewhere.\n-  223: \t\t\t}\n-  224: \t\t}\n+  134: \t\treturn extractorBinder.isDefaultExtractorPath(\n+  135: \t\t\t\tintrospector,\n+  136: \t\t\t\tpropertyModel.getTypeModel(),\n+  137: \t\t\t\toriginalSideBoundExtractorPath.getExtractorPath()\n+  138: \t\t);\n+  139: \t}\n+  140: \n+  141: \tprivate Optional<PojoModelPathValueNode> findInverseSidePathFromOriginalSide(\n+  142: \t\t\tBoundPojoModelPathValueNode<?, ?, ?> pathToInvert) {\n+  214: \t\t\t\t\t\t);\n+  215: \n+  216: \t\t\t\tif ( inverseSidePathOptional.isPresent() ) {\n+  217: \t\t\t\t\treturn inverseSidePathOptional;\n+  218: \t\t\t\t}\n+  219: \t\t\t\t// else: continue the loop, maybe we'll find the inverse path elsewhere.\n+  220: \t\t\t}\n+  221: \t\t}\n+  222: \n+  223: \t\treturn Optional.empty();\n+  224: \t}\n",
        "uniqueId": "b369ab192fd1c80ab666c63e65224bf90f658292_132_142_214_224_132_139",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 10,
                "covered": 14
            },
            "BRANCH": {
                "missed": 3,
                "covered": 1
            },
            "LINE": {
                "missed": 1,
                "covered": 5
            },
            "COMPLEXITY": {
                "missed": 2,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate processMatching(event LogEvent) : void extracted from package process(event LogEvent) : void in class org.hibernate.search.util.impl.test.rule.log4j.LogChecker",
        "diffLocations": [
            {
                "filePath": "util/internal/test/common/src/main/java/org/hibernate/search/util/impl/test/rule/log4j/LogChecker.java",
                "startLine": 47,
                "endLine": 67,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/test/common/src/main/java/org/hibernate/search/util/impl/test/rule/log4j/LogChecker.java",
                "startLine": 46,
                "endLine": 54,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/test/common/src/main/java/org/hibernate/search/util/impl/test/rule/log4j/LogChecker.java",
                "startLine": 57,
                "endLine": 71,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "synchronized void process(LogEvent event) {\n\t\tif ( expectation.getMaxExpectedCount() == null && expectation.getMinExpectedCount() <= count ) {\n\t\t\t// We don't care about events anymore, expectations are met and it won't change\n\t\t\treturn;\n\t\t}\n\t\tif ( expectation.getMatcher().matches( event ) ) {\n\t\t\t++count;\n\t\t\tif ( expectation.getMaxExpectedCount() != null && count > expectation.getMaxExpectedCount() ) {\n\t\t\t\tif ( extraEvents == null ) {\n\t\t\t\t\textraEvents = new ArrayList<>();\n\t\t\t\t}\n\t\t\t\textraEvents.add( event.toImmutable() );\n\t\t\t}\n\t\t\telse {\n\t\t\t\tif ( matchingEvents == null ) {\n\t\t\t\t\tmatchingEvents = new ArrayList<>();\n\t\t\t\t}\n\t\t\t\tmatchingEvents.add( event.toImmutable() );\n\t\t\t}\n\t\t}\n\t}",
        "filePathBefore": "util/internal/test/common/src/main/java/org/hibernate/search/util/impl/test/rule/log4j/LogChecker.java",
        "isPureRefactoring": true,
        "commitId": "3e3dc3475796d7fa2258843391d552647ec596fd",
        "packageNameBefore": "org.hibernate.search.util.impl.test.rule.log4j",
        "classNameBefore": "org.hibernate.search.util.impl.test.rule.log4j.LogChecker",
        "methodNameBefore": "org.hibernate.search.util.impl.test.rule.log4j.LogChecker#process",
        "classSignatureBefore": "public class LogChecker ",
        "methodNameBeforeSet": [
            "org.hibernate.search.util.impl.test.rule.log4j.LogChecker#process"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.util.impl.test.rule.log4j.LogChecker"
        ],
        "classSignatureBeforeSet": [
            "public class LogChecker "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.test.rule.log4j;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.apache.logging.log4j.core.LogEvent;\nimport org.hamcrest.Description;\n\npublic class LogChecker {\n\n\tprivate final LogExpectation expectation;\n\tprivate int count = 0;\n\tprivate List<LogEvent> matchingEvents;\n\tprivate List<LogEvent> extraEvents;\n\n\tLogChecker(LogExpectation expectation) {\n\t\tthis.expectation = expectation;\n\t}\n\n\tpublic void appendFailure(Description description, String newline) {\n\t\tdescription.appendText( newline );\n\t\tif ( count < expectation.getMinExpectedCount() ) {\n\t\t\tdescription.appendText( \"Expected at least \" + expectation.getMinExpectedCount() + \" time(s) \" );\n\t\t\texpectation.getMatcher().describeTo( description );\n\t\t\tdescription.appendText( \" but only got \" + count + \" such event(s).\" );\n\t\t\tdescription.appendText( \" Matching events: \" );\n\t\t\tappendEvents( description, newline, matchingEvents );\n\t\t}\n\t\tif ( expectation.getMaxExpectedCount() != null && expectation.getMaxExpectedCount() < count ) {\n\t\t\tdescription.appendText( \"Expected at most \" + expectation.getMaxExpectedCount() + \" time(s) \" );\n\t\t\texpectation.getMatcher().describeTo( description );\n\t\t\tdescription.appendText( \" but got \" + count + \" such event(s).\" );\n\t\t\tdescription.appendText( \" Extra events: \" );\n\t\t\tappendEvents( description, newline, extraEvents );\n\t\t\tdescription.appendText( \" Matching events: \" );\n\t\t\tappendEvents( description, newline, matchingEvents );\n\t\t}\n\t}\n\n\t// This must be synchronized to avoid problems when multiple threads issue log events concurrently\n\tsynchronized void process(LogEvent event) {\n\t\tif ( expectation.getMaxExpectedCount() == null && expectation.getMinExpectedCount() <= count ) {\n\t\t\t// We don't care about events anymore, expectations are met and it won't change\n\t\t\treturn;\n\t\t}\n\t\tif ( expectation.getMatcher().matches( event ) ) {\n\t\t\t++count;\n\t\t\tif ( expectation.getMaxExpectedCount() != null && count > expectation.getMaxExpectedCount() ) {\n\t\t\t\tif ( extraEvents == null ) {\n\t\t\t\t\textraEvents = new ArrayList<>();\n\t\t\t\t}\n\t\t\t\textraEvents.add( event.toImmutable() );\n\t\t\t}\n\t\t\telse {\n\t\t\t\tif ( matchingEvents == null ) {\n\t\t\t\t\tmatchingEvents = new ArrayList<>();\n\t\t\t\t}\n\t\t\t\tmatchingEvents.add( event.toImmutable() );\n\t\t\t}\n\t\t}\n\t}\n\n\tboolean areExpectationsMet() {\n\t\treturn expectation.getMinExpectedCount() <= count\n\t\t\t\t&& ( expectation.getMaxExpectedCount() == null || count <= expectation.getMaxExpectedCount() );\n\t}\n\n\tprivate static void appendEvents(Description description, String newline, List<LogEvent> events) {\n\t\tif ( events == null || events.isEmpty() ) {\n\t\t\tdescription.appendText( \"<none>\" );\n\t\t\treturn;\n\t\t}\n\t\tfor ( LogEvent event : events ) {\n\t\t\tdescription.appendText( newline );\n\t\t\tdescription.appendText( \"\\t - \" );\n\t\t\tdescription.appendText( event.getMessage().getFormattedMessage() );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "util/internal/test/common/src/main/java/org/hibernate/search/util/impl/test/rule/log4j/LogChecker.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.test.rule.log4j;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport org.apache.logging.log4j.core.LogEvent;\nimport org.hamcrest.Description;\n\npublic class LogChecker {\n\n\tprivate final LogExpectation expectation;\n\tprivate volatile int count = 0;\n\tprivate List<LogEvent> matchingEvents;\n\tprivate List<LogEvent> extraEvents;\n\n\tLogChecker(LogExpectation expectation) {\n\t\tthis.expectation = expectation;\n\t}\n\n\tpublic void appendFailure(Description description, String newline) {\n\t\tdescription.appendText( newline );\n\t\tif ( count < expectation.getMinExpectedCount() ) {\n\t\t\tdescription.appendText( \"Expected at least \" + expectation.getMinExpectedCount() + \" time(s) \" );\n\t\t\texpectation.getMatcher().describeTo( description );\n\t\t\tdescription.appendText( \" but only got \" + count + \" such event(s).\" );\n\t\t\tdescription.appendText( \" Matching events: \" );\n\t\t\tappendEvents( description, newline, matchingEvents );\n\t\t}\n\t\tif ( expectation.getMaxExpectedCount() != null && expectation.getMaxExpectedCount() < count ) {\n\t\t\tdescription.appendText( \"Expected at most \" + expectation.getMaxExpectedCount() + \" time(s) \" );\n\t\t\texpectation.getMatcher().describeTo( description );\n\t\t\tdescription.appendText( \" but got \" + count + \" such event(s).\" );\n\t\t\tdescription.appendText( \" Extra events: \" );\n\t\t\tappendEvents( description, newline, extraEvents );\n\t\t\tdescription.appendText( \" Matching events: \" );\n\t\t\tappendEvents( description, newline, matchingEvents );\n\t\t}\n\t}\n\n\tvoid process(LogEvent event) {\n\t\tif ( expectation.getMaxExpectedCount() == null && expectation.getMinExpectedCount() <= count ) {\n\t\t\t// We don't care about events anymore, expectations are met and it won't change\n\t\t\treturn;\n\t\t}\n\t\tif ( expectation.getMatcher().matches( event ) ) {\n\t\t\tprocessMatching( event );\n\t\t}\n\t}\n\n\t// This must be synchronized to avoid problems when multiple threads issue log events concurrently\n\tprivate synchronized void processMatching(LogEvent event) {\n\t\t++count;\n\t\tif ( expectation.getMaxExpectedCount() != null && count > expectation.getMaxExpectedCount() ) {\n\t\t\tif ( extraEvents == null ) {\n\t\t\t\textraEvents = new ArrayList<>();\n\t\t\t}\n\t\t\textraEvents.add( event.toImmutable() );\n\t\t}\n\t\telse {\n\t\t\tif ( matchingEvents == null ) {\n\t\t\t\tmatchingEvents = new ArrayList<>();\n\t\t\t}\n\t\t\tmatchingEvents.add( event.toImmutable() );\n\t\t}\n\t}\n\n\tboolean areExpectationsMet() {\n\t\treturn expectation.getMinExpectedCount() <= count\n\t\t\t\t&& ( expectation.getMaxExpectedCount() == null || count <= expectation.getMaxExpectedCount() );\n\t}\n\n\tprivate static void appendEvents(Description description, String newline, List<LogEvent> events) {\n\t\tif ( events == null || events.isEmpty() ) {\n\t\t\tdescription.appendText( \"<none>\" );\n\t\t\treturn;\n\t\t}\n\t\tfor ( LogEvent event : events ) {\n\t\t\tdescription.appendText( newline );\n\t\t\tdescription.appendText( \"\\t - \" );\n\t\t\tdescription.appendText( event.getMessage().getFormattedMessage() );\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private synchronized void processMatching(LogEvent event) {\n\t\t++count;\n\t\tif ( expectation.getMaxExpectedCount() != null && count > expectation.getMaxExpectedCount() ) {\n\t\t\tif ( extraEvents == null ) {\n\t\t\t\textraEvents = new ArrayList<>();\n\t\t\t}\n\t\t\textraEvents.add( event.toImmutable() );\n\t\t}\n\t\telse {\n\t\t\tif ( matchingEvents == null ) {\n\t\t\t\tmatchingEvents = new ArrayList<>();\n\t\t\t}\n\t\t\tmatchingEvents.add( event.toImmutable() );\n\t\t}\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "void process(LogEvent event) {\n\t\tif ( expectation.getMaxExpectedCount() == null && expectation.getMinExpectedCount() <= count ) {\n\t\t\t// We don't care about events anymore, expectations are met and it won't change\n\t\t\treturn;\n\t\t}\n\t\tif ( expectation.getMatcher().matches( event ) ) {\n\t\t\tprocessMatching( event );\n\t\t}\n\t}\nprivate synchronized void processMatching(LogEvent event) {\n\t\t++count;\n\t\tif ( expectation.getMaxExpectedCount() != null && count > expectation.getMaxExpectedCount() ) {\n\t\t\tif ( extraEvents == null ) {\n\t\t\t\textraEvents = new ArrayList<>();\n\t\t\t}\n\t\t\textraEvents.add( event.toImmutable() );\n\t\t}\n\t\telse {\n\t\t\tif ( matchingEvents == null ) {\n\t\t\t\tmatchingEvents = new ArrayList<>();\n\t\t\t}\n\t\t\tmatchingEvents.add( event.toImmutable() );\n\t\t}\n\t}",
        "diffSourceCode": "-   46: \t// This must be synchronized to avoid problems when multiple threads issue log events concurrently\n-   47: \tsynchronized void process(LogEvent event) {\n-   48: \t\tif ( expectation.getMaxExpectedCount() == null && expectation.getMinExpectedCount() <= count ) {\n-   49: \t\t\t// We don't care about events anymore, expectations are met and it won't change\n-   50: \t\t\treturn;\n-   51: \t\t}\n-   52: \t\tif ( expectation.getMatcher().matches( event ) ) {\n-   53: \t\t\t++count;\n-   54: \t\t\tif ( expectation.getMaxExpectedCount() != null && count > expectation.getMaxExpectedCount() ) {\n-   55: \t\t\t\tif ( extraEvents == null ) {\n-   56: \t\t\t\t\textraEvents = new ArrayList<>();\n-   57: \t\t\t\t}\n-   58: \t\t\t\textraEvents.add( event.toImmutable() );\n-   59: \t\t\t}\n-   60: \t\t\telse {\n-   61: \t\t\t\tif ( matchingEvents == null ) {\n-   62: \t\t\t\t\tmatchingEvents = new ArrayList<>();\n-   63: \t\t\t\t}\n-   64: \t\t\t\tmatchingEvents.add( event.toImmutable() );\n-   65: \t\t\t}\n-   66: \t\t}\n-   67: \t}\n-   68: \n-   69: \tboolean areExpectationsMet() {\n-   70: \t\treturn expectation.getMinExpectedCount() <= count\n-   71: \t\t\t\t&& ( expectation.getMaxExpectedCount() == null || count <= expectation.getMaxExpectedCount() );\n+   46: \tvoid process(LogEvent event) {\n+   47: \t\tif ( expectation.getMaxExpectedCount() == null && expectation.getMinExpectedCount() <= count ) {\n+   48: \t\t\t// We don't care about events anymore, expectations are met and it won't change\n+   49: \t\t\treturn;\n+   50: \t\t}\n+   51: \t\tif ( expectation.getMatcher().matches( event ) ) {\n+   52: \t\t\tprocessMatching( event );\n+   53: \t\t}\n+   54: \t}\n+   55: \n+   56: \t// This must be synchronized to avoid problems when multiple threads issue log events concurrently\n+   57: \tprivate synchronized void processMatching(LogEvent event) {\n+   58: \t\t++count;\n+   59: \t\tif ( expectation.getMaxExpectedCount() != null && count > expectation.getMaxExpectedCount() ) {\n+   60: \t\t\tif ( extraEvents == null ) {\n+   61: \t\t\t\textraEvents = new ArrayList<>();\n+   62: \t\t\t}\n+   63: \t\t\textraEvents.add( event.toImmutable() );\n+   64: \t\t}\n+   65: \t\telse {\n+   66: \t\t\tif ( matchingEvents == null ) {\n+   67: \t\t\t\tmatchingEvents = new ArrayList<>();\n+   68: \t\t\t}\n+   69: \t\t\tmatchingEvents.add( event.toImmutable() );\n+   70: \t\t}\n+   71: \t}\n",
        "uniqueId": "3e3dc3475796d7fa2258843391d552647ec596fd_47_67_57_71_46_54",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11
    },
    {
        "type": "Move Method",
        "description": "Move Method\tprotected getPersistenceUnitName() : String from class org.hibernate.search.integrationtest.batch.jsr352.massindexing.AbstractBatchIndexingIT to protected getPersistenceUnitName() : String from class org.hibernate.search.integrationtest.batch.jsr352.massindexing.EntityManagerFactoryRetrievalIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm-batch-jsr352/src/test/java/org/hibernate/search/integrationtest/batch/jsr352/massindexing/AbstractBatchIndexingIT.java",
                "startLine": 92,
                "endLine": 94,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm-batch-jsr352/src/test/java/org/hibernate/search/integrationtest/batch/jsr352/massindexing/EntityManagerFactoryRetrievalIT.java",
                "startLine": 90,
                "endLine": 92,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "protected String getPersistenceUnitName() {\n\t\treturn PERSISTENCE_UNIT_NAME;\n\t}",
        "filePathBefore": "integrationtest/mapper/orm-batch-jsr352/src/test/java/org/hibernate/search/integrationtest/batch/jsr352/massindexing/AbstractBatchIndexingIT.java",
        "isPureRefactoring": true,
        "commitId": "314cff098d6147b783fa091ce3ebcc54a87522aa",
        "packageNameBefore": "org.hibernate.search.integrationtest.batch.jsr352.massindexing",
        "classNameBefore": "org.hibernate.search.integrationtest.batch.jsr352.massindexing.AbstractBatchIndexingIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.batch.jsr352.massindexing.AbstractBatchIndexingIT#getPersistenceUnitName",
        "classSignatureBefore": "public abstract class AbstractBatchIndexingIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.batch.jsr352.massindexing.AbstractBatchIndexingIT#getPersistenceUnitName"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.batch.jsr352.massindexing.AbstractBatchIndexingIT"
        ],
        "classSignatureBeforeSet": [
            "public abstract class AbstractBatchIndexingIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.batch.jsr352.massindexing;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport javax.batch.operations.JobOperator;\nimport javax.persistence.EntityManager;\nimport javax.persistence.EntityManagerFactory;\nimport javax.persistence.Persistence;\nimport javax.persistence.criteria.CriteriaBuilder;\nimport javax.persistence.criteria.CriteriaQuery;\nimport javax.persistence.criteria.Path;\nimport javax.persistence.criteria.Root;\n\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.Company;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.Person;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.WhoAmI;\nimport org.hibernate.search.integrationtest.batch.jsr352.util.JobTestUtil;\nimport org.hibernate.search.integrationtest.batch.jsr352.util.PersistenceUnitTestUtil;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.session.SearchSession;\nimport org.hibernate.search.mapper.orm.work.SearchIndexingPlan;\n\nimport org.junit.After;\nimport org.junit.Before;\n\n/**\n * @author Mincong Huang\n */\npublic abstract class AbstractBatchIndexingIT {\n\n\tprivate static final String PERSISTENCE_UNIT_NAME = PersistenceUnitTestUtil.getPersistenceUnitName();\n\n\tprotected static final int INSTANCES_PER_DATA_TEMPLATE = 100;\n\n\t// We have three data templates per entity type (see setup)\n\tprotected static final int INSTANCE_PER_ENTITY_TYPE = INSTANCES_PER_DATA_TEMPLATE * 3;\n\n\tstatic final int JOB_TIMEOUT_MS = 30_000;\n\n\tprotected JobOperator jobOperator;\n\tprotected EntityManagerFactory emf;\n\n\t@Before\n\tpublic void setup() {\n\t\tjobOperator = JobTestUtil.getAndCheckRuntime();\n\t\tList<Company> companies = new ArrayList();\n\t\tList<Person> people = new ArrayList();\n\t\tList<WhoAmI> whos = new ArrayList();\n\t\tfor ( int i = 0; i < INSTANCE_PER_ENTITY_TYPE; i += 3 ) {\n\t\t\tint index1 = i;\n\t\t\tint index2 = i + 1;\n\t\t\tint index3 = i + 2;\n\t\t\tcompanies.add( new Company( \"Google \" + index1 ) );\n\t\t\tcompanies.add( new Company( \"Red Hat \" + index2 ) );\n\t\t\tcompanies.add( new Company( \"Microsoft \" + index3 ) );\n\t\t\tpeople.add( new Person( \"BG \" + index1, \"Bill\", \"Gates\" ) );\n\t\t\tpeople.add( new Person( \"LT \" + index2, \"Linus\", \"Torvalds\" ) );\n\t\t\tpeople.add( new Person( \"SJ \" + index3, \"Steven\", \"Jobs\" ) );\n\t\t\twhos.add( new WhoAmI( \"cid01 \" + index1, \"id01 \" + index1, \"uid01 \" + index1 ) );\n\t\t\twhos.add( new WhoAmI( \"cid02 \" + index2, \"id02 \" + index2, \"uid02 \" + index2 ) );\n\t\t\twhos.add( new WhoAmI( \"cid03 \" + index3, \"id03 \" + index3, \"uid03 \" + index3 ) );\n\t\t}\n\t\tEntityManager em = null;\n\n\t\ttry {\n\t\t\temf = Persistence.createEntityManagerFactory( getPersistenceUnitName() );\n\t\t\tem = emf.createEntityManager();\n\t\t\tem.getTransaction().begin();\n\t\t\tcompanies.forEach( em::persist );\n\t\t\tpeople.forEach( em::persist );\n\t\t\twhos.forEach( em::persist );\n\t\t\tem.getTransaction().commit();\n\t\t}\n\t\tfinally {\n\t\t\tif ( em != null ) {\n\t\t\t\tem.close();\n\t\t\t}\n\t\t}\n\t}\n\n\t@After\n\tpublic void shutdown() {\n\t\temf.close();\n\t}\n\n\tprotected String getPersistenceUnitName() {\n\t\treturn PERSISTENCE_UNIT_NAME;\n\t}\n\n\tprotected final void indexSomeCompanies(int count) {\n\t\tEntityManager em = null;\n\t\ttry {\n\t\t\tem = emf.createEntityManager();\n\t\t\tem.getTransaction().begin();\n\t\t\tCriteriaBuilder criteriaBuilder = em.getCriteriaBuilder();\n\t\t\tCriteriaQuery<Company> criteria = criteriaBuilder.createQuery( Company.class );\n\t\t\tRoot<Company> root = criteria.from( Company.class );\n\t\t\tPath<Integer> id = root.get( root.getModel().getId( int.class ) );\n\t\t\tcriteria.orderBy( criteriaBuilder.asc( id ) );\n\t\t\tList<Company> companies = em.createQuery( criteria ).setMaxResults( count ).getResultList();\n\t\t\tSearchSession session = Search.session( em );\n\n\t\t\tSearchIndexingPlan indexingPlan = session.indexingPlan();\n\t\t\tfor ( Company company : companies ) {\n\t\t\t\tindexingPlan.addOrUpdate( company );\n\t\t\t}\n\t\t\tem.getTransaction().commit();\n\t\t}\n\t\tfinally {\n\t\t\tif ( em != null ) {\n\t\t\t\tem.close();\n\t\t\t}\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/orm-batch-jsr352/src/test/java/org/hibernate/search/integrationtest/batch/jsr352/massindexing/EntityManagerFactoryRetrievalIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.batch.jsr352.massindexing;\n\nimport static org.hibernate.search.integrationtest.batch.jsr352.util.JobTestUtil.JOB_TIMEOUT_MS;\nimport static org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils.with;\nimport static org.junit.Assert.assertEquals;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport javax.batch.operations.JobOperator;\nimport javax.batch.runtime.JobExecution;\nimport javax.persistence.EntityManagerFactory;\nimport javax.persistence.Persistence;\n\nimport org.hibernate.search.batch.jsr352.core.massindexing.MassIndexingJob;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.Person;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.WhoAmI;\nimport org.hibernate.search.integrationtest.batch.jsr352.util.JobTestUtil;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.Company;\nimport org.hibernate.search.integrationtest.batch.jsr352.util.PersistenceUnitTestUtil;\n\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\n\n/**\n * @author Mincong Huang\n */\npublic class EntityManagerFactoryRetrievalIT {\n\n\tprivate static final String PERSISTENCE_UNIT_NAME = PersistenceUnitTestUtil.getPersistenceUnitName();\n\n\tprotected static final int INSTANCES_PER_DATA_TEMPLATE = 100;\n\n\t// We have three data templates per entity type (see setup)\n\tprotected static final int INSTANCE_PER_ENTITY_TYPE = INSTANCES_PER_DATA_TEMPLATE * 3;\n\n\t/*\n\t * Make sure to have more than one checkpoint,\n\t * because we had errors related to that in the past.\n\t */\n\tprivate static final int CHECKPOINT_INTERVAL = 10;\n\n\tprivate static final String SESSION_FACTORY_NAME = \"primary_session_factory\";\n\n\tprotected JobOperator jobOperator;\n\tprotected EntityManagerFactory emf;\n\n\t@Before\n\tpublic void setup() {\n\t\tjobOperator = JobTestUtil.getAndCheckRuntime();\n\t\tList<Company> companies = new ArrayList<>();\n\t\tList<Person> people = new ArrayList<>();\n\t\tList<WhoAmI> whos = new ArrayList<>();\n\t\tfor ( int i = 0; i < INSTANCE_PER_ENTITY_TYPE; i += 3 ) {\n\t\t\tint index1 = i;\n\t\t\tint index2 = i + 1;\n\t\t\tint index3 = i + 2;\n\t\t\tcompanies.add( new Company( \"Google \" + index1 ) );\n\t\t\tcompanies.add( new Company( \"Red Hat \" + index2 ) );\n\t\t\tcompanies.add( new Company( \"Microsoft \" + index3 ) );\n\t\t\tpeople.add( new Person( \"BG \" + index1, \"Bill\", \"Gates\" ) );\n\t\t\tpeople.add( new Person( \"LT \" + index2, \"Linus\", \"Torvalds\" ) );\n\t\t\tpeople.add( new Person( \"SJ \" + index3, \"Steven\", \"Jobs\" ) );\n\t\t\twhos.add( new WhoAmI( \"cid01 \" + index1, \"id01 \" + index1, \"uid01 \" + index1 ) );\n\t\t\twhos.add( new WhoAmI( \"cid02 \" + index2, \"id02 \" + index2, \"uid02 \" + index2 ) );\n\t\t\twhos.add( new WhoAmI( \"cid03 \" + index3, \"id03 \" + index3, \"uid03 \" + index3 ) );\n\t\t}\n\n\t\temf = Persistence.createEntityManagerFactory( getPersistenceUnitName() );\n\t\twith( emf ).runInTransaction( em -> {\n\t\t\tcompanies.forEach( em::persist );\n\t\t\tpeople.forEach( em::persist );\n\t\t\twhos.forEach( em::persist );\n\t\t} );\n\t}\n\n\t@After\n\tpublic void shutdown() {\n\t\tif ( emf != null ) {\n\t\t\temf.close();\n\t\t}\n\t}\n\n\tprotected String getPersistenceUnitName() {\n\t\treturn PERSISTENCE_UNIT_NAME;\n\t}\n\n\t@Test\n\tpublic void defaultNamespace() throws Exception {\n\t\tList<Company> companies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( 0, companies.size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntity( Company.class )\n\t\t\t\t\t\t.checkpointInterval( CHECKPOINT_INTERVAL )\n\t\t\t\t\t\t.entityManagerFactoryReference( getPersistenceUnitName() )\n\t\t\t\t\t\t.build()\n\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tJobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\n\t\tcompanies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( INSTANCES_PER_DATA_TEMPLATE, companies.size() );\n\t}\n\n\t@Test\n\tpublic void persistenceUnitNamespace() throws Exception {\n\t\tList<Company> companies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( 0, companies.size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntity( Company.class )\n\t\t\t\t\t\t.checkpointInterval( CHECKPOINT_INTERVAL )\n\t\t\t\t\t\t.entityManagerFactoryNamespace( \"persistence-unit-name\" )\n\t\t\t\t\t\t.entityManagerFactoryReference( getPersistenceUnitName() )\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tJobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\n\t\tcompanies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( INSTANCES_PER_DATA_TEMPLATE, companies.size() );\n\t}\n\n\t@Test\n\tpublic void sessionFactoryNamespace() throws Exception {\n\t\tList<Company> companies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( 0, companies.size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntity( Company.class )\n\t\t\t\t\t\t.checkpointInterval( CHECKPOINT_INTERVAL )\n\t\t\t\t\t\t.entityManagerFactoryNamespace( \"session-factory-name\" )\n\t\t\t\t\t\t.entityManagerFactoryReference( SESSION_FACTORY_NAME )\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tJobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\n\t\tcompanies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( INSTANCES_PER_DATA_TEMPLATE, companies.size() );\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "protected String getPersistenceUnitName() {\n\t\treturn PERSISTENCE_UNIT_NAME;\n\t}",
        "diffSourceCode": "-   90: \t}\n-   91: \n-   92: \tprotected String getPersistenceUnitName() {\n-   93: \t\treturn PERSISTENCE_UNIT_NAME;\n-   94: \t}\n+   90: \tprotected String getPersistenceUnitName() {\n+   91: \t\treturn PERSISTENCE_UNIT_NAME;\n+   92: \t}\n+   93: \n+   94: \t@Test\n",
        "uniqueId": "314cff098d6147b783fa091ce3ebcc54a87522aa_92_94__90_92",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11
    },
    {
        "type": "Move And Rename Method",
        "description": "Move And Rename Method\tpublic shutDown() : void from class org.hibernate.search.integrationtest.batch.jsr352.component.HibernateSearchPartitionMapperComponentIT to public shutdown() : void from class org.hibernate.search.integrationtest.batch.jsr352.massindexing.EntityManagerFactoryRetrievalIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm-batch-jsr352/src/test/java/org/hibernate/search/integrationtest/batch/jsr352/component/HibernateSearchPartitionMapperComponentIT.java",
                "startLine": 90,
                "endLine": 95,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm-batch-jsr352/src/test/java/org/hibernate/search/integrationtest/batch/jsr352/massindexing/EntityManagerFactoryRetrievalIT.java",
                "startLine": 83,
                "endLine": 88,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@After\n\tpublic void shutDown() {\n\t\tif ( emf.isOpen() ) {\n\t\t\temf.close();\n\t\t}\n\t}",
        "filePathBefore": "integrationtest/mapper/orm-batch-jsr352/src/test/java/org/hibernate/search/integrationtest/batch/jsr352/component/HibernateSearchPartitionMapperComponentIT.java",
        "isPureRefactoring": true,
        "commitId": "314cff098d6147b783fa091ce3ebcc54a87522aa",
        "packageNameBefore": "org.hibernate.search.integrationtest.batch.jsr352.component",
        "classNameBefore": "org.hibernate.search.integrationtest.batch.jsr352.component.HibernateSearchPartitionMapperComponentIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.batch.jsr352.component.HibernateSearchPartitionMapperComponentIT#shutDown",
        "classSignatureBefore": "public class HibernateSearchPartitionMapperComponentIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.batch.jsr352.component.HibernateSearchPartitionMapperComponentIT#shutDown"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.batch.jsr352.component.HibernateSearchPartitionMapperComponentIT"
        ],
        "classSignatureBeforeSet": [
            "public class HibernateSearchPartitionMapperComponentIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.batch.jsr352.component;\n\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertNotNull;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.when;\n\nimport java.util.Arrays;\nimport java.util.Properties;\nimport javax.batch.api.partition.PartitionPlan;\nimport javax.batch.runtime.context.JobContext;\nimport javax.persistence.EntityManager;\nimport javax.persistence.EntityManagerFactory;\nimport javax.persistence.Persistence;\n\nimport org.hibernate.search.batch.jsr352.core.massindexing.impl.JobContextData;\nimport org.hibernate.search.batch.jsr352.core.massindexing.step.impl.HibernateSearchPartitionMapper;\nimport org.hibernate.search.batch.jsr352.core.massindexing.util.impl.MassIndexingPartitionProperties;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.Company;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.Person;\nimport org.hibernate.search.integrationtest.batch.jsr352.util.JobTestUtil;\nimport org.hibernate.search.integrationtest.batch.jsr352.util.PersistenceUnitTestUtil;\n\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\n\n/**\n * Single-component test for partition plan validation.\n *\n * @author Mincong Huang\n */\npublic class HibernateSearchPartitionMapperComponentIT {\n\n\tprivate static final String PERSISTENCE_UNIT_NAME = PersistenceUnitTestUtil.getPersistenceUnitName();\n\tprivate static final int COMP_ROWS = 3;\n\tprivate static final int PERS_ROWS = 8;\n\n\tprivate EntityManagerFactory emf;\n\n\tprivate JobContext mockedJobContext;\n\n\tprivate HibernateSearchPartitionMapper partitionMapper;\n\n\t@Before\n\tpublic void setUp() {\n\t\tEntityManager em = null;\n\t\ttry {\n\t\t\temf = Persistence.createEntityManagerFactory( PERSISTENCE_UNIT_NAME );\n\t\t\tem = emf.createEntityManager();\n\t\t\tem.getTransaction().begin();\n\t\t\tfor ( int i = 1; i <= COMP_ROWS; i++ ) {\n\t\t\t\tem.persist( new Company( \"C\" + i ) );\n\t\t\t}\n\t\t\tfor ( int i = 1; i <= PERS_ROWS; i++ ) {\n\t\t\t\tem.persist( new Person( \"P\" + i, \"\", \"\" ) );\n\t\t\t}\n\t\t\tem.getTransaction().commit();\n\t\t}\n\t\tfinally {\n\t\t\tif ( em != null ) {\n\t\t\t\tem.close();\n\t\t\t}\n\t\t}\n\n\t\tfinal String fetchSize = String.valueOf( 200 * 1000 );\n\t\tfinal String hql = null;\n\t\tfinal String maxThreads = String.valueOf( 1 );\n\t\tfinal String rowsPerPartition = String.valueOf( 3 );\n\n\t\tmockedJobContext = mock( JobContext.class );\n\t\tpartitionMapper = new HibernateSearchPartitionMapper(\n\t\t\t\tfetchSize,\n\t\t\t\thql,\n\t\t\t\tmaxThreads,\n\t\t\t\tnull,\n\t\t\t\trowsPerPartition,\n\t\t\t\tnull,\n\t\t\t\tnull,\n\t\t\t\tmockedJobContext\n\t\t);\n\t}\n\n\t@After\n\tpublic void shutDown() {\n\t\tif ( emf.isOpen() ) {\n\t\t\temf.close();\n\t\t}\n\t}\n\n\t/**\n\t * Prove that there're N partitions for each root entity,\n\t * where N stands for the ceiling number of the division\n\t * between the rows to index and the max rows per partition.\n\t */\n\t@Test\n\tpublic void testMapPartitions() throws Exception {\n\t\tJobContextData jobData = new JobContextData();\n\t\tjobData.setEntityManagerFactory( emf );\n\t\tjobData.setEntityTypeDescriptors( Arrays.asList(\n\t\t\t\tJobTestUtil.createSimpleEntityTypeDescriptor( emf, Company.class ),\n\t\t\t\tJobTestUtil.createSimpleEntityTypeDescriptor( emf, Person.class )\n\t\t\t\t) );\n\t\twhen( mockedJobContext.getTransientUserData() ).thenReturn( jobData );\n\n\t\tPartitionPlan partitionPlan = partitionMapper.mapPartitions();\n\n\t\tint compPartitions = 0;\n\t\tint persPartitions = 0;\n\t\tfor ( Properties p : partitionPlan.getPartitionProperties() ) {\n\t\t\tString entityName = p.getProperty( MassIndexingPartitionProperties.ENTITY_NAME );\n\t\t\tif ( entityName.equals( Company.class.getName() ) ) {\n\t\t\t\tcompPartitions++;\n\t\t\t}\n\t\t\tif ( entityName.equals( Person.class.getName() ) ) {\n\t\t\t\tpersPartitions++;\n\t\t\t}\n\t\t\t/*\n\t\t\t * The checkpoint interval should have defaulted to the value of rowsPerPartition,\n\t\t\t * since the value of rowsPerPartition is lower than the static default for checkpoint interval.\n\t\t\t */\n\t\t\tString checkpointInterval = p.getProperty( MassIndexingPartitionProperties.CHECKPOINT_INTERVAL );\n\t\t\tassertNotNull( checkpointInterval );\n\t\t\tassertEquals( \"3\", checkpointInterval );\n\t\t}\n\n\t\t// nbPartitions = rows / rowsPerPartition\n\t\tassertEquals( 1, compPartitions ); // 3 / 3 => 1 partition\n\t\tassertEquals( 3, persPartitions ); // 8 / 3 => 3 partitions\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/orm-batch-jsr352/src/test/java/org/hibernate/search/integrationtest/batch/jsr352/massindexing/EntityManagerFactoryRetrievalIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.batch.jsr352.massindexing;\n\nimport static org.hibernate.search.integrationtest.batch.jsr352.util.JobTestUtil.JOB_TIMEOUT_MS;\nimport static org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils.with;\nimport static org.junit.Assert.assertEquals;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport javax.batch.operations.JobOperator;\nimport javax.batch.runtime.JobExecution;\nimport javax.persistence.EntityManagerFactory;\nimport javax.persistence.Persistence;\n\nimport org.hibernate.search.batch.jsr352.core.massindexing.MassIndexingJob;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.Person;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.WhoAmI;\nimport org.hibernate.search.integrationtest.batch.jsr352.util.JobTestUtil;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.Company;\nimport org.hibernate.search.integrationtest.batch.jsr352.util.PersistenceUnitTestUtil;\n\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\n\n/**\n * @author Mincong Huang\n */\npublic class EntityManagerFactoryRetrievalIT {\n\n\tprivate static final String PERSISTENCE_UNIT_NAME = PersistenceUnitTestUtil.getPersistenceUnitName();\n\n\tprotected static final int INSTANCES_PER_DATA_TEMPLATE = 100;\n\n\t// We have three data templates per entity type (see setup)\n\tprotected static final int INSTANCE_PER_ENTITY_TYPE = INSTANCES_PER_DATA_TEMPLATE * 3;\n\n\t/*\n\t * Make sure to have more than one checkpoint,\n\t * because we had errors related to that in the past.\n\t */\n\tprivate static final int CHECKPOINT_INTERVAL = 10;\n\n\tprivate static final String SESSION_FACTORY_NAME = \"primary_session_factory\";\n\n\tprotected JobOperator jobOperator;\n\tprotected EntityManagerFactory emf;\n\n\t@Before\n\tpublic void setup() {\n\t\tjobOperator = JobTestUtil.getAndCheckRuntime();\n\t\tList<Company> companies = new ArrayList<>();\n\t\tList<Person> people = new ArrayList<>();\n\t\tList<WhoAmI> whos = new ArrayList<>();\n\t\tfor ( int i = 0; i < INSTANCE_PER_ENTITY_TYPE; i += 3 ) {\n\t\t\tint index1 = i;\n\t\t\tint index2 = i + 1;\n\t\t\tint index3 = i + 2;\n\t\t\tcompanies.add( new Company( \"Google \" + index1 ) );\n\t\t\tcompanies.add( new Company( \"Red Hat \" + index2 ) );\n\t\t\tcompanies.add( new Company( \"Microsoft \" + index3 ) );\n\t\t\tpeople.add( new Person( \"BG \" + index1, \"Bill\", \"Gates\" ) );\n\t\t\tpeople.add( new Person( \"LT \" + index2, \"Linus\", \"Torvalds\" ) );\n\t\t\tpeople.add( new Person( \"SJ \" + index3, \"Steven\", \"Jobs\" ) );\n\t\t\twhos.add( new WhoAmI( \"cid01 \" + index1, \"id01 \" + index1, \"uid01 \" + index1 ) );\n\t\t\twhos.add( new WhoAmI( \"cid02 \" + index2, \"id02 \" + index2, \"uid02 \" + index2 ) );\n\t\t\twhos.add( new WhoAmI( \"cid03 \" + index3, \"id03 \" + index3, \"uid03 \" + index3 ) );\n\t\t}\n\n\t\temf = Persistence.createEntityManagerFactory( getPersistenceUnitName() );\n\t\twith( emf ).runInTransaction( em -> {\n\t\t\tcompanies.forEach( em::persist );\n\t\t\tpeople.forEach( em::persist );\n\t\t\twhos.forEach( em::persist );\n\t\t} );\n\t}\n\n\t@After\n\tpublic void shutdown() {\n\t\tif ( emf != null ) {\n\t\t\temf.close();\n\t\t}\n\t}\n\n\tprotected String getPersistenceUnitName() {\n\t\treturn PERSISTENCE_UNIT_NAME;\n\t}\n\n\t@Test\n\tpublic void defaultNamespace() throws Exception {\n\t\tList<Company> companies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( 0, companies.size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntity( Company.class )\n\t\t\t\t\t\t.checkpointInterval( CHECKPOINT_INTERVAL )\n\t\t\t\t\t\t.entityManagerFactoryReference( getPersistenceUnitName() )\n\t\t\t\t\t\t.build()\n\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tJobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\n\t\tcompanies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( INSTANCES_PER_DATA_TEMPLATE, companies.size() );\n\t}\n\n\t@Test\n\tpublic void persistenceUnitNamespace() throws Exception {\n\t\tList<Company> companies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( 0, companies.size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntity( Company.class )\n\t\t\t\t\t\t.checkpointInterval( CHECKPOINT_INTERVAL )\n\t\t\t\t\t\t.entityManagerFactoryNamespace( \"persistence-unit-name\" )\n\t\t\t\t\t\t.entityManagerFactoryReference( getPersistenceUnitName() )\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tJobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\n\t\tcompanies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( INSTANCES_PER_DATA_TEMPLATE, companies.size() );\n\t}\n\n\t@Test\n\tpublic void sessionFactoryNamespace() throws Exception {\n\t\tList<Company> companies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( 0, companies.size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntity( Company.class )\n\t\t\t\t\t\t.checkpointInterval( CHECKPOINT_INTERVAL )\n\t\t\t\t\t\t.entityManagerFactoryNamespace( \"session-factory-name\" )\n\t\t\t\t\t\t.entityManagerFactoryReference( SESSION_FACTORY_NAME )\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tJobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\n\t\tcompanies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( INSTANCES_PER_DATA_TEMPLATE, companies.size() );\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "@After\n\tpublic void shutdown() {\n\t\tif ( emf != null ) {\n\t\t\temf.close();\n\t\t}\n\t}",
        "diffSourceCode": "-   83: \t\t\t\trowsPerPartition,\n-   84: \t\t\t\tnull,\n-   85: \t\t\t\tnull,\n-   86: \t\t\t\tmockedJobContext\n-   87: \t\t);\n+   83: \t@After\n+   84: \tpublic void shutdown() {\n+   85: \t\tif ( emf != null ) {\n+   86: \t\t\temf.close();\n+   87: \t\t}\n    88: \t}\n-   90: \t@After\n-   91: \tpublic void shutDown() {\n-   92: \t\tif ( emf.isOpen() ) {\n-   93: \t\t\temf.close();\n-   94: \t\t}\n-   95: \t}\n+   90: \tprotected String getPersistenceUnitName() {\n+   91: \t\treturn PERSISTENCE_UNIT_NAME;\n+   92: \t}\n+   93: \n+   94: \t@Test\n+   95: \tpublic void defaultNamespace() throws Exception {\n",
        "uniqueId": "314cff098d6147b783fa091ce3ebcc54a87522aa_90_95__83_88",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Move And Rename Method",
        "description": "Move And Rename Method\tpublic shutDown() : void from class org.hibernate.search.integrationtest.batch.jsr352.component.EntityReaderComponentIT to public shutdown() : void from class org.hibernate.search.integrationtest.batch.jsr352.massindexing.EntityManagerFactoryRetrievalIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm-batch-jsr352/src/test/java/org/hibernate/search/integrationtest/batch/jsr352/component/EntityReaderComponentIT.java",
                "startLine": 109,
                "endLine": 114,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm-batch-jsr352/src/test/java/org/hibernate/search/integrationtest/batch/jsr352/massindexing/EntityManagerFactoryRetrievalIT.java",
                "startLine": 83,
                "endLine": 88,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@After\n\tpublic void shutDown() {\n\t\tif ( emf.isOpen() ) {\n\t\t\temf.close();\n\t\t}\n\t}",
        "filePathBefore": "integrationtest/mapper/orm-batch-jsr352/src/test/java/org/hibernate/search/integrationtest/batch/jsr352/component/EntityReaderComponentIT.java",
        "isPureRefactoring": true,
        "commitId": "314cff098d6147b783fa091ce3ebcc54a87522aa",
        "packageNameBefore": "org.hibernate.search.integrationtest.batch.jsr352.component",
        "classNameBefore": "org.hibernate.search.integrationtest.batch.jsr352.component.EntityReaderComponentIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.batch.jsr352.component.EntityReaderComponentIT#shutDown",
        "classSignatureBefore": "public class EntityReaderComponentIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.batch.jsr352.component.EntityReaderComponentIT#shutDown"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.batch.jsr352.component.EntityReaderComponentIT"
        ],
        "classSignatureBeforeSet": [
            "public class EntityReaderComponentIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.batch.jsr352.component;\n\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertNull;\nimport static org.mockito.ArgumentMatchers.any;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.when;\n\nimport java.util.Arrays;\nimport java.util.List;\nimport javax.batch.runtime.context.JobContext;\nimport javax.batch.runtime.context.StepContext;\nimport javax.persistence.EntityManager;\nimport javax.persistence.EntityManagerFactory;\nimport javax.persistence.Persistence;\n\nimport org.hibernate.CacheMode;\nimport org.hibernate.search.batch.jsr352.core.massindexing.impl.JobContextData;\nimport org.hibernate.search.batch.jsr352.core.massindexing.step.impl.IndexScope;\nimport org.hibernate.search.batch.jsr352.core.massindexing.step.spi.EntityReader;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.Company;\nimport org.hibernate.search.integrationtest.batch.jsr352.util.JobTestUtil;\nimport org.hibernate.search.integrationtest.batch.jsr352.util.PersistenceUnitTestUtil;\n\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.mockito.junit.MockitoJUnit;\nimport org.mockito.junit.MockitoRule;\nimport org.mockito.quality.Strictness;\n\n/**\n * Single-component test for item reader validation.\n *\n * @author Mincong Huang\n */\npublic class EntityReaderComponentIT {\n\n\tprivate static final String PERSISTENCE_UNIT_NAME = PersistenceUnitTestUtil.getPersistenceUnitName();\n\n\tprivate static final List<Company> COMPANIES = Arrays.asList(\n\t\t\tnew Company( \"Red Hat\" ),\n\t\t\tnew Company( \"Google\" ),\n\t\t\tnew Company( \"Microsoft\" )\n\t);\n\n\t@Rule\n\tpublic final MockitoRule mockito = MockitoJUnit.rule().strictness( Strictness.STRICT_STUBS );\n\n\tprivate EntityManagerFactory emf;\n\n\tprivate JobContext mockedJobContext;\n\n\tprivate StepContext mockedStepContext;\n\n\tprivate EntityReader entityReader;\n\n\t@Before\n\tpublic void setUp() {\n\t\tEntityManager em = null;\n\t\ttry {\n\t\t\temf = Persistence.createEntityManagerFactory( PERSISTENCE_UNIT_NAME );\n\t\t\tem = emf.createEntityManager();\n\t\t\tem.getTransaction().begin();\n\t\t\tCOMPANIES.forEach( em::persist );\n\t\t\tem.getTransaction().commit();\n\t\t}\n\t\tfinally {\n\t\t\tif ( em != null ) {\n\t\t\t\tem.close();\n\t\t\t}\n\t\t}\n\n\t\tfinal String cacheMode = CacheMode.IGNORE.name();\n\t\tfinal String entityName = Company.class.getName();\n\t\tfinal String entityFetchSize = String.valueOf( 1000 );\n\t\tfinal String checkpointInterval = String.valueOf( 1000 );\n\t\tfinal String sessionClearInterval = String.valueOf( 100 );\n\t\tfinal String hql = null;\n\t\tfinal String maxResults = String.valueOf( Integer.MAX_VALUE );\n\t\tfinal String partitionId = String.valueOf( 0 );\n\n\t\tmockedJobContext = mock( JobContext.class );\n\t\tmockedStepContext = mock( StepContext.class );\n\n\t\tentityReader = new EntityReader( cacheMode,\n\t\t\t\tentityName,\n\t\t\t\tentityFetchSize,\n\t\t\t\tcheckpointInterval,\n\t\t\t\tsessionClearInterval,\n\t\t\t\thql,\n\t\t\t\tmaxResults,\n\t\t\t\tpartitionId,\n\t\t\t\tnull,\n\t\t\t\tnull,\n\t\t\t\tIndexScope.FULL_ENTITY.name(),\n\t\t\t\tmockedJobContext,\n\t\t\t\tmockedStepContext );\n\t}\n\n\t@After\n\tpublic void shutDown() {\n\t\tif ( emf.isOpen() ) {\n\t\t\temf.close();\n\t\t}\n\t}\n\n\t@Test\n\tpublic void testReadItem_withoutBoundary() throws Exception {\n\t\tJobContextData jobData = new JobContextData();\n\t\tjobData.setEntityManagerFactory( emf );\n\t\tjobData.setEntityTypeDescriptors( Arrays.asList( JobTestUtil.createSimpleEntityTypeDescriptor( emf, Company.class ) ) );\n\n\t\twhen( mockedJobContext.getTransientUserData() ).thenReturn( jobData );\n\t\tmockedStepContext.setTransientUserData( any() );\n\n\t\ttry {\n\t\t\tentityReader.open( null );\n\t\t\tfor ( Company expected : COMPANIES ) {\n\t\t\t\tCompany actual = (Company) entityReader.readItem();\n\t\t\t\tassertEquals( expected.getName(), actual.getName() );\n\t\t\t}\n\t\t\t// no more item\n\t\t\tassertNull( entityReader.readItem() );\n\t\t}\n\t\tfinally {\n\t\t\tentityReader.close();\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/orm-batch-jsr352/src/test/java/org/hibernate/search/integrationtest/batch/jsr352/massindexing/EntityManagerFactoryRetrievalIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.batch.jsr352.massindexing;\n\nimport static org.hibernate.search.integrationtest.batch.jsr352.util.JobTestUtil.JOB_TIMEOUT_MS;\nimport static org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils.with;\nimport static org.junit.Assert.assertEquals;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport javax.batch.operations.JobOperator;\nimport javax.batch.runtime.JobExecution;\nimport javax.persistence.EntityManagerFactory;\nimport javax.persistence.Persistence;\n\nimport org.hibernate.search.batch.jsr352.core.massindexing.MassIndexingJob;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.Person;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.WhoAmI;\nimport org.hibernate.search.integrationtest.batch.jsr352.util.JobTestUtil;\nimport org.hibernate.search.integrationtest.batch.jsr352.massindexing.entity.Company;\nimport org.hibernate.search.integrationtest.batch.jsr352.util.PersistenceUnitTestUtil;\n\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\n\n/**\n * @author Mincong Huang\n */\npublic class EntityManagerFactoryRetrievalIT {\n\n\tprivate static final String PERSISTENCE_UNIT_NAME = PersistenceUnitTestUtil.getPersistenceUnitName();\n\n\tprotected static final int INSTANCES_PER_DATA_TEMPLATE = 100;\n\n\t// We have three data templates per entity type (see setup)\n\tprotected static final int INSTANCE_PER_ENTITY_TYPE = INSTANCES_PER_DATA_TEMPLATE * 3;\n\n\t/*\n\t * Make sure to have more than one checkpoint,\n\t * because we had errors related to that in the past.\n\t */\n\tprivate static final int CHECKPOINT_INTERVAL = 10;\n\n\tprivate static final String SESSION_FACTORY_NAME = \"primary_session_factory\";\n\n\tprotected JobOperator jobOperator;\n\tprotected EntityManagerFactory emf;\n\n\t@Before\n\tpublic void setup() {\n\t\tjobOperator = JobTestUtil.getAndCheckRuntime();\n\t\tList<Company> companies = new ArrayList<>();\n\t\tList<Person> people = new ArrayList<>();\n\t\tList<WhoAmI> whos = new ArrayList<>();\n\t\tfor ( int i = 0; i < INSTANCE_PER_ENTITY_TYPE; i += 3 ) {\n\t\t\tint index1 = i;\n\t\t\tint index2 = i + 1;\n\t\t\tint index3 = i + 2;\n\t\t\tcompanies.add( new Company( \"Google \" + index1 ) );\n\t\t\tcompanies.add( new Company( \"Red Hat \" + index2 ) );\n\t\t\tcompanies.add( new Company( \"Microsoft \" + index3 ) );\n\t\t\tpeople.add( new Person( \"BG \" + index1, \"Bill\", \"Gates\" ) );\n\t\t\tpeople.add( new Person( \"LT \" + index2, \"Linus\", \"Torvalds\" ) );\n\t\t\tpeople.add( new Person( \"SJ \" + index3, \"Steven\", \"Jobs\" ) );\n\t\t\twhos.add( new WhoAmI( \"cid01 \" + index1, \"id01 \" + index1, \"uid01 \" + index1 ) );\n\t\t\twhos.add( new WhoAmI( \"cid02 \" + index2, \"id02 \" + index2, \"uid02 \" + index2 ) );\n\t\t\twhos.add( new WhoAmI( \"cid03 \" + index3, \"id03 \" + index3, \"uid03 \" + index3 ) );\n\t\t}\n\n\t\temf = Persistence.createEntityManagerFactory( getPersistenceUnitName() );\n\t\twith( emf ).runInTransaction( em -> {\n\t\t\tcompanies.forEach( em::persist );\n\t\t\tpeople.forEach( em::persist );\n\t\t\twhos.forEach( em::persist );\n\t\t} );\n\t}\n\n\t@After\n\tpublic void shutdown() {\n\t\tif ( emf != null ) {\n\t\t\temf.close();\n\t\t}\n\t}\n\n\tprotected String getPersistenceUnitName() {\n\t\treturn PERSISTENCE_UNIT_NAME;\n\t}\n\n\t@Test\n\tpublic void defaultNamespace() throws Exception {\n\t\tList<Company> companies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( 0, companies.size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntity( Company.class )\n\t\t\t\t\t\t.checkpointInterval( CHECKPOINT_INTERVAL )\n\t\t\t\t\t\t.entityManagerFactoryReference( getPersistenceUnitName() )\n\t\t\t\t\t\t.build()\n\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tJobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\n\t\tcompanies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( INSTANCES_PER_DATA_TEMPLATE, companies.size() );\n\t}\n\n\t@Test\n\tpublic void persistenceUnitNamespace() throws Exception {\n\t\tList<Company> companies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( 0, companies.size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntity( Company.class )\n\t\t\t\t\t\t.checkpointInterval( CHECKPOINT_INTERVAL )\n\t\t\t\t\t\t.entityManagerFactoryNamespace( \"persistence-unit-name\" )\n\t\t\t\t\t\t.entityManagerFactoryReference( getPersistenceUnitName() )\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tJobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\n\t\tcompanies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( INSTANCES_PER_DATA_TEMPLATE, companies.size() );\n\t}\n\n\t@Test\n\tpublic void sessionFactoryNamespace() throws Exception {\n\t\tList<Company> companies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( 0, companies.size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntity( Company.class )\n\t\t\t\t\t\t.checkpointInterval( CHECKPOINT_INTERVAL )\n\t\t\t\t\t\t.entityManagerFactoryNamespace( \"session-factory-name\" )\n\t\t\t\t\t\t.entityManagerFactoryReference( SESSION_FACTORY_NAME )\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tJobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\n\t\tcompanies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n\t\tassertEquals( INSTANCES_PER_DATA_TEMPLATE, companies.size() );\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "@After\n\tpublic void shutdown() {\n\t\tif ( emf != null ) {\n\t\t\temf.close();\n\t\t}\n\t}",
        "diffSourceCode": "-   83: \t\tfinal String entityName = Company.class.getName();\n-   84: \t\tfinal String entityFetchSize = String.valueOf( 1000 );\n-   85: \t\tfinal String checkpointInterval = String.valueOf( 1000 );\n-   86: \t\tfinal String sessionClearInterval = String.valueOf( 100 );\n-   87: \t\tfinal String hql = null;\n-   88: \t\tfinal String maxResults = String.valueOf( Integer.MAX_VALUE );\n-  109: \t@After\n-  110: \tpublic void shutDown() {\n-  111: \t\tif ( emf.isOpen() ) {\n-  112: \t\t\temf.close();\n-  113: \t\t}\n-  114: \t}\n+   83: \t@After\n+   84: \tpublic void shutdown() {\n+   85: \t\tif ( emf != null ) {\n+   86: \t\t\temf.close();\n+   87: \t\t}\n+   88: \t}\n+  109: \n+  110: \t\tcompanies = JobTestUtil.findIndexedResults( emf, Company.class, \"name\", \"Google\" );\n+  111: \t\tassertEquals( INSTANCES_PER_DATA_TEMPLATE, companies.size() );\n+  112: \t}\n+  113: \n+  114: \t@Test\n",
        "uniqueId": "314cff098d6147b783fa091ce3ebcc54a87522aa_109_114__83_88",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Move And Inline Method",
        "description": "Move And Inline Method\tpublic rawValue(value F) : Object moved from class org.hibernate.search.integrationtest.backend.tck.testsupport.types.FieldTypeDescriptor to class org.hibernate.search.integrationtest.backend.lucene.testsupport.util.LuceneTckBackendFeatures & inlined to public toRawValue(descriptor FieldTypeDescriptor<F,?>, value F) : Object",
        "diffLocations": [
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/testsupport/util/LuceneTckBackendFeatures.java",
                "startLine": 67,
                "endLine": 70,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/testsupport/util/LuceneTckBackendFeatures.java",
                "startLine": 103,
                "endLine": 163,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/testsupport/util/LuceneTckBackendFeatures.java",
                "startLine": 228,
                "endLine": 230,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "",
        "filePathBefore": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/testsupport/util/LuceneTckBackendFeatures.java",
        "isPureRefactoring": true,
        "commitId": "6b65d5b3d028b6cf672822b29edb9610db6adca4",
        "packageNameBefore": "org.hibernate.search.integrationtest.backend.tck.testsupport.types",
        "classNameBefore": "org.hibernate.search.integrationtest.backend.tck.testsupport.types.FieldTypeDescriptor",
        "methodNameBefore": "org.hibernate.search.integrationtest.backend.tck.testsupport.types.FieldTypeDescriptor#rawValue",
        "classSignatureBefore": "public abstract class FieldTypeDescriptor<F, S extends SearchableProjectableIndexFieldTypeOptionsStep<?, F>> ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.tck.testsupport.types.FieldTypeDescriptor#rawValue"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.tck.testsupport.types.FieldTypeDescriptor"
        ],
        "classSignatureBeforeSet": [
            "public abstract class FieldTypeDescriptor<F, S extends SearchableProjectableIndexFieldTypeOptionsStep<?, F>> "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * SPDX-License-Identifier: Apache-2.0\n * Copyright Red Hat Inc. and Hibernate Authors\n */\npackage org.hibernate.search.integrationtest.backend.lucene.testsupport.util;\n\nimport org.hibernate.search.engine.backend.types.ObjectStructure;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.FieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.TckBackendFeatures;\n\nclass LuceneTckBackendFeatures extends TckBackendFeatures {\n\n\t@Override\n\tpublic boolean nonDefaultOrderInTermsAggregations() {\n\t\t// TODO HSEARCH-3666 Lucene terms aggregations (discrete facets) may return wrong results for any sort other than the default one\n\t\treturn false;\n\t}\n\n\t@Override\n\tpublic boolean projectionPreservesNulls() {\n\t\treturn false;\n\t}\n\n\t@Override\n\tpublic boolean fieldsProjectableByDefault() {\n\t\treturn false;\n\t}\n\n\t@Override\n\tpublic boolean projectionPreservesEmptySingleValuedObject(ObjectStructure structure) {\n\t\t// For single-valued, flattened object fields,\n\t\t// we cannot distinguish between an empty object (non-null object, but no subfield carries a value)\n\t\t// and an empty object.\n\t\treturn ObjectStructure.NESTED.equals( structure );\n\t}\n\n\t@Override\n\tpublic boolean reliesOnNestedDocumentsForMultiValuedObjectProjection() {\n\t\treturn true;\n\t}\n\n\t@Override\n\tpublic boolean supportsHighlightableWithoutProjectable() {\n\t\t// The Lucene backend relies on stored values for highlighting\n\t\treturn false;\n\t}\n\n\t@Override\n\tpublic boolean supportsHighlighterUnifiedTypeNoMatchSize() {\n\t\t// Lucene default unified highlighter does not support no-match-size setting.\n\t\t// While in ES a custom highlighter is used that allows for such option.\n\t\treturn false;\n\t}\n\n\t@Override\n\tpublic boolean supportsHighlighterUnifiedTypeFragmentSize() {\n\t\t// Break iterators from `java.text.BreakIterator` do not allow for such config.\n\t\t// While in ES a custom iterator is available that wraps sentence and word break iterators and is using the max size option.\n\t\treturn false;\n\t}\n\n\t@Override\n\tpublic boolean supportsHighlighterUnifiedPhraseMatching() {\n\t\treturn true;\n\t}\n\n\t@Override\n\tpublic <F> Object toRawValue(FieldTypeDescriptor<F, ?> descriptor, F value) {\n\t\treturn descriptor.rawValue( value );\n\t}\n}\n",
        "filePathAfter": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/testsupport/util/LuceneTckBackendFeatures.java",
        "sourceCodeAfterForWhole": "/*\n * SPDX-License-Identifier: Apache-2.0\n * Copyright Red Hat Inc. and Hibernate Authors\n */\npackage org.hibernate.search.integrationtest.backend.lucene.testsupport.util;\n\nimport java.math.BigDecimal;\nimport java.math.BigInteger;\nimport java.math.RoundingMode;\nimport java.time.Instant;\nimport java.time.LocalDate;\nimport java.time.LocalDateTime;\nimport java.time.LocalTime;\nimport java.time.MonthDay;\nimport java.time.OffsetDateTime;\nimport java.time.OffsetTime;\nimport java.time.Year;\nimport java.time.YearMonth;\nimport java.time.ZoneOffset;\nimport java.time.ZonedDateTime;\nimport java.time.temporal.ChronoField;\nimport java.time.temporal.TemporalAccessor;\n\nimport org.hibernate.search.engine.backend.types.ObjectStructure;\nimport org.hibernate.search.engine.spatial.GeoPoint;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.BigDecimalFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.BigIntegerFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.BooleanFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.ByteFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.FieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.GeoPointFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.InstantFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.LocalDateFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.LocalDateTimeFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.LocalTimeFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.MonthDayFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.OffsetDateTimeFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.OffsetTimeFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.ShortFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.YearFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.YearMonthFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.types.ZonedDateTimeFieldTypeDescriptor;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.TckBackendFeatures;\n\nimport org.apache.lucene.document.DoublePoint;\n\nclass LuceneTckBackendFeatures extends TckBackendFeatures {\n\n\t@Override\n\tpublic boolean nonDefaultOrderInTermsAggregations() {\n\t\t// TODO HSEARCH-3666 Lucene terms aggregations (discrete facets) may return wrong results for any sort other than the default one\n\t\treturn false;\n\t}\n\n\t@Override\n\tpublic boolean projectionPreservesNulls() {\n\t\treturn false;\n\t}\n\n\t@Override\n\tpublic boolean fieldsProjectableByDefault() {\n\t\treturn false;\n\t}\n\n\t@Override\n\tpublic boolean projectionPreservesEmptySingleValuedObject(ObjectStructure structure) {\n\t\t// For single-valued, flattened object fields,\n\t\t// we cannot distinguish between an empty object (non-null object, but no subfield carries a value)\n\t\t// and an empty object.\n\t\treturn ObjectStructure.NESTED.equals( structure );\n\t}\n\n\t@Override\n\tpublic boolean reliesOnNestedDocumentsForMultiValuedObjectProjection() {\n\t\treturn true;\n\t}\n\n\t@Override\n\tpublic boolean supportsHighlightableWithoutProjectable() {\n\t\t// The Lucene backend relies on stored values for highlighting\n\t\treturn false;\n\t}\n\n\t@Override\n\tpublic boolean supportsHighlighterUnifiedTypeNoMatchSize() {\n\t\t// Lucene default unified highlighter does not support no-match-size setting.\n\t\t// While in ES a custom highlighter is used that allows for such option.\n\t\treturn false;\n\t}\n\n\t@Override\n\tpublic boolean supportsHighlighterUnifiedTypeFragmentSize() {\n\t\t// Break iterators from `java.text.BreakIterator` do not allow for such config.\n\t\t// While in ES a custom iterator is available that wraps sentence and word break iterators and is using the max size option.\n\t\treturn false;\n\t}\n\n\t@Override\n\tpublic boolean supportsHighlighterUnifiedPhraseMatching() {\n\t\treturn true;\n\t}\n\n\t@Override\n\tpublic <F> Object toRawValue(FieldTypeDescriptor<F, ?> descriptor, F value) {\n\t\tif ( value == null ) {\n\t\t\treturn null;\n\t\t}\n\t\tif ( BigIntegerFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn new BigDecimal( ( (BigInteger) value ) ).setScale( -2, RoundingMode.HALF_UP ).unscaledValue().longValue();\n\t\t}\n\t\tif ( BigDecimalFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (BigDecimal) value ).setScale( 2, RoundingMode.HALF_UP ).unscaledValue().longValue();\n\t\t}\n\t\tif ( GeoPointFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\tbyte[] bytes = new byte[2 * Double.BYTES];\n\t\t\tDoublePoint.encodeDimension( ( (GeoPoint) value ).latitude(), bytes, 0 );\n\t\t\tDoublePoint.encodeDimension( ( (GeoPoint) value ).longitude(), bytes, Double.BYTES );\n\t\t\treturn bytes;\n\t\t}\n\t\tif ( InstantFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (Instant) value ).toEpochMilli();\n\t\t}\n\t\tif ( OffsetTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\t// see private method OffsetTime#toEpochNano:\n\t\t\tlong nod = ( (OffsetTime) value ).toLocalTime().toNanoOfDay();\n\t\t\tlong offsetNanos = ( (OffsetTime) value ).getOffset().getTotalSeconds() * 1_000_000_000L;\n\t\t\treturn nod - offsetNanos;\n\t\t}\n\t\tif ( YearFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (Year) value ).getValue();\n\t\t}\n\t\tif ( ZonedDateTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (ZonedDateTime) value ).toInstant().toEpochMilli();\n\t\t}\n\t\tif ( LocalDateTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (LocalDateTime) value ).toInstant( ZoneOffset.UTC ).toEpochMilli();\n\t\t}\n\t\tif ( LocalTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (LocalTime) value ).toNanoOfDay();\n\t\t}\n\t\tif ( LocalDateFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (LocalDate) value ).toEpochDay();\n\t\t}\n\t\tif ( BooleanFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn Boolean.TRUE.equals( value ) ? 1 : 0;\n\t\t}\n\t\tif ( MonthDayFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn 100 * ( (MonthDay) value ).getMonthValue() + ( (MonthDay) value ).getDayOfMonth();\n\t\t}\n\t\tif ( YearMonthFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (YearMonth) value ).getLong( ChronoField.PROLEPTIC_MONTH );\n\t\t}\n\t\tif ( OffsetDateTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (OffsetDateTime) value ).toInstant().toEpochMilli();\n\t\t}\n\t\tif ( ShortFieldTypeDescriptor.INSTANCE.equals( descriptor )\n\t\t\t\t|| ByteFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (Number) value ).intValue();\n\t\t}\n\n\n\t\treturn value;\n\t}\n\n\t@Override\n\tpublic <F> Class<?> rawType(FieldTypeDescriptor<F, ?> descriptor) {\n\t\tif ( BooleanFieldTypeDescriptor.INSTANCE.equals( descriptor )\n\t\t\t\t|| ByteFieldTypeDescriptor.INSTANCE.equals( descriptor )\n\t\t\t\t|| ShortFieldTypeDescriptor.INSTANCE.equals( descriptor )\n\t\t\t\t|| MonthDayFieldTypeDescriptor.INSTANCE.equals( descriptor )\n\t\t\t\t|| YearFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn Integer.class;\n\t\t}\n\t\tif ( GeoPointFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn byte[].class;\n\t\t}\n\t\tif ( TemporalAccessor.class.isAssignableFrom( descriptor.getJavaType() )\n\t\t\t\t|| BigDecimalFieldTypeDescriptor.INSTANCE.equals( descriptor )\n\t\t\t\t|| BigIntegerFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn Long.class;\n\t\t}\n\t\treturn descriptor.getJavaType();\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "@Override\n\tpublic <F> Object toRawValue(FieldTypeDescriptor<F, ?> descriptor, F value) {\n\t\tif ( value == null ) {\n\t\t\treturn null;\n\t\t}\n\t\tif ( BigIntegerFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn new BigDecimal( ( (BigInteger) value ) ).setScale( -2, RoundingMode.HALF_UP ).unscaledValue().longValue();\n\t\t}\n\t\tif ( BigDecimalFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (BigDecimal) value ).setScale( 2, RoundingMode.HALF_UP ).unscaledValue().longValue();\n\t\t}\n\t\tif ( GeoPointFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\tbyte[] bytes = new byte[2 * Double.BYTES];\n\t\t\tDoublePoint.encodeDimension( ( (GeoPoint) value ).latitude(), bytes, 0 );\n\t\t\tDoublePoint.encodeDimension( ( (GeoPoint) value ).longitude(), bytes, Double.BYTES );\n\t\t\treturn bytes;\n\t\t}\n\t\tif ( InstantFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (Instant) value ).toEpochMilli();\n\t\t}\n\t\tif ( OffsetTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\t// see private method OffsetTime#toEpochNano:\n\t\t\tlong nod = ( (OffsetTime) value ).toLocalTime().toNanoOfDay();\n\t\t\tlong offsetNanos = ( (OffsetTime) value ).getOffset().getTotalSeconds() * 1_000_000_000L;\n\t\t\treturn nod - offsetNanos;\n\t\t}\n\t\tif ( YearFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (Year) value ).getValue();\n\t\t}\n\t\tif ( ZonedDateTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (ZonedDateTime) value ).toInstant().toEpochMilli();\n\t\t}\n\t\tif ( LocalDateTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (LocalDateTime) value ).toInstant( ZoneOffset.UTC ).toEpochMilli();\n\t\t}\n\t\tif ( LocalTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (LocalTime) value ).toNanoOfDay();\n\t\t}\n\t\tif ( LocalDateFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (LocalDate) value ).toEpochDay();\n\t\t}\n\t\tif ( BooleanFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn Boolean.TRUE.equals( value ) ? 1 : 0;\n\t\t}\n\t\tif ( MonthDayFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn 100 * ( (MonthDay) value ).getMonthValue() + ( (MonthDay) value ).getDayOfMonth();\n\t\t}\n\t\tif ( YearMonthFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (YearMonth) value ).getLong( ChronoField.PROLEPTIC_MONTH );\n\t\t}\n\t\tif ( OffsetDateTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (OffsetDateTime) value ).toInstant().toEpochMilli();\n\t\t}\n\t\tif ( ShortFieldTypeDescriptor.INSTANCE.equals( descriptor )\n\t\t\t\t|| ByteFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n\t\t\treturn ( (Number) value ).intValue();\n\t\t}\n\n\n\t\treturn value;\n\t}",
        "diffSourceCode": "-   67: \t@Override\n-   68: \tpublic <F> Object toRawValue(FieldTypeDescriptor<F, ?> descriptor, F value) {\n-   69: \t\treturn descriptor.rawValue( value );\n-   70: \t}\n+   67: \t\t// For single-valued, flattened object fields,\n+   68: \t\t// we cannot distinguish between an empty object (non-null object, but no subfield carries a value)\n+   69: \t\t// and an empty object.\n+   70: \t\treturn ObjectStructure.NESTED.equals( structure );\n+  103: \t@Override\n+  104: \tpublic <F> Object toRawValue(FieldTypeDescriptor<F, ?> descriptor, F value) {\n+  105: \t\tif ( value == null ) {\n+  106: \t\t\treturn null;\n+  107: \t\t}\n+  108: \t\tif ( BigIntegerFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  109: \t\t\treturn new BigDecimal( ( (BigInteger) value ) ).setScale( -2, RoundingMode.HALF_UP ).unscaledValue().longValue();\n+  110: \t\t}\n+  111: \t\tif ( BigDecimalFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  112: \t\t\treturn ( (BigDecimal) value ).setScale( 2, RoundingMode.HALF_UP ).unscaledValue().longValue();\n+  113: \t\t}\n+  114: \t\tif ( GeoPointFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  115: \t\t\tbyte[] bytes = new byte[2 * Double.BYTES];\n+  116: \t\t\tDoublePoint.encodeDimension( ( (GeoPoint) value ).latitude(), bytes, 0 );\n+  117: \t\t\tDoublePoint.encodeDimension( ( (GeoPoint) value ).longitude(), bytes, Double.BYTES );\n+  118: \t\t\treturn bytes;\n+  119: \t\t}\n+  120: \t\tif ( InstantFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  121: \t\t\treturn ( (Instant) value ).toEpochMilli();\n+  122: \t\t}\n+  123: \t\tif ( OffsetTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  124: \t\t\t// see private method OffsetTime#toEpochNano:\n+  125: \t\t\tlong nod = ( (OffsetTime) value ).toLocalTime().toNanoOfDay();\n+  126: \t\t\tlong offsetNanos = ( (OffsetTime) value ).getOffset().getTotalSeconds() * 1_000_000_000L;\n+  127: \t\t\treturn nod - offsetNanos;\n+  128: \t\t}\n+  129: \t\tif ( YearFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  130: \t\t\treturn ( (Year) value ).getValue();\n+  131: \t\t}\n+  132: \t\tif ( ZonedDateTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  133: \t\t\treturn ( (ZonedDateTime) value ).toInstant().toEpochMilli();\n+  134: \t\t}\n+  135: \t\tif ( LocalDateTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  136: \t\t\treturn ( (LocalDateTime) value ).toInstant( ZoneOffset.UTC ).toEpochMilli();\n+  137: \t\t}\n+  138: \t\tif ( LocalTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  139: \t\t\treturn ( (LocalTime) value ).toNanoOfDay();\n+  140: \t\t}\n+  141: \t\tif ( LocalDateFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  142: \t\t\treturn ( (LocalDate) value ).toEpochDay();\n+  143: \t\t}\n+  144: \t\tif ( BooleanFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  145: \t\t\treturn Boolean.TRUE.equals( value ) ? 1 : 0;\n+  146: \t\t}\n+  147: \t\tif ( MonthDayFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  148: \t\t\treturn 100 * ( (MonthDay) value ).getMonthValue() + ( (MonthDay) value ).getDayOfMonth();\n+  149: \t\t}\n+  150: \t\tif ( YearMonthFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  151: \t\t\treturn ( (YearMonth) value ).getLong( ChronoField.PROLEPTIC_MONTH );\n+  152: \t\t}\n+  153: \t\tif ( OffsetDateTimeFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  154: \t\t\treturn ( (OffsetDateTime) value ).toInstant().toEpochMilli();\n+  155: \t\t}\n+  156: \t\tif ( ShortFieldTypeDescriptor.INSTANCE.equals( descriptor )\n+  157: \t\t\t\t|| ByteFieldTypeDescriptor.INSTANCE.equals( descriptor ) ) {\n+  158: \t\t\treturn ( (Number) value ).intValue();\n+  159: \t\t}\n+  160: \n+  161: \n+  162: \t\treturn value;\n+  163: \t}\n",
        "uniqueId": "6b65d5b3d028b6cf672822b29edb9610db6adca4_67_70__103_163_228_230",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Inline Method",
        "description": "Inline Method\tprivate doBefore() : void inlined to public beforeAll(extensionContext ExtensionContext) : void in class org.hibernate.search.util.impl.integrationtest.common.extension.BackendMock",
        "diffLocations": [
            {
                "filePath": "util/internal/integrationtest/common/src/main/java/org/hibernate/search/util/impl/integrationtest/common/extension/BackendMock.java",
                "startLine": 67,
                "endLine": 71,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/integrationtest/common/src/main/java/org/hibernate/search/util/impl/integrationtest/common/extension/BackendMock.java",
                "startLine": 66,
                "endLine": 70,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/integrationtest/common/src/main/java/org/hibernate/search/util/impl/integrationtest/common/extension/BackendMock.java",
                "startLine": 94,
                "endLine": 96,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void doBefore() {\n\t\tstarted = true;\n\t}",
        "filePathBefore": "util/internal/integrationtest/common/src/main/java/org/hibernate/search/util/impl/integrationtest/common/extension/BackendMock.java",
        "isPureRefactoring": true,
        "commitId": "7340d5e3553f69f655aa71a003f99f5d05abbeec",
        "packageNameBefore": "org.hibernate.search.util.impl.integrationtest.common.extension",
        "classNameBefore": "org.hibernate.search.util.impl.integrationtest.common.extension.BackendMock",
        "methodNameBefore": "org.hibernate.search.util.impl.integrationtest.common.extension.BackendMock#doBefore",
        "classSignatureBefore": "public class BackendMock implements BeforeEachCallback, AfterEachCallback, BeforeAllCallback, AfterAllCallback ",
        "methodNameBeforeSet": [
            "org.hibernate.search.util.impl.integrationtest.common.extension.BackendMock#doBefore"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.util.impl.integrationtest.common.extension.BackendMock"
        ],
        "classSignatureBeforeSet": [
            "public class BackendMock implements BeforeEachCallback, AfterEachCallback, BeforeAllCallback, AfterAllCallback "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.integrationtest.common.extension;\n\nimport static org.hibernate.search.util.common.impl.CollectionHelper.asSetIgnoreNull;\n\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.LinkedHashSet;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.function.Consumer;\nimport java.util.function.Supplier;\n\nimport org.hibernate.search.engine.backend.common.DocumentReference;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.impl.integrationtest.common.assertion.StubDocumentWorkAssert;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.StubTreeNodeDiffer;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.BackendMappingHandle;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.document.StubDocumentNode;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.document.model.StubIndexSchemaDataNode;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.document.model.impl.StubIndexModel;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubDocumentWork;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubIndexScaleWork;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubSchemaManagementWork;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.impl.StubBackendBuildContext;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.impl.StubBackendFactory;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.impl.StubIndexCreateContext;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.search.query.impl.StubSearchWork;\n\nimport org.junit.jupiter.api.extension.AfterAllCallback;\nimport org.junit.jupiter.api.extension.AfterEachCallback;\nimport org.junit.jupiter.api.extension.BeforeAllCallback;\nimport org.junit.jupiter.api.extension.BeforeEachCallback;\nimport org.junit.jupiter.api.extension.ExtensionContext;\n\nimport org.opentest4j.TestAbortedException;\n\npublic class BackendMock implements BeforeEachCallback, AfterEachCallback, BeforeAllCallback, AfterAllCallback {\n\n\tprivate final VerifyingStubBackendBehavior backendBehavior =\n\t\t\tnew VerifyingStubBackendBehavior( this::indexingWorkExpectations );\n\n\tprivate volatile boolean started = false;\n\tprivate boolean callOncePerClass = false;\n\n\tprivate volatile BackendIndexingWorkExpectations indexingWorkExpectations = BackendIndexingWorkExpectations.sync();\n\n\tprivate final Map<String, StubTreeNodeDiffer<StubDocumentNode>> documentDiffers = new ConcurrentHashMap<>();\n\n\tpublic static BackendMock create() {\n\t\treturn new BackendMock();\n\t}\n\n\tprotected BackendMock() {\n\t}\n\n\t@Override\n\tpublic void beforeAll(ExtensionContext context) {\n\t\tcallOncePerClass = true;\n\t\tdoBefore();\n\t}\n\n\t@Override\n\tpublic void afterAll(ExtensionContext context) {\n\t\tif ( callOncePerClass ) {\n\t\t\tdoAfter( context );\n\t\t}\n\t}\n\n\t@Override\n\tpublic void beforeEach(ExtensionContext context) {\n\t\tif ( !callOncePerClass ) {\n\t\t\tdoBefore();\n\t\t}\n\t}\n\n\t@Override\n\tpublic void afterEach(ExtensionContext context) {\n\t\tif ( !callOncePerClass ) {\n\t\t\tdoAfter( context );\n\t\t}\n\t}\n\n\tprivate void doBefore() {\n\t\tstarted = true;\n\t}\n\n\tprivate void doAfter(ExtensionContext context) {\n\t\ttry {\n\t\t\tif ( context.getExecutionException().map( e -> e instanceof TestAbortedException ).orElse( Boolean.FALSE ) ) {\n\t\t\t\t// test was aborted - hence let's not do verification. cleanups will happen in finally block.\n\t\t\t\treturn;\n\t\t\t}\n\t\t\t// Workaround for a problem in Hibernate ORM's CustomRunner\n\t\t\t// (used by BytecodeEnhancerRunner in particular)\n\t\t\t// which applies class rules twices, resulting in \"started\" being false\n\t\t\t// when we get here in the outermost statement...\n\t\t\tif ( started ) {\n\t\t\t\tverifyExpectationsMet();\n\t\t\t}\n\t\t}\n\t\tfinally {\n\t\t\tif ( started ) {\n\t\t\t\tresetExpectations();\n\t\t\t\tstarted = false;\n\t\t\t\tbackendBehavior.resetBackends();\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic BackendMock ignoreSchema() {\n\t\tbackendBehavior.ignoreSchema( true );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock documentDiffer(String indexName, StubTreeNodeDiffer<StubDocumentNode> differ) {\n\t\tdocumentDiffers.put( indexName, differ );\n\t\treturn this;\n\t}\n\n\tpublic StubBackendFactory factory(CompletionStage<BackendMappingHandle> mappingHandlePromise) {\n\t\treturn new StubBackendFactory( backendBehavior, mappingHandlePromise );\n\t}\n\n\tpublic void indexingWorkExpectations(BackendIndexingWorkExpectations expectations) {\n\t\tindexingWorkExpectations = expectations;\n\t}\n\n\tpublic BackendIndexingWorkExpectations indexingWorkExpectations() {\n\t\treturn indexingWorkExpectations;\n\t}\n\n\tpublic void resetExpectations() {\n\t\tbackendBehavior().resetExpectations();\n\t}\n\n\tpublic void verifyExpectationsMet() {\n\t\tbackendBehavior().verifyExpectationsMet();\n\t}\n\n\tpublic long remainingExpectedIndexingCount() {\n\t\treturn backendBehavior().getDocumentWorkExecuteCalls().values().stream()\n\t\t\t\t.mapToLong( CallQueue::remainingExpectedCallCount )\n\t\t\t\t.sum();\n\t}\n\n\tpublic void inLenientMode(Runnable action) {\n\t\tbackendBehavior().lenient( true );\n\t\ttry {\n\t\t\taction.run();\n\t\t}\n\t\tfinally {\n\t\t\tbackendBehavior().lenient( false );\n\t\t}\n\t}\n\n\tpublic BackendMock onCreate(Consumer<StubBackendBuildContext> behavior) {\n\t\tbackendBehavior().addCreateBackendBehavior( context -> {\n\t\t\tbehavior.accept( context );\n\t\t\treturn null;\n\t\t} );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock onStop(Runnable behavior) {\n\t\tbackendBehavior().addStopBackendBehavior( () -> {\n\t\t\tbehavior.run();\n\t\t\treturn null;\n\t\t} );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock onCreateIndex(Consumer<StubIndexCreateContext> behavior) {\n\t\tbackendBehavior().addCreateIndexBehavior( context -> {\n\t\t\tbehavior.accept( context );\n\t\t\treturn null;\n\t\t} );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock expectFailingField(String indexName, String absoluteFieldPath,\n\t\t\tSupplier<RuntimeException> exceptionSupplier) {\n\t\tbackendBehavior().setIndexFieldAddBehavior( indexName, absoluteFieldPath, () -> {\n\t\t\tthrow exceptionSupplier.get();\n\t\t} );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock expectSchema(String indexName, Consumer<StubIndexSchemaDataNode.Builder> contributor) {\n\t\treturn expectSchema( indexName, contributor, ignored -> {} );\n\t}\n\n\tpublic BackendMock expectSchema(String indexName, Consumer<StubIndexSchemaDataNode.Builder> contributor,\n\t\t\tConsumer<StubIndexModel> capture) {\n\t\tCallQueue<SchemaDefinitionCall> callQueue = backendBehavior().getSchemaDefinitionCalls( indexName );\n\t\tStubIndexSchemaDataNode.Builder builder = StubIndexSchemaDataNode.schema();\n\t\tcontributor.accept( builder );\n\t\tcallQueue.expectOutOfOrder( new SchemaDefinitionCall( indexName, builder.build(), capture ) );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock expectAnySchema(String indexName) {\n\t\tCallQueue<SchemaDefinitionCall> callQueue = backendBehavior().getSchemaDefinitionCalls( indexName );\n\t\tcallQueue.expectOutOfOrder( new SchemaDefinitionCall( indexName, null, null ) );\n\t\treturn this;\n\t}\n\n\tpublic SchemaManagementWorkCallListContext expectSchemaManagementWorks(String indexName) {\n\t\tCallQueue<SchemaManagementWorkCall> callQueue = backendBehavior().getSchemaManagementWorkCalls( indexName );\n\t\treturn new SchemaManagementWorkCallListContext(\n\t\t\t\tindexName,\n\t\t\t\tcallQueue::expectInOrder\n\t\t);\n\t}\n\n\tpublic DocumentWorkCallListContext expectWorks(String indexName) {\n\t\treturn expectWorks( indexName, null );\n\t}\n\n\tpublic DocumentWorkCallListContext expectWorks(String indexName, String tenantId) {\n\t\t// Default to force commit and no refresh, which is what the mapper should use by default\n\t\treturn expectWorks( indexName, tenantId, DocumentCommitStrategy.FORCE, DocumentRefreshStrategy.NONE );\n\t}\n\n\tpublic DocumentWorkCallListContext expectWorks(String indexName,\n\t\t\tDocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\n\t\treturn expectWorks( indexName, null, commitStrategy, refreshStrategy );\n\t}\n\n\tpublic DocumentWorkCallListContext expectWorks(String indexName, String tenantId,\n\t\t\tDocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\n\t\treturn new DocumentWorkCallListContext(\n\t\t\t\tindexName, tenantId,\n\t\t\t\tcommitStrategy, refreshStrategy,\n\t\t\t\tDocumentWorkCallKind.CREATE_AND_EXECUTE, CompletableFuture.completedFuture( null )\n\t\t);\n\t}\n\n\tpublic IndexScaleWorkCallListContext expectIndexScaleWorks(String indexName, String... tenantIds) {\n\t\tCallQueue<IndexScaleWorkCall> callQueue = backendBehavior().getIndexScaleWorkCalls( indexName );\n\t\treturn new IndexScaleWorkCallListContext(\n\t\t\t\tindexName,\n\t\t\t\tasSetIgnoreNull( tenantIds ),\n\t\t\t\tcallQueue::expectInOrder\n\t\t);\n\t}\n\n\tpublic BackendMock expectSearchReferences(Collection<String> indexNames,\n\t\t\tStubSearchWorkBehavior<DocumentReference> behavior) {\n\t\treturn expectSearch( indexNames, b -> {}, behavior );\n\t}\n\n\tpublic BackendMock expectSearchReferences(Collection<String> indexNames,\n\t\t\tConsumer<StubSearchWork.Builder> contributor,\n\t\t\tStubSearchWorkBehavior<DocumentReference> behavior) {\n\t\treturn expectSearch( indexNames, contributor, behavior );\n\t}\n\n\tpublic BackendMock expectSearchIds(Collection<String> indexNames, Consumer<StubSearchWork.Builder> contributor,\n\t\t\tStubSearchWorkBehavior<String> behavior) {\n\t\treturn expectSearch( indexNames, contributor, behavior );\n\t}\n\n\tpublic BackendMock expectSearchObjects(String indexName, StubSearchWorkBehavior<DocumentReference> behavior) {\n\t\treturn expectSearch( Collections.singleton( indexName ), ignored -> {}, behavior );\n\t}\n\n\tpublic BackendMock expectSearchObjects(Collection<String> indexNames, Consumer<StubSearchWork.Builder> contributor,\n\t\t\tStubSearchWorkBehavior<DocumentReference> behavior) {\n\t\treturn expectSearch( indexNames, contributor, behavior );\n\t}\n\n\tpublic BackendMock expectSearchProjection(String indexNames, StubSearchWorkBehavior<?> behavior) {\n\t\treturn expectSearch( Collections.singleton( indexNames ), ignored -> {}, behavior );\n\t}\n\n\tpublic BackendMock expectSearchProjection(String indexNames, Consumer<StubSearchWork.Builder> contributor,\n\t\t\tStubSearchWorkBehavior<?> behavior) {\n\t\treturn expectSearch( Collections.singleton( indexNames ), contributor, behavior );\n\t}\n\n\tpublic BackendMock expectSearchProjection(Collection<String> indexNames, StubSearchWorkBehavior<?> behavior) {\n\t\treturn expectSearch( indexNames, ignored -> {}, behavior );\n\t}\n\n\tpublic BackendMock expectSearchProjection(Collection<String> indexNames,\n\t\t\tConsumer<StubSearchWork.Builder> contributor,\n\t\t\tStubSearchWorkBehavior<?> behavior) {\n\t\treturn expectSearch( indexNames, contributor, behavior );\n\t}\n\n\tprivate BackendMock expectSearch(Collection<String> indexNames, Consumer<StubSearchWork.Builder> contributor,\n\t\t\tStubSearchWorkBehavior<?> behavior) {\n\t\tCallQueue<SearchWorkCall<?>> callQueue = backendBehavior().getSearchWorkCalls();\n\t\tStubSearchWork.Builder builder = StubSearchWork.builder();\n\t\tcontributor.accept( builder );\n\t\tcallQueue.expectInOrder( new SearchWorkCall<>( new LinkedHashSet<>( indexNames ), builder.build(), behavior ) );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock expectCount(Collection<String> indexNames, long expectedResult) {\n\t\tCallQueue<CountWorkCall> callQueue = backendBehavior().getCountWorkCalls();\n\t\tcallQueue.expectInOrder( new CountWorkCall( new LinkedHashSet<>( indexNames ), expectedResult ) );\n\t\treturn this;\n\t}\n\n\tVerifyingStubBackendBehavior backendBehavior() {\n\t\tif ( !started ) {\n\t\t\tthrow new AssertionFailure( \"The backend mock was not configured as a JUnit Extension,\"\n\t\t\t\t\t+ \" or its statement wrapper hasn't started executing yet,\"\n\t\t\t\t\t+ \" or its statement wrapper has finished executing.\"\n\t\t\t\t\t+ \" Double check the @RegisterExtension annotation and the execution order of extensions.\" );\n\t\t}\n\t\treturn backendBehavior;\n\t}\n\n\tpublic BackendMock expectScrollObjects(Collection<String> indexNames, int chunkSize,\n\t\t\tConsumer<StubSearchWork.Builder> contributor) {\n\t\treturn expectScroll( indexNames, contributor, chunkSize );\n\t}\n\n\tpublic BackendMock expectScrollProjections(Collection<String> indexNames, int chunkSize,\n\t\t\tConsumer<StubSearchWork.Builder> contributor) {\n\t\treturn expectScroll( indexNames, contributor, chunkSize );\n\t}\n\n\tprivate BackendMock expectScroll(Collection<String> indexNames, Consumer<StubSearchWork.Builder> contributor,\n\t\t\tint chunkSize) {\n\t\tCallQueue<ScrollWorkCall<?>> callQueue = backendBehavior().getScrollCalls();\n\t\tStubSearchWork.Builder builder = StubSearchWork.builder();\n\t\tcontributor.accept( builder );\n\t\tcallQueue.expectInOrder(\n\t\t\t\tnew ScrollWorkCall<>( new LinkedHashSet<>( indexNames ), builder.build(), chunkSize ) );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock expectCloseScroll(Collection<String> indexNames) {\n\t\tCallQueue<CloseScrollWorkCall> callQueue = backendBehavior().getCloseScrollCalls();\n\t\tcallQueue.expectInOrder( new CloseScrollWorkCall( new LinkedHashSet<>( indexNames ) ) );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock expectNextScroll(Collection<String> indexNames, StubNextScrollWorkBehavior<?> behavior) {\n\t\tCallQueue<NextScrollWorkCall<?>> callQueue = backendBehavior().getNextScrollCalls();\n\t\tcallQueue.expectInOrder( new NextScrollWorkCall<>( new LinkedHashSet<>( indexNames ), behavior ) );\n\t\treturn this;\n\t}\n\n\tpublic static class SchemaManagementWorkCallListContext {\n\t\tprivate final String indexName;\n\t\tprivate final Consumer<SchemaManagementWorkCall> expectationConsumer;\n\n\t\tprivate SchemaManagementWorkCallListContext(String indexName,\n\t\t\t\tConsumer<SchemaManagementWorkCall> expectationConsumer) {\n\t\t\tthis.indexName = indexName;\n\t\t\tthis.expectationConsumer = expectationConsumer;\n\t\t}\n\n\t\tpublic SchemaManagementWorkCallListContext work(StubSchemaManagementWork.Type type) {\n\t\t\treturn work( type, CompletableFuture.completedFuture( null ) );\n\t\t}\n\n\t\tpublic SchemaManagementWorkCallListContext work(StubSchemaManagementWork.Type type,\n\t\t\t\tCompletableFuture<?> future) {\n\t\t\treturn work( type, failureCollector -> future );\n\t\t}\n\n\t\tpublic SchemaManagementWorkCallListContext work(StubSchemaManagementWork.Type type,\n\t\t\t\tSchemaManagementWorkBehavior behavior) {\n\t\t\tStubSchemaManagementWork work = StubSchemaManagementWork.builder( type )\n\t\t\t\t\t.build();\n\t\t\texpectationConsumer.accept( new SchemaManagementWorkCall( indexName, work, behavior ) );\n\t\t\treturn this;\n\t\t}\n\t}\n\n\tprivate enum DocumentWorkCallKind {\n\t\tCREATE,\n\t\tDISCARD,\n\t\tEXECUTE,\n\t\tCREATE_AND_DISCARD,\n\t\tCREATE_AND_EXECUTE,\n\t\tCREATE_AND_EXECUTE_OUT_OF_ORDER;\n\t}\n\n\tpublic class DocumentWorkCallListContext {\n\t\tprivate final String indexName;\n\t\tprivate final String tenantId;\n\t\tprivate final DocumentCommitStrategy commitStrategyForDocumentWorks;\n\t\tprivate final DocumentRefreshStrategy refreshStrategyForDocumentWorks;\n\n\t\tprivate final DocumentWorkCallKind kind;\n\t\tprivate final CompletableFuture<?> executionFuture;\n\n\t\tprivate DocumentWorkCallListContext(String indexName, String tenantId,\n\t\t\t\tDocumentCommitStrategy commitStrategyForDocumentWorks,\n\t\t\t\tDocumentRefreshStrategy refreshStrategyForDocumentWorks,\n\t\t\t\tDocumentWorkCallKind kind, CompletableFuture<?> executionFuture) {\n\t\t\tthis.indexName = indexName;\n\t\t\tthis.tenantId = tenantId;\n\t\t\tthis.commitStrategyForDocumentWorks = commitStrategyForDocumentWorks;\n\t\t\tthis.refreshStrategyForDocumentWorks = refreshStrategyForDocumentWorks;\n\t\t\tthis.kind = kind;\n\t\t\tthis.executionFuture = executionFuture;\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext createFollowingWorks() {\n\t\t\treturn newContext( DocumentWorkCallKind.CREATE );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext discardFollowingWorks() {\n\t\t\treturn newContext( DocumentWorkCallKind.DISCARD );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext executeFollowingWorks() {\n\t\t\treturn newContext( DocumentWorkCallKind.EXECUTE );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext executeFollowingWorks(CompletableFuture<?> executionFuture) {\n\t\t\treturn newContext( DocumentWorkCallKind.EXECUTE, executionFuture );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext createAndExecuteFollowingWorks() {\n\t\t\treturn newContext( DocumentWorkCallKind.CREATE_AND_EXECUTE );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext createAndExecuteFollowingWorks(CompletableFuture<?> executionFuture) {\n\t\t\treturn newContext( DocumentWorkCallKind.CREATE_AND_EXECUTE, executionFuture );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext createAndExecuteFollowingWorksOutOfOrder() {\n\t\t\treturn newContext( DocumentWorkCallKind.CREATE_AND_EXECUTE_OUT_OF_ORDER );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext createAndDiscardFollowingWorks() {\n\t\t\treturn newContext( DocumentWorkCallKind.CREATE_AND_DISCARD );\n\t\t}\n\n\t\tprivate DocumentWorkCallListContext newContext(DocumentWorkCallKind kind) {\n\t\t\treturn newContext( kind, CompletableFuture.completedFuture( null ) );\n\t\t}\n\n\t\tprivate DocumentWorkCallListContext newContext(DocumentWorkCallKind kind,\n\t\t\t\tCompletableFuture<?> executionFuture) {\n\t\t\treturn new DocumentWorkCallListContext( indexName, tenantId,\n\t\t\t\t\tcommitStrategyForDocumentWorks, refreshStrategyForDocumentWorks,\n\t\t\t\t\tkind, executionFuture\n\t\t\t);\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext add(Consumer<StubDocumentWork.Builder> contributor) {\n\t\t\treturn documentWork( indexingWorkExpectations.addWorkType, contributor );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext add(String id, Consumer<StubDocumentNode.Builder> documentContributor) {\n\t\t\treturn documentWork( indexingWorkExpectations.addWorkType, id, documentContributor );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext addOrUpdate(Consumer<StubDocumentWork.Builder> contributor) {\n\t\t\treturn documentWork( StubDocumentWork.Type.ADD_OR_UPDATE, contributor );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext addOrUpdate(String id,\n\t\t\t\tConsumer<StubDocumentNode.Builder> documentContributor) {\n\t\t\treturn documentWork( StubDocumentWork.Type.ADD_OR_UPDATE, id, documentContributor );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext delete(String id) {\n\t\t\treturn documentWork( StubDocumentWork.Type.DELETE, b -> b.identifier( id ) );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext delete(Consumer<StubDocumentWork.Builder> contributor) {\n\t\t\treturn documentWork( StubDocumentWork.Type.DELETE, contributor );\n\t\t}\n\n\t\tDocumentWorkCallListContext documentWork(StubDocumentWork.Type type,\n\t\t\t\tConsumer<StubDocumentWork.Builder> contributor) {\n\t\t\tStubDocumentWork.Builder builder = StubDocumentWork.builder( type );\n\t\t\tbuilder.tenantIdentifier( tenantId );\n\t\t\tcontributor.accept( builder );\n\t\t\tbuilder.commit( commitStrategyForDocumentWorks );\n\t\t\tbuilder.refresh( refreshStrategyForDocumentWorks );\n\t\t\treturn work( builder.build() );\n\t\t}\n\n\t\tDocumentWorkCallListContext documentWork(StubDocumentWork.Type type, String id,\n\t\t\t\tConsumer<StubDocumentNode.Builder> documentContributor) {\n\t\t\treturn documentWork( type, b -> {\n\t\t\t\tb.identifier( id );\n\t\t\t\tStubDocumentNode.Builder documentBuilder = StubDocumentNode.document();\n\t\t\t\tdocumentContributor.accept( documentBuilder );\n\t\t\t\tb.document( documentBuilder.build() );\n\t\t\t} );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext work(StubDocumentWork work) {\n\t\t\tStubTreeNodeDiffer<StubDocumentNode> documentDiffer =\n\t\t\t\t\tdocumentDiffers.getOrDefault( indexName, StubDocumentWorkAssert.DEFAULT_DOCUMENT_DIFFER );\n\t\t\tswitch ( kind ) {\n\t\t\t\tcase CREATE:\n\t\t\t\t\texpect( new DocumentWorkCreateCall( indexName, work, documentDiffer ) );\n\t\t\t\t\tbreak;\n\t\t\t\tcase DISCARD:\n\t\t\t\t\texpect( new DocumentWorkDiscardCall( indexName, work, documentDiffer ) );\n\t\t\t\t\tbreak;\n\t\t\t\tcase EXECUTE:\n\t\t\t\t\texpect( new DocumentWorkExecuteCall( indexName, work, documentDiffer, executionFuture ) );\n\t\t\t\t\tbreak;\n\t\t\t\tcase CREATE_AND_DISCARD:\n\t\t\t\t\texpect( new DocumentWorkCreateCall( indexName, work, documentDiffer ) );\n\t\t\t\t\texpect( new DocumentWorkDiscardCall( indexName, work, documentDiffer ) );\n\t\t\t\t\tbreak;\n\t\t\t\tcase CREATE_AND_EXECUTE:\n\t\t\t\t\texpect( new DocumentWorkCreateCall( indexName, work, documentDiffer ) );\n\t\t\t\t\texpect( new DocumentWorkExecuteCall( indexName, work, documentDiffer, executionFuture ) );\n\t\t\t\t\tbreak;\n\t\t\t\tcase CREATE_AND_EXECUTE_OUT_OF_ORDER:\n\t\t\t\t\texpectOutOfOrder( new DocumentWorkCreateCall( indexName, work, documentDiffer ) );\n\t\t\t\t\texpectOutOfOrder( new DocumentWorkExecuteCall( indexName, work, documentDiffer, executionFuture ) );\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\treturn this;\n\t\t}\n\n\t\tprivate void expect(DocumentWorkCreateCall call) {\n\t\t\tbackendBehavior().getDocumentWorkCreateCalls( call.documentKey() ).expectInOrder( call );\n\t\t}\n\n\t\tprivate void expectOutOfOrder(DocumentWorkCreateCall call) {\n\t\t\tbackendBehavior().getDocumentWorkCreateCalls( call.documentKey() ).expectOutOfOrder( call );\n\t\t}\n\n\t\tprivate void expect(DocumentWorkDiscardCall call) {\n\t\t\tbackendBehavior().getDocumentWorkDiscardCalls( call.documentKey() ).expectInOrder( call );\n\t\t}\n\n\t\tprivate void expect(DocumentWorkExecuteCall call) {\n\t\t\tbackendBehavior().getDocumentWorkExecuteCalls( call.documentKey() ).expectInOrder( call );\n\t\t}\n\n\t\tprivate void expectOutOfOrder(DocumentWorkExecuteCall call) {\n\t\t\tbackendBehavior().getDocumentWorkExecuteCalls( call.documentKey() ).expectOutOfOrder( call );\n\t\t}\n\t}\n\n\tpublic static class IndexScaleWorkCallListContext {\n\t\tprivate final String indexName;\n\t\tprivate final Set<String> tenantIdentifiers;\n\t\tprivate final Consumer<IndexScaleWorkCall> expectationConsumer;\n\n\t\tprivate IndexScaleWorkCallListContext(String indexName,\n\t\t\t\tSet<String> tenantIdentifiers,\n\t\t\t\tConsumer<IndexScaleWorkCall> expectationConsumer) {\n\t\t\tthis.indexName = indexName;\n\t\t\tthis.tenantIdentifiers = tenantIdentifiers;\n\t\t\tthis.expectationConsumer = expectationConsumer;\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext mergeSegments() {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext mergeSegments(CompletableFuture<?> future) {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, future );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext purge() {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.PURGE );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext purge(CompletableFuture<?> future) {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.PURGE, future );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext purge(Set<String> routingKeys) {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.PURGE, routingKeys );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext purge(Set<String> routingKeys, CompletableFuture<?> future) {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.PURGE, routingKeys, future );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext flush() {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.FLUSH );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext flush(CompletableFuture<?> future) {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.FLUSH, future );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext refresh() {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.REFRESH );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext refresh(CompletableFuture<?> future) {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.REFRESH, future );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext indexScaleWork(StubIndexScaleWork.Type type) {\n\t\t\treturn indexScaleWork( type, Collections.emptySet() );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext indexScaleWork(StubIndexScaleWork.Type type, CompletableFuture<?> future) {\n\t\t\treturn indexScaleWork( type, Collections.emptySet(), future );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext indexScaleWork(StubIndexScaleWork.Type type, Set<String> routingKeys) {\n\t\t\treturn indexScaleWork( type, routingKeys, CompletableFuture.completedFuture( null ) );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext indexScaleWork(StubIndexScaleWork.Type type, Set<String> routingKeys,\n\t\t\t\tCompletableFuture<?> future) {\n\t\t\tStubIndexScaleWork work = StubIndexScaleWork.builder( type )\n\t\t\t\t\t.tenantIdentifiers( tenantIdentifiers )\n\t\t\t\t\t.routingKeys( routingKeys )\n\t\t\t\t\t.build();\n\t\t\texpectationConsumer.accept( new IndexScaleWorkCall( indexName, work, future ) );\n\t\t\treturn this;\n\t\t}\n\t}\n\n}\n",
        "filePathAfter": "util/internal/integrationtest/common/src/main/java/org/hibernate/search/util/impl/integrationtest/common/extension/BackendMock.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.integrationtest.common.extension;\n\nimport static org.hibernate.search.util.common.impl.CollectionHelper.asSetIgnoreNull;\n\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.LinkedHashSet;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\nimport java.util.concurrent.ConcurrentHashMap;\nimport java.util.function.Consumer;\nimport java.util.function.Supplier;\n\nimport org.hibernate.search.engine.backend.common.DocumentReference;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.impl.integrationtest.common.assertion.StubDocumentWorkAssert;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.StubTreeNodeDiffer;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.BackendMappingHandle;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.document.StubDocumentNode;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.document.model.StubIndexSchemaDataNode;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.document.model.impl.StubIndexModel;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubDocumentWork;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubIndexScaleWork;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubSchemaManagementWork;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.impl.StubBackendBuildContext;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.impl.StubBackendFactory;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.impl.StubIndexCreateContext;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.search.query.impl.StubSearchWork;\nimport org.hibernate.search.util.impl.test.extension.ExtensionScope;\n\nimport org.junit.jupiter.api.extension.AfterEachCallback;\nimport org.junit.jupiter.api.extension.BeforeAllCallback;\nimport org.junit.jupiter.api.extension.BeforeEachCallback;\nimport org.junit.jupiter.api.extension.BeforeTestExecutionCallback;\nimport org.junit.jupiter.api.extension.ExtensionContext;\n\nimport org.opentest4j.TestAbortedException;\n\npublic class BackendMock implements BeforeTestExecutionCallback, AfterEachCallback, BeforeAllCallback, BeforeEachCallback {\n\n\tprivate final VerifyingStubBackendBehavior backendBehavior =\n\t\t\tnew VerifyingStubBackendBehavior( this::indexingWorkExpectations );\n\tprivate volatile boolean started = false;\n\tprivate ExtensionScope startingScope = ExtensionScope.TEST;\n\tprivate volatile BackendIndexingWorkExpectations indexingWorkExpectations = BackendIndexingWorkExpectations.sync();\n\n\tprivate final Map<String, StubTreeNodeDiffer<StubDocumentNode>> documentDiffers = new ConcurrentHashMap<>();\n\n\tpublic static BackendMock create() {\n\t\treturn new BackendMock();\n\t}\n\n\tprotected BackendMock() {\n\t}\n\n\t@Override\n\tpublic void beforeAll(ExtensionContext extensionContext) {\n\t\tstarted = true;\n\t\tstartingScope = ExtensionScope.CLASS;\n\t}\n\n\t@Override\n\tpublic void beforeEach(ExtensionContext extensionContext) {\n\t\tstarted = true;\n\t}\n\n\t@Override\n\tpublic void beforeTestExecution(ExtensionContext extensionContext) throws Exception {\n\t\t// this means we are done with any @Before setups\n\t\t// and we want to make sure that anything we've expected there is OK.\n\t\t// And if we don't do it here then any unmet expectations from @Before will only fail after the test is executed\n\t\t// and it'll be harder to track the mismatch.\n\t\tverifyAndReset();\n\t}\n\n\t@Override\n\tpublic void afterEach(ExtensionContext context) {\n\t\tif ( context.getExecutionException().map( e -> e instanceof TestAbortedException ).orElse( Boolean.FALSE ) ) {\n\t\t\t// test was aborted - hence let's not do verification. cleanups will happen in finally block.\n\t\t\treturn;\n\t\t}\n\t\tverifyAndReset();\n\t\tif ( ExtensionScope.TEST.equals( startingScope ) ) {\n\t\t\tbackendBehavior().resetBackends();\n\t\t}\n\t\tstarted = false;\n\t}\n\n\tprivate void verifyAndReset() {\n\t\ttry {\n\t\t\tbackendBehavior().verifyExpectationsMet();\n\t\t}\n\t\tfinally {\n\t\t\tbackendBehavior().resetExpectations();\n\t\t}\n\t}\n\n\tpublic BackendMock ignoreSchema() {\n\t\tbackendBehavior.ignoreSchema( true );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock documentDiffer(String indexName, StubTreeNodeDiffer<StubDocumentNode> differ) {\n\t\tdocumentDiffers.put( indexName, differ );\n\t\treturn this;\n\t}\n\n\tpublic StubBackendFactory factory(CompletionStage<BackendMappingHandle> mappingHandlePromise) {\n\t\treturn new StubBackendFactory( backendBehavior, mappingHandlePromise );\n\t}\n\n\tpublic void indexingWorkExpectations(BackendIndexingWorkExpectations expectations) {\n\t\tindexingWorkExpectations = expectations;\n\t}\n\n\tpublic BackendIndexingWorkExpectations indexingWorkExpectations() {\n\t\treturn indexingWorkExpectations;\n\t}\n\n\tpublic void resetExpectations() {\n\t\tbackendBehavior().resetExpectations();\n\t}\n\n\tpublic void verifyExpectationsMet() {\n\t\tbackendBehavior().verifyExpectationsMet();\n\t}\n\n\tpublic long remainingExpectedIndexingCount() {\n\t\treturn backendBehavior().getDocumentWorkExecuteCalls().values().stream()\n\t\t\t\t.mapToLong( CallQueue::remainingExpectedCallCount )\n\t\t\t\t.sum();\n\t}\n\n\tpublic void inLenientMode(Runnable action) {\n\t\tbackendBehavior().lenient( true );\n\t\ttry {\n\t\t\taction.run();\n\t\t}\n\t\tfinally {\n\t\t\tbackendBehavior().lenient( false );\n\t\t}\n\t}\n\n\tpublic BackendMock onCreate(Consumer<StubBackendBuildContext> behavior) {\n\t\tbackendBehavior().addCreateBackendBehavior( context -> {\n\t\t\tbehavior.accept( context );\n\t\t\treturn null;\n\t\t} );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock onStop(Runnable behavior) {\n\t\tbackendBehavior().addStopBackendBehavior( () -> {\n\t\t\tbehavior.run();\n\t\t\treturn null;\n\t\t} );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock onCreateIndex(Consumer<StubIndexCreateContext> behavior) {\n\t\tbackendBehavior().addCreateIndexBehavior( context -> {\n\t\t\tbehavior.accept( context );\n\t\t\treturn null;\n\t\t} );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock expectFailingField(String indexName, String absoluteFieldPath,\n\t\t\tSupplier<RuntimeException> exceptionSupplier) {\n\t\tbackendBehavior().setIndexFieldAddBehavior( indexName, absoluteFieldPath, () -> {\n\t\t\tthrow exceptionSupplier.get();\n\t\t} );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock expectSchema(String indexName, Consumer<StubIndexSchemaDataNode.Builder> contributor) {\n\t\treturn expectSchema( indexName, contributor, ignored -> {} );\n\t}\n\n\tpublic BackendMock expectSchema(String indexName, Consumer<StubIndexSchemaDataNode.Builder> contributor,\n\t\t\tConsumer<StubIndexModel> capture) {\n\t\tCallQueue<SchemaDefinitionCall> callQueue = backendBehavior().getSchemaDefinitionCalls( indexName );\n\t\tStubIndexSchemaDataNode.Builder builder = StubIndexSchemaDataNode.schema();\n\t\tcontributor.accept( builder );\n\t\tcallQueue.expectOutOfOrder( new SchemaDefinitionCall( indexName, builder.build(), capture ) );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock expectAnySchema(String indexName) {\n\t\tCallQueue<SchemaDefinitionCall> callQueue = backendBehavior().getSchemaDefinitionCalls( indexName );\n\t\tcallQueue.expectOutOfOrder( new SchemaDefinitionCall( indexName, null, null ) );\n\t\treturn this;\n\t}\n\n\tpublic SchemaManagementWorkCallListContext expectSchemaManagementWorks(String indexName) {\n\t\tCallQueue<SchemaManagementWorkCall> callQueue = backendBehavior().getSchemaManagementWorkCalls( indexName );\n\t\treturn new SchemaManagementWorkCallListContext(\n\t\t\t\tindexName,\n\t\t\t\tcallQueue::expectInOrder\n\t\t);\n\t}\n\n\tpublic DocumentWorkCallListContext expectWorks(String indexName) {\n\t\treturn expectWorks( indexName, null );\n\t}\n\n\tpublic DocumentWorkCallListContext expectWorks(String indexName, String tenantId) {\n\t\t// Default to force commit and no refresh, which is what the mapper should use by default\n\t\treturn expectWorks( indexName, tenantId, DocumentCommitStrategy.FORCE, DocumentRefreshStrategy.NONE );\n\t}\n\n\tpublic DocumentWorkCallListContext expectWorks(String indexName,\n\t\t\tDocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\n\t\treturn expectWorks( indexName, null, commitStrategy, refreshStrategy );\n\t}\n\n\tpublic DocumentWorkCallListContext expectWorks(String indexName, String tenantId,\n\t\t\tDocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\n\t\treturn new DocumentWorkCallListContext(\n\t\t\t\tindexName, tenantId,\n\t\t\t\tcommitStrategy, refreshStrategy,\n\t\t\t\tDocumentWorkCallKind.CREATE_AND_EXECUTE, CompletableFuture.completedFuture( null )\n\t\t);\n\t}\n\n\tpublic IndexScaleWorkCallListContext expectIndexScaleWorks(String indexName, String... tenantIds) {\n\t\tCallQueue<IndexScaleWorkCall> callQueue = backendBehavior().getIndexScaleWorkCalls( indexName );\n\t\treturn new IndexScaleWorkCallListContext(\n\t\t\t\tindexName,\n\t\t\t\tasSetIgnoreNull( tenantIds ),\n\t\t\t\tcallQueue::expectInOrder\n\t\t);\n\t}\n\n\tpublic BackendMock expectSearchReferences(Collection<String> indexNames,\n\t\t\tStubSearchWorkBehavior<DocumentReference> behavior) {\n\t\treturn expectSearch( indexNames, b -> {}, behavior );\n\t}\n\n\tpublic BackendMock expectSearchReferences(Collection<String> indexNames,\n\t\t\tConsumer<StubSearchWork.Builder> contributor,\n\t\t\tStubSearchWorkBehavior<DocumentReference> behavior) {\n\t\treturn expectSearch( indexNames, contributor, behavior );\n\t}\n\n\tpublic BackendMock expectSearchIds(Collection<String> indexNames, Consumer<StubSearchWork.Builder> contributor,\n\t\t\tStubSearchWorkBehavior<String> behavior) {\n\t\treturn expectSearch( indexNames, contributor, behavior );\n\t}\n\n\tpublic BackendMock expectSearchObjects(String indexName, StubSearchWorkBehavior<DocumentReference> behavior) {\n\t\treturn expectSearch( Collections.singleton( indexName ), ignored -> {}, behavior );\n\t}\n\n\tpublic BackendMock expectSearchObjects(Collection<String> indexNames, Consumer<StubSearchWork.Builder> contributor,\n\t\t\tStubSearchWorkBehavior<DocumentReference> behavior) {\n\t\treturn expectSearch( indexNames, contributor, behavior );\n\t}\n\n\tpublic BackendMock expectSearchProjection(String indexNames, StubSearchWorkBehavior<?> behavior) {\n\t\treturn expectSearch( Collections.singleton( indexNames ), ignored -> {}, behavior );\n\t}\n\n\tpublic BackendMock expectSearchProjection(String indexNames, Consumer<StubSearchWork.Builder> contributor,\n\t\t\tStubSearchWorkBehavior<?> behavior) {\n\t\treturn expectSearch( Collections.singleton( indexNames ), contributor, behavior );\n\t}\n\n\tpublic BackendMock expectSearchProjection(Collection<String> indexNames, StubSearchWorkBehavior<?> behavior) {\n\t\treturn expectSearch( indexNames, ignored -> {}, behavior );\n\t}\n\n\tpublic BackendMock expectSearchProjection(Collection<String> indexNames,\n\t\t\tConsumer<StubSearchWork.Builder> contributor,\n\t\t\tStubSearchWorkBehavior<?> behavior) {\n\t\treturn expectSearch( indexNames, contributor, behavior );\n\t}\n\n\tprivate BackendMock expectSearch(Collection<String> indexNames, Consumer<StubSearchWork.Builder> contributor,\n\t\t\tStubSearchWorkBehavior<?> behavior) {\n\t\tCallQueue<SearchWorkCall<?>> callQueue = backendBehavior().getSearchWorkCalls();\n\t\tStubSearchWork.Builder builder = StubSearchWork.builder();\n\t\tcontributor.accept( builder );\n\t\tcallQueue.expectInOrder( new SearchWorkCall<>( new LinkedHashSet<>( indexNames ), builder.build(), behavior ) );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock expectCount(Collection<String> indexNames, long expectedResult) {\n\t\tCallQueue<CountWorkCall> callQueue = backendBehavior().getCountWorkCalls();\n\t\tcallQueue.expectInOrder( new CountWorkCall( new LinkedHashSet<>( indexNames ), expectedResult ) );\n\t\treturn this;\n\t}\n\n\tVerifyingStubBackendBehavior backendBehavior() {\n\t\tif ( !started ) {\n\t\t\tthrow new AssertionFailure( \"The backend mock was not configured as a JUnit Extension,\"\n\t\t\t\t\t+ \" or its statement wrapper hasn't started executing yet,\"\n\t\t\t\t\t+ \" or its statement wrapper has finished executing.\"\n\t\t\t\t\t+ \" Double check the @RegisterExtension annotation and the execution order of extensions.\" );\n\t\t}\n\t\treturn backendBehavior;\n\t}\n\n\tpublic BackendMock expectScrollObjects(Collection<String> indexNames, int chunkSize,\n\t\t\tConsumer<StubSearchWork.Builder> contributor) {\n\t\treturn expectScroll( indexNames, contributor, chunkSize );\n\t}\n\n\tpublic BackendMock expectScrollProjections(Collection<String> indexNames, int chunkSize,\n\t\t\tConsumer<StubSearchWork.Builder> contributor) {\n\t\treturn expectScroll( indexNames, contributor, chunkSize );\n\t}\n\n\tprivate BackendMock expectScroll(Collection<String> indexNames, Consumer<StubSearchWork.Builder> contributor,\n\t\t\tint chunkSize) {\n\t\tCallQueue<ScrollWorkCall<?>> callQueue = backendBehavior().getScrollCalls();\n\t\tStubSearchWork.Builder builder = StubSearchWork.builder();\n\t\tcontributor.accept( builder );\n\t\tcallQueue.expectInOrder(\n\t\t\t\tnew ScrollWorkCall<>( new LinkedHashSet<>( indexNames ), builder.build(), chunkSize ) );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock expectCloseScroll(Collection<String> indexNames) {\n\t\tCallQueue<CloseScrollWorkCall> callQueue = backendBehavior().getCloseScrollCalls();\n\t\tcallQueue.expectInOrder( new CloseScrollWorkCall( new LinkedHashSet<>( indexNames ) ) );\n\t\treturn this;\n\t}\n\n\tpublic BackendMock expectNextScroll(Collection<String> indexNames, StubNextScrollWorkBehavior<?> behavior) {\n\t\tCallQueue<NextScrollWorkCall<?>> callQueue = backendBehavior().getNextScrollCalls();\n\t\tcallQueue.expectInOrder( new NextScrollWorkCall<>( new LinkedHashSet<>( indexNames ), behavior ) );\n\t\treturn this;\n\t}\n\n\tpublic static class SchemaManagementWorkCallListContext {\n\t\tprivate final String indexName;\n\t\tprivate final Consumer<SchemaManagementWorkCall> expectationConsumer;\n\n\t\tprivate SchemaManagementWorkCallListContext(String indexName,\n\t\t\t\tConsumer<SchemaManagementWorkCall> expectationConsumer) {\n\t\t\tthis.indexName = indexName;\n\t\t\tthis.expectationConsumer = expectationConsumer;\n\t\t}\n\n\t\tpublic SchemaManagementWorkCallListContext work(StubSchemaManagementWork.Type type) {\n\t\t\treturn work( type, CompletableFuture.completedFuture( null ) );\n\t\t}\n\n\t\tpublic SchemaManagementWorkCallListContext work(StubSchemaManagementWork.Type type,\n\t\t\t\tCompletableFuture<?> future) {\n\t\t\treturn work( type, failureCollector -> future );\n\t\t}\n\n\t\tpublic SchemaManagementWorkCallListContext work(StubSchemaManagementWork.Type type,\n\t\t\t\tSchemaManagementWorkBehavior behavior) {\n\t\t\tStubSchemaManagementWork work = StubSchemaManagementWork.builder( type )\n\t\t\t\t\t.build();\n\t\t\texpectationConsumer.accept( new SchemaManagementWorkCall( indexName, work, behavior ) );\n\t\t\treturn this;\n\t\t}\n\t}\n\n\tprivate enum DocumentWorkCallKind {\n\t\tCREATE,\n\t\tDISCARD,\n\t\tEXECUTE,\n\t\tCREATE_AND_DISCARD,\n\t\tCREATE_AND_EXECUTE,\n\t\tCREATE_AND_EXECUTE_OUT_OF_ORDER;\n\t}\n\n\tpublic class DocumentWorkCallListContext {\n\t\tprivate final String indexName;\n\t\tprivate final String tenantId;\n\t\tprivate final DocumentCommitStrategy commitStrategyForDocumentWorks;\n\t\tprivate final DocumentRefreshStrategy refreshStrategyForDocumentWorks;\n\n\t\tprivate final DocumentWorkCallKind kind;\n\t\tprivate final CompletableFuture<?> executionFuture;\n\n\t\tprivate DocumentWorkCallListContext(String indexName, String tenantId,\n\t\t\t\tDocumentCommitStrategy commitStrategyForDocumentWorks,\n\t\t\t\tDocumentRefreshStrategy refreshStrategyForDocumentWorks,\n\t\t\t\tDocumentWorkCallKind kind, CompletableFuture<?> executionFuture) {\n\t\t\tthis.indexName = indexName;\n\t\t\tthis.tenantId = tenantId;\n\t\t\tthis.commitStrategyForDocumentWorks = commitStrategyForDocumentWorks;\n\t\t\tthis.refreshStrategyForDocumentWorks = refreshStrategyForDocumentWorks;\n\t\t\tthis.kind = kind;\n\t\t\tthis.executionFuture = executionFuture;\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext createFollowingWorks() {\n\t\t\treturn newContext( DocumentWorkCallKind.CREATE );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext discardFollowingWorks() {\n\t\t\treturn newContext( DocumentWorkCallKind.DISCARD );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext executeFollowingWorks() {\n\t\t\treturn newContext( DocumentWorkCallKind.EXECUTE );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext executeFollowingWorks(CompletableFuture<?> executionFuture) {\n\t\t\treturn newContext( DocumentWorkCallKind.EXECUTE, executionFuture );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext createAndExecuteFollowingWorks() {\n\t\t\treturn newContext( DocumentWorkCallKind.CREATE_AND_EXECUTE );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext createAndExecuteFollowingWorks(CompletableFuture<?> executionFuture) {\n\t\t\treturn newContext( DocumentWorkCallKind.CREATE_AND_EXECUTE, executionFuture );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext createAndExecuteFollowingWorksOutOfOrder() {\n\t\t\treturn newContext( DocumentWorkCallKind.CREATE_AND_EXECUTE_OUT_OF_ORDER );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext createAndDiscardFollowingWorks() {\n\t\t\treturn newContext( DocumentWorkCallKind.CREATE_AND_DISCARD );\n\t\t}\n\n\t\tprivate DocumentWorkCallListContext newContext(DocumentWorkCallKind kind) {\n\t\t\treturn newContext( kind, CompletableFuture.completedFuture( null ) );\n\t\t}\n\n\t\tprivate DocumentWorkCallListContext newContext(DocumentWorkCallKind kind,\n\t\t\t\tCompletableFuture<?> executionFuture) {\n\t\t\treturn new DocumentWorkCallListContext( indexName, tenantId,\n\t\t\t\t\tcommitStrategyForDocumentWorks, refreshStrategyForDocumentWorks,\n\t\t\t\t\tkind, executionFuture\n\t\t\t);\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext add(Consumer<StubDocumentWork.Builder> contributor) {\n\t\t\treturn documentWork( indexingWorkExpectations.addWorkType, contributor );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext add(String id, Consumer<StubDocumentNode.Builder> documentContributor) {\n\t\t\treturn documentWork( indexingWorkExpectations.addWorkType, id, documentContributor );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext addOrUpdate(Consumer<StubDocumentWork.Builder> contributor) {\n\t\t\treturn documentWork( StubDocumentWork.Type.ADD_OR_UPDATE, contributor );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext addOrUpdate(String id,\n\t\t\t\tConsumer<StubDocumentNode.Builder> documentContributor) {\n\t\t\treturn documentWork( StubDocumentWork.Type.ADD_OR_UPDATE, id, documentContributor );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext delete(String id) {\n\t\t\treturn documentWork( StubDocumentWork.Type.DELETE, b -> b.identifier( id ) );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext delete(Consumer<StubDocumentWork.Builder> contributor) {\n\t\t\treturn documentWork( StubDocumentWork.Type.DELETE, contributor );\n\t\t}\n\n\t\tDocumentWorkCallListContext documentWork(StubDocumentWork.Type type,\n\t\t\t\tConsumer<StubDocumentWork.Builder> contributor) {\n\t\t\tStubDocumentWork.Builder builder = StubDocumentWork.builder( type );\n\t\t\tbuilder.tenantIdentifier( tenantId );\n\t\t\tcontributor.accept( builder );\n\t\t\tbuilder.commit( commitStrategyForDocumentWorks );\n\t\t\tbuilder.refresh( refreshStrategyForDocumentWorks );\n\t\t\treturn work( builder.build() );\n\t\t}\n\n\t\tDocumentWorkCallListContext documentWork(StubDocumentWork.Type type, String id,\n\t\t\t\tConsumer<StubDocumentNode.Builder> documentContributor) {\n\t\t\treturn documentWork( type, b -> {\n\t\t\t\tb.identifier( id );\n\t\t\t\tStubDocumentNode.Builder documentBuilder = StubDocumentNode.document();\n\t\t\t\tdocumentContributor.accept( documentBuilder );\n\t\t\t\tb.document( documentBuilder.build() );\n\t\t\t} );\n\t\t}\n\n\t\tpublic DocumentWorkCallListContext work(StubDocumentWork work) {\n\t\t\tStubTreeNodeDiffer<StubDocumentNode> documentDiffer =\n\t\t\t\t\tdocumentDiffers.getOrDefault( indexName, StubDocumentWorkAssert.DEFAULT_DOCUMENT_DIFFER );\n\t\t\tswitch ( kind ) {\n\t\t\t\tcase CREATE:\n\t\t\t\t\texpect( new DocumentWorkCreateCall( indexName, work, documentDiffer ) );\n\t\t\t\t\tbreak;\n\t\t\t\tcase DISCARD:\n\t\t\t\t\texpect( new DocumentWorkDiscardCall( indexName, work, documentDiffer ) );\n\t\t\t\t\tbreak;\n\t\t\t\tcase EXECUTE:\n\t\t\t\t\texpect( new DocumentWorkExecuteCall( indexName, work, documentDiffer, executionFuture ) );\n\t\t\t\t\tbreak;\n\t\t\t\tcase CREATE_AND_DISCARD:\n\t\t\t\t\texpect( new DocumentWorkCreateCall( indexName, work, documentDiffer ) );\n\t\t\t\t\texpect( new DocumentWorkDiscardCall( indexName, work, documentDiffer ) );\n\t\t\t\t\tbreak;\n\t\t\t\tcase CREATE_AND_EXECUTE:\n\t\t\t\t\texpect( new DocumentWorkCreateCall( indexName, work, documentDiffer ) );\n\t\t\t\t\texpect( new DocumentWorkExecuteCall( indexName, work, documentDiffer, executionFuture ) );\n\t\t\t\t\tbreak;\n\t\t\t\tcase CREATE_AND_EXECUTE_OUT_OF_ORDER:\n\t\t\t\t\texpectOutOfOrder( new DocumentWorkCreateCall( indexName, work, documentDiffer ) );\n\t\t\t\t\texpectOutOfOrder( new DocumentWorkExecuteCall( indexName, work, documentDiffer, executionFuture ) );\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\treturn this;\n\t\t}\n\n\t\tprivate void expect(DocumentWorkCreateCall call) {\n\t\t\tbackendBehavior().getDocumentWorkCreateCalls( call.documentKey() ).expectInOrder( call );\n\t\t}\n\n\t\tprivate void expectOutOfOrder(DocumentWorkCreateCall call) {\n\t\t\tbackendBehavior().getDocumentWorkCreateCalls( call.documentKey() ).expectOutOfOrder( call );\n\t\t}\n\n\t\tprivate void expect(DocumentWorkDiscardCall call) {\n\t\t\tbackendBehavior().getDocumentWorkDiscardCalls( call.documentKey() ).expectInOrder( call );\n\t\t}\n\n\t\tprivate void expect(DocumentWorkExecuteCall call) {\n\t\t\tbackendBehavior().getDocumentWorkExecuteCalls( call.documentKey() ).expectInOrder( call );\n\t\t}\n\n\t\tprivate void expectOutOfOrder(DocumentWorkExecuteCall call) {\n\t\t\tbackendBehavior().getDocumentWorkExecuteCalls( call.documentKey() ).expectOutOfOrder( call );\n\t\t}\n\t}\n\n\tpublic static class IndexScaleWorkCallListContext {\n\t\tprivate final String indexName;\n\t\tprivate final Set<String> tenantIdentifiers;\n\t\tprivate final Consumer<IndexScaleWorkCall> expectationConsumer;\n\n\t\tprivate IndexScaleWorkCallListContext(String indexName,\n\t\t\t\tSet<String> tenantIdentifiers,\n\t\t\t\tConsumer<IndexScaleWorkCall> expectationConsumer) {\n\t\t\tthis.indexName = indexName;\n\t\t\tthis.tenantIdentifiers = tenantIdentifiers;\n\t\t\tthis.expectationConsumer = expectationConsumer;\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext mergeSegments() {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext mergeSegments(CompletableFuture<?> future) {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, future );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext purge() {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.PURGE );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext purge(CompletableFuture<?> future) {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.PURGE, future );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext purge(Set<String> routingKeys) {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.PURGE, routingKeys );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext purge(Set<String> routingKeys, CompletableFuture<?> future) {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.PURGE, routingKeys, future );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext flush() {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.FLUSH );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext flush(CompletableFuture<?> future) {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.FLUSH, future );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext refresh() {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.REFRESH );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext refresh(CompletableFuture<?> future) {\n\t\t\treturn indexScaleWork( StubIndexScaleWork.Type.REFRESH, future );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext indexScaleWork(StubIndexScaleWork.Type type) {\n\t\t\treturn indexScaleWork( type, Collections.emptySet() );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext indexScaleWork(StubIndexScaleWork.Type type, CompletableFuture<?> future) {\n\t\t\treturn indexScaleWork( type, Collections.emptySet(), future );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext indexScaleWork(StubIndexScaleWork.Type type, Set<String> routingKeys) {\n\t\t\treturn indexScaleWork( type, routingKeys, CompletableFuture.completedFuture( null ) );\n\t\t}\n\n\t\tpublic IndexScaleWorkCallListContext indexScaleWork(StubIndexScaleWork.Type type, Set<String> routingKeys,\n\t\t\t\tCompletableFuture<?> future) {\n\t\t\tStubIndexScaleWork work = StubIndexScaleWork.builder( type )\n\t\t\t\t\t.tenantIdentifiers( tenantIdentifiers )\n\t\t\t\t\t.routingKeys( routingKeys )\n\t\t\t\t\t.build();\n\t\t\texpectationConsumer.accept( new IndexScaleWorkCall( indexName, work, future ) );\n\t\t\treturn this;\n\t\t}\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "@Override\n\tpublic void beforeAll(ExtensionContext extensionContext) {\n\t\tstarted = true;\n\t\tstartingScope = ExtensionScope.CLASS;\n\t}",
        "diffSourceCode": "-   66: \n-   67: \t@Override\n-   68: \tpublic void beforeAll(ExtensionContext context) {\n-   69: \t\tcallOncePerClass = true;\n-   70: \t\tdoBefore();\n-   71: \t}\n-   94: \tprivate void doBefore() {\n-   95: \t\tstarted = true;\n-   96: \t}\n+   66: \t@Override\n+   67: \tpublic void beforeAll(ExtensionContext extensionContext) {\n+   68: \t\tstarted = true;\n+   69: \t\tstartingScope = ExtensionScope.CLASS;\n+   70: \t}\n+   71: \n+   94: \t\t\tbackendBehavior().resetBackends();\n+   95: \t\t}\n+   96: \t\tstarted = false;\n",
        "uniqueId": "7340d5e3553f69f655aa71a003f99f5d05abbeec_67_71__66_70_94_96",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 4
            },
            "LINE": {
                "missed": 0,
                "covered": 2
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Inline Method",
        "description": "Inline Method\tprivate doBefore() : void inlined to public beforeEach(extensionContext ExtensionContext) : void in class org.hibernate.search.test.testsupport.StaticIndexingSwitch",
        "diffLocations": [
            {
                "filePath": "integrationtest/v5migrationhelper/orm/src/test/java/org/hibernate/search/test/testsupport/StaticIndexingSwitch.java",
                "startLine": 72,
                "endLine": 77,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/v5migrationhelper/orm/src/test/java/org/hibernate/search/test/testsupport/StaticIndexingSwitch.java",
                "startLine": 59,
                "endLine": 67,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/v5migrationhelper/orm/src/test/java/org/hibernate/search/test/testsupport/StaticIndexingSwitch.java",
                "startLine": 79,
                "endLine": 86,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void doBefore() {\n\t\tif ( activeInstance != null ) {\n\t\t\tthrow new IllegalStateException( \"Using StaticCounters twice in a single test is forbidden.\"\n\t\t\t\t\t+ \" Make sure you added one (and only one)\"\n\t\t\t\t\t+ \" '@Rule public StaticIndexingSwitch indexingSwitch = new StaticIndexingSwitch()' to your test.\" );\n\t\t}\n\t\tactiveInstance = StaticIndexingSwitch.this;\n\t}",
        "filePathBefore": "integrationtest/v5migrationhelper/orm/src/test/java/org/hibernate/search/test/testsupport/StaticIndexingSwitch.java",
        "isPureRefactoring": true,
        "commitId": "7340d5e3553f69f655aa71a003f99f5d05abbeec",
        "packageNameBefore": "org.hibernate.search.test.testsupport",
        "classNameBefore": "org.hibernate.search.test.testsupport.StaticIndexingSwitch",
        "methodNameBefore": "org.hibernate.search.test.testsupport.StaticIndexingSwitch#doBefore",
        "classSignatureBefore": "public class StaticIndexingSwitch\n\t\timplements BeforeEachCallback, AfterEachCallback, BeforeAllCallback, AfterAllCallback ",
        "methodNameBeforeSet": [
            "org.hibernate.search.test.testsupport.StaticIndexingSwitch#doBefore"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.test.testsupport.StaticIndexingSwitch"
        ],
        "classSignatureBeforeSet": [
            "public class StaticIndexingSwitch\n\t\timplements BeforeEachCallback, AfterEachCallback, BeforeAllCallback, AfterAllCallback "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.test.testsupport;\n\nimport org.hibernate.search.mapper.pojo.bridge.RoutingBridge;\nimport org.hibernate.search.mapper.pojo.bridge.binding.RoutingBindingContext;\nimport org.hibernate.search.mapper.pojo.bridge.mapping.programmatic.RoutingBinder;\nimport org.hibernate.search.mapper.pojo.bridge.runtime.RoutingBridgeRouteContext;\nimport org.hibernate.search.mapper.pojo.route.DocumentRoutes;\n\nimport org.junit.jupiter.api.extension.AfterAllCallback;\nimport org.junit.jupiter.api.extension.AfterEachCallback;\nimport org.junit.jupiter.api.extension.BeforeAllCallback;\nimport org.junit.jupiter.api.extension.BeforeEachCallback;\nimport org.junit.jupiter.api.extension.ExtensionContext;\n\npublic class StaticIndexingSwitch\n\t\timplements BeforeEachCallback, AfterEachCallback, BeforeAllCallback, AfterAllCallback {\n\n\tprivate static StaticIndexingSwitch activeInstance = null;\n\n\tprivate boolean enabled = true;\n\tprivate boolean callOncePerClass = false;\n\n\tpublic void enable(boolean enabled) {\n\t\tthis.enabled = enabled;\n\t}\n\n\tprivate boolean enabled() {\n\t\treturn enabled;\n\t}\n\n\tprivate void reset() {\n\t\tenabled = true;\n\t}\n\n\tpublic static StaticIndexingSwitch activeSwitch() {\n\t\tif ( activeInstance == null ) {\n\t\t\tthrow new IllegalStateException( \"Using StaticCounters without an appropriate @RegisterExtension is forbidden.\"\n\t\t\t\t\t+ \" Make sure you added one (and only one)\"\n\t\t\t\t\t+ \" '@RegisterExtension public StaticIndexingSwitch indexingSwitch = new StaticIndexingSwitch()' to your test.\" );\n\t\t}\n\t\treturn activeInstance;\n\t}\n\n\t@Override\n\tpublic void afterAll(ExtensionContext extensionContext) throws Exception {\n\t\tif ( callOncePerClass ) {\n\t\t\tactiveInstance = null;\n\t\t\treset();\n\t\t}\n\t}\n\n\t@Override\n\tpublic void afterEach(ExtensionContext extensionContext) throws Exception {\n\t\tif ( !callOncePerClass ) {\n\t\t\tactiveInstance = null;\n\t\t\treset();\n\t\t}\n\t}\n\n\t@Override\n\tpublic void beforeAll(ExtensionContext extensionContext) throws Exception {\n\t\tcallOncePerClass = true;\n\t\tdoBefore();\n\t}\n\n\t@Override\n\tpublic void beforeEach(ExtensionContext extensionContext) throws Exception {\n\t\tif ( !callOncePerClass ) {\n\t\t\tdoBefore();\n\t\t}\n\t}\n\n\tprivate void doBefore() {\n\t\tif ( activeInstance != null ) {\n\t\t\tthrow new IllegalStateException( \"Using StaticCounters twice in a single test is forbidden.\"\n\t\t\t\t\t+ \" Make sure you added one (and only one)\"\n\t\t\t\t\t+ \" '@Rule public StaticIndexingSwitch indexingSwitch = new StaticIndexingSwitch()' to your test.\" );\n\t\t}\n\t\tactiveInstance = StaticIndexingSwitch.this;\n\t}\n\n\tpublic static class Binder implements RoutingBinder {\n\t\t@Override\n\t\tpublic void bind(RoutingBindingContext context) {\n\t\t\tcontext.dependencies().useRootOnly();\n\t\t\tcontext.bridge( Object.class, new Bridge() );\n\t\t}\n\t}\n\n\tprivate static class Bridge implements RoutingBridge<Object> {\n\t\t@Override\n\t\tpublic void route(DocumentRoutes routes, Object entityIdentifier, Object indexedEntity,\n\t\t\t\tRoutingBridgeRouteContext context) {\n\t\t\tif ( activeSwitch().enabled() ) {\n\t\t\t\troutes.addRoute();\n\t\t\t}\n\t\t\telse {\n\t\t\t\troutes.notIndexed();\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic void previousRoutes(DocumentRoutes routes, Object entityIdentifier, Object indexedEntity,\n\t\t\t\tRoutingBridgeRouteContext context) {\n\t\t\tif ( activeSwitch().enabled() ) {\n\t\t\t\troutes.addRoute();\n\t\t\t}\n\t\t\telse {\n\t\t\t\troutes.notIndexed();\n\t\t\t}\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/v5migrationhelper/orm/src/test/java/org/hibernate/search/test/testsupport/StaticIndexingSwitch.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.test.testsupport;\n\nimport org.hibernate.search.mapper.pojo.bridge.RoutingBridge;\nimport org.hibernate.search.mapper.pojo.bridge.binding.RoutingBindingContext;\nimport org.hibernate.search.mapper.pojo.bridge.mapping.programmatic.RoutingBinder;\nimport org.hibernate.search.mapper.pojo.bridge.runtime.RoutingBridgeRouteContext;\nimport org.hibernate.search.mapper.pojo.route.DocumentRoutes;\n\nimport org.junit.jupiter.api.extension.AfterEachCallback;\nimport org.junit.jupiter.api.extension.BeforeAllCallback;\nimport org.junit.jupiter.api.extension.BeforeEachCallback;\nimport org.junit.jupiter.api.extension.ExtensionContext;\n\npublic class StaticIndexingSwitch implements BeforeEachCallback, AfterEachCallback, BeforeAllCallback {\n\n\tprivate static StaticIndexingSwitch activeInstance = null;\n\n\tprivate boolean enabled = true;\n\n\tpublic void enable(boolean enabled) {\n\t\tthis.enabled = enabled;\n\t}\n\n\tprivate boolean enabled() {\n\t\treturn enabled;\n\t}\n\n\tprivate void reset() {\n\t\tenabled = true;\n\t}\n\n\tpublic static StaticIndexingSwitch activeSwitch() {\n\t\tif ( activeInstance == null ) {\n\t\t\tthrow new IllegalStateException( \"Using StaticCounters without an appropriate @RegisterExtension is forbidden.\"\n\t\t\t\t\t+ \" Make sure you added one (and only one)\"\n\t\t\t\t\t+ \" '@RegisterExtension public StaticIndexingSwitch indexingSwitch = new StaticIndexingSwitch()' to your test.\" );\n\t\t}\n\t\treturn activeInstance;\n\t}\n\n\t@Override\n\tpublic void afterEach(ExtensionContext extensionContext) {\n\t\tactiveInstance = null;\n\t\treset();\n\t}\n\n\t@Override\n\tpublic void beforeAll(ExtensionContext extensionContext) {\n\t\tthrow new IllegalStateException(\n\t\t\t\t\"StaticIndexingSwitch is only available as nonstatic extension, i.e. @RegisterExtension StaticIndexingSwitch staticIndexingSwitch = new StaticIndexingSwitch();\" );\n\t}\n\n\t@Override\n\tpublic void beforeEach(ExtensionContext extensionContext) {\n\t\tif ( activeInstance != null ) {\n\t\t\tthrow new IllegalStateException( \"Using StaticCounters twice in a single test is forbidden.\"\n\t\t\t\t\t+ \" Make sure you added one (and only one)\"\n\t\t\t\t\t+ \" '@Rule public StaticIndexingSwitch indexingSwitch = new StaticIndexingSwitch()' to your test.\" );\n\t\t}\n\t\tactiveInstance = StaticIndexingSwitch.this;\n\t}\n\n\tpublic static class Binder implements RoutingBinder {\n\t\t@Override\n\t\tpublic void bind(RoutingBindingContext context) {\n\t\t\tcontext.dependencies().useRootOnly();\n\t\t\tcontext.bridge( Object.class, new Bridge() );\n\t\t}\n\t}\n\n\tprivate static class Bridge implements RoutingBridge<Object> {\n\t\t@Override\n\t\tpublic void route(DocumentRoutes routes, Object entityIdentifier, Object indexedEntity,\n\t\t\t\tRoutingBridgeRouteContext context) {\n\t\t\tif ( activeSwitch().enabled() ) {\n\t\t\t\troutes.addRoute();\n\t\t\t}\n\t\t\telse {\n\t\t\t\troutes.notIndexed();\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic void previousRoutes(DocumentRoutes routes, Object entityIdentifier, Object indexedEntity,\n\t\t\t\tRoutingBridgeRouteContext context) {\n\t\t\tif ( activeSwitch().enabled() ) {\n\t\t\t\troutes.addRoute();\n\t\t\t}\n\t\t\telse {\n\t\t\t\troutes.notIndexed();\n\t\t\t}\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "@Override\n\tpublic void beforeEach(ExtensionContext extensionContext) {\n\t\tif ( activeInstance != null ) {\n\t\t\tthrow new IllegalStateException( \"Using StaticCounters twice in a single test is forbidden.\"\n\t\t\t\t\t+ \" Make sure you added one (and only one)\"\n\t\t\t\t\t+ \" '@Rule public StaticIndexingSwitch indexingSwitch = new StaticIndexingSwitch()' to your test.\" );\n\t\t}\n\t\tactiveInstance = StaticIndexingSwitch.this;\n\t}",
        "diffSourceCode": "-   59: \tpublic void afterEach(ExtensionContext extensionContext) throws Exception {\n-   60: \t\tif ( !callOncePerClass ) {\n-   61: \t\t\tactiveInstance = null;\n-   62: \t\t\treset();\n-   63: \t\t}\n-   64: \t}\n-   65: \n-   66: \t@Override\n-   67: \tpublic void beforeAll(ExtensionContext extensionContext) throws Exception {\n-   72: \t@Override\n-   73: \tpublic void beforeEach(ExtensionContext extensionContext) throws Exception {\n-   74: \t\tif ( !callOncePerClass ) {\n-   75: \t\t\tdoBefore();\n-   76: \t\t}\n-   77: \t}\n-   79: \tprivate void doBefore() {\n-   80: \t\tif ( activeInstance != null ) {\n-   81: \t\t\tthrow new IllegalStateException( \"Using StaticCounters twice in a single test is forbidden.\"\n-   82: \t\t\t\t\t+ \" Make sure you added one (and only one)\"\n-   83: \t\t\t\t\t+ \" '@Rule public StaticIndexingSwitch indexingSwitch = new StaticIndexingSwitch()' to your test.\" );\n-   84: \t\t}\n-   85: \t\tactiveInstance = StaticIndexingSwitch.this;\n-   86: \t}\n+   59: \t@Override\n+   60: \tpublic void beforeEach(ExtensionContext extensionContext) {\n+   61: \t\tif ( activeInstance != null ) {\n+   62: \t\t\tthrow new IllegalStateException( \"Using StaticCounters twice in a single test is forbidden.\"\n+   63: \t\t\t\t\t+ \" Make sure you added one (and only one)\"\n+   64: \t\t\t\t\t+ \" '@Rule public StaticIndexingSwitch indexingSwitch = new StaticIndexingSwitch()' to your test.\" );\n+   65: \t\t}\n+   66: \t\tactiveInstance = StaticIndexingSwitch.this;\n+   67: \t}\n+   72: \t\t\tcontext.dependencies().useRootOnly();\n+   73: \t\t\tcontext.bridge( Object.class, new Bridge() );\n+   74: \t\t}\n+   75: \t}\n+   76: \n+   77: \tprivate static class Bridge implements RoutingBridge<Object> {\n+   79: \t\tpublic void route(DocumentRoutes routes, Object entityIdentifier, Object indexedEntity,\n+   80: \t\t\t\tRoutingBridgeRouteContext context) {\n+   81: \t\t\tif ( activeSwitch().enabled() ) {\n+   82: \t\t\t\troutes.addRoute();\n+   83: \t\t\t}\n+   84: \t\t\telse {\n+   85: \t\t\t\troutes.notIndexed();\n+   86: \t\t\t}\n",
        "uniqueId": "7340d5e3553f69f655aa71a003f99f5d05abbeec_72_77__59_67_79_86",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Inline Method",
        "description": "Inline Method\tprivate doAfter(extensionContext ExtensionContext) : void inlined to public afterEach(extensionContext ExtensionContext) : void in class org.hibernate.search.testsupport.junit.SearchFactoryHolder",
        "diffLocations": [
            {
                "filePath": "integrationtest/v5migrationhelper/engine/src/test/java/org/hibernate/search/testsupport/junit/SearchFactoryHolder.java",
                "startLine": 69,
                "endLine": 74,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/v5migrationhelper/engine/src/test/java/org/hibernate/search/testsupport/junit/SearchFactoryHolder.java",
                "startLine": 60,
                "endLine": 65,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/v5migrationhelper/engine/src/test/java/org/hibernate/search/testsupport/junit/SearchFactoryHolder.java",
                "startLine": 76,
                "endLine": 80,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void doAfter(ExtensionContext extensionContext) throws Exception {\n\t\tmapping = null;\n\t\tsearchIntegrator = null;\n\t\tsetupHelper.afterAll( extensionContext );\n\t}",
        "filePathBefore": "integrationtest/v5migrationhelper/engine/src/test/java/org/hibernate/search/testsupport/junit/SearchFactoryHolder.java",
        "isPureRefactoring": true,
        "commitId": "7340d5e3553f69f655aa71a003f99f5d05abbeec",
        "packageNameBefore": "org.hibernate.search.testsupport.junit",
        "classNameBefore": "org.hibernate.search.testsupport.junit.SearchFactoryHolder",
        "methodNameBefore": "org.hibernate.search.testsupport.junit.SearchFactoryHolder#doAfter",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.backend.tck.testsupport.util.extension.SearchSetupHelper#afterAll\n methodBody: public void afterAll(ExtensionContext context) throws Exception {\nconfigurationProvider.afterAll(context);\nif(!runningInNestedContext(context) && callOncePerClass){cleanUp();\n}}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.BackendMock#afterAll\n methodBody: public void afterAll(ExtensionContext context) {\nif(callOncePerClass){doAfter(context);\n}}\nmethodSignature: org.hibernate.search.test.testsupport.StaticIndexingSwitch#afterAll\n methodBody: public void afterAll(ExtensionContext extensionContext) throws Exception {\nif(callOncePerClass){activeInstance=null;\nreset();\n}}\nmethodSignature: org.hibernate.search.test.util.FullTextSessionBuilder#afterAll\n methodBody: public void afterAll(ExtensionContext extensionContext) throws Exception {\nif(callOncePerClass){sessionFactory=null;\nsetupHelper.afterEach(extensionContext);\n}}\nmethodSignature: org.hibernate.search.testsupport.junit.SearchFactoryHolder#afterAll\n methodBody: public void afterAll(ExtensionContext extensionContext) throws Exception {\nif(callOncePerClass){doAfter(extensionContext);\n}}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.MappingSetupHelper#afterAll\n methodBody: public void afterAll(ExtensionContext context) throws Exception {\nconfigurationProvider.afterAll(context);\ncleanUp(ExtensionScope.CLASS);\n}",
        "classSignatureBefore": "public class SearchFactoryHolder implements AfterAllCallback, BeforeAllCallback, BeforeEachCallback, AfterEachCallback ",
        "methodNameBeforeSet": [
            "org.hibernate.search.testsupport.junit.SearchFactoryHolder#doAfter"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.testsupport.junit.SearchFactoryHolder"
        ],
        "classSignatureBeforeSet": [
            "public class SearchFactoryHolder implements AfterAllCallback, BeforeAllCallback, BeforeEachCallback, AfterEachCallback "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.testsupport.junit;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\nimport org.hibernate.search.mapper.pojo.standalone.mapping.SearchMapping;\nimport org.hibernate.search.spi.SearchIntegrator;\nimport org.hibernate.search.testsupport.migration.V5MigrationStandalonePojoSearchIntegratorAdapter;\n\nimport org.junit.jupiter.api.extension.AfterAllCallback;\nimport org.junit.jupiter.api.extension.AfterEachCallback;\nimport org.junit.jupiter.api.extension.BeforeAllCallback;\nimport org.junit.jupiter.api.extension.BeforeEachCallback;\nimport org.junit.jupiter.api.extension.ExtensionContext;\n\n/**\n * Testing SearchFactoryHolder.\n *\n * <p>Automatically retrieves configuration options from the classpath file \"/test-defaults.properties\".\n *\n * @author Sanne Grinovero\n * @since 4.1\n */\npublic class SearchFactoryHolder implements AfterAllCallback, BeforeAllCallback, BeforeEachCallback, AfterEachCallback {\n\n\tprivate final V5MigrationHelperEngineSetupHelper setupHelper = V5MigrationHelperEngineSetupHelper.create();\n\n\tprivate final Class<?>[] entities;\n\tprivate final Map<String, Object> configuration;\n\n\tprivate SearchMapping mapping;\n\tprivate SearchIntegrator searchIntegrator;\n\tprivate boolean callOncePerClass = false;\n\n\tpublic SearchFactoryHolder(Class<?>... entities) {\n\t\tthis.entities = entities;\n\t\tthis.configuration = new HashMap<>();\n\t}\n\n\tpublic SearchIntegrator getSearchFactory() {\n\t\treturn searchIntegrator;\n\t}\n\n\tpublic SearchMapping getMapping() {\n\t\treturn mapping;\n\t}\n\n\tpublic SearchFactoryHolder withProperty(String key, Object value) {\n\t\tassertThat( mapping ).as( \"Mapping already initialized\" ).isNotNull();\n\t\tconfiguration.put( key, value );\n\t\treturn this;\n\t}\n\n\t@Override\n\tpublic void afterAll(ExtensionContext extensionContext) throws Exception {\n\t\tif ( callOncePerClass ) {\n\t\t\tdoAfter( extensionContext );\n\t\t}\n\t}\n\n\t@Override\n\tpublic void afterEach(ExtensionContext extensionContext) throws Exception {\n\t\tif ( !callOncePerClass ) {\n\t\t\tdoAfter( extensionContext );\n\t\t}\n\t}\n\n\tprivate void doAfter(ExtensionContext extensionContext) throws Exception {\n\t\tmapping = null;\n\t\tsearchIntegrator = null;\n\t\tsetupHelper.afterAll( extensionContext );\n\t}\n\n\t@Override\n\tpublic void beforeAll(ExtensionContext extensionContext) throws Exception {\n\t\tcallOncePerClass = true;\n\t\tdoBefore( extensionContext );\n\t}\n\n\t@Override\n\tpublic void beforeEach(ExtensionContext extensionContext) throws Exception {\n\t\tif ( !callOncePerClass ) {\n\t\t\tdoBefore( extensionContext );\n\t\t}\n\t}\n\n\tprivate void doBefore(ExtensionContext extensionContext) throws Exception {\n\t\tsetupHelper.beforeAll( extensionContext );\n\n\t\tmapping = setupHelper.start()\n\t\t\t\t.withProperties( configuration )\n\t\t\t\t.setup( entities );\n\t\tsearchIntegrator = new V5MigrationStandalonePojoSearchIntegratorAdapter( mapping );\n\t}\n}\n",
        "filePathAfter": "integrationtest/v5migrationhelper/engine/src/test/java/org/hibernate/search/testsupport/junit/SearchFactoryHolder.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.testsupport.junit;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\nimport org.hibernate.search.mapper.pojo.standalone.mapping.SearchMapping;\nimport org.hibernate.search.spi.SearchIntegrator;\nimport org.hibernate.search.testsupport.migration.V5MigrationStandalonePojoSearchIntegratorAdapter;\n\nimport org.junit.jupiter.api.extension.AfterEachCallback;\nimport org.junit.jupiter.api.extension.BeforeAllCallback;\nimport org.junit.jupiter.api.extension.BeforeEachCallback;\nimport org.junit.jupiter.api.extension.ExtensionContext;\n\n/**\n * Testing SearchFactoryHolder.\n *\n * <p>Automatically retrieves configuration options from the classpath file \"/test-defaults.properties\".\n *\n * @author Sanne Grinovero\n * @since 4.1\n */\npublic class SearchFactoryHolder implements BeforeAllCallback, BeforeEachCallback, AfterEachCallback {\n\n\tprivate final V5MigrationHelperEngineSetupHelper setupHelper = V5MigrationHelperEngineSetupHelper.create();\n\n\tprivate final Class<?>[] entities;\n\tprivate final Map<String, Object> configuration;\n\n\tprivate SearchMapping mapping;\n\tprivate SearchIntegrator searchIntegrator;\n\n\tpublic SearchFactoryHolder(Class<?>... entities) {\n\t\tthis.entities = entities;\n\t\tthis.configuration = new HashMap<>();\n\t}\n\n\tpublic SearchIntegrator getSearchFactory() {\n\t\treturn searchIntegrator;\n\t}\n\n\tpublic SearchMapping getMapping() {\n\t\treturn mapping;\n\t}\n\n\tpublic SearchFactoryHolder withProperty(String key, Object value) {\n\t\tassertThat( mapping ).as( \"Mapping already initialized\" ).isNotNull();\n\t\tconfiguration.put( key, value );\n\t\treturn this;\n\t}\n\n\t@Override\n\tpublic void afterEach(ExtensionContext extensionContext) throws Exception {\n\t\tmapping = null;\n\t\tsearchIntegrator = null;\n\t\tsetupHelper.afterAll( extensionContext );\n\t}\n\n\t@Override\n\tpublic void beforeAll(ExtensionContext extensionContext) {\n\t\tthrow new IllegalStateException(\n\t\t\t\t\"SearchFactoryHolder is only available as nonstatic extension, i.e. @RegisterExtension SearchFactoryHolder searchFactoryHolder = new SearchFactoryHolder();\" );\n\t}\n\n\t@Override\n\tpublic void beforeEach(ExtensionContext extensionContext) throws Exception {\n\t\tsetupHelper.beforeEach( extensionContext );\n\n\t\tmapping = setupHelper.start()\n\t\t\t\t.withProperties( configuration )\n\t\t\t\t.setup( entities );\n\t\tsearchIntegrator = new V5MigrationStandalonePojoSearchIntegratorAdapter( mapping );\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.backend.tck.testsupport.util.extension.SearchSetupHelper#afterAll\n methodBody: public void afterAll(ExtensionContext context) throws Exception {\nconfigurationProvider.afterAll(context);\nif(!runningInNestedContext(context) && callOncePerClass){cleanUp();\n}}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.BackendMock#afterAll\n methodBody: public void afterAll(ExtensionContext context) {\nif(callOncePerClass){doAfter(context);\n}}",
            "methodSignature: org.hibernate.search.test.testsupport.StaticIndexingSwitch#afterAll\n methodBody: public void afterAll(ExtensionContext extensionContext) throws Exception {\nif(callOncePerClass){activeInstance=null;\nreset();\n}}",
            "methodSignature: org.hibernate.search.test.util.FullTextSessionBuilder#afterAll\n methodBody: public void afterAll(ExtensionContext extensionContext) throws Exception {\nif(callOncePerClass){sessionFactory=null;\nsetupHelper.afterEach(extensionContext);\n}}",
            "methodSignature: org.hibernate.search.testsupport.junit.SearchFactoryHolder#afterAll\n methodBody: public void afterAll(ExtensionContext extensionContext) throws Exception {\nif(callOncePerClass){doAfter(extensionContext);\n}}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.MappingSetupHelper#afterAll\n methodBody: public void afterAll(ExtensionContext context) throws Exception {\nconfigurationProvider.afterAll(context);\ncleanUp(ExtensionScope.CLASS);\n}"
        ],
        "sourceCodeAfterRefactoring": "@Override\n\tpublic void afterEach(ExtensionContext extensionContext) throws Exception {\n\t\tmapping = null;\n\t\tsearchIntegrator = null;\n\t\tsetupHelper.afterAll( extensionContext );\n\t}",
        "diffSourceCode": "-   60: \t}\n-   61: \n-   62: \t@Override\n-   63: \tpublic void afterAll(ExtensionContext extensionContext) throws Exception {\n-   64: \t\tif ( callOncePerClass ) {\n-   65: \t\t\tdoAfter( extensionContext );\n-   69: \t@Override\n-   70: \tpublic void afterEach(ExtensionContext extensionContext) throws Exception {\n-   71: \t\tif ( !callOncePerClass ) {\n-   72: \t\t\tdoAfter( extensionContext );\n-   73: \t\t}\n-   74: \t}\n-   76: \tprivate void doAfter(ExtensionContext extensionContext) throws Exception {\n-   77: \t\tmapping = null;\n-   78: \t\tsearchIntegrator = null;\n-   79: \t\tsetupHelper.afterAll( extensionContext );\n-   80: \t}\n+   60: \t@Override\n+   61: \tpublic void afterEach(ExtensionContext extensionContext) throws Exception {\n+   62: \t\tmapping = null;\n+   63: \t\tsearchIntegrator = null;\n+   64: \t\tsetupHelper.afterAll( extensionContext );\n+   65: \t}\n+   69: \t\tthrow new IllegalStateException(\n+   70: \t\t\t\t\"SearchFactoryHolder is only available as nonstatic extension, i.e. @RegisterExtension SearchFactoryHolder searchFactoryHolder = new SearchFactoryHolder();\" );\n+   71: \t}\n+   72: \n+   73: \t@Override\n+   74: \tpublic void beforeEach(ExtensionContext extensionContext) throws Exception {\n+   76: \n+   77: \t\tmapping = setupHelper.start()\n+   78: \t\t\t\t.withProperties( configuration )\n+   79: \t\t\t\t.setup( entities );\n+   80: \t\tsearchIntegrator = new V5MigrationStandalonePojoSearchIntegratorAdapter( mapping );\n",
        "uniqueId": "7340d5e3553f69f655aa71a003f99f5d05abbeec_69_74__60_65_76_80",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tpublic dataClearingIndexOnly() : SetupContext extracted from public init() : void in class org.hibernate.search.integrationtest.jakarta.batch.component.HibernateSearchPartitionMapperComponentIT & moved to class org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper.SetupContext",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm-jakarta-batch/src/test/java/org/hibernate/search/integrationtest/jakarta/batch/component/HibernateSearchPartitionMapperComponentIT.java",
                "startLine": 57,
                "endLine": 86,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm-jakarta-batch/src/test/java/org/hibernate/search/integrationtest/jakarta/batch/component/HibernateSearchPartitionMapperComponentIT.java",
                "startLine": 57,
                "endLine": 86,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm-jakarta-batch/src/test/java/org/hibernate/search/integrationtest/jakarta/batch/component/HibernateSearchPartitionMapperComponentIT.java",
                "startLine": 264,
                "endLine": 266,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@BeforeAll\n\tpublic void init() {\n\t\temf = ormSetupHelper.start().withAnnotatedTypes( Company.class, Person.class, CompanyGroup.class )\n\t\t\t\t.withProperty( HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED, false )\n\t\t\t\t.dataClearing( config -> config.clearDatabaseData( false ).clearIndexData( true ) )\n\t\t\t\t.setup();\n\n\t\twith( emf ).runInTransaction( session -> {\n\t\t\tfor ( int i = 1; i <= COMP_ROWS; i++ ) {\n\t\t\t\tsession.persist( new Company( \"C\" + i ) );\n\t\t\t}\n\t\t\tfor ( int i = 1; i <= PERS_ROWS; i++ ) {\n\t\t\t\tsession.persist( new Person( \"P\" + i, \"\", \"\" ) );\n\t\t\t}\n\t\t} );\n\n\t\tfinal String maxThreads = String.valueOf( 1 );\n\t\tfinal String rowsPerPartition = String.valueOf( 3 );\n\n\t\tmockedJobContext = mock( JobContext.class );\n\t\tpartitionMapper = new HibernateSearchPartitionMapper(\n\t\t\t\tnull, null,\n\t\t\t\tmaxThreads,\n\t\t\t\tnull,\n\t\t\t\trowsPerPartition,\n\t\t\t\tnull,\n\t\t\t\tnull,\n\t\t\t\tmockedJobContext\n\t\t);\n\t}",
        "filePathBefore": "integrationtest/mapper/orm-jakarta-batch/src/test/java/org/hibernate/search/integrationtest/jakarta/batch/component/HibernateSearchPartitionMapperComponentIT.java",
        "isPureRefactoring": true,
        "commitId": "da64c4e549b8dfd9c17e7753247d8028bc022529",
        "packageNameBefore": "org.hibernate.search.integrationtest.jakarta.batch.component",
        "classNameBefore": "org.hibernate.search.integrationtest.jakarta.batch.component.HibernateSearchPartitionMapperComponentIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.jakarta.batch.component.HibernateSearchPartitionMapperComponentIT#init",
        "invokedMethod": "methodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.BackendSetupStrategy#start\n methodBody: C extends MappingSetupHelper<C, ?, ?, ?, ?>.AbstractSetupContext> C start(C setupContext,\n\t\t\tTestConfigurationProvider configurationProvider,\n\t\t\tCompletionStage<BackendMappingHandle> mappingHandlePromise);\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.association.AutomaticIndexingAssociationDeletionIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(AssociationOwner.NAME);\nbackendMock.expectAnySchema(AssociationNonOwner.NAME);\nOrmSetupHelper.SetupContext setupContext=ormSetupHelper.start().withAnnotatedTypes(AssociationOwner.class,AssociationNonOwner.class).dataClearing(config -> config.clearOrder(AssociationOwner.class,AssociationNonOwner.class));\nsessionFactory=additionalSetup(setupContext).setup();\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper.SetupContext#setup\n methodBody: public SessionFactory setup(Class<?>... annotatedTypes) {\nreturn withAnnotatedTypes(annotatedTypes).setup();\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.BackendMockSetupStrategy#start\n methodBody: public <C extends MappingSetupHelper<C, ?, ?, ?, ?>.AbstractSetupContext> C start(C setupContext,\n\t\t\tTestConfigurationProvider configurationProvider,\n\t\t\tCompletionStage<BackendMappingHandle> mappingHandlePromise) {\nif(defaultBackendMock != null){setupContext=setupContext.withBackendProperty(\"type\",defaultBackendMock.factory(mappingHandlePromise));\n}for(Map.Entry<String,BackendMock> entry: namedBackendMocks.entrySet()){BackendMock backendMock=entry.getValue();\nsetupContext=setupContext.withBackendProperty(entry.getKey(),\"type\",backendMock.factory(mappingHandlePromise));\n}return setupContext;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.workspace.AbstractSearchWorkspaceSimpleOperationIT#setup\n methodBody: void setup() {\ndefaultBackendMock.expectAnySchema(IndexedEntity1.INDEX_NAME);\nbackend2Mock.expectAnySchema(IndexedEntity2.INDEX_NAME);\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntity1.class,IndexedEntity2.class).setup();\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.session.AutomaticIndexingOutOfTransactionIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(IndexedEntity.INDEX_NAME);\nsessionFactory=ormSetupHelper.start().withProperty(AvailableSettings.ALLOW_UPDATE_OUTSIDE_TRANSACTION,true).withAnnotatedTypes(IndexedEntity.class).setup();\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.MappingSetupHelper.AbstractSetupContext#setup\n methodBody: public final R setup() {\nif(setupCalled){throw new IllegalStateException(\"SetupContext#setup() was called multiple times on the same context\");\n}setupCalled=true;\nB builder=createBuilder();\nconsumeBeforeBuildConfigurations(builder,configurations.stream().map(c -> c.beforeBuild).collect(Collectors.toList()));\ntryR result=build(builder);\ntoClose.add(result);\nbackendMappingHandlePromise.complete(toBackendMappingHandle(result));\nconfigurations.forEach(c -> c.afterBuild(result));\nreturn result;\ncatch(Throwable t)backendMappingHandlePromise.complete(null);\nthrow t;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingMappedSuperclassIT#setup\n methodBody: void setup() {\nbackendMock.expectSchema(IndexedEntity.INDEX,b -> b.objectField(\"containedSingle\",b2 -> b2.field(\"includedInSingle\",String.class)));\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntityMappedSuperclass.class,IndexedEntity.class,ContainedEntity.class).dataClearing(config -> config.clearDatabaseData(false)).setup();\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.association.AutomaticIndexingPolymorphicOriginalSideAssociationIT#setup\n methodBody: void setup() {\nbackendMock.expectSchema(IndexedEntity.INDEX,b -> b.objectField(\"child\",b3 -> b3.objectField(\"containedSingle\",b2 -> b2.field(\"includedInSingle\",String.class))));\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntity.class,ContainingEntity.class,FirstMiddleContainingEntity.class,SecondMiddleContainingEntity.class,ContainedEntity.class).dataClearing(config -> config.clearDatabaseData(false)).setup();\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.array.AbstractAutomaticIndexingArrayIT#setup\n methodBody: void setup() {\nbackendMock.expectSchema(primitives.getIndexName(),b -> b.field(\"serializedArray\",primitives.getExpectedIndexFieldType(),b2 -> b2.multiValued(true)).field(\"elementCollectionArray\",primitives.getExpectedIndexFieldType(),b2 -> b2.multiValued(true)));\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(primitives.getIndexedClass()).setup();\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.proxy.ReindexingResolverProxiedAssociatedEntityIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(IndexedEntity.NAME);\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntity.class,ContainedLevel1Entity.class,ContainedLevel2Entity.class).dataClearing(config -> config.clearOrder(IndexedEntity.class,ContainedLevel2Entity.class,ContainedLevel1Entity.class)).setup();\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper.SetupContext#dataClearing\n methodBody: public SetupContext dataClearing(boolean reset, Consumer<DataClearConfig> configurer) {\nif(reset){ormSetupHelperCleaner=OrmSetupHelperCleaner.create(callOncePerClass);\n}ormSetupHelperCleaner.appendConfiguration(configurer);\nreturn thisAsC();\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.search.SearchQueryBaseIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(Book.NAME);\nbackendMock.expectAnySchema(Author.NAME);\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(Book.class,Author.class,NotIndexed.class).dataClearing(config -> config.clearOrder(Book.class,Author.class,NotIndexed.class)).setup();\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper.SetupContext#withAnnotatedTypes\n methodBody: public SetupContext withAnnotatedTypes(Class<?>... annotatedTypes) {\nreturn withConfiguration(builder -> builder.addAnnotatedClasses(Arrays.asList(annotatedTypes)));\n}\nmethodSignature: org.hibernate.search.integrationtest.jakarta.batch.massindexing.MassIndexingJobIT#setup\n methodBody: void setup() {\nemf=ormSetupHelper.start().withAnnotatedTypes(Company.class,Person.class,WhoAmI.class,CompanyGroup.class).withProperty(HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED,false).dataClearing(config -> config.clearOrder(CompanyGroup.class,Company.class).clearIndexData(true)).setup();\n}\nmethodSignature: org.hibernate.search.integrationtest.jakarta.batch.massindexing.RestartChunkIT#setup\n methodBody: void setup() {\nemf=ormSetupHelper.start().withAnnotatedTypes(SimulatedFailureCompany.class).withProperty(HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED,false).dataClearing(config -> config.clearIndexData(true)).setup();\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.hibernateormapis.ToHibernateOrmQueryIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(IndexedEntity.NAME);\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntity.class,ContainedEntity.class).dataClearing(config -> config.preClear(ContainedEntity.class,c -> c.setContainingLazy(null)).clearOrder(IndexedEntity.class,ContainedEntity.class)).setup();\n}\nmethodSignature: org.hibernate.search.integrationtest.jakarta.batch.component.ValidationUtilComponentIT#setup\n methodBody: void setup() {\normSetupHelper.start().withAnnotatedTypes(Company.class,Person.class).withProperty(HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED,false).dataClearing(config -> config.clearIndexData(true)).setup();\n}\nmethodSignature: org.hibernate.search.integrationtest.jakarta.batch.massindexing.MassIndexingJobWithCompositeIdIT#setup\n methodBody: void setup() {\nemf=ormSetupHelper.start().withAnnotatedTypes(EntityWithIdClass.class,EntityWithEmbeddedId.class).withProperty(HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED,false).dataClearing(config -> config.clearDatabaseData(false).clearIndexData(true)).setup();\nwith(emf).runInTransaction(entityManager -> {\n  for (LocalDate d=START; d.isBefore(END); d=d.plusDays(1)) {\n    entityManager.persist(new EntityWithIdClass(d));\n    entityManager.persist(new EntityWithEmbeddedId(d));\n  }\n}\n);\nassertThat(JobTestUtil.nbDocumentsInIndex(emf,EntityWithIdClass.class)).isEqualTo(0);\nassertThat(JobTestUtil.nbDocumentsInIndex(emf,EntityWithEmbeddedId.class)).isEqualTo(0);\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.search.loading.SearchQueryEntityLoadingGraphIT#setup\n methodBody: void setup(SingleTypeLoadingModel<T> model, SingleTypeLoadingMapping mapping) {\nthis.model=model;\nthis.mapping=mapping;\nbackendMock.expectAnySchema(model.getIndexName());\nsessionFactory=ormSetupHelper.start().withConfiguration(c -> mapping.configure(c,model)).dataClearing(true,config -> config.preClear(model.getIndexedClass(),model::clearContainedEager).clearOrder(model.getContainedClass(),model.getIndexedClass())).setup();\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.MappingSetupHelper#start\n methodBody: public C start(SV setupVariant) {\nC setupContext=createSetupContext(setupVariant);\nreturn backendSetupStrategy.start(setupContext,configurationProvider,setupContext.backendMappingHandlePromise);\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.ActualBackendSetupStrategy#start\n methodBody: public <C extends MappingSetupHelper<C, ?, ?, ?, ?>.AbstractSetupContext> C start(C setupContext,\n\t\t\tTestConfigurationProvider configurationProvider,\n\t\t\t// The mapping handle is not used by actual backends (only by BackendMock).\n\t\t\tCompletionStage<BackendMappingHandle> mappingHandlePromise) {\nif(defaultBackendConfiguration != null){setupContext=defaultBackendConfiguration.setup(setupContext,null,configurationProvider);\n}for(Map.Entry<String,BackendConfiguration> entry: namedBackendConfigurations.entrySet()){String name=entry.getKey();\nBackendConfiguration configuration=entry.getValue();\nsetupContext=configuration.setup(setupContext,name,configurationProvider);\n}return setupContext;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingOverReindexingIT#setup\n methodBody: void setup() {\nbackendMock.expectSchema(Level1Entity.INDEX,b -> b.field(\"property1FromBridge\",String.class));\nbackendMock.expectSchema(Level2Entity.INDEX,b -> b.objectField(\"level3\",b3 -> b3.field(\"property2\",String.class)));\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(Level1Entity.class,Level2Entity.class,Level3Entity.class).setup();\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.session.SearchIndexingPlanBaseIT#setup\n methodBody: void setup() {\ndefaultBackendMock.expectAnySchema(IndexedEntity1.INDEX_NAME);\nbackend2Mock.expectAnySchema(IndexedEntity2.INDEX_NAME);\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntity1.class,IndexedEntity2.class,ContainedEntity.class).setup();\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper.SetupContext#withProperty\n methodBody: public SetupContext withProperty(String key, Object value) {\noverriddenProperties.put(key,value);\nreturn thisAsC();\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingIdClassIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(IdClassEntity.INDEX);\nsessionFactory=ormSetupHelper.start().withPropertyRadical(HibernateOrmMapperSettings.Radicals.INDEXING_LISTENERS_ENABLED,false).withAnnotatedTypes(IdClassEntity.class).setup();\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelperCleaner.DataClearConfigImpl#clearIndexData\n methodBody: public DataClearConfig clearIndexData(boolean clear) {\nthis.clearIndexData=clear;\nreturn this;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.hibernateormapis.ToJpaQueryIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(IndexedEntity.NAME);\nsessionFactory=ormSetupHelper.start().withProperty(AvailableSettings.JPA_QUERY_COMPLIANCE,true).withAnnotatedTypes(IndexedEntity.class,ContainedEntity.class).dataClearing(config -> config.preClear(ContainedEntity.class,c -> c.setContainingLazy(null)).clearOrder(IndexedEntity.class,ContainedEntity.class)).setup();\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.MappingSetupHelper.AbstractSetupContext#withProperty\n methodBody: public abstract C withProperty(String keyRadical, Object value);\nmethodSignature: org.hibernate.search.integrationtest.jakarta.batch.massindexing.MassIndexingJobWithMultiTenancyIT#setup\n methodBody: public void setup() {\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(Company.class).withProperty(HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED,false).withBackendProperty(\"multi_tenancy.strategy\",\"discriminator\").tenants(TARGET_TENANT_ID,UNUSED_TENANT_ID).setup();\nwith(sessionFactory,TARGET_TENANT_ID).runInTransaction(session -> companies.forEach(session::persist));\nwith(sessionFactory,TARGET_TENANT_ID).runNoTransaction(session -> Search.session(session).workspace(Company.class).purge());\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.model.BytecodeEnhancementIT#setup\n methodBody: public void setup() {\nbackendMock.expectSchema(IndexedEntity.INDEX,b -> b.field(\"mappedSuperClassText\",String.class).field(\"entitySuperClassText\",String.class).field(\"id\",Integer.class).objectField(\"containedEntityList\",b2 -> b2.multiValued(true).field(\"text\",String.class)).objectField(\"containedEmbeddable\",b2 -> b2.field(\"text\",String.class)).field(\"text1\",String.class).field(\"text2\",String.class).field(\"primitiveInteger\",Integer.class).field(\"primitiveLong\",Long.class).field(\"primitiveBoolean\",Boolean.class).field(\"primitiveFloat\",Float.class).field(\"primitiveDouble\",Double.class).field(\"transientText\",String.class));\nOrmSetupHelper.SetupContext setupContext=ormSetupHelper.start().withTcclLookupPrecedenceBefore().withAnnotatedTypes(IndexedMappedSuperClass.class,IndexedEntitySuperClass.class,IndexedEntity.class,ContainedEntity.class,ContainedEmbeddable.class);\nsessionFactory=setupContext.setup();\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.MappingSetupHelper.AbstractSetupContext#with\n methodBody: public final C with(UnaryOperator<C> config) {\nreturn config.apply(thisAsC());\n}\nmethodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelperCleaner.DataClearConfigImpl#clearDatabaseData\n methodBody: public DataClearConfig clearDatabaseData(boolean clear) {\nthis.clearDatabaseData=clear;\nreturn this;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.association.AutomaticIndexingGenericPolymorphicAssociationIT#setup\n methodBody: void setup() {\nbackendMock.expectSchema(IndexedEntity.INDEX,b -> b.objectField(\"child\",b3 -> b3.objectField(\"containedSingle\",b2 -> b2.field(\"includedInSingle\",String.class))));\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntity.class,ContainingEntity.class,MiddleContainingEntity.class,UnrelatedContainingEntity.class,ContainedEntity.class).dataClearing(config -> config.clearOrder(IndexedEntity.class,ContainingEntity.class,MiddleContainingEntity.class,UnrelatedContainingEntity.class,ContainedEntity.class).preClear(session -> session.createQuery(\"select e from indexed e \",IndexedEntity.class).getResultList().forEach(e -> {\n  e.getChild().setParent(null);\n  e.setChild(null);\n}\n))).setup();\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.smoke.ProgrammaticMappingSmokeIT#setup\n methodBody: void setup() {\nbackendMock.expectSchema(OtherIndexedEntity.NAME,b -> b.field(\"numeric\",Integer.class).field(\"numericAsString\",String.class));\nbackendMock.expectSchema(YetAnotherIndexedEntity.NAME,b -> b.objectField(\"customBridgeOnProperty\",b2 -> b2.field(\"date\",LocalDate.class).field(\"text\",String.class)).field(\"myLocalDateField\",LocalDate.class).field(\"numeric\",Integer.class).objectField(\"myEmbeddedList\",b2 -> b2.multiValued(true).objectField(\"myEmbedded\",b3 -> b3.objectField(\"customBridgeOnClass\",b4 -> b4.field(\"text\",String.class)))).field(\"embeddedMapKeys\",String.class,b2 -> b2.multiValued(true)).objectField(\"embeddedMap\",b2 -> b2.multiValued(true).objectField(\"myEmbedded\",b3 -> b3.field(\"myLocalDateField\",LocalDate.class))));\nbackendMock.expectSchema(IndexedEntity.NAME,b -> b.objectField(\"customBridgeOnClass\",b2 -> b2.field(\"date\",LocalDate.class).field(\"text\",String.class)).objectField(\"customBridgeOnProperty\",b2 -> b2.field(\"date\",LocalDate.class).field(\"text\",String.class)).objectField(\"myEmbedded\",b2 -> b2.objectField(\"customBridgeOnClass\",b3 -> b3.field(\"date\",LocalDate.class).field(\"text\",String.class)).objectField(\"customBridgeOnProperty\",b3 -> b3.field(\"date\",LocalDate.class).field(\"text\",String.class)).objectField(\"myEmbedded\",b3 -> b3.objectField(\"customBridgeOnClass\",b4 -> b4.field(\"text\",String.class))).field(\"myLocalDateField\",LocalDate.class).field(\"myTextField\",String.class)).field(\"myTextField\",String.class).field(\"myLocalDateField\",LocalDate.class));\nsessionFactory=ormSetupHelper.start().withProperty(HibernateOrmMapperSettings.MAPPING_CONFIGURER,new MyMappingConfigurer()).withAnnotatedTypes(IndexedEntity.class,ParentIndexedEntity.class,OtherIndexedEntity.class,YetAnotherIndexedEntity.class).dataClearing(config -> config.clearDatabaseData(false)).setup();\n}",
        "classSignatureBefore": "class HibernateSearchPartitionMapperComponentIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.jakarta.batch.component.HibernateSearchPartitionMapperComponentIT#init"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.jakarta.batch.component.HibernateSearchPartitionMapperComponentIT"
        ],
        "classSignatureBeforeSet": [
            "class HibernateSearchPartitionMapperComponentIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.jakarta.batch.component;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils.with;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.when;\n\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.Properties;\n\nimport jakarta.batch.api.partition.PartitionPlan;\nimport jakarta.batch.runtime.context.JobContext;\nimport jakarta.persistence.EntityManagerFactory;\n\nimport org.hibernate.search.integrationtest.jakarta.batch.massindexing.entity.Company;\nimport org.hibernate.search.integrationtest.jakarta.batch.massindexing.entity.CompanyGroup;\nimport org.hibernate.search.integrationtest.jakarta.batch.massindexing.entity.Person;\nimport org.hibernate.search.integrationtest.jakarta.batch.util.BackendConfigurations;\nimport org.hibernate.search.integrationtest.jakarta.batch.util.JobTestUtil;\nimport org.hibernate.search.jakarta.batch.core.massindexing.impl.JobContextData;\nimport org.hibernate.search.jakarta.batch.core.massindexing.step.impl.HibernateSearchPartitionMapper;\nimport org.hibernate.search.jakarta.batch.core.massindexing.util.impl.MassIndexingPartitionProperties;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\n\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\nimport org.junit.jupiter.api.TestInstance;\nimport org.junit.jupiter.api.extension.RegisterExtension;\n\n/**\n * Single-component test for partition plan validation.\n *\n * @author Mincong Huang\n */\n@TestInstance(TestInstance.Lifecycle.PER_CLASS)\nclass HibernateSearchPartitionMapperComponentIT {\n\n\tprivate static final int COMP_ROWS = 3;\n\tprivate static final int PERS_ROWS = 8;\n\n\t@RegisterExtension\n\tpublic static OrmSetupHelper ormSetupHelper = OrmSetupHelper.withSingleBackend( BackendConfigurations.simple() );\n\tprivate EntityManagerFactory emf;\n\n\tprivate JobContext mockedJobContext;\n\n\tprivate HibernateSearchPartitionMapper partitionMapper;\n\n\t@BeforeAll\n\tpublic void init() {\n\t\temf = ormSetupHelper.start().withAnnotatedTypes( Company.class, Person.class, CompanyGroup.class )\n\t\t\t\t.withProperty( HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED, false )\n\t\t\t\t.dataClearing( config -> config.clearDatabaseData( false ).clearIndexData( true ) )\n\t\t\t\t.setup();\n\n\t\twith( emf ).runInTransaction( session -> {\n\t\t\tfor ( int i = 1; i <= COMP_ROWS; i++ ) {\n\t\t\t\tsession.persist( new Company( \"C\" + i ) );\n\t\t\t}\n\t\t\tfor ( int i = 1; i <= PERS_ROWS; i++ ) {\n\t\t\t\tsession.persist( new Person( \"P\" + i, \"\", \"\" ) );\n\t\t\t}\n\t\t} );\n\n\t\tfinal String maxThreads = String.valueOf( 1 );\n\t\tfinal String rowsPerPartition = String.valueOf( 3 );\n\n\t\tmockedJobContext = mock( JobContext.class );\n\t\tpartitionMapper = new HibernateSearchPartitionMapper(\n\t\t\t\tnull, null,\n\t\t\t\tmaxThreads,\n\t\t\t\tnull,\n\t\t\t\trowsPerPartition,\n\t\t\t\tnull,\n\t\t\t\tnull,\n\t\t\t\tmockedJobContext\n\t\t);\n\t}\n\n\t/**\n\t * Prove that there are N partitions for each root entity,\n\t * where N stands for the ceiling number of the division\n\t * between the rows to index and the max rows per partition.\n\t */\n\t@Test\n\tvoid simple() throws Exception {\n\t\tJobContextData jobData = new JobContextData();\n\t\tjobData.setEntityManagerFactory( emf );\n\t\tvar companyType = JobTestUtil.createEntityTypeDescriptor( emf, Company.class );\n\t\tvar personType = JobTestUtil.createEntityTypeDescriptor( emf, Person.class );\n\t\tjobData.setEntityTypeDescriptors( Arrays.asList( companyType, personType ) );\n\t\twhen( mockedJobContext.getTransientUserData() ).thenReturn( jobData );\n\n\t\tPartitionPlan partitionPlan = partitionMapper.mapPartitions();\n\n\t\tint compPartitions = 0;\n\t\tint persPartitions = 0;\n\t\tfor ( Properties p : partitionPlan.getPartitionProperties() ) {\n\t\t\tString entityName = p.getProperty( MassIndexingPartitionProperties.ENTITY_NAME );\n\t\t\tif ( entityName.equals( companyType.jpaEntityName() ) ) {\n\t\t\t\tcompPartitions++;\n\t\t\t}\n\t\t\tif ( entityName.equals( personType.jpaEntityName() ) ) {\n\t\t\t\tpersPartitions++;\n\t\t\t}\n\t\t\t/*\n\t\t\t * The checkpoint interval should have defaulted to the value of rowsPerPartition,\n\t\t\t * since the value of rowsPerPartition is lower than the static default for checkpoint interval.\n\t\t\t */\n\t\t\tString checkpointInterval = p.getProperty( MassIndexingPartitionProperties.CHECKPOINT_INTERVAL );\n\t\t\tassertThat( checkpointInterval ).isNotNull();\n\t\t\tassertThat( checkpointInterval ).isEqualTo( \"3\" );\n\t\t}\n\n\t\t// nbPartitions = rows / rowsPerPartition\n\t\tassertThat( compPartitions ).isEqualTo( 1 ); // 3 / 3 => 1 partition\n\t\tassertThat( persPartitions ).isEqualTo( 3 ); // 8 / 3 => 3 partitions\n\t}\n\n\t@Test\n\tvoid noData() throws Exception {\n\t\tJobContextData jobData = new JobContextData();\n\t\tjobData.setEntityManagerFactory( emf );\n\t\tvar companyGroupType = JobTestUtil.createEntityTypeDescriptor( emf, CompanyGroup.class );\n\t\tjobData.setEntityTypeDescriptors( Collections.singletonList( companyGroupType ) );\n\t\twhen( mockedJobContext.getTransientUserData() ).thenReturn( jobData );\n\n\t\tPartitionPlan partitionPlan = partitionMapper.mapPartitions();\n\n\t\tint compGroupPartitions = 0;\n\t\tfor ( Properties p : partitionPlan.getPartitionProperties() ) {\n\t\t\tString entityName = p.getProperty( MassIndexingPartitionProperties.ENTITY_NAME );\n\t\t\tif ( entityName.equals( companyGroupType.jpaEntityName() ) ) {\n\t\t\t\tcompGroupPartitions++;\n\t\t\t}\n\t\t}\n\n\t\t// Did not find anything in the ResultSet at index \"rowsPerPartition\"\n\t\t// => 1 partition covering the whole range.\n\t\t// We'll notice there is no data later, when reading IDs to reindex.\n\t\tassertThat( compGroupPartitions ).isEqualTo( 1 );\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/orm-jakarta-batch/src/test/java/org/hibernate/search/integrationtest/jakarta/batch/component/HibernateSearchPartitionMapperComponentIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.jakarta.batch.component;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils.with;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.when;\n\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.Properties;\n\nimport jakarta.batch.api.partition.PartitionPlan;\nimport jakarta.batch.runtime.context.JobContext;\nimport jakarta.persistence.EntityManagerFactory;\n\nimport org.hibernate.search.integrationtest.jakarta.batch.massindexing.entity.Company;\nimport org.hibernate.search.integrationtest.jakarta.batch.massindexing.entity.CompanyGroup;\nimport org.hibernate.search.integrationtest.jakarta.batch.massindexing.entity.Person;\nimport org.hibernate.search.integrationtest.jakarta.batch.util.BackendConfigurations;\nimport org.hibernate.search.integrationtest.jakarta.batch.util.JobTestUtil;\nimport org.hibernate.search.jakarta.batch.core.massindexing.impl.JobContextData;\nimport org.hibernate.search.jakarta.batch.core.massindexing.step.impl.HibernateSearchPartitionMapper;\nimport org.hibernate.search.jakarta.batch.core.massindexing.util.impl.MassIndexingPartitionProperties;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\n\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\nimport org.junit.jupiter.api.TestInstance;\nimport org.junit.jupiter.api.extension.RegisterExtension;\n\n/**\n * Single-component test for partition plan validation.\n *\n * @author Mincong Huang\n */\n@TestInstance(TestInstance.Lifecycle.PER_CLASS)\nclass HibernateSearchPartitionMapperComponentIT {\n\n\tprivate static final int COMP_ROWS = 3;\n\tprivate static final int PERS_ROWS = 8;\n\n\t@RegisterExtension\n\tpublic static OrmSetupHelper ormSetupHelper = OrmSetupHelper.withSingleBackend( BackendConfigurations.simple() );\n\tprivate EntityManagerFactory emf;\n\n\tprivate JobContext mockedJobContext;\n\n\tprivate HibernateSearchPartitionMapper partitionMapper;\n\n\t@BeforeAll\n\tpublic void init() {\n\t\temf = ormSetupHelper.start().withAnnotatedTypes( Company.class, Person.class, CompanyGroup.class )\n\t\t\t\t.withProperty( HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED, false )\n\t\t\t\t.dataClearingIndexOnly()\n\t\t\t\t.setup();\n\n\t\twith( emf ).runInTransaction( session -> {\n\t\t\tfor ( int i = 1; i <= COMP_ROWS; i++ ) {\n\t\t\t\tsession.persist( new Company( \"C\" + i ) );\n\t\t\t}\n\t\t\tfor ( int i = 1; i <= PERS_ROWS; i++ ) {\n\t\t\t\tsession.persist( new Person( \"P\" + i, \"\", \"\" ) );\n\t\t\t}\n\t\t} );\n\n\t\tfinal String maxThreads = String.valueOf( 1 );\n\t\tfinal String rowsPerPartition = String.valueOf( 3 );\n\n\t\tmockedJobContext = mock( JobContext.class );\n\t\tpartitionMapper = new HibernateSearchPartitionMapper(\n\t\t\t\tnull, null,\n\t\t\t\tmaxThreads,\n\t\t\t\tnull,\n\t\t\t\trowsPerPartition,\n\t\t\t\tnull,\n\t\t\t\tnull,\n\t\t\t\tmockedJobContext\n\t\t);\n\t}\n\n\t/**\n\t * Prove that there are N partitions for each root entity,\n\t * where N stands for the ceiling number of the division\n\t * between the rows to index and the max rows per partition.\n\t */\n\t@Test\n\tvoid simple() throws Exception {\n\t\tJobContextData jobData = new JobContextData();\n\t\tjobData.setEntityManagerFactory( emf );\n\t\tvar companyType = JobTestUtil.createEntityTypeDescriptor( emf, Company.class );\n\t\tvar personType = JobTestUtil.createEntityTypeDescriptor( emf, Person.class );\n\t\tjobData.setEntityTypeDescriptors( Arrays.asList( companyType, personType ) );\n\t\twhen( mockedJobContext.getTransientUserData() ).thenReturn( jobData );\n\n\t\tPartitionPlan partitionPlan = partitionMapper.mapPartitions();\n\n\t\tint compPartitions = 0;\n\t\tint persPartitions = 0;\n\t\tfor ( Properties p : partitionPlan.getPartitionProperties() ) {\n\t\t\tString entityName = p.getProperty( MassIndexingPartitionProperties.ENTITY_NAME );\n\t\t\tif ( entityName.equals( companyType.jpaEntityName() ) ) {\n\t\t\t\tcompPartitions++;\n\t\t\t}\n\t\t\tif ( entityName.equals( personType.jpaEntityName() ) ) {\n\t\t\t\tpersPartitions++;\n\t\t\t}\n\t\t\t/*\n\t\t\t * The checkpoint interval should have defaulted to the value of rowsPerPartition,\n\t\t\t * since the value of rowsPerPartition is lower than the static default for checkpoint interval.\n\t\t\t */\n\t\t\tString checkpointInterval = p.getProperty( MassIndexingPartitionProperties.CHECKPOINT_INTERVAL );\n\t\t\tassertThat( checkpointInterval ).isNotNull();\n\t\t\tassertThat( checkpointInterval ).isEqualTo( \"3\" );\n\t\t}\n\n\t\t// nbPartitions = rows / rowsPerPartition\n\t\tassertThat( compPartitions ).isEqualTo( 1 ); // 3 / 3 => 1 partition\n\t\tassertThat( persPartitions ).isEqualTo( 3 ); // 8 / 3 => 3 partitions\n\t}\n\n\t@Test\n\tvoid noData() throws Exception {\n\t\tJobContextData jobData = new JobContextData();\n\t\tjobData.setEntityManagerFactory( emf );\n\t\tvar companyGroupType = JobTestUtil.createEntityTypeDescriptor( emf, CompanyGroup.class );\n\t\tjobData.setEntityTypeDescriptors( Collections.singletonList( companyGroupType ) );\n\t\twhen( mockedJobContext.getTransientUserData() ).thenReturn( jobData );\n\n\t\tPartitionPlan partitionPlan = partitionMapper.mapPartitions();\n\n\t\tint compGroupPartitions = 0;\n\t\tfor ( Properties p : partitionPlan.getPartitionProperties() ) {\n\t\t\tString entityName = p.getProperty( MassIndexingPartitionProperties.ENTITY_NAME );\n\t\t\tif ( entityName.equals( companyGroupType.jpaEntityName() ) ) {\n\t\t\t\tcompGroupPartitions++;\n\t\t\t}\n\t\t}\n\n\t\t// Did not find anything in the ResultSet at index \"rowsPerPartition\"\n\t\t// => 1 partition covering the whole range.\n\t\t// We'll notice there is no data later, when reading IDs to reindex.\n\t\tassertThat( compGroupPartitions ).isEqualTo( 1 );\n\t}\n}\n",
        "diffSourceCodeSet": [
            ""
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.BackendSetupStrategy#start\n methodBody: C extends MappingSetupHelper<C, ?, ?, ?, ?>.AbstractSetupContext> C start(C setupContext,\n\t\t\tTestConfigurationProvider configurationProvider,\n\t\t\tCompletionStage<BackendMappingHandle> mappingHandlePromise);",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.association.AutomaticIndexingAssociationDeletionIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(AssociationOwner.NAME);\nbackendMock.expectAnySchema(AssociationNonOwner.NAME);\nOrmSetupHelper.SetupContext setupContext=ormSetupHelper.start().withAnnotatedTypes(AssociationOwner.class,AssociationNonOwner.class).dataClearing(config -> config.clearOrder(AssociationOwner.class,AssociationNonOwner.class));\nsessionFactory=additionalSetup(setupContext).setup();\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper.SetupContext#setup\n methodBody: public SessionFactory setup(Class<?>... annotatedTypes) {\nreturn withAnnotatedTypes(annotatedTypes).setup();\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.BackendMockSetupStrategy#start\n methodBody: public <C extends MappingSetupHelper<C, ?, ?, ?, ?>.AbstractSetupContext> C start(C setupContext,\n\t\t\tTestConfigurationProvider configurationProvider,\n\t\t\tCompletionStage<BackendMappingHandle> mappingHandlePromise) {\nif(defaultBackendMock != null){setupContext=setupContext.withBackendProperty(\"type\",defaultBackendMock.factory(mappingHandlePromise));\n}for(Map.Entry<String,BackendMock> entry: namedBackendMocks.entrySet()){BackendMock backendMock=entry.getValue();\nsetupContext=setupContext.withBackendProperty(entry.getKey(),\"type\",backendMock.factory(mappingHandlePromise));\n}return setupContext;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.workspace.AbstractSearchWorkspaceSimpleOperationIT#setup\n methodBody: void setup() {\ndefaultBackendMock.expectAnySchema(IndexedEntity1.INDEX_NAME);\nbackend2Mock.expectAnySchema(IndexedEntity2.INDEX_NAME);\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntity1.class,IndexedEntity2.class).setup();\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.session.AutomaticIndexingOutOfTransactionIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(IndexedEntity.INDEX_NAME);\nsessionFactory=ormSetupHelper.start().withProperty(AvailableSettings.ALLOW_UPDATE_OUTSIDE_TRANSACTION,true).withAnnotatedTypes(IndexedEntity.class).setup();\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.MappingSetupHelper.AbstractSetupContext#setup\n methodBody: public final R setup() {\nif(setupCalled){throw new IllegalStateException(\"SetupContext#setup() was called multiple times on the same context\");\n}setupCalled=true;\nB builder=createBuilder();\nconsumeBeforeBuildConfigurations(builder,configurations.stream().map(c -> c.beforeBuild).collect(Collectors.toList()));\ntryR result=build(builder);\ntoClose.add(result);\nbackendMappingHandlePromise.complete(toBackendMappingHandle(result));\nconfigurations.forEach(c -> c.afterBuild(result));\nreturn result;\ncatch(Throwable t)backendMappingHandlePromise.complete(null);\nthrow t;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingMappedSuperclassIT#setup\n methodBody: void setup() {\nbackendMock.expectSchema(IndexedEntity.INDEX,b -> b.objectField(\"containedSingle\",b2 -> b2.field(\"includedInSingle\",String.class)));\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntityMappedSuperclass.class,IndexedEntity.class,ContainedEntity.class).dataClearing(config -> config.clearDatabaseData(false)).setup();\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.association.AutomaticIndexingPolymorphicOriginalSideAssociationIT#setup\n methodBody: void setup() {\nbackendMock.expectSchema(IndexedEntity.INDEX,b -> b.objectField(\"child\",b3 -> b3.objectField(\"containedSingle\",b2 -> b2.field(\"includedInSingle\",String.class))));\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntity.class,ContainingEntity.class,FirstMiddleContainingEntity.class,SecondMiddleContainingEntity.class,ContainedEntity.class).dataClearing(config -> config.clearDatabaseData(false)).setup();\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.array.AbstractAutomaticIndexingArrayIT#setup\n methodBody: void setup() {\nbackendMock.expectSchema(primitives.getIndexName(),b -> b.field(\"serializedArray\",primitives.getExpectedIndexFieldType(),b2 -> b2.multiValued(true)).field(\"elementCollectionArray\",primitives.getExpectedIndexFieldType(),b2 -> b2.multiValued(true)));\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(primitives.getIndexedClass()).setup();\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.proxy.ReindexingResolverProxiedAssociatedEntityIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(IndexedEntity.NAME);\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntity.class,ContainedLevel1Entity.class,ContainedLevel2Entity.class).dataClearing(config -> config.clearOrder(IndexedEntity.class,ContainedLevel2Entity.class,ContainedLevel1Entity.class)).setup();\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper.SetupContext#dataClearing\n methodBody: public SetupContext dataClearing(boolean reset, Consumer<DataClearConfig> configurer) {\nif(reset){ormSetupHelperCleaner=OrmSetupHelperCleaner.create(callOncePerClass);\n}ormSetupHelperCleaner.appendConfiguration(configurer);\nreturn thisAsC();\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.search.SearchQueryBaseIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(Book.NAME);\nbackendMock.expectAnySchema(Author.NAME);\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(Book.class,Author.class,NotIndexed.class).dataClearing(config -> config.clearOrder(Book.class,Author.class,NotIndexed.class)).setup();\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper.SetupContext#withAnnotatedTypes\n methodBody: public SetupContext withAnnotatedTypes(Class<?>... annotatedTypes) {\nreturn withConfiguration(builder -> builder.addAnnotatedClasses(Arrays.asList(annotatedTypes)));\n}",
            "methodSignature: org.hibernate.search.integrationtest.jakarta.batch.massindexing.MassIndexingJobIT#setup\n methodBody: void setup() {\nemf=ormSetupHelper.start().withAnnotatedTypes(Company.class,Person.class,WhoAmI.class,CompanyGroup.class).withProperty(HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED,false).dataClearing(config -> config.clearOrder(CompanyGroup.class,Company.class).clearIndexData(true)).setup();\n}",
            "methodSignature: org.hibernate.search.integrationtest.jakarta.batch.massindexing.RestartChunkIT#setup\n methodBody: void setup() {\nemf=ormSetupHelper.start().withAnnotatedTypes(SimulatedFailureCompany.class).withProperty(HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED,false).dataClearing(config -> config.clearIndexData(true)).setup();\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.hibernateormapis.ToHibernateOrmQueryIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(IndexedEntity.NAME);\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntity.class,ContainedEntity.class).dataClearing(config -> config.preClear(ContainedEntity.class,c -> c.setContainingLazy(null)).clearOrder(IndexedEntity.class,ContainedEntity.class)).setup();\n}",
            "methodSignature: org.hibernate.search.integrationtest.jakarta.batch.component.ValidationUtilComponentIT#setup\n methodBody: void setup() {\normSetupHelper.start().withAnnotatedTypes(Company.class,Person.class).withProperty(HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED,false).dataClearing(config -> config.clearIndexData(true)).setup();\n}",
            "methodSignature: org.hibernate.search.integrationtest.jakarta.batch.massindexing.MassIndexingJobWithCompositeIdIT#setup\n methodBody: void setup() {\nemf=ormSetupHelper.start().withAnnotatedTypes(EntityWithIdClass.class,EntityWithEmbeddedId.class).withProperty(HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED,false).dataClearing(config -> config.clearDatabaseData(false).clearIndexData(true)).setup();\nwith(emf).runInTransaction(entityManager -> {\n  for (LocalDate d=START; d.isBefore(END); d=d.plusDays(1)) {\n    entityManager.persist(new EntityWithIdClass(d));\n    entityManager.persist(new EntityWithEmbeddedId(d));\n  }\n}\n);\nassertThat(JobTestUtil.nbDocumentsInIndex(emf,EntityWithIdClass.class)).isEqualTo(0);\nassertThat(JobTestUtil.nbDocumentsInIndex(emf,EntityWithEmbeddedId.class)).isEqualTo(0);\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.search.loading.SearchQueryEntityLoadingGraphIT#setup\n methodBody: void setup(SingleTypeLoadingModel<T> model, SingleTypeLoadingMapping mapping) {\nthis.model=model;\nthis.mapping=mapping;\nbackendMock.expectAnySchema(model.getIndexName());\nsessionFactory=ormSetupHelper.start().withConfiguration(c -> mapping.configure(c,model)).dataClearing(true,config -> config.preClear(model.getIndexedClass(),model::clearContainedEager).clearOrder(model.getContainedClass(),model.getIndexedClass())).setup();\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.MappingSetupHelper#start\n methodBody: public C start(SV setupVariant) {\nC setupContext=createSetupContext(setupVariant);\nreturn backendSetupStrategy.start(setupContext,configurationProvider,setupContext.backendMappingHandlePromise);\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.ActualBackendSetupStrategy#start\n methodBody: public <C extends MappingSetupHelper<C, ?, ?, ?, ?>.AbstractSetupContext> C start(C setupContext,\n\t\t\tTestConfigurationProvider configurationProvider,\n\t\t\t// The mapping handle is not used by actual backends (only by BackendMock).\n\t\t\tCompletionStage<BackendMappingHandle> mappingHandlePromise) {\nif(defaultBackendConfiguration != null){setupContext=defaultBackendConfiguration.setup(setupContext,null,configurationProvider);\n}for(Map.Entry<String,BackendConfiguration> entry: namedBackendConfigurations.entrySet()){String name=entry.getKey();\nBackendConfiguration configuration=entry.getValue();\nsetupContext=configuration.setup(setupContext,name,configurationProvider);\n}return setupContext;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingOverReindexingIT#setup\n methodBody: void setup() {\nbackendMock.expectSchema(Level1Entity.INDEX,b -> b.field(\"property1FromBridge\",String.class));\nbackendMock.expectSchema(Level2Entity.INDEX,b -> b.objectField(\"level3\",b3 -> b3.field(\"property2\",String.class)));\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(Level1Entity.class,Level2Entity.class,Level3Entity.class).setup();\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.session.SearchIndexingPlanBaseIT#setup\n methodBody: void setup() {\ndefaultBackendMock.expectAnySchema(IndexedEntity1.INDEX_NAME);\nbackend2Mock.expectAnySchema(IndexedEntity2.INDEX_NAME);\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntity1.class,IndexedEntity2.class,ContainedEntity.class).setup();\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper.SetupContext#withProperty\n methodBody: public SetupContext withProperty(String key, Object value) {\noverriddenProperties.put(key,value);\nreturn thisAsC();\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingIdClassIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(IdClassEntity.INDEX);\nsessionFactory=ormSetupHelper.start().withPropertyRadical(HibernateOrmMapperSettings.Radicals.INDEXING_LISTENERS_ENABLED,false).withAnnotatedTypes(IdClassEntity.class).setup();\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelperCleaner.DataClearConfigImpl#clearIndexData\n methodBody: public DataClearConfig clearIndexData(boolean clear) {\nthis.clearIndexData=clear;\nreturn this;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.hibernateormapis.ToJpaQueryIT#setup\n methodBody: void setup() {\nbackendMock.expectAnySchema(IndexedEntity.NAME);\nsessionFactory=ormSetupHelper.start().withProperty(AvailableSettings.JPA_QUERY_COMPLIANCE,true).withAnnotatedTypes(IndexedEntity.class,ContainedEntity.class).dataClearing(config -> config.preClear(ContainedEntity.class,c -> c.setContainingLazy(null)).clearOrder(IndexedEntity.class,ContainedEntity.class)).setup();\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.MappingSetupHelper.AbstractSetupContext#withProperty\n methodBody: public abstract C withProperty(String keyRadical, Object value);",
            "methodSignature: org.hibernate.search.integrationtest.jakarta.batch.massindexing.MassIndexingJobWithMultiTenancyIT#setup\n methodBody: public void setup() {\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(Company.class).withProperty(HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED,false).withBackendProperty(\"multi_tenancy.strategy\",\"discriminator\").tenants(TARGET_TENANT_ID,UNUSED_TENANT_ID).setup();\nwith(sessionFactory,TARGET_TENANT_ID).runInTransaction(session -> companies.forEach(session::persist));\nwith(sessionFactory,TARGET_TENANT_ID).runNoTransaction(session -> Search.session(session).workspace(Company.class).purge());\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.model.BytecodeEnhancementIT#setup\n methodBody: public void setup() {\nbackendMock.expectSchema(IndexedEntity.INDEX,b -> b.field(\"mappedSuperClassText\",String.class).field(\"entitySuperClassText\",String.class).field(\"id\",Integer.class).objectField(\"containedEntityList\",b2 -> b2.multiValued(true).field(\"text\",String.class)).objectField(\"containedEmbeddable\",b2 -> b2.field(\"text\",String.class)).field(\"text1\",String.class).field(\"text2\",String.class).field(\"primitiveInteger\",Integer.class).field(\"primitiveLong\",Long.class).field(\"primitiveBoolean\",Boolean.class).field(\"primitiveFloat\",Float.class).field(\"primitiveDouble\",Double.class).field(\"transientText\",String.class));\nOrmSetupHelper.SetupContext setupContext=ormSetupHelper.start().withTcclLookupPrecedenceBefore().withAnnotatedTypes(IndexedMappedSuperClass.class,IndexedEntitySuperClass.class,IndexedEntity.class,ContainedEntity.class,ContainedEmbeddable.class);\nsessionFactory=setupContext.setup();\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.common.extension.MappingSetupHelper.AbstractSetupContext#with\n methodBody: public final C with(UnaryOperator<C> config) {\nreturn config.apply(thisAsC());\n}",
            "methodSignature: org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelperCleaner.DataClearConfigImpl#clearDatabaseData\n methodBody: public DataClearConfig clearDatabaseData(boolean clear) {\nthis.clearDatabaseData=clear;\nreturn this;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.association.AutomaticIndexingGenericPolymorphicAssociationIT#setup\n methodBody: void setup() {\nbackendMock.expectSchema(IndexedEntity.INDEX,b -> b.objectField(\"child\",b3 -> b3.objectField(\"containedSingle\",b2 -> b2.field(\"includedInSingle\",String.class))));\nsessionFactory=ormSetupHelper.start().withAnnotatedTypes(IndexedEntity.class,ContainingEntity.class,MiddleContainingEntity.class,UnrelatedContainingEntity.class,ContainedEntity.class).dataClearing(config -> config.clearOrder(IndexedEntity.class,ContainingEntity.class,MiddleContainingEntity.class,UnrelatedContainingEntity.class,ContainedEntity.class).preClear(session -> session.createQuery(\"select e from indexed e \",IndexedEntity.class).getResultList().forEach(e -> {\n  e.getChild().setParent(null);\n  e.setChild(null);\n}\n))).setup();\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.smoke.ProgrammaticMappingSmokeIT#setup\n methodBody: void setup() {\nbackendMock.expectSchema(OtherIndexedEntity.NAME,b -> b.field(\"numeric\",Integer.class).field(\"numericAsString\",String.class));\nbackendMock.expectSchema(YetAnotherIndexedEntity.NAME,b -> b.objectField(\"customBridgeOnProperty\",b2 -> b2.field(\"date\",LocalDate.class).field(\"text\",String.class)).field(\"myLocalDateField\",LocalDate.class).field(\"numeric\",Integer.class).objectField(\"myEmbeddedList\",b2 -> b2.multiValued(true).objectField(\"myEmbedded\",b3 -> b3.objectField(\"customBridgeOnClass\",b4 -> b4.field(\"text\",String.class)))).field(\"embeddedMapKeys\",String.class,b2 -> b2.multiValued(true)).objectField(\"embeddedMap\",b2 -> b2.multiValued(true).objectField(\"myEmbedded\",b3 -> b3.field(\"myLocalDateField\",LocalDate.class))));\nbackendMock.expectSchema(IndexedEntity.NAME,b -> b.objectField(\"customBridgeOnClass\",b2 -> b2.field(\"date\",LocalDate.class).field(\"text\",String.class)).objectField(\"customBridgeOnProperty\",b2 -> b2.field(\"date\",LocalDate.class).field(\"text\",String.class)).objectField(\"myEmbedded\",b2 -> b2.objectField(\"customBridgeOnClass\",b3 -> b3.field(\"date\",LocalDate.class).field(\"text\",String.class)).objectField(\"customBridgeOnProperty\",b3 -> b3.field(\"date\",LocalDate.class).field(\"text\",String.class)).objectField(\"myEmbedded\",b3 -> b3.objectField(\"customBridgeOnClass\",b4 -> b4.field(\"text\",String.class))).field(\"myLocalDateField\",LocalDate.class).field(\"myTextField\",String.class)).field(\"myTextField\",String.class).field(\"myLocalDateField\",LocalDate.class));\nsessionFactory=ormSetupHelper.start().withProperty(HibernateOrmMapperSettings.MAPPING_CONFIGURER,new MyMappingConfigurer()).withAnnotatedTypes(IndexedEntity.class,ParentIndexedEntity.class,OtherIndexedEntity.class,YetAnotherIndexedEntity.class).dataClearing(config -> config.clearDatabaseData(false)).setup();\n}"
        ],
        "sourceCodeAfterRefactoring": "@BeforeAll\n\tpublic void init() {\n\t\temf = ormSetupHelper.start().withAnnotatedTypes( Company.class, Person.class, CompanyGroup.class )\n\t\t\t\t.withProperty( HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED, false )\n\t\t\t\t.dataClearingIndexOnly()\n\t\t\t\t.setup();\n\n\t\twith( emf ).runInTransaction( session -> {\n\t\t\tfor ( int i = 1; i <= COMP_ROWS; i++ ) {\n\t\t\t\tsession.persist( new Company( \"C\" + i ) );\n\t\t\t}\n\t\t\tfor ( int i = 1; i <= PERS_ROWS; i++ ) {\n\t\t\t\tsession.persist( new Person( \"P\" + i, \"\", \"\" ) );\n\t\t\t}\n\t\t} );\n\n\t\tfinal String maxThreads = String.valueOf( 1 );\n\t\tfinal String rowsPerPartition = String.valueOf( 3 );\n\n\t\tmockedJobContext = mock( JobContext.class );\n\t\tpartitionMapper = new HibernateSearchPartitionMapper(\n\t\t\t\tnull, null,\n\t\t\t\tmaxThreads,\n\t\t\t\tnull,\n\t\t\t\trowsPerPartition,\n\t\t\t\tnull,\n\t\t\t\tnull,\n\t\t\t\tmockedJobContext\n\t\t);\n\t}\n",
        "diffSourceCode": "    57: \t@BeforeAll\n    58: \tpublic void init() {\n    59: \t\temf = ormSetupHelper.start().withAnnotatedTypes( Company.class, Person.class, CompanyGroup.class )\n    60: \t\t\t\t.withProperty( HibernateOrmMapperSettings.INDEXING_LISTENERS_ENABLED, false )\n-   61: \t\t\t\t.dataClearing( config -> config.clearDatabaseData( false ).clearIndexData( true ) )\n+   61: \t\t\t\t.dataClearingIndexOnly()\n    62: \t\t\t\t.setup();\n    63: \n    64: \t\twith( emf ).runInTransaction( session -> {\n    65: \t\t\tfor ( int i = 1; i <= COMP_ROWS; i++ ) {\n    66: \t\t\t\tsession.persist( new Company( \"C\" + i ) );\n    67: \t\t\t}\n    68: \t\t\tfor ( int i = 1; i <= PERS_ROWS; i++ ) {\n    69: \t\t\t\tsession.persist( new Person( \"P\" + i, \"\", \"\" ) );\n    70: \t\t\t}\n    71: \t\t} );\n    72: \n    73: \t\tfinal String maxThreads = String.valueOf( 1 );\n    74: \t\tfinal String rowsPerPartition = String.valueOf( 3 );\n    75: \n    76: \t\tmockedJobContext = mock( JobContext.class );\n    77: \t\tpartitionMapper = new HibernateSearchPartitionMapper(\n    78: \t\t\t\tnull, null,\n    79: \t\t\t\tmaxThreads,\n    80: \t\t\t\tnull,\n    81: \t\t\t\trowsPerPartition,\n    82: \t\t\t\tnull,\n    83: \t\t\t\tnull,\n    84: \t\t\t\tmockedJobContext\n    85: \t\t);\n    86: \t}\n",
        "uniqueId": "da64c4e549b8dfd9c17e7753247d8028bc022529_57_86_264_266_57_86",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tpublic getDistanceSortIndex(absoluteFieldPath String, location GeoPoint) : Integer extracted from package getDistanceSortIndex(absoluteFieldPath String, location GeoPoint) : Integer in class org.hibernate.search.backend.elasticsearch.search.projection.impl.SearchProjectionExtractContext & moved to class org.hibernate.search.backend.elasticsearch.search.query.impl.ElasticsearchSearchQueryRequestContext",
        "diffLocations": [
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/SearchProjectionExtractContext.java",
                "startLine": 23,
                "endLine": 29,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/SearchProjectionExtractContext.java",
                "startLine": 19,
                "endLine": 21,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/SearchProjectionExtractContext.java",
                "startLine": 46,
                "endLine": 53,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "Integer getDistanceSortIndex(String absoluteFieldPath, GeoPoint location) {\n\t\tif ( distanceSorts == null ) {\n\t\t\treturn null;\n\t\t}\n\n\t\treturn distanceSorts.get( new DistanceSortKey( absoluteFieldPath, location ) );\n\t}",
        "filePathBefore": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/SearchProjectionExtractContext.java",
        "isPureRefactoring": true,
        "commitId": "05c12b8fd2eb002ad9246efd04d8fa27c9379a4b",
        "packageNameBefore": "org.hibernate.search.backend.elasticsearch.search.projection.impl",
        "classNameBefore": "org.hibernate.search.backend.elasticsearch.search.projection.impl.SearchProjectionExtractContext",
        "methodNameBefore": "org.hibernate.search.backend.elasticsearch.search.projection.impl.SearchProjectionExtractContext#getDistanceSortIndex",
        "classSignatureBefore": "public class SearchProjectionExtractContext ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.search.projection.impl.SearchProjectionExtractContext#getDistanceSortIndex"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.search.projection.impl.SearchProjectionExtractContext"
        ],
        "classSignatureBeforeSet": [
            "public class SearchProjectionExtractContext "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.search.projection.impl;\n\nimport java.util.Collections;\nimport java.util.Map;\nimport java.util.Objects;\n\nimport org.hibernate.search.engine.spatial.GeoPoint;\n\npublic class SearchProjectionExtractContext {\n\n\tprivate final Map<DistanceSortKey, Integer> distanceSorts;\n\n\tpublic SearchProjectionExtractContext(Map<DistanceSortKey, Integer> distanceSorts) {\n\t\tthis.distanceSorts = distanceSorts != null ? Collections.unmodifiableMap( distanceSorts ) : null;\n\t}\n\n\tInteger getDistanceSortIndex(String absoluteFieldPath, GeoPoint location) {\n\t\tif ( distanceSorts == null ) {\n\t\t\treturn null;\n\t\t}\n\n\t\treturn distanceSorts.get( new DistanceSortKey( absoluteFieldPath, location ) );\n\t}\n\n\tpublic static class DistanceSortKey {\n\n\t\tprivate final String absoluteFieldPath;\n\n\t\tprivate final GeoPoint location;\n\n\t\tpublic DistanceSortKey(String absoluteFieldPath, GeoPoint location) {\n\t\t\tthis.absoluteFieldPath = absoluteFieldPath;\n\t\t\tthis.location = location;\n\t\t}\n\n\t\t@Override\n\t\tpublic boolean equals(Object obj) {\n\t\t\tif ( obj == this ) {\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\tif ( !(obj instanceof DistanceSortKey) ) {\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tDistanceSortKey other = (DistanceSortKey) obj;\n\n\t\t\treturn Objects.equals( this.absoluteFieldPath, other.absoluteFieldPath )\n\t\t\t\t\t&& Objects.equals( this.location, other.location );\n\t\t}\n\n\t\t@Override\n\t\tpublic int hashCode() {\n\t\t\treturn Objects.hash( absoluteFieldPath, location );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/SearchProjectionExtractContext.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.search.projection.impl;\n\nimport org.hibernate.search.engine.spatial.GeoPoint;\n\npublic class SearchProjectionExtractContext {\n\n\tprivate final SearchProjectionRequestContext requestContext;\n\n\tpublic SearchProjectionExtractContext(SearchProjectionRequestContext requestContext) {\n\t\tthis.requestContext = requestContext;\n\t}\n\n\tInteger getDistanceSortIndex(String absoluteFieldPath, GeoPoint location) {\n\t\treturn requestContext.getDistanceSortIndex( absoluteFieldPath, location );\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            ""
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "Integer getDistanceSortIndex(String absoluteFieldPath, GeoPoint location) {\n\t\treturn requestContext.getDistanceSortIndex( absoluteFieldPath, location );\n\t}\n",
        "diffSourceCode": "-   19: \tpublic SearchProjectionExtractContext(Map<DistanceSortKey, Integer> distanceSorts) {\n-   20: \t\tthis.distanceSorts = distanceSorts != null ? Collections.unmodifiableMap( distanceSorts ) : null;\n+   19: \tInteger getDistanceSortIndex(String absoluteFieldPath, GeoPoint location) {\n+   20: \t\treturn requestContext.getDistanceSortIndex( absoluteFieldPath, location );\n    21: \t}\n-   23: \tInteger getDistanceSortIndex(String absoluteFieldPath, GeoPoint location) {\n-   24: \t\tif ( distanceSorts == null ) {\n-   25: \t\t\treturn null;\n-   26: \t\t}\n-   27: \n-   28: \t\treturn distanceSorts.get( new DistanceSortKey( absoluteFieldPath, location ) );\n-   29: \t}\n-   46: \t\t\t}\n-   47: \t\t\tif ( !(obj instanceof DistanceSortKey) ) {\n-   48: \t\t\t\treturn false;\n-   49: \t\t\t}\n-   50: \n-   51: \t\t\tDistanceSortKey other = (DistanceSortKey) obj;\n-   52: \n-   53: \t\t\treturn Objects.equals( this.absoluteFieldPath, other.absoluteFieldPath )\n+   23: }\n",
        "uniqueId": "05c12b8fd2eb002ad9246efd04d8fa27c9379a4b_23_29_46_53_19_21",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 2,
                "covered": 13
            },
            "BRANCH": {
                "missed": 1,
                "covered": 1
            },
            "LINE": {
                "missed": 1,
                "covered": 2
            },
            "COMPLEXITY": {
                "missed": 1,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Move And Inline Method",
        "description": "Move And Inline Method\tpublic toSearchProjectionExecutionContext() : SearchProjectionExtractContext moved from class org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector to class org.hibernate.search.backend.elasticsearch.search.query.impl.ElasticsearchSearchQueryBuilder & inlined to public build() : ElasticsearchSearchQuery<H>",
        "diffLocations": [
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/search/query/impl/ElasticsearchSearchQueryBuilder.java",
                "startLine": 78,
                "endLine": 111,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/search/query/impl/ElasticsearchSearchQueryBuilder.java",
                "startLine": 118,
                "endLine": 152,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/search/query/impl/ElasticsearchSearchQueryBuilder.java",
                "startLine": 76,
                "endLine": 78,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "}\n\n\t@Override",
        "filePathBefore": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/search/query/impl/ElasticsearchSearchQueryBuilder.java",
        "isPureRefactoring": true,
        "commitId": "0c6c0d87a14968fd7a417b4943835c7dfadeed0f",
        "packageNameBefore": "org.hibernate.search.backend.elasticsearch.search.impl",
        "classNameBefore": "org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector",
        "methodNameBefore": "org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector#toSearchProjectionExecutionContext",
        "classSignatureBefore": "public class ElasticsearchSearchQueryElementCollector\n\t\timplements ElasticsearchSearchPredicateCollector, ElasticsearchSearchSortCollector ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector#toSearchProjectionExecutionContext"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector"
        ],
        "classSignatureBeforeSet": [
            "public class ElasticsearchSearchQueryElementCollector\n\t\timplements ElasticsearchSearchPredicateCollector, ElasticsearchSearchSortCollector "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.search.query.impl;\n\nimport java.util.HashSet;\nimport java.util.Set;\n\nimport org.hibernate.search.backend.elasticsearch.multitenancy.impl.MultiTenancyStrategy;\nimport org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchWorkOrchestrator;\nimport org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchContext;\nimport org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector;\nimport org.hibernate.search.backend.elasticsearch.search.projection.impl.ElasticsearchSearchProjection;\nimport org.hibernate.search.backend.elasticsearch.search.projection.impl.SearchProjectionExtractContext;\nimport org.hibernate.search.backend.elasticsearch.search.query.ElasticsearchSearchQuery;\nimport org.hibernate.search.backend.elasticsearch.work.builder.factory.impl.ElasticsearchWorkBuilderFactory;\nimport org.hibernate.search.backend.elasticsearch.work.impl.ElasticsearchSearchResultExtractor;\nimport org.hibernate.search.engine.mapper.session.context.spi.SessionContextImplementor;\nimport org.hibernate.search.engine.search.loading.context.spi.LoadingContext;\nimport org.hibernate.search.engine.search.loading.context.spi.LoadingContextBuilder;\nimport org.hibernate.search.engine.search.query.spi.SearchQueryBuilder;\n\nimport com.google.gson.JsonArray;\nimport com.google.gson.JsonObject;\n\npublic class ElasticsearchSearchQueryBuilder<H>\n\t\timplements SearchQueryBuilder<H, ElasticsearchSearchQueryElementCollector> {\n\n\tprivate final ElasticsearchWorkBuilderFactory workFactory;\n\tprivate final ElasticsearchSearchResultExtractorFactory searchResultExtractorFactory;\n\tprivate final ElasticsearchWorkOrchestrator queryOrchestrator;\n\tprivate final MultiTenancyStrategy multiTenancyStrategy;\n\n\tprivate final ElasticsearchSearchContext searchContext;\n\tprivate final SessionContextImplementor sessionContext;\n\tprivate final Set<String> routingKeys;\n\n\tprivate final ElasticsearchSearchQueryElementCollector elementCollector;\n\tprivate final LoadingContextBuilder<?, ?> loadingContextBuilder;\n\tprivate final ElasticsearchSearchProjection<?, H> rootProjection;\n\n\tpublic ElasticsearchSearchQueryBuilder(\n\t\t\tElasticsearchWorkBuilderFactory workFactory,\n\t\t\tElasticsearchSearchResultExtractorFactory searchResultExtractorFactory,\n\t\t\tElasticsearchWorkOrchestrator queryOrchestrator,\n\t\t\tMultiTenancyStrategy multiTenancyStrategy,\n\t\t\tElasticsearchSearchContext searchContext,\n\t\t\tSessionContextImplementor sessionContext,\n\t\t\tLoadingContextBuilder<?, ?> loadingContextBuilder,\n\t\t\tElasticsearchSearchProjection<?, H> rootProjection) {\n\t\tthis.workFactory = workFactory;\n\t\tthis.searchResultExtractorFactory = searchResultExtractorFactory;\n\t\tthis.queryOrchestrator = queryOrchestrator;\n\t\tthis.multiTenancyStrategy = multiTenancyStrategy;\n\n\t\tthis.searchContext = searchContext;\n\t\tthis.sessionContext = sessionContext;\n\t\tthis.routingKeys = new HashSet<>();\n\n\t\tthis.elementCollector = new ElasticsearchSearchQueryElementCollector( sessionContext );\n\t\tthis.loadingContextBuilder = loadingContextBuilder;\n\t\tthis.rootProjection = rootProjection;\n\t}\n\n\t@Override\n\tpublic ElasticsearchSearchQueryElementCollector getQueryElementCollector() {\n\t\treturn elementCollector;\n\t}\n\n\t@Override\n\tpublic void addRoutingKey(String routingKey) {\n\t\tthis.routingKeys.add( routingKey );\n\t}\n\n\t@Override\n\tpublic ElasticsearchSearchQuery<H> build() {\n\t\tJsonObject payload = new JsonObject();\n\n\t\tJsonObject jsonQuery = getJsonQuery();\n\t\tif ( jsonQuery != null ) {\n\t\t\tpayload.add( \"query\", jsonQuery );\n\t\t}\n\n\t\tJsonArray jsonSort = elementCollector.toJsonSort();\n\t\tif ( jsonSort != null ) {\n\t\t\tpayload.add( \"sort\", jsonSort );\n\t\t}\n\n\t\tSearchProjectionExtractContext searchProjectionExecutionContext = elementCollector\n\t\t\t\t.toSearchProjectionExecutionContext();\n\n\t\trootProjection.contributeRequest( payload, searchProjectionExecutionContext );\n\n\t\tLoadingContext<?, ?> loadingContext = loadingContextBuilder.build();\n\n\t\tElasticsearchSearchResultExtractor<H> searchResultExtractor =\n\t\t\t\tsearchResultExtractorFactory.createResultExtractor(\n\t\t\t\t\t\tloadingContext,\n\t\t\t\t\t\trootProjection, searchProjectionExecutionContext\n\t\t\t\t);\n\n\t\treturn new ElasticsearchSearchQueryImpl<>(\n\t\t\t\tworkFactory, queryOrchestrator,\n\t\t\t\tsearchContext, sessionContext, loadingContext, routingKeys,\n\t\t\t\tpayload,\n\t\t\t\tsearchResultExtractor\n\t\t);\n\t}\n\n\tprivate JsonObject getJsonQuery() {\n\t\treturn multiTenancyStrategy.decorateJsonQuery( elementCollector.toJsonPredicate(), sessionContext.getTenantIdentifier() );\n\t}\n}\n",
        "filePathAfter": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/search/query/impl/ElasticsearchSearchQueryBuilder.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.search.query.impl;\n\nimport java.util.HashSet;\nimport java.util.Map;\nimport java.util.Set;\n\nimport org.hibernate.search.backend.elasticsearch.multitenancy.impl.MultiTenancyStrategy;\nimport org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchWorkOrchestrator;\nimport org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchContext;\nimport org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector;\nimport org.hibernate.search.backend.elasticsearch.search.predicate.impl.ElasticsearchSearchPredicateContext;\nimport org.hibernate.search.backend.elasticsearch.search.projection.impl.ElasticsearchSearchProjection;\nimport org.hibernate.search.backend.elasticsearch.search.projection.impl.SearchProjectionExtractContext;\nimport org.hibernate.search.backend.elasticsearch.search.query.ElasticsearchSearchQuery;\nimport org.hibernate.search.backend.elasticsearch.work.builder.factory.impl.ElasticsearchWorkBuilderFactory;\nimport org.hibernate.search.backend.elasticsearch.work.impl.ElasticsearchSearchResultExtractor;\nimport org.hibernate.search.engine.mapper.session.context.spi.SessionContextImplementor;\nimport org.hibernate.search.engine.search.loading.context.spi.LoadingContext;\nimport org.hibernate.search.engine.search.loading.context.spi.LoadingContextBuilder;\nimport org.hibernate.search.engine.search.query.spi.SearchQueryBuilder;\nimport org.hibernate.search.engine.spatial.GeoPoint;\nimport org.hibernate.search.util.common.impl.CollectionHelper;\n\nimport com.google.gson.JsonArray;\nimport com.google.gson.JsonElement;\nimport com.google.gson.JsonObject;\n\npublic class ElasticsearchSearchQueryBuilder<H>\n\t\timplements SearchQueryBuilder<H, ElasticsearchSearchQueryElementCollector>,\n\t\t\t\tElasticsearchSearchQueryElementCollector {\n\n\tprivate final ElasticsearchWorkBuilderFactory workFactory;\n\tprivate final ElasticsearchSearchResultExtractorFactory searchResultExtractorFactory;\n\tprivate final ElasticsearchWorkOrchestrator queryOrchestrator;\n\tprivate final MultiTenancyStrategy multiTenancyStrategy;\n\n\tprivate final ElasticsearchSearchContext searchContext;\n\tprivate final SessionContextImplementor sessionContext;\n\n\tprivate final ElasticsearchSearchPredicateContext rootPredicateContext;\n\tprivate final LoadingContextBuilder<?, ?> loadingContextBuilder;\n\tprivate final ElasticsearchSearchProjection<?, H> rootProjection;\n\n\tprivate final Set<String> routingKeys;\n\tprivate JsonObject jsonPredicate;\n\tprivate JsonArray jsonSort;\n\tprivate Map<SearchProjectionExtractContext.DistanceSortKey, Integer> distanceSorts;\n\n\tpublic ElasticsearchSearchQueryBuilder(\n\t\t\tElasticsearchWorkBuilderFactory workFactory,\n\t\t\tElasticsearchSearchResultExtractorFactory searchResultExtractorFactory,\n\t\t\tElasticsearchWorkOrchestrator queryOrchestrator,\n\t\t\tMultiTenancyStrategy multiTenancyStrategy,\n\t\t\tElasticsearchSearchContext searchContext,\n\t\t\tSessionContextImplementor sessionContext,\n\t\t\tLoadingContextBuilder<?, ?> loadingContextBuilder,\n\t\t\tElasticsearchSearchProjection<?, H> rootProjection) {\n\t\tthis.workFactory = workFactory;\n\t\tthis.searchResultExtractorFactory = searchResultExtractorFactory;\n\t\tthis.queryOrchestrator = queryOrchestrator;\n\t\tthis.multiTenancyStrategy = multiTenancyStrategy;\n\n\t\tthis.searchContext = searchContext;\n\t\tthis.sessionContext = sessionContext;\n\t\tthis.routingKeys = new HashSet<>();\n\n\t\tthis.rootPredicateContext = new ElasticsearchSearchPredicateContext( sessionContext );\n\t\tthis.loadingContextBuilder = loadingContextBuilder;\n\t\tthis.rootProjection = rootProjection;\n\t}\n\n\t@Override\n\tpublic ElasticsearchSearchQueryElementCollector toQueryElementCollector() {\n\t\treturn this;\n\t}\n\n\t@Override\n\tpublic void addRoutingKey(String routingKey) {\n\t\tthis.routingKeys.add( routingKey );\n\t}\n\n\t@Override\n\tpublic ElasticsearchSearchPredicateContext getRootPredicateContext() {\n\t\treturn rootPredicateContext;\n\t}\n\n\t@Override\n\tpublic void collectPredicate(JsonObject jsonQuery) {\n\t\tthis.jsonPredicate = jsonQuery;\n\t}\n\n\t@Override\n\tpublic void collectSort(JsonElement sort) {\n\t\tif ( jsonSort == null ) {\n\t\t\tjsonSort = new JsonArray();\n\t\t}\n\t\tthis.jsonSort.add( sort );\n\t}\n\n\t@Override\n\tpublic void collectDistanceSort(JsonElement sort, String absoluteFieldPath, GeoPoint center) {\n\t\tcollectSort( sort );\n\n\t\tint index = jsonSort.size() - 1;\n\t\tif ( distanceSorts == null ) {\n\t\t\tdistanceSorts = CollectionHelper.newHashMap( 3 );\n\t\t}\n\n\t\tdistanceSorts.put( new SearchProjectionExtractContext.DistanceSortKey( absoluteFieldPath, center ), index );\n\t}\n\n\t@Override\n\tpublic ElasticsearchSearchQuery<H> build() {\n\t\tJsonObject payload = new JsonObject();\n\n\t\tJsonObject jsonQuery = multiTenancyStrategy.decorateJsonQuery(\n\t\t\t\tjsonPredicate, sessionContext.getTenantIdentifier()\n\t\t);\n\t\tif ( jsonQuery != null ) {\n\t\t\tpayload.add( \"query\", jsonQuery );\n\t\t}\n\n\t\tif ( jsonSort != null ) {\n\t\t\tpayload.add( \"sort\", jsonSort );\n\t\t}\n\n\t\tSearchProjectionExtractContext searchProjectionExecutionContext =\n\t\t\t\tnew SearchProjectionExtractContext( distanceSorts );\n\n\t\trootProjection.contributeRequest( payload, searchProjectionExecutionContext );\n\n\t\tLoadingContext<?, ?> loadingContext = loadingContextBuilder.build();\n\n\t\tElasticsearchSearchResultExtractor<H> searchResultExtractor =\n\t\t\t\tsearchResultExtractorFactory.createResultExtractor(\n\t\t\t\t\t\tloadingContext,\n\t\t\t\t\t\trootProjection, searchProjectionExecutionContext\n\t\t\t\t);\n\n\t\treturn new ElasticsearchSearchQueryImpl<>(\n\t\t\t\tworkFactory, queryOrchestrator,\n\t\t\t\tsearchContext, sessionContext, loadingContext, routingKeys,\n\t\t\t\tpayload,\n\t\t\t\tsearchResultExtractor\n\t\t);\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "@Override\n\tpublic ElasticsearchSearchQuery<H> build() {\n\t\tJsonObject payload = new JsonObject();\n\n\t\tJsonObject jsonQuery = multiTenancyStrategy.decorateJsonQuery(\n\t\t\t\tjsonPredicate, sessionContext.getTenantIdentifier()\n\t\t);\n\t\tif ( jsonQuery != null ) {\n\t\t\tpayload.add( \"query\", jsonQuery );\n\t\t}\n\n\t\tif ( jsonSort != null ) {\n\t\t\tpayload.add( \"sort\", jsonSort );\n\t\t}\n\n\t\tSearchProjectionExtractContext searchProjectionExecutionContext =\n\t\t\t\tnew SearchProjectionExtractContext( distanceSorts );\n\n\t\trootProjection.contributeRequest( payload, searchProjectionExecutionContext );\n\n\t\tLoadingContext<?, ?> loadingContext = loadingContextBuilder.build();\n\n\t\tElasticsearchSearchResultExtractor<H> searchResultExtractor =\n\t\t\t\tsearchResultExtractorFactory.createResultExtractor(\n\t\t\t\t\t\tloadingContext,\n\t\t\t\t\t\trootProjection, searchProjectionExecutionContext\n\t\t\t\t);\n\n\t\treturn new ElasticsearchSearchQueryImpl<>(\n\t\t\t\tworkFactory, queryOrchestrator,\n\t\t\t\tsearchContext, sessionContext, loadingContext, routingKeys,\n\t\t\t\tpayload,\n\t\t\t\tsearchResultExtractor\n\t\t);\n\t}",
        "diffSourceCode": "    76: \t}\n    77: \n    78: \t@Override\n-   79: \tpublic ElasticsearchSearchQuery<H> build() {\n-   80: \t\tJsonObject payload = new JsonObject();\n-   81: \n-   82: \t\tJsonObject jsonQuery = getJsonQuery();\n-   83: \t\tif ( jsonQuery != null ) {\n-   84: \t\t\tpayload.add( \"query\", jsonQuery );\n-   85: \t\t}\n-   86: \n-   87: \t\tJsonArray jsonSort = elementCollector.toJsonSort();\n-   88: \t\tif ( jsonSort != null ) {\n-   89: \t\t\tpayload.add( \"sort\", jsonSort );\n-   90: \t\t}\n-   91: \n-   92: \t\tSearchProjectionExtractContext searchProjectionExecutionContext = elementCollector\n-   93: \t\t\t\t.toSearchProjectionExecutionContext();\n-   94: \n-   95: \t\trootProjection.contributeRequest( payload, searchProjectionExecutionContext );\n-   96: \n-   97: \t\tLoadingContext<?, ?> loadingContext = loadingContextBuilder.build();\n-   98: \n-   99: \t\tElasticsearchSearchResultExtractor<H> searchResultExtractor =\n-  100: \t\t\t\tsearchResultExtractorFactory.createResultExtractor(\n-  101: \t\t\t\t\t\tloadingContext,\n-  102: \t\t\t\t\t\trootProjection, searchProjectionExecutionContext\n-  103: \t\t\t\t);\n-  104: \n-  105: \t\treturn new ElasticsearchSearchQueryImpl<>(\n-  106: \t\t\t\tworkFactory, queryOrchestrator,\n-  107: \t\t\t\tsearchContext, sessionContext, loadingContext, routingKeys,\n-  108: \t\t\t\tpayload,\n-  109: \t\t\t\tsearchResultExtractor\n-  110: \t\t);\n-  111: \t}\n+   79: \tpublic ElasticsearchSearchQueryElementCollector toQueryElementCollector() {\n+   80: \t\treturn this;\n+   81: \t}\n+   82: \n+   83: \t@Override\n+   84: \tpublic void addRoutingKey(String routingKey) {\n+   85: \t\tthis.routingKeys.add( routingKey );\n+   86: \t}\n+   87: \n+   88: \t@Override\n+   89: \tpublic ElasticsearchSearchPredicateContext getRootPredicateContext() {\n+   90: \t\treturn rootPredicateContext;\n+   91: \t}\n+   92: \n+   93: \t@Override\n+   94: \tpublic void collectPredicate(JsonObject jsonQuery) {\n+   95: \t\tthis.jsonPredicate = jsonQuery;\n+   96: \t}\n+   97: \n+   98: \t@Override\n+   99: \tpublic void collectSort(JsonElement sort) {\n+  100: \t\tif ( jsonSort == null ) {\n+  101: \t\t\tjsonSort = new JsonArray();\n+  102: \t\t}\n+  103: \t\tthis.jsonSort.add( sort );\n+  104: \t}\n+  105: \n+  106: \t@Override\n+  107: \tpublic void collectDistanceSort(JsonElement sort, String absoluteFieldPath, GeoPoint center) {\n+  108: \t\tcollectSort( sort );\n+  109: \n+  110: \t\tint index = jsonSort.size() - 1;\n+  111: \t\tif ( distanceSorts == null ) {\n+  118: \t@Override\n+  119: \tpublic ElasticsearchSearchQuery<H> build() {\n+  120: \t\tJsonObject payload = new JsonObject();\n+  121: \n+  122: \t\tJsonObject jsonQuery = multiTenancyStrategy.decorateJsonQuery(\n+  123: \t\t\t\tjsonPredicate, sessionContext.getTenantIdentifier()\n+  124: \t\t);\n+  125: \t\tif ( jsonQuery != null ) {\n+  126: \t\t\tpayload.add( \"query\", jsonQuery );\n+  127: \t\t}\n+  128: \n+  129: \t\tif ( jsonSort != null ) {\n+  130: \t\t\tpayload.add( \"sort\", jsonSort );\n+  131: \t\t}\n+  132: \n+  133: \t\tSearchProjectionExtractContext searchProjectionExecutionContext =\n+  134: \t\t\t\tnew SearchProjectionExtractContext( distanceSorts );\n+  135: \n+  136: \t\trootProjection.contributeRequest( payload, searchProjectionExecutionContext );\n+  137: \n+  138: \t\tLoadingContext<?, ?> loadingContext = loadingContextBuilder.build();\n+  139: \n+  140: \t\tElasticsearchSearchResultExtractor<H> searchResultExtractor =\n+  141: \t\t\t\tsearchResultExtractorFactory.createResultExtractor(\n+  142: \t\t\t\t\t\tloadingContext,\n+  143: \t\t\t\t\t\trootProjection, searchProjectionExecutionContext\n+  144: \t\t\t\t);\n+  145: \n+  146: \t\treturn new ElasticsearchSearchQueryImpl<>(\n+  147: \t\t\t\tworkFactory, queryOrchestrator,\n+  148: \t\t\t\tsearchContext, sessionContext, loadingContext, routingKeys,\n+  149: \t\t\t\tpayload,\n+  150: \t\t\t\tsearchResultExtractor\n+  151: \t\t);\n+  152: \t}\n",
        "uniqueId": "0c6c0d87a14968fd7a417b4943835c7dfadeed0f_78_111__118_152_76_78",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 6
            },
            "LINE": {
                "missed": 0,
                "covered": 1
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate createExtractContext(distanceSorts Map<DistanceSortKey,Integer>) : SearchProjectionExtractContext extracted from public projection_sort() : void in class org.hibernate.search.backend.elasticsearch.search.projection.impl.DistanceToFieldSearchProjectionTest",
        "diffLocations": [
            {
                "filePath": "backend/elasticsearch/src/test/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/DistanceToFieldSearchProjectionTest.java",
                "startLine": 51,
                "endLine": 72,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/test/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/DistanceToFieldSearchProjectionTest.java",
                "startLine": 50,
                "endLine": 68,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/test/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/DistanceToFieldSearchProjectionTest.java",
                "startLine": 70,
                "endLine": 72,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void projection_sort() {\n\t\tSessionContextImplementor sessionContext = createMock( SessionContextImplementor.class );\n\t\tElasticsearchSearchQueryElementCollector elementCollector = new ElasticsearchSearchQueryElementCollector( sessionContext );\n\t\telementCollector.collectSort( new JsonObject() );\n\t\telementCollector.collectDistanceSort( new JsonObject(), FIELD, LOCATION );\n\n\t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n\t\t\t\tLOCATION, DistanceUnit.METERS );\n\n\t\tJsonObject requestBody = new JsonObject();\n\n\t\tresetAll();\n\t\treplayAll();\n\t\tSearchProjectionExtractContext searchProjectionExecutionContext =\n\t\t\t\telementCollector.toSearchProjectionExecutionContext();\n\t\tprojection.contributeRequest( requestBody, searchProjectionExecutionContext );\n\t\tverifyAll();\n\n\t\tassertThat( searchProjectionExecutionContext.getDistanceSortIndex( FIELD, LOCATION ) ).isEqualTo( 1 );\n\t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNull();\n\t}",
        "filePathBefore": "backend/elasticsearch/src/test/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/DistanceToFieldSearchProjectionTest.java",
        "isPureRefactoring": true,
        "commitId": "0c6c0d87a14968fd7a417b4943835c7dfadeed0f",
        "packageNameBefore": "org.hibernate.search.backend.elasticsearch.search.projection.impl",
        "classNameBefore": "org.hibernate.search.backend.elasticsearch.search.projection.impl.DistanceToFieldSearchProjectionTest",
        "methodNameBefore": "org.hibernate.search.backend.elasticsearch.search.projection.impl.DistanceToFieldSearchProjectionTest#projection_sort",
        "invokedMethod": "methodSignature: org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector#collectSort\n methodBody: public void collectSort(JsonElement sort) {\nif(jsonSort == null){jsonSort=new JsonArray();\n}this.jsonSort.add(sort);\n}\nmethodSignature: org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector#collectDistanceSort\n methodBody: public void collectDistanceSort(JsonElement sort, String absoluteFieldPath, GeoPoint center) {\ncollectSort(sort);\nint index=jsonSort.size() - 1;\nif(distanceSorts == null){distanceSorts=CollectionHelper.newHashMap(3);\n}distanceSorts.put(new DistanceSortKey(absoluteFieldPath,center),index);\n}\nmethodSignature: org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector#toSearchProjectionExecutionContext\n methodBody: public SearchProjectionExtractContext toSearchProjectionExecutionContext() {\nreturn new SearchProjectionExtractContext(distanceSorts);\n}",
        "classSignatureBefore": "public class DistanceToFieldSearchProjectionTest extends EasyMockSupport ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.search.projection.impl.DistanceToFieldSearchProjectionTest#projection_sort"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.search.projection.impl.DistanceToFieldSearchProjectionTest"
        ],
        "classSignatureBeforeSet": [
            "public class DistanceToFieldSearchProjectionTest extends EasyMockSupport "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Tolerable changes in the body\nOverlapped refactoring - can be identical by undoing the overlapped refactoring\n- Rename Variable-",
                "description": "Rename Variable on top of the extract method - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.search.projection.impl;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport java.util.Collections;\nimport java.util.Set;\n\nimport org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector;\nimport org.hibernate.search.engine.mapper.session.context.spi.SessionContextImplementor;\nimport org.hibernate.search.engine.spatial.DistanceUnit;\nimport org.hibernate.search.engine.spatial.GeoPoint;\nimport org.junit.Test;\n\nimport com.google.gson.JsonObject;\nimport org.easymock.EasyMockSupport;\n\npublic class DistanceToFieldSearchProjectionTest extends EasyMockSupport {\n\n\tprivate static final Set<String> INDEX_NAMES = Collections.singleton( \"myIndexName\" );\n\tprivate static final String FIELD = \"myField\";\n\n\tprivate static final GeoPoint LOCATION = GeoPoint.of( 43, 4 );\n\n\t@Test\n\tpublic void projection_script() {\n\t\tSessionContextImplementor sessionContext = createMock( SessionContextImplementor.class );\n\t\tElasticsearchSearchQueryElementCollector elementCollector = new ElasticsearchSearchQueryElementCollector( sessionContext );\n\n\t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n\t\t\t\tLOCATION, DistanceUnit.METERS );\n\n\t\tJsonObject requestBody = new JsonObject();\n\n\t\tresetAll();\n\t\treplayAll();\n\t\tSearchProjectionExtractContext searchProjectionExecutionContext =\n\t\t\t\telementCollector.toSearchProjectionExecutionContext();\n\t\tprojection.contributeRequest( requestBody, searchProjectionExecutionContext );\n\t\tverifyAll();\n\n\t\tassertThat( searchProjectionExecutionContext.getDistanceSortIndex( FIELD, LOCATION ) ).isNull();\n\t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNotNull();\n\t}\n\n\t@Test\n\tpublic void projection_sort() {\n\t\tSessionContextImplementor sessionContext = createMock( SessionContextImplementor.class );\n\t\tElasticsearchSearchQueryElementCollector elementCollector = new ElasticsearchSearchQueryElementCollector( sessionContext );\n\t\telementCollector.collectSort( new JsonObject() );\n\t\telementCollector.collectDistanceSort( new JsonObject(), FIELD, LOCATION );\n\n\t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n\t\t\t\tLOCATION, DistanceUnit.METERS );\n\n\t\tJsonObject requestBody = new JsonObject();\n\n\t\tresetAll();\n\t\treplayAll();\n\t\tSearchProjectionExtractContext searchProjectionExecutionContext =\n\t\t\t\telementCollector.toSearchProjectionExecutionContext();\n\t\tprojection.contributeRequest( requestBody, searchProjectionExecutionContext );\n\t\tverifyAll();\n\n\t\tassertThat( searchProjectionExecutionContext.getDistanceSortIndex( FIELD, LOCATION ) ).isEqualTo( 1 );\n\t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNull();\n\t}\n}\n",
        "filePathAfter": "backend/elasticsearch/src/test/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/DistanceToFieldSearchProjectionTest.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.search.projection.impl;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Set;\n\nimport org.hibernate.search.backend.elasticsearch.search.projection.impl.SearchProjectionExtractContext.DistanceSortKey;\nimport org.hibernate.search.engine.spatial.DistanceUnit;\nimport org.hibernate.search.engine.spatial.GeoPoint;\n\nimport org.junit.Test;\n\nimport com.google.gson.JsonObject;\nimport org.easymock.EasyMockSupport;\n\npublic class DistanceToFieldSearchProjectionTest extends EasyMockSupport {\n\n\tprivate static final Set<String> INDEX_NAMES = Collections.singleton( \"myIndexName\" );\n\tprivate static final String FIELD = \"myField\";\n\n\tprivate static final GeoPoint LOCATION = GeoPoint.of( 43, 4 );\n\n\t@Test\n\tpublic void projection_script() {\n\t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n\t\t\t\tLOCATION, DistanceUnit.METERS );\n\n\t\tMap<DistanceSortKey, Integer> distanceSorts = Collections.emptyMap();\n\t\tSearchProjectionExtractContext extractContext = createExtractContext( distanceSorts );\n\t\tassertThat( extractContext.getDistanceSortIndex( FIELD, LOCATION ) ).isNull();\n\n\t\tJsonObject requestBody = new JsonObject();\n\t\tresetAll();\n\t\treplayAll();\n\t\tprojection.contributeRequest( requestBody, extractContext );\n\t\tverifyAll();\n\n\t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNotNull();\n\t}\n\n\t@Test\n\tpublic void projection_sort() {\n\t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n\t\t\t\tLOCATION, DistanceUnit.METERS );\n\n\t\tMap<DistanceSortKey, Integer> distanceSorts = new HashMap<>();\n\t\tdistanceSorts.put( new DistanceSortKey( FIELD, LOCATION ), 1 );\n\t\tSearchProjectionExtractContext extractContext = createExtractContext( distanceSorts );\n\t\tassertThat( extractContext.getDistanceSortIndex( FIELD, LOCATION ) ).isEqualTo( 1 );\n\n\t\tJsonObject requestBody = new JsonObject();\n\n\t\tresetAll();\n\t\treplayAll();\n\t\tprojection.contributeRequest( requestBody, extractContext );\n\t\tverifyAll();\n\n\t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNull();\n\t}\n\n\tprivate SearchProjectionExtractContext createExtractContext(Map<DistanceSortKey, Integer> distanceSorts) {\n\t\treturn new SearchProjectionExtractContext( distanceSorts );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private SearchProjectionExtractContext createExtractContext(Map<DistanceSortKey, Integer> distanceSorts) {\n\t\treturn new SearchProjectionExtractContext( distanceSorts );\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector#collectSort\n methodBody: public void collectSort(JsonElement sort) {\nif(jsonSort == null){jsonSort=new JsonArray();\n}this.jsonSort.add(sort);\n}",
            "methodSignature: org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector#collectDistanceSort\n methodBody: public void collectDistanceSort(JsonElement sort, String absoluteFieldPath, GeoPoint center) {\ncollectSort(sort);\nint index=jsonSort.size() - 1;\nif(distanceSorts == null){distanceSorts=CollectionHelper.newHashMap(3);\n}distanceSorts.put(new DistanceSortKey(absoluteFieldPath,center),index);\n}",
            "methodSignature: org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector#toSearchProjectionExecutionContext\n methodBody: public SearchProjectionExtractContext toSearchProjectionExecutionContext() {\nreturn new SearchProjectionExtractContext(distanceSorts);\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void projection_sort() {\n\t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n\t\t\t\tLOCATION, DistanceUnit.METERS );\n\n\t\tMap<DistanceSortKey, Integer> distanceSorts = new HashMap<>();\n\t\tdistanceSorts.put( new DistanceSortKey( FIELD, LOCATION ), 1 );\n\t\tSearchProjectionExtractContext extractContext = createExtractContext( distanceSorts );\n\t\tassertThat( extractContext.getDistanceSortIndex( FIELD, LOCATION ) ).isEqualTo( 1 );\n\n\t\tJsonObject requestBody = new JsonObject();\n\n\t\tresetAll();\n\t\treplayAll();\n\t\tprojection.contributeRequest( requestBody, extractContext );\n\t\tverifyAll();\n\n\t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNull();\n\t}\nprivate SearchProjectionExtractContext createExtractContext(Map<DistanceSortKey, Integer> distanceSorts) {\n\t\treturn new SearchProjectionExtractContext( distanceSorts );\n\t}",
        "diffSourceCode": "-   50: \n-   51: \t@Test\n-   52: \tpublic void projection_sort() {\n-   53: \t\tSessionContextImplementor sessionContext = createMock( SessionContextImplementor.class );\n-   54: \t\tElasticsearchSearchQueryElementCollector elementCollector = new ElasticsearchSearchQueryElementCollector( sessionContext );\n-   55: \t\telementCollector.collectSort( new JsonObject() );\n-   56: \t\telementCollector.collectDistanceSort( new JsonObject(), FIELD, LOCATION );\n-   57: \n-   58: \t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n-   59: \t\t\t\tLOCATION, DistanceUnit.METERS );\n-   60: \n-   61: \t\tJsonObject requestBody = new JsonObject();\n-   62: \n-   63: \t\tresetAll();\n-   64: \t\treplayAll();\n-   65: \t\tSearchProjectionExtractContext searchProjectionExecutionContext =\n-   66: \t\t\t\telementCollector.toSearchProjectionExecutionContext();\n-   67: \t\tprojection.contributeRequest( requestBody, searchProjectionExecutionContext );\n-   68: \t\tverifyAll();\n+   50: \t@Test\n+   51: \tpublic void projection_sort() {\n+   52: \t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n+   53: \t\t\t\tLOCATION, DistanceUnit.METERS );\n+   54: \n+   55: \t\tMap<DistanceSortKey, Integer> distanceSorts = new HashMap<>();\n+   56: \t\tdistanceSorts.put( new DistanceSortKey( FIELD, LOCATION ), 1 );\n+   57: \t\tSearchProjectionExtractContext extractContext = createExtractContext( distanceSorts );\n+   58: \t\tassertThat( extractContext.getDistanceSortIndex( FIELD, LOCATION ) ).isEqualTo( 1 );\n+   59: \n+   60: \t\tJsonObject requestBody = new JsonObject();\n+   61: \n+   62: \t\tresetAll();\n+   63: \t\treplayAll();\n+   64: \t\tprojection.contributeRequest( requestBody, extractContext );\n+   65: \t\tverifyAll();\n+   66: \n+   67: \t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNull();\n+   68: \t}\n    69: \n-   70: \t\tassertThat( searchProjectionExecutionContext.getDistanceSortIndex( FIELD, LOCATION ) ).isEqualTo( 1 );\n-   71: \t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNull();\n+   70: \tprivate SearchProjectionExtractContext createExtractContext(Map<DistanceSortKey, Integer> distanceSorts) {\n+   71: \t\treturn new SearchProjectionExtractContext( distanceSorts );\n    72: \t}\n",
        "uniqueId": "0c6c0d87a14968fd7a417b4943835c7dfadeed0f_51_72_70_72_50_68",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate createExtractContext(distanceSorts Map<DistanceSortKey,Integer>) : SearchProjectionExtractContext extracted from public projection_script() : void in class org.hibernate.search.backend.elasticsearch.search.projection.impl.DistanceToFieldSearchProjectionTest",
        "diffLocations": [
            {
                "filePath": "backend/elasticsearch/src/test/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/DistanceToFieldSearchProjectionTest.java",
                "startLine": 30,
                "endLine": 49,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/test/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/DistanceToFieldSearchProjectionTest.java",
                "startLine": 32,
                "endLine": 48,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/test/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/DistanceToFieldSearchProjectionTest.java",
                "startLine": 70,
                "endLine": 72,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void projection_script() {\n\t\tSessionContextImplementor sessionContext = createMock( SessionContextImplementor.class );\n\t\tElasticsearchSearchQueryElementCollector elementCollector = new ElasticsearchSearchQueryElementCollector( sessionContext );\n\n\t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n\t\t\t\tLOCATION, DistanceUnit.METERS );\n\n\t\tJsonObject requestBody = new JsonObject();\n\n\t\tresetAll();\n\t\treplayAll();\n\t\tSearchProjectionExtractContext searchProjectionExecutionContext =\n\t\t\t\telementCollector.toSearchProjectionExecutionContext();\n\t\tprojection.contributeRequest( requestBody, searchProjectionExecutionContext );\n\t\tverifyAll();\n\n\t\tassertThat( searchProjectionExecutionContext.getDistanceSortIndex( FIELD, LOCATION ) ).isNull();\n\t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNotNull();\n\t}",
        "filePathBefore": "backend/elasticsearch/src/test/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/DistanceToFieldSearchProjectionTest.java",
        "isPureRefactoring": true,
        "commitId": "0c6c0d87a14968fd7a417b4943835c7dfadeed0f",
        "packageNameBefore": "org.hibernate.search.backend.elasticsearch.search.projection.impl",
        "classNameBefore": "org.hibernate.search.backend.elasticsearch.search.projection.impl.DistanceToFieldSearchProjectionTest",
        "methodNameBefore": "org.hibernate.search.backend.elasticsearch.search.projection.impl.DistanceToFieldSearchProjectionTest#projection_script",
        "invokedMethod": "methodSignature: org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector#toSearchProjectionExecutionContext\n methodBody: public SearchProjectionExtractContext toSearchProjectionExecutionContext() {\nreturn new SearchProjectionExtractContext(distanceSorts);\n}",
        "classSignatureBefore": "public class DistanceToFieldSearchProjectionTest extends EasyMockSupport ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.search.projection.impl.DistanceToFieldSearchProjectionTest#projection_script"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.search.projection.impl.DistanceToFieldSearchProjectionTest"
        ],
        "classSignatureBeforeSet": [
            "public class DistanceToFieldSearchProjectionTest extends EasyMockSupport "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Tolerable changes in the body\nOverlapped refactoring - can be identical by undoing the overlapped refactoring\n- Rename Variable-",
                "description": "Rename Variable on top of the extract method - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.search.projection.impl;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport java.util.Collections;\nimport java.util.Set;\n\nimport org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector;\nimport org.hibernate.search.engine.mapper.session.context.spi.SessionContextImplementor;\nimport org.hibernate.search.engine.spatial.DistanceUnit;\nimport org.hibernate.search.engine.spatial.GeoPoint;\nimport org.junit.Test;\n\nimport com.google.gson.JsonObject;\nimport org.easymock.EasyMockSupport;\n\npublic class DistanceToFieldSearchProjectionTest extends EasyMockSupport {\n\n\tprivate static final Set<String> INDEX_NAMES = Collections.singleton( \"myIndexName\" );\n\tprivate static final String FIELD = \"myField\";\n\n\tprivate static final GeoPoint LOCATION = GeoPoint.of( 43, 4 );\n\n\t@Test\n\tpublic void projection_script() {\n\t\tSessionContextImplementor sessionContext = createMock( SessionContextImplementor.class );\n\t\tElasticsearchSearchQueryElementCollector elementCollector = new ElasticsearchSearchQueryElementCollector( sessionContext );\n\n\t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n\t\t\t\tLOCATION, DistanceUnit.METERS );\n\n\t\tJsonObject requestBody = new JsonObject();\n\n\t\tresetAll();\n\t\treplayAll();\n\t\tSearchProjectionExtractContext searchProjectionExecutionContext =\n\t\t\t\telementCollector.toSearchProjectionExecutionContext();\n\t\tprojection.contributeRequest( requestBody, searchProjectionExecutionContext );\n\t\tverifyAll();\n\n\t\tassertThat( searchProjectionExecutionContext.getDistanceSortIndex( FIELD, LOCATION ) ).isNull();\n\t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNotNull();\n\t}\n\n\t@Test\n\tpublic void projection_sort() {\n\t\tSessionContextImplementor sessionContext = createMock( SessionContextImplementor.class );\n\t\tElasticsearchSearchQueryElementCollector elementCollector = new ElasticsearchSearchQueryElementCollector( sessionContext );\n\t\telementCollector.collectSort( new JsonObject() );\n\t\telementCollector.collectDistanceSort( new JsonObject(), FIELD, LOCATION );\n\n\t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n\t\t\t\tLOCATION, DistanceUnit.METERS );\n\n\t\tJsonObject requestBody = new JsonObject();\n\n\t\tresetAll();\n\t\treplayAll();\n\t\tSearchProjectionExtractContext searchProjectionExecutionContext =\n\t\t\t\telementCollector.toSearchProjectionExecutionContext();\n\t\tprojection.contributeRequest( requestBody, searchProjectionExecutionContext );\n\t\tverifyAll();\n\n\t\tassertThat( searchProjectionExecutionContext.getDistanceSortIndex( FIELD, LOCATION ) ).isEqualTo( 1 );\n\t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNull();\n\t}\n}\n",
        "filePathAfter": "backend/elasticsearch/src/test/java/org/hibernate/search/backend/elasticsearch/search/projection/impl/DistanceToFieldSearchProjectionTest.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.search.projection.impl;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Set;\n\nimport org.hibernate.search.backend.elasticsearch.search.projection.impl.SearchProjectionExtractContext.DistanceSortKey;\nimport org.hibernate.search.engine.spatial.DistanceUnit;\nimport org.hibernate.search.engine.spatial.GeoPoint;\n\nimport org.junit.Test;\n\nimport com.google.gson.JsonObject;\nimport org.easymock.EasyMockSupport;\n\npublic class DistanceToFieldSearchProjectionTest extends EasyMockSupport {\n\n\tprivate static final Set<String> INDEX_NAMES = Collections.singleton( \"myIndexName\" );\n\tprivate static final String FIELD = \"myField\";\n\n\tprivate static final GeoPoint LOCATION = GeoPoint.of( 43, 4 );\n\n\t@Test\n\tpublic void projection_script() {\n\t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n\t\t\t\tLOCATION, DistanceUnit.METERS );\n\n\t\tMap<DistanceSortKey, Integer> distanceSorts = Collections.emptyMap();\n\t\tSearchProjectionExtractContext extractContext = createExtractContext( distanceSorts );\n\t\tassertThat( extractContext.getDistanceSortIndex( FIELD, LOCATION ) ).isNull();\n\n\t\tJsonObject requestBody = new JsonObject();\n\t\tresetAll();\n\t\treplayAll();\n\t\tprojection.contributeRequest( requestBody, extractContext );\n\t\tverifyAll();\n\n\t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNotNull();\n\t}\n\n\t@Test\n\tpublic void projection_sort() {\n\t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n\t\t\t\tLOCATION, DistanceUnit.METERS );\n\n\t\tMap<DistanceSortKey, Integer> distanceSorts = new HashMap<>();\n\t\tdistanceSorts.put( new DistanceSortKey( FIELD, LOCATION ), 1 );\n\t\tSearchProjectionExtractContext extractContext = createExtractContext( distanceSorts );\n\t\tassertThat( extractContext.getDistanceSortIndex( FIELD, LOCATION ) ).isEqualTo( 1 );\n\n\t\tJsonObject requestBody = new JsonObject();\n\n\t\tresetAll();\n\t\treplayAll();\n\t\tprojection.contributeRequest( requestBody, extractContext );\n\t\tverifyAll();\n\n\t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNull();\n\t}\n\n\tprivate SearchProjectionExtractContext createExtractContext(Map<DistanceSortKey, Integer> distanceSorts) {\n\t\treturn new SearchProjectionExtractContext( distanceSorts );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private SearchProjectionExtractContext createExtractContext(Map<DistanceSortKey, Integer> distanceSorts) {\n\t\treturn new SearchProjectionExtractContext( distanceSorts );\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.backend.elasticsearch.search.impl.ElasticsearchSearchQueryElementCollector#toSearchProjectionExecutionContext\n methodBody: public SearchProjectionExtractContext toSearchProjectionExecutionContext() {\nreturn new SearchProjectionExtractContext(distanceSorts);\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void projection_script() {\n\t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n\t\t\t\tLOCATION, DistanceUnit.METERS );\n\n\t\tMap<DistanceSortKey, Integer> distanceSorts = Collections.emptyMap();\n\t\tSearchProjectionExtractContext extractContext = createExtractContext( distanceSorts );\n\t\tassertThat( extractContext.getDistanceSortIndex( FIELD, LOCATION ) ).isNull();\n\n\t\tJsonObject requestBody = new JsonObject();\n\t\tresetAll();\n\t\treplayAll();\n\t\tprojection.contributeRequest( requestBody, extractContext );\n\t\tverifyAll();\n\n\t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNotNull();\n\t}\nprivate SearchProjectionExtractContext createExtractContext(Map<DistanceSortKey, Integer> distanceSorts) {\n\t\treturn new SearchProjectionExtractContext( distanceSorts );\n\t}",
        "diffSourceCode": "-   30: \t@Test\n-   31: \tpublic void projection_script() {\n-   32: \t\tSessionContextImplementor sessionContext = createMock( SessionContextImplementor.class );\n-   33: \t\tElasticsearchSearchQueryElementCollector elementCollector = new ElasticsearchSearchQueryElementCollector( sessionContext );\n-   34: \n-   35: \t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n-   36: \t\t\t\tLOCATION, DistanceUnit.METERS );\n-   37: \n-   38: \t\tJsonObject requestBody = new JsonObject();\n-   39: \n-   40: \t\tresetAll();\n-   41: \t\treplayAll();\n-   42: \t\tSearchProjectionExtractContext searchProjectionExecutionContext =\n-   43: \t\t\t\telementCollector.toSearchProjectionExecutionContext();\n-   44: \t\tprojection.contributeRequest( requestBody, searchProjectionExecutionContext );\n+   30: \tprivate static final GeoPoint LOCATION = GeoPoint.of( 43, 4 );\n+   31: \n+   32: \t@Test\n+   33: \tpublic void projection_script() {\n+   34: \t\tElasticsearchDistanceToFieldProjection projection = new ElasticsearchDistanceToFieldProjection( INDEX_NAMES, FIELD, null,\n+   35: \t\t\t\tLOCATION, DistanceUnit.METERS );\n+   36: \n+   37: \t\tMap<DistanceSortKey, Integer> distanceSorts = Collections.emptyMap();\n+   38: \t\tSearchProjectionExtractContext extractContext = createExtractContext( distanceSorts );\n+   39: \t\tassertThat( extractContext.getDistanceSortIndex( FIELD, LOCATION ) ).isNull();\n+   40: \n+   41: \t\tJsonObject requestBody = new JsonObject();\n+   42: \t\tresetAll();\n+   43: \t\treplayAll();\n+   44: \t\tprojection.contributeRequest( requestBody, extractContext );\n    45: \t\tverifyAll();\n    46: \n-   47: \t\tassertThat( searchProjectionExecutionContext.getDistanceSortIndex( FIELD, LOCATION ) ).isNull();\n-   48: \t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNotNull();\n-   49: \t}\n-   70: \t\tassertThat( searchProjectionExecutionContext.getDistanceSortIndex( FIELD, LOCATION ) ).isEqualTo( 1 );\n-   71: \t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNull();\n+   47: \t\tassertThat( requestBody.get( \"script_fields\" ) ).as( \"script_fields\" ).isNotNull();\n+   48: \t}\n+   49: \n+   70: \tprivate SearchProjectionExtractContext createExtractContext(Map<DistanceSortKey, Integer> distanceSorts) {\n+   71: \t\treturn new SearchProjectionExtractContext( distanceSorts );\n    72: \t}\n",
        "uniqueId": "0c6c0d87a14968fd7a417b4943835c7dfadeed0f_30_49_70_72_32_48",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate setupAndUpdateIndex(customSettingsFile String) : void extracted from private setupAndUpdateIndex() : void in class org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/schema/management/ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT.java",
                "startLine": 196,
                "endLine": 214,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/schema/management/ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT.java",
                "startLine": 220,
                "endLine": 222,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/schema/management/ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT.java",
                "startLine": 224,
                "endLine": 242,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void setupAndUpdateIndex() {\n\t\tsetupHelper.start()\n\t\t\t\t.withSchemaManagement( StubMappingSchemaManagementStrategy.DROP_ON_SHUTDOWN_ONLY )\n\t\t\t\t.withBackendProperty(\n\t\t\t\t\t\t// use an empty analysis configurer,\n\t\t\t\t\t\t// so that we have only the custom settings definitions\n\t\t\t\t\t\tElasticsearchIndexSettings.ANALYSIS_CONFIGURER,\n\t\t\t\t\t\t(ElasticsearchAnalysisConfigurer) (ElasticsearchAnalysisConfigurationContext context) -> {\n\t\t\t\t\t\t\t// No-op\n\t\t\t\t\t\t}\n\t\t\t\t)\n\t\t\t\t.withIndexProperty( index.name(), ElasticsearchIndexSettings.SCHEMA_MANAGEMENT_SETTINGS_FILE,\n\t\t\t\t\t\t\"custom-index-settings/valid.json\"\n\t\t\t\t)\n\t\t\t\t.withIndex( index )\n\t\t\t\t.setup();\n\n\t\tFutures.unwrappedExceptionJoin( index.schemaManager().createOrUpdate() );\n\t}",
        "filePathBefore": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/schema/management/ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT.java",
        "isPureRefactoring": true,
        "commitId": "7a3db306af472c7a12f3c539621de72bdf0190dc",
        "packageNameBefore": "org.hibernate.search.integrationtest.backend.elasticsearch.schema.management",
        "classNameBefore": "org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT#setupAndUpdateIndex",
        "classSignatureBefore": "public class ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT#setupAndUpdateIndex"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.elasticsearch.schema.management.ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT"
        ],
        "classSignatureBeforeSet": [
            "public class ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.elasticsearch.schema.management;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.hibernate.search.util.impl.test.JsonHelper.assertJsonEquals;\n\nimport org.hibernate.search.backend.elasticsearch.analysis.ElasticsearchAnalysisConfigurationContext;\nimport org.hibernate.search.backend.elasticsearch.analysis.ElasticsearchAnalysisConfigurer;\nimport org.hibernate.search.backend.elasticsearch.cfg.ElasticsearchIndexSettings;\nimport org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.categories.RequiresIndexOpenClose;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.rule.SearchSetupHelper;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.rule.TestElasticsearchClient;\nimport org.hibernate.search.util.impl.integrationtest.mapper.stub.StubMappedIndex;\nimport org.hibernate.search.util.impl.integrationtest.mapper.stub.StubMappingSchemaManagementStrategy;\nimport org.hibernate.search.util.impl.test.annotation.TestForIssue;\n\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.experimental.categories.Category;\n\n/**\n * Tests related to index custom settings when updating indexes.\n */\n@Category(RequiresIndexOpenClose.class)\n@TestForIssue(jiraKey = \"HSEARCH-3934\")\npublic class ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT {\n\n\t@Rule\n\tpublic final SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic TestElasticsearchClient elasticsearchClient = new TestElasticsearchClient();\n\n\tprivate final StubMappedIndex index = StubMappedIndex.withoutFields();\n\n\t@Test\n\tpublic void nothingToDo() {\n\t\telasticsearchClient.index( index.name() ).deleteAndCreate( \"index\",\n\t\t\t\t\" { \" +\n\t\t\t\t\"   'number_of_shards': '3', \" +\n\t\t\t\t\"   'number_of_replicas': '3', \" +\n\t\t\t\t\"   'analysis': { \" +\n\t\t\t\t\"     'analyzer': { \" +\n\t\t\t\t\"       'my_standard-english': { \" +\n\t\t\t\t\"         'type': 'standard', \" +\n\t\t\t\t\"         'stopwords': '_english_' \" +\n\t\t\t\t\"       }, \" +\n\t\t\t\t\"       'my_analyzer_ngram': { \" +\n\t\t\t\t\"         'type': 'custom', \" +\n\t\t\t\t\"         'tokenizer': 'my_analyzer_ngram_tokenizer' \" +\n\t\t\t\t\"       } \" +\n\t\t\t\t\"     }, \" +\n\t\t\t\t\"     'tokenizer': { \" +\n\t\t\t\t\"       'my_analyzer_ngram_tokenizer': { \" +\n\t\t\t\t\"         'type': 'ngram', \" +\n\t\t\t\t\"         'min_gram': '5', \" +\n\t\t\t\t\"         'max_gram': '6' \" +\n\t\t\t\t\"       } \" +\n\t\t\t\t\"     } \" +\n\t\t\t\t\"   } \" +\n\t\t\t\t\" } \"\n\t\t);\n\n\t\tsetupAndUpdateIndex();\n\n\t\tassertJsonEquals(\n\t\t\t\t\" { \" +\n\t\t\t\t\" \t'analyzer': { \" +\n\t\t\t\t\" \t\t'my_standard-english': { \" +\n\t\t\t\t\" \t\t\t'type': 'standard', \" +\n\t\t\t\t\" \t\t\t'stopwords': '_english_' \" +\n\t\t\t\t\" \t\t}, \" +\n\t\t\t\t\" \t\t'my_analyzer_ngram': { \" +\n\t\t\t\t\" \t\t\t'type': 'custom', \" +\n\t\t\t\t\" \t\t\t'tokenizer': 'my_analyzer_ngram_tokenizer' \" +\n\t\t\t\t\" \t\t} \" +\n\t\t\t\t\" \t}, \" +\n\t\t\t\t\" \t'tokenizer': { \" +\n\t\t\t\t\" \t\t'my_analyzer_ngram_tokenizer': { \" +\n\t\t\t\t\" \t\t\t'type': 'ngram', \" +\n\t\t\t\t\" \t\t\t'min_gram': '5', \" +\n\t\t\t\t\" \t\t\t'max_gram': '6' \" +\n\t\t\t\t\" \t\t} \" +\n\t\t\t\t\" \t} \" +\n\t\t\t\t\" } \",\n\t\t\t\telasticsearchClient.index( index.name() ).settings( \"index.analysis\" ).get()\n\t\t);\n\n\t\tassertThat( elasticsearchClient.index( index.name() ).settings( \"index.number_of_shards\" ).get() )\n\t\t\t\t.isEqualTo( \"\\\"3\\\"\" );\n\t\tassertThat( elasticsearchClient.index( index.name() ).settings( \"index.number_of_replicas\" ).get() )\n\t\t\t\t.isEqualTo( \"\\\"3\\\"\" );\n\t}\n\n\t@Test\n\tpublic void change_analysis() {\n\t\telasticsearchClient.index( index.name() ).deleteAndCreate( \"index\",\n\t\t\t\t\" { \" +\n\t\t\t\t\"   'number_of_shards': '3', \" +\n\t\t\t\t\"   'number_of_replicas': '3', \" +\n\t\t\t\t\"   'analysis': { \" +\n\t\t\t\t\"     'analyzer': { \" +\n\t\t\t\t\"       'my_standard-english': { \" +\n\t\t\t\t\"         'type': 'standard', \" +\n\t\t\t\t\"         'stopwords': '_english_' \" +\n\t\t\t\t\"       }, \" +\n\t\t\t\t\"       'my_analyzer_ngram': { \" +\n\t\t\t\t\"         'type': 'custom', \" +\n\t\t\t\t\"         'tokenizer': 'my_analyzer_ngram_tokenizer' \" +\n\t\t\t\t\"       } \" +\n\t\t\t\t\"     }, \" +\n\t\t\t\t\"     'tokenizer': { \" +\n\t\t\t\t\"       'my_analyzer_ngram_tokenizer': { \" +\n\t\t\t\t\"         'type': 'ngram', \" +\n\t\t\t\t\"         'min_gram': '2', \" +\n\t\t\t\t\"         'max_gram': '3' \" +\n\t\t\t\t\"       } \" +\n\t\t\t\t\"     } \" +\n\t\t\t\t\"   } \" +\n\t\t\t\t\" } \"\n\t\t);\n\n\t\tsetupAndUpdateIndex();\n\n\t\tassertJsonEquals(\n\t\t\t\t\" { \" +\n\t\t\t\t\" \t'analyzer': { \" +\n\t\t\t\t\" \t\t'my_standard-english': { \" +\n\t\t\t\t\" \t\t\t'type': 'standard', \" +\n\t\t\t\t\" \t\t\t'stopwords': '_english_' \" +\n\t\t\t\t\" \t\t}, \" +\n\t\t\t\t\" \t\t'my_analyzer_ngram': { \" +\n\t\t\t\t\" \t\t\t'type': 'custom', \" +\n\t\t\t\t\" \t\t\t'tokenizer': 'my_analyzer_ngram_tokenizer' \" +\n\t\t\t\t\" \t\t} \" +\n\t\t\t\t\" \t}, \" +\n\t\t\t\t\" \t'tokenizer': { \" +\n\t\t\t\t\" \t\t'my_analyzer_ngram_tokenizer': { \" +\n\t\t\t\t\" \t\t\t'type': 'ngram', \" +\n\t\t\t\t\" \t\t\t'min_gram': '5', \" +\n\t\t\t\t\" \t\t\t'max_gram': '6' \" +\n\t\t\t\t\" \t\t} \" +\n\t\t\t\t\" \t} \" +\n\t\t\t\t\" } \",\n\t\t\t\telasticsearchClient.index( index.name() ).settings( \"index.analysis\" ).get()\n\t\t);\n\n\t\tassertThat( elasticsearchClient.index( index.name() ).settings( \"index.number_of_shards\" ).get() )\n\t\t\t\t.isEqualTo( \"\\\"3\\\"\" );\n\t\tassertThat( elasticsearchClient.index( index.name() ).settings( \"index.number_of_replicas\" ).get() )\n\t\t\t\t.isEqualTo( \"\\\"3\\\"\" );\n\t}\n\n\t@Test\n\tpublic void change_numberOfShards() {\n\t\telasticsearchClient.index( index.name() ).deleteAndCreate( \"index\",\n\t\t\t\t\" { \" +\n\t\t\t\t\"   'number_of_shards': '7', \" +\n\t\t\t\t\"   'number_of_replicas': '3', \" +\n\t\t\t\t\"   'analysis': { \" +\n\t\t\t\t\"     'analyzer': { \" +\n\t\t\t\t\"       'my_standard-english': { \" +\n\t\t\t\t\"         'type': 'standard', \" +\n\t\t\t\t\"         'stopwords': '_english_' \" +\n\t\t\t\t\"       }, \" +\n\t\t\t\t\"       'my_analyzer_ngram': { \" +\n\t\t\t\t\"         'type': 'custom', \" +\n\t\t\t\t\"         'tokenizer': 'my_analyzer_ngram_tokenizer' \" +\n\t\t\t\t\"       } \" +\n\t\t\t\t\"     }, \" +\n\t\t\t\t\"     'tokenizer': { \" +\n\t\t\t\t\"       'my_analyzer_ngram_tokenizer': { \" +\n\t\t\t\t\"         'type': 'ngram', \" +\n\t\t\t\t\"         'min_gram': '2', \" +\n\t\t\t\t\"         'max_gram': '3' \" +\n\t\t\t\t\"       } \" +\n\t\t\t\t\"     } \" +\n\t\t\t\t\"   } \" +\n\t\t\t\t\" } \"\n\t\t);\n\n\t\tassertThatThrownBy( () -> setupAndUpdateIndex() )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\"Unable to update settings\", \"index.number_of_shards\" );\n\t}\n\n\tprivate void setupAndUpdateIndex() {\n\t\tsetupHelper.start()\n\t\t\t\t.withSchemaManagement( StubMappingSchemaManagementStrategy.DROP_ON_SHUTDOWN_ONLY )\n\t\t\t\t.withBackendProperty(\n\t\t\t\t\t\t// use an empty analysis configurer,\n\t\t\t\t\t\t// so that we have only the custom settings definitions\n\t\t\t\t\t\tElasticsearchIndexSettings.ANALYSIS_CONFIGURER,\n\t\t\t\t\t\t(ElasticsearchAnalysisConfigurer) (ElasticsearchAnalysisConfigurationContext context) -> {\n\t\t\t\t\t\t\t// No-op\n\t\t\t\t\t\t}\n\t\t\t\t)\n\t\t\t\t.withIndexProperty( index.name(), ElasticsearchIndexSettings.SCHEMA_MANAGEMENT_SETTINGS_FILE,\n\t\t\t\t\t\t\"custom-index-settings/valid.json\"\n\t\t\t\t)\n\t\t\t\t.withIndex( index )\n\t\t\t\t.setup();\n\n\t\tFutures.unwrappedExceptionJoin( index.schemaManager().createOrUpdate() );\n\t}\n\n}\n",
        "filePathAfter": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/schema/management/ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.elasticsearch.schema.management;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.hibernate.search.util.impl.test.JsonHelper.assertJsonEquals;\n\nimport org.hibernate.search.backend.elasticsearch.analysis.ElasticsearchAnalysisConfigurationContext;\nimport org.hibernate.search.backend.elasticsearch.analysis.ElasticsearchAnalysisConfigurer;\nimport org.hibernate.search.backend.elasticsearch.cfg.ElasticsearchIndexSettings;\nimport org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.categories.RequiresIndexOpenClose;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.rule.SearchSetupHelper;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.rule.TestElasticsearchClient;\nimport org.hibernate.search.util.impl.integrationtest.mapper.stub.StubMappedIndex;\nimport org.hibernate.search.util.impl.integrationtest.mapper.stub.StubMappingSchemaManagementStrategy;\nimport org.hibernate.search.util.impl.test.annotation.TestForIssue;\n\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.experimental.categories.Category;\n\n/**\n * Tests related to index custom settings when updating indexes.\n */\n@Category(RequiresIndexOpenClose.class)\n@TestForIssue(jiraKey = \"HSEARCH-3934\")\npublic class ElasticsearchIndexSchemaManagerUpdateCustomSettingsIT {\n\n\t@Rule\n\tpublic final SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic TestElasticsearchClient elasticsearchClient = new TestElasticsearchClient();\n\n\tprivate final StubMappedIndex index = StubMappedIndex.withoutFields();\n\n\t@Test\n\tpublic void nothingToDo() {\n\t\telasticsearchClient.index( index.name() ).deleteAndCreate( \"index\",\n\t\t\t\t\" { \" +\n\t\t\t\t\"   'number_of_shards': '3', \" +\n\t\t\t\t\"   'number_of_replicas': '3', \" +\n\t\t\t\t\"   'analysis': { \" +\n\t\t\t\t\"     'analyzer': { \" +\n\t\t\t\t\"       'my_standard-english': { \" +\n\t\t\t\t\"         'type': 'standard', \" +\n\t\t\t\t\"         'stopwords': '_english_' \" +\n\t\t\t\t\"       }, \" +\n\t\t\t\t\"       'my_analyzer_ngram': { \" +\n\t\t\t\t\"         'type': 'custom', \" +\n\t\t\t\t\"         'tokenizer': 'my_analyzer_ngram_tokenizer' \" +\n\t\t\t\t\"       } \" +\n\t\t\t\t\"     }, \" +\n\t\t\t\t\"     'tokenizer': { \" +\n\t\t\t\t\"       'my_analyzer_ngram_tokenizer': { \" +\n\t\t\t\t\"         'type': 'ngram', \" +\n\t\t\t\t\"         'min_gram': '5', \" +\n\t\t\t\t\"         'max_gram': '6' \" +\n\t\t\t\t\"       } \" +\n\t\t\t\t\"     } \" +\n\t\t\t\t\"   } \" +\n\t\t\t\t\" } \"\n\t\t);\n\n\t\tsetupAndUpdateIndex();\n\n\t\tassertJsonEquals(\n\t\t\t\t\" { \" +\n\t\t\t\t\" \t'analyzer': { \" +\n\t\t\t\t\" \t\t'my_standard-english': { \" +\n\t\t\t\t\" \t\t\t'type': 'standard', \" +\n\t\t\t\t\" \t\t\t'stopwords': '_english_' \" +\n\t\t\t\t\" \t\t}, \" +\n\t\t\t\t\" \t\t'my_analyzer_ngram': { \" +\n\t\t\t\t\" \t\t\t'type': 'custom', \" +\n\t\t\t\t\" \t\t\t'tokenizer': 'my_analyzer_ngram_tokenizer' \" +\n\t\t\t\t\" \t\t} \" +\n\t\t\t\t\" \t}, \" +\n\t\t\t\t\" \t'tokenizer': { \" +\n\t\t\t\t\" \t\t'my_analyzer_ngram_tokenizer': { \" +\n\t\t\t\t\" \t\t\t'type': 'ngram', \" +\n\t\t\t\t\" \t\t\t'min_gram': '5', \" +\n\t\t\t\t\" \t\t\t'max_gram': '6' \" +\n\t\t\t\t\" \t\t} \" +\n\t\t\t\t\" \t} \" +\n\t\t\t\t\" } \",\n\t\t\t\telasticsearchClient.index( index.name() ).settings( \"index.analysis\" ).get()\n\t\t);\n\n\t\tassertThat( elasticsearchClient.index( index.name() ).settings( \"index.number_of_shards\" ).get() )\n\t\t\t\t.isEqualTo( \"\\\"3\\\"\" );\n\t\tassertThat( elasticsearchClient.index( index.name() ).settings( \"index.number_of_replicas\" ).get() )\n\t\t\t\t.isEqualTo( \"\\\"3\\\"\" );\n\t}\n\n\t@Test\n\tpublic void change_analysis() {\n\t\telasticsearchClient.index( index.name() ).deleteAndCreate( \"index\",\n\t\t\t\t\" { \" +\n\t\t\t\t\"   'number_of_shards': '3', \" +\n\t\t\t\t\"   'number_of_replicas': '3', \" +\n\t\t\t\t\"   'analysis': { \" +\n\t\t\t\t\"     'analyzer': { \" +\n\t\t\t\t\"       'my_standard-english': { \" +\n\t\t\t\t\"         'type': 'standard', \" +\n\t\t\t\t\"         'stopwords': '_english_' \" +\n\t\t\t\t\"       }, \" +\n\t\t\t\t\"       'my_analyzer_ngram': { \" +\n\t\t\t\t\"         'type': 'custom', \" +\n\t\t\t\t\"         'tokenizer': 'my_analyzer_ngram_tokenizer' \" +\n\t\t\t\t\"       } \" +\n\t\t\t\t\"     }, \" +\n\t\t\t\t\"     'tokenizer': { \" +\n\t\t\t\t\"       'my_analyzer_ngram_tokenizer': { \" +\n\t\t\t\t\"         'type': 'ngram', \" +\n\t\t\t\t\"         'min_gram': '2', \" +\n\t\t\t\t\"         'max_gram': '3' \" +\n\t\t\t\t\"       } \" +\n\t\t\t\t\"     } \" +\n\t\t\t\t\"   } \" +\n\t\t\t\t\" } \"\n\t\t);\n\n\t\tsetupAndUpdateIndex();\n\n\t\tassertJsonEquals(\n\t\t\t\t\" { \" +\n\t\t\t\t\" \t'analyzer': { \" +\n\t\t\t\t\" \t\t'my_standard-english': { \" +\n\t\t\t\t\" \t\t\t'type': 'standard', \" +\n\t\t\t\t\" \t\t\t'stopwords': '_english_' \" +\n\t\t\t\t\" \t\t}, \" +\n\t\t\t\t\" \t\t'my_analyzer_ngram': { \" +\n\t\t\t\t\" \t\t\t'type': 'custom', \" +\n\t\t\t\t\" \t\t\t'tokenizer': 'my_analyzer_ngram_tokenizer' \" +\n\t\t\t\t\" \t\t} \" +\n\t\t\t\t\" \t}, \" +\n\t\t\t\t\" \t'tokenizer': { \" +\n\t\t\t\t\" \t\t'my_analyzer_ngram_tokenizer': { \" +\n\t\t\t\t\" \t\t\t'type': 'ngram', \" +\n\t\t\t\t\" \t\t\t'min_gram': '5', \" +\n\t\t\t\t\" \t\t\t'max_gram': '6' \" +\n\t\t\t\t\" \t\t} \" +\n\t\t\t\t\" \t} \" +\n\t\t\t\t\" } \",\n\t\t\t\telasticsearchClient.index( index.name() ).settings( \"index.analysis\" ).get()\n\t\t);\n\n\t\tassertThat( elasticsearchClient.index( index.name() ).settings( \"index.number_of_shards\" ).get() )\n\t\t\t\t.isEqualTo( \"\\\"3\\\"\" );\n\t\tassertThat( elasticsearchClient.index( index.name() ).settings( \"index.number_of_replicas\" ).get() )\n\t\t\t\t.isEqualTo( \"\\\"3\\\"\" );\n\t}\n\n\t@Test\n\tpublic void change_numberOfShards() {\n\t\telasticsearchClient.index( index.name() ).deleteAndCreate( \"index\",\n\t\t\t\t\" { \" +\n\t\t\t\t\"   'number_of_shards': '7', \" +\n\t\t\t\t\"   'number_of_replicas': '3', \" +\n\t\t\t\t\"   'analysis': { \" +\n\t\t\t\t\"     'analyzer': { \" +\n\t\t\t\t\"       'my_standard-english': { \" +\n\t\t\t\t\"         'type': 'standard', \" +\n\t\t\t\t\"         'stopwords': '_english_' \" +\n\t\t\t\t\"       }, \" +\n\t\t\t\t\"       'my_analyzer_ngram': { \" +\n\t\t\t\t\"         'type': 'custom', \" +\n\t\t\t\t\"         'tokenizer': 'my_analyzer_ngram_tokenizer' \" +\n\t\t\t\t\"       } \" +\n\t\t\t\t\"     }, \" +\n\t\t\t\t\"     'tokenizer': { \" +\n\t\t\t\t\"       'my_analyzer_ngram_tokenizer': { \" +\n\t\t\t\t\"         'type': 'ngram', \" +\n\t\t\t\t\"         'min_gram': '2', \" +\n\t\t\t\t\"         'max_gram': '3' \" +\n\t\t\t\t\"       } \" +\n\t\t\t\t\"     } \" +\n\t\t\t\t\"   } \" +\n\t\t\t\t\" } \"\n\t\t);\n\n\t\tassertThatThrownBy( () -> setupAndUpdateIndex() )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\"Unable to update settings\", \"index.number_of_shards\" );\n\t}\n\n\t@Test\n\tpublic void change_maxResultWindow() {\n\t\telasticsearchClient.index( index.name() ).deleteAndCreate( \"index\", \"{ 'max_result_window': '20000' }\" );\n\n\t\tsetupAndUpdateIndex( \"max-result-window.json\" );\n\n\t\tassertJsonEquals(\n\t\t\t\t\"\\\"250\\\"\",\n\t\t\t\telasticsearchClient.index( index.name() ).settings( \"index.max_result_window\" ).get()\n\t\t);\n\t}\n\n\t@Test\n\tpublic void set_maxResultWindow() {\n\t\telasticsearchClient.index( index.name() ).deleteAndCreate( \"index\", \"{ }\" );\n\n\t\tsetupAndUpdateIndex( \"max-result-window.json\" );\n\n\t\tassertJsonEquals(\n\t\t\t\t\"\\\"250\\\"\",\n\t\t\t\telasticsearchClient.index( index.name() ).settings( \"index.max_result_window\" ).get()\n\t\t);\n\t}\n\n\tprivate void setupAndUpdateIndex() {\n\t\tsetupAndUpdateIndex( \"valid.json\" );\n\t}\n\n\tprivate void setupAndUpdateIndex(String customSettingsFile) {\n\t\tsetupHelper.start()\n\t\t\t\t.withSchemaManagement( StubMappingSchemaManagementStrategy.DROP_ON_SHUTDOWN_ONLY )\n\t\t\t\t.withBackendProperty(\n\t\t\t\t\t\t// use an empty analysis configurer,\n\t\t\t\t\t\t// so that we have only the custom settings definitions\n\t\t\t\t\t\tElasticsearchIndexSettings.ANALYSIS_CONFIGURER,\n\t\t\t\t\t\t(ElasticsearchAnalysisConfigurer) (ElasticsearchAnalysisConfigurationContext context) -> {\n\t\t\t\t\t\t\t// No-op\n\t\t\t\t\t\t}\n\t\t\t\t)\n\t\t\t\t.withIndexProperty( index.name(), ElasticsearchIndexSettings.SCHEMA_MANAGEMENT_SETTINGS_FILE,\n\t\t\t\t\t\t\"custom-index-settings/\" + customSettingsFile\n\t\t\t\t)\n\t\t\t\t.withIndex( index )\n\t\t\t\t.setup();\n\n\t\tFutures.unwrappedExceptionJoin( index.schemaManager().createOrUpdate() );\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "private void setupAndUpdateIndex(String customSettingsFile) {\n\t\tsetupHelper.start()\n\t\t\t\t.withSchemaManagement( StubMappingSchemaManagementStrategy.DROP_ON_SHUTDOWN_ONLY )\n\t\t\t\t.withBackendProperty(\n\t\t\t\t\t\t// use an empty analysis configurer,\n\t\t\t\t\t\t// so that we have only the custom settings definitions\n\t\t\t\t\t\tElasticsearchIndexSettings.ANALYSIS_CONFIGURER,\n\t\t\t\t\t\t(ElasticsearchAnalysisConfigurer) (ElasticsearchAnalysisConfigurationContext context) -> {\n\t\t\t\t\t\t\t// No-op\n\t\t\t\t\t\t}\n\t\t\t\t)\n\t\t\t\t.withIndexProperty( index.name(), ElasticsearchIndexSettings.SCHEMA_MANAGEMENT_SETTINGS_FILE,\n\t\t\t\t\t\t\"custom-index-settings/\" + customSettingsFile\n\t\t\t\t)\n\t\t\t\t.withIndex( index )\n\t\t\t\t.setup();\n\n\t\tFutures.unwrappedExceptionJoin( index.schemaManager().createOrUpdate() );\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "private void setupAndUpdateIndex() {\n\t\tsetupAndUpdateIndex( \"valid.json\" );\n\t}\nprivate void setupAndUpdateIndex(String customSettingsFile) {\n\t\tsetupHelper.start()\n\t\t\t\t.withSchemaManagement( StubMappingSchemaManagementStrategy.DROP_ON_SHUTDOWN_ONLY )\n\t\t\t\t.withBackendProperty(\n\t\t\t\t\t\t// use an empty analysis configurer,\n\t\t\t\t\t\t// so that we have only the custom settings definitions\n\t\t\t\t\t\tElasticsearchIndexSettings.ANALYSIS_CONFIGURER,\n\t\t\t\t\t\t(ElasticsearchAnalysisConfigurer) (ElasticsearchAnalysisConfigurationContext context) -> {\n\t\t\t\t\t\t\t// No-op\n\t\t\t\t\t\t}\n\t\t\t\t)\n\t\t\t\t.withIndexProperty( index.name(), ElasticsearchIndexSettings.SCHEMA_MANAGEMENT_SETTINGS_FILE,\n\t\t\t\t\t\t\"custom-index-settings/\" + customSettingsFile\n\t\t\t\t)\n\t\t\t\t.withIndex( index )\n\t\t\t\t.setup();\n\n\t\tFutures.unwrappedExceptionJoin( index.schemaManager().createOrUpdate() );\n\t}",
        "diffSourceCode": "-  196: \tprivate void setupAndUpdateIndex() {\n-  197: \t\tsetupHelper.start()\n-  198: \t\t\t\t.withSchemaManagement( StubMappingSchemaManagementStrategy.DROP_ON_SHUTDOWN_ONLY )\n-  199: \t\t\t\t.withBackendProperty(\n-  200: \t\t\t\t\t\t// use an empty analysis configurer,\n-  201: \t\t\t\t\t\t// so that we have only the custom settings definitions\n-  202: \t\t\t\t\t\tElasticsearchIndexSettings.ANALYSIS_CONFIGURER,\n-  203: \t\t\t\t\t\t(ElasticsearchAnalysisConfigurer) (ElasticsearchAnalysisConfigurationContext context) -> {\n-  204: \t\t\t\t\t\t\t// No-op\n-  205: \t\t\t\t\t\t}\n-  206: \t\t\t\t)\n-  207: \t\t\t\t.withIndexProperty( index.name(), ElasticsearchIndexSettings.SCHEMA_MANAGEMENT_SETTINGS_FILE,\n-  208: \t\t\t\t\t\t\"custom-index-settings/valid.json\"\n-  209: \t\t\t\t)\n-  210: \t\t\t\t.withIndex( index )\n-  211: \t\t\t\t.setup();\n-  212: \n-  213: \t\tFutures.unwrappedExceptionJoin( index.schemaManager().createOrUpdate() );\n-  214: \t}\n+  196: \t@Test\n+  197: \tpublic void change_maxResultWindow() {\n+  198: \t\telasticsearchClient.index( index.name() ).deleteAndCreate( \"index\", \"{ 'max_result_window': '20000' }\" );\n+  199: \n+  200: \t\tsetupAndUpdateIndex( \"max-result-window.json\" );\n+  201: \n+  202: \t\tassertJsonEquals(\n+  203: \t\t\t\t\"\\\"250\\\"\",\n+  204: \t\t\t\telasticsearchClient.index( index.name() ).settings( \"index.max_result_window\" ).get()\n+  205: \t\t);\n+  206: \t}\n+  207: \n+  208: \t@Test\n+  209: \tpublic void set_maxResultWindow() {\n+  210: \t\telasticsearchClient.index( index.name() ).deleteAndCreate( \"index\", \"{ }\" );\n+  211: \n+  212: \t\tsetupAndUpdateIndex( \"max-result-window.json\" );\n+  213: \n+  214: \t\tassertJsonEquals(\n+  220: \tprivate void setupAndUpdateIndex() {\n+  221: \t\tsetupAndUpdateIndex( \"valid.json\" );\n+  222: \t}\n+  224: \tprivate void setupAndUpdateIndex(String customSettingsFile) {\n+  225: \t\tsetupHelper.start()\n+  226: \t\t\t\t.withSchemaManagement( StubMappingSchemaManagementStrategy.DROP_ON_SHUTDOWN_ONLY )\n+  227: \t\t\t\t.withBackendProperty(\n+  228: \t\t\t\t\t\t// use an empty analysis configurer,\n+  229: \t\t\t\t\t\t// so that we have only the custom settings definitions\n+  230: \t\t\t\t\t\tElasticsearchIndexSettings.ANALYSIS_CONFIGURER,\n+  231: \t\t\t\t\t\t(ElasticsearchAnalysisConfigurer) (ElasticsearchAnalysisConfigurationContext context) -> {\n+  232: \t\t\t\t\t\t\t// No-op\n+  233: \t\t\t\t\t\t}\n+  234: \t\t\t\t)\n+  235: \t\t\t\t.withIndexProperty( index.name(), ElasticsearchIndexSettings.SCHEMA_MANAGEMENT_SETTINGS_FILE,\n+  236: \t\t\t\t\t\t\"custom-index-settings/\" + customSettingsFile\n+  237: \t\t\t\t)\n+  238: \t\t\t\t.withIndex( index )\n+  239: \t\t\t\t.setup();\n+  240: \n+  241: \t\tFutures.unwrappedExceptionJoin( index.schemaManager().createOrUpdate() );\n+  242: \t}\n",
        "uniqueId": "7a3db306af472c7a12f3c539621de72bdf0190dc_196_214_224_242_220_222",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11
    },
    {
        "type": "Inline Method",
        "description": "Inline Method\tprivate try_multipleHosts_failover_fault() : void inlined to public multipleHosts_failover_fault() : void in class org.hibernate.search.integrationtest.backend.elasticsearch.client.ElasticsearchClientFactoryImplIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/client/ElasticsearchClientFactoryImplIT.java",
                "startLine": 659,
                "endLine": 668,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/client/ElasticsearchClientFactoryImplIT.java",
                "startLine": 627,
                "endLine": 663,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/client/ElasticsearchClientFactoryImplIT.java",
                "startLine": 670,
                "endLine": 704,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void try_multipleHosts_failover_fault() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withFault( Fault.MALFORMED_RESPONSE_CHUNK ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1, wireMockRule2 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 1, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\n\t\t\twireMockRule1.resetRequests();\n\t\t\twireMockRule2.resetRequests();\n\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t// Must not use the failing node anymore\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 0, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}",
        "filePathBefore": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/client/ElasticsearchClientFactoryImplIT.java",
        "isPureRefactoring": true,
        "commitId": "280cdd4a54bba61ac2dd9e927d0599ae0a263e8b",
        "packageNameBefore": "org.hibernate.search.integrationtest.backend.elasticsearch.client",
        "classNameBefore": "org.hibernate.search.integrationtest.backend.elasticsearch.client.ElasticsearchClientFactoryImplIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.backend.elasticsearch.client.ElasticsearchClientFactoryImplIT#try_multipleHosts_failover_fault",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.client.ElasticsearchClientFactoryImplIT#doPost\n methodBody: private ElasticsearchResponse doPost(ElasticsearchClient client, String path, String payload) {\ntryreturn client.submit(buildRequest(ElasticsearchRequest.post(),path,payload)).join();\ncatch(RuntimeException e)throw new AssertionFailure(\"Unexpected exception during POST: \" + e.getMessage(),e);\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.client.ElasticsearchClientFactoryImplIT#elasticsearchResponse\n methodBody: private static ResponseDefinitionBuilder elasticsearchResponse() {\nreturn ResponseDefinitionBuilder.okForEmptyJson();\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.client.ElasticsearchClientFactoryImplIT#createClient\n methodBody: private ElasticsearchClientImplementor createClient(Consumer<BiConsumer<String, Object>> additionalProperties) {\nMap<String,?> defaultBackendProperties=new ElasticsearchTckBackendHelper().createDefaultBackendSetupStrategy().createBackendConfigurationProperties(testConfigurationProvider);\nMap<String,Object> clientProperties=new HashMap<>(defaultBackendProperties);\nclientProperties.put(ElasticsearchBackendSettings.HOSTS,httpHostAndPortFor(wireMockRule1));\nclientProperties.put(ElasticsearchBackendSettings.PROTOCOL,\"http\");\nadditionalProperties.accept(clientProperties::put);\nConfigurationPropertySource clientPropertySource=ConfigurationPropertySource.fromMap(clientProperties);\nMap<String,Object> beanResolverConfiguration=new HashMap<>();\nbeanResolverConfiguration.put(EngineSpiSettings.Radicals.BEAN_CONFIGURERS,Collections.singletonList(elasticsearchSslBeanConfigurer()));\nBeanResolver beanResolver=testConfigurationProvider.createBeanResolverForTest(ConfigurationPropertySource.fromMap(beanResolverConfiguration));\ntry(BeanHolder<ElasticsearchClientFactory> factoryHolder=beanResolver.resolve(ElasticsearchClientFactoryImpl.REFERENCE))return factoryHolder.get().create(beanResolver,clientPropertySource,threadPoolProvider.threadProvider(),\"Client\",timeoutExecutorService,GsonProvider.create(GsonBuilder::new,true));\n}",
        "classSignatureBefore": "public class ElasticsearchClientFactoryImplIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.elasticsearch.client.ElasticsearchClientFactoryImplIT#try_multipleHosts_failover_fault"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.elasticsearch.client.ElasticsearchClientFactoryImplIT"
        ],
        "classSignatureBeforeSet": [
            "public class ElasticsearchClientFactoryImplIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.elasticsearch.client;\n\nimport static com.github.tomakehurst.wiremock.client.WireMock.equalToJson;\nimport static com.github.tomakehurst.wiremock.client.WireMock.get;\nimport static com.github.tomakehurst.wiremock.client.WireMock.post;\nimport static com.github.tomakehurst.wiremock.client.WireMock.postRequestedFor;\nimport static com.github.tomakehurst.wiremock.client.WireMock.urlPathMatching;\nimport static com.github.tomakehurst.wiremock.core.WireMockConfiguration.wireMockConfig;\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.awaitility.Awaitility.await;\nimport static org.hibernate.search.util.impl.test.JsonHelper.assertJsonEquals;\n\nimport java.io.IOException;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.concurrent.CompletionException;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.function.BiConsumer;\nimport java.util.function.Consumer;\nimport java.util.stream.Collectors;\nimport javax.net.ssl.SSLContext;\n\nimport org.hibernate.search.backend.elasticsearch.cfg.ElasticsearchBackendSettings;\nimport org.hibernate.search.backend.elasticsearch.client.impl.ElasticsearchClientFactoryImpl;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchClient;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchClientFactory;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchClientImplementor;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchHttpClientConfigurationContext;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchHttpClientConfigurer;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchRequest;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchResponse;\nimport org.hibernate.search.backend.elasticsearch.gson.spi.GsonProvider;\nimport org.hibernate.search.backend.elasticsearch.util.spi.URLEncodedString;\nimport org.hibernate.search.engine.cfg.spi.ConfigurationPropertySource;\nimport org.hibernate.search.engine.cfg.spi.EngineSpiSettings;\nimport org.hibernate.search.engine.environment.bean.BeanHolder;\nimport org.hibernate.search.engine.environment.bean.BeanReference;\nimport org.hibernate.search.engine.environment.bean.BeanResolver;\nimport org.hibernate.search.engine.environment.bean.spi.BeanConfigurer;\nimport org.hibernate.search.engine.environment.thread.impl.EmbeddedThreadProvider;\nimport org.hibernate.search.engine.environment.thread.impl.ThreadPoolProviderImpl;\nimport org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.categories.RequiresNoAutomaticAuthenticationHeader;\nimport org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util.ElasticsearchTckBackendHelper;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.dialect.ElasticsearchTestDialect;\nimport org.hibernate.search.util.impl.integrationtest.common.TestConfigurationProvider;\nimport org.hibernate.search.util.impl.test.SubTest;\nimport org.hibernate.search.util.impl.test.annotation.PortedFromSearch5;\nimport org.hibernate.search.util.impl.test.annotation.TestForIssue;\nimport org.hibernate.search.util.impl.test.rule.ExpectedLog4jLog;\n\nimport org.junit.After;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.experimental.categories.Category;\n\nimport com.github.tomakehurst.wiremock.client.ResponseDefinitionBuilder;\nimport com.github.tomakehurst.wiremock.extension.Parameters;\nimport com.github.tomakehurst.wiremock.http.Fault;\nimport com.github.tomakehurst.wiremock.http.Request;\nimport com.github.tomakehurst.wiremock.junit.WireMockRule;\nimport com.github.tomakehurst.wiremock.matching.MatchResult;\nimport com.github.tomakehurst.wiremock.matching.RequestMatcherExtension;\nimport com.google.gson.GsonBuilder;\nimport com.google.gson.JsonArray;\nimport com.google.gson.JsonObject;\nimport com.google.gson.JsonParser;\nimport com.google.gson.JsonSyntaxException;\nimport org.apache.http.conn.ssl.NoopHostnameVerifier;\nimport org.apache.http.conn.ssl.TrustSelfSignedStrategy;\nimport org.apache.http.ssl.SSLContexts;\nimport org.apache.logging.log4j.Level;\nimport org.assertj.core.api.InstanceOfAssertFactories;\n\n@PortedFromSearch5(original = \"org.hibernate.search.elasticsearch.test.DefaultElasticsearchClientFactoryTest\")\npublic class ElasticsearchClientFactoryImplIT {\n\n\tprivate static final JsonParser JSON_PARSER = new JsonParser();\n\n\t@Rule\n\tpublic ExpectedLog4jLog logged = ExpectedLog4jLog.create();\n\n\t@Rule\n\tpublic WireMockRule wireMockRule1 = new WireMockRule( wireMockConfig().port( 0 )\n\t\t\t.httpsPort( 0 ) /* Automatic port selection */ );\n\n\t@Rule\n\tpublic WireMockRule wireMockRule2 = new WireMockRule( wireMockConfig().port( 0 ).httpsPort( 0 ) /* Automatic port selection */ );\n\n\t@Rule\n\tpublic TestConfigurationProvider testConfigurationProvider = new TestConfigurationProvider();\n\n\tprivate final ThreadPoolProviderImpl threadPoolProvider = new ThreadPoolProviderImpl(\n\t\t\tBeanHolder.of( new EmbeddedThreadProvider( ElasticsearchClientFactoryImplIT.class.getName() + \": \" ) )\n\t);\n\n\tprivate final ScheduledExecutorService timeoutExecutorService =\n\t\t\tthreadPoolProvider.newScheduledExecutor( 1, \"Timeout - \" );\n\n\t@After\n\tpublic void cleanup() {\n\t\ttimeoutExecutorService.shutdownNow();\n\t\tthreadPoolProvider.close();\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2274\")\n\tpublic void simple_http() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessage\";\n\t\tString responseBody = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t\t\t.withBody( responseBody ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient() ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\n\t\t\twireMockRule1.verify(\n\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-4099\")\n\tpublic void uris_http() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessage\";\n\t\tString responseBody = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t\t\t.withBody( responseBody ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, httpUrisFor( wireMockRule1 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\n\t\t\twireMockRule1.verify(\n\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-4051\")\n\tpublic void pathPrefix_http() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessage\";\n\t\tString responseBody = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/bla/bla/bla/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t\t\t.withBody( responseBody ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PATH_PREFIX, \"bla/bla/bla\" );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\n\t\t\twireMockRule1.verify(\n\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/bla/bla/bla/myIndex/myType\" ) )\n\t\t\t\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-4099\")\n\tpublic void pathPrefix_uris() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessage\";\n\t\tString responseBody = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/bla/bla/bla/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t\t\t.withBody( responseBody ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PATH_PREFIX, \"bla/bla/bla\" );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, httpUrisFor( wireMockRule1 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\n\t\t\twireMockRule1.verify(\n\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/bla/bla/bla/myIndex/myType\" ) )\n\t\t\t\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2274\")\n\tpublic void simple_https() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessage\";\n\t\tString responseBody = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t\t\t.withBody( responseBody ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpsHostAndPortFor( wireMockRule1 ) );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, \"https\" );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\n\t\t\twireMockRule1.verify(\n\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2274\")\n\tpublic void uris_https() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessage\";\n\t\tString responseBody = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t\t\t.withBody( responseBody ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, httpsUrisFor( wireMockRule1 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\n\t\t\twireMockRule1.verify(\n\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\tpublic void error() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString responseBody = \"{ \\\"error\\\": \\\"ErrorMessageExplainingTheError\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn(\n\t\t\t\t\t\telasticsearchResponse().withStatus( 500 )\n\t\t\t\t\t\t.withBody( responseBody )\n\t\t\t\t) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient() ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 500 );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\t\t}\n\t}\n\n\t@Test\n\tpublic void unparseable() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn(\n\t\t\t\t\t\telasticsearchResponse()\n\t\t\t\t\t\t.withBody( \"'unparseable\" )\n\t\t\t\t) );\n\n\t\tassertThatThrownBy( () -> {\n\t\t\ttry ( ElasticsearchClientImplementor client = createClient() ) {\n\t\t\t\tdoPost( client, \"/myIndex/myType\", payload );\n\t\t\t}\n\t\t} )\n\t\t\t\t.isInstanceOf( AssertionFailure.class )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( CompletionException.class )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"HSEARCH400089\" )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( JsonSyntaxException.class );\n\t}\n\n\t@Test\n\tpublic void timeout_read() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn(\n\t\t\t\t\t\telasticsearchResponse()\n\t\t\t\t\t\t.withFixedDelay( 2000 )\n\t\t\t\t) );\n\n\t\tassertThatThrownBy( () -> {\n\t\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\t\tproperties -> {\n\t\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.READ_TIMEOUT, \"1000\" );\n\t\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.REQUEST_TIMEOUT, \"99999\" );\n\t\t\t\t\t}\n\t\t\t) ) {\n\t\t\t\tdoPost( client, \"/myIndex/myType\", payload );\n\t\t\t}\n\t\t} )\n\t\t\t\t.isInstanceOf( AssertionFailure.class )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( CompletionException.class )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( IOException.class );\n\t}\n\n\t@Test\n\tpublic void timeout_request() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn(\n\t\t\t\t\t\telasticsearchResponse()\n\t\t\t\t\t\t.withFixedDelay( 2000 )\n\t\t\t\t) );\n\n\t\tassertThatThrownBy( () -> {\n\t\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\t\tproperties -> {\n\t\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.READ_TIMEOUT, \"99999\" );\n\t\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.REQUEST_TIMEOUT, \"1000\" );\n\t\t\t\t\t}\n\t\t\t) ) {\n\t\t\t\tdoPost( client, \"/myIndex/myType\", payload );\n\t\t\t}\n\t\t} )\n\t\t\t\t.isInstanceOf( AssertionFailure.class )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( CompletionException.class )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll( \"Request execution exceeded the timeout of 1s, 0ms and 0ns\",\n\t\t\t\t\t\t\"Request was POST /myIndex/myType with parameters {}\" );\n\t}\n\n\t/**\n\t * Verify that by default, even when the client is clogged (many pending requests),\n\t * we don't trigger timeouts just because requests spend a long time waiting;\n\t * timeouts are only related to how long the *server* takes to answer.\n\t */\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2836\")\n\tpublic void cloggedClient_noTimeout_read() {\n\t\tSubTest.expectSuccessAfterRetry(\n\t\t\t\t// This test is flaky, for some reason once in a while wiremock takes a very long time to answer\n\t\t\t\t// even though no delay was configured.\n\t\t\t\t// The exact reason is unknown though, so just try multiple times...\n\t\t\t\tthis::try_cloggedClient_noTimeout_read\n\t\t);\n\t}\n\n\tprivate void try_cloggedClient_noTimeout_read() throws Exception {\n\t\twireMockRule1.resetRequests();\n\t\twireMockRule1.resetMappings();\n\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/long\" ) )\n\t\t\t\t.willReturn( elasticsearchResponse()\n\t\t\t\t\t\t.withFixedDelay( 300 /* 300ms => should not time out, but will still clog up the client */ ) ) );\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withFixedDelay( 100 /* 100ms => should not time out */ ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1 ) );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.MAX_CONNECTIONS_PER_ROUTE, \"1\" );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.READ_TIMEOUT, \"1000\" /* 1s */ );\n\t\t\t\t}\n\t\t) ) {\n\t\t\t// Clog up the client: put many requests in the queue, to be executed asynchronously,\n\t\t\t// so that we're sure the next request will have to wait in the queue\n\t\t\t// for more that the configured timeout before it ends up being executed.\n\t\t\tfor ( int i = 0 ; i < 10 ; ++i ) {\n\t\t\t\tclient.submit( buildRequest( ElasticsearchRequest.post(), \"/long\", payload ) );\n\t\t\t}\n\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( 1, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}\n\n\t/**\n\t * Verify that when a request timeout is set, and when the client is clogged (many pending requests),\n\t * we do trigger timeouts just because requests spend a long time waiting.\n\t */\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2836\")\n\tpublic void cloggedClient_timeout_request() {\n\t\tSubTest.expectSuccessAfterRetry(\n\t\t\t\t// This test is flaky, for some reason once in a while wiremock takes a very long time to answer\n\t\t\t\t// even though no delay was configured.\n\t\t\t\t// The exact reason is unknown though, so just try multiple times...\n\t\t\t\tthis::try_cloggedClient_timeout_request\n\t\t);\n\t}\n\n\tprivate void try_cloggedClient_timeout_request() throws Exception {\n\t\twireMockRule1.resetRequests();\n\t\twireMockRule1.resetMappings();\n\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/long\" ) )\n\t\t\t\t.willReturn( elasticsearchResponse()\n\t\t\t\t\t\t.withFixedDelay( 300 /* 300ms => should not time out, but will still clog up the client */ ) ) );\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withFixedDelay( 100 /* 100ms => should not time out */ ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1 ) );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.MAX_CONNECTIONS_PER_ROUTE, \"1\" );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.REQUEST_TIMEOUT, \"1000\" /* 1s */ );\n\t\t\t\t}\n\t\t) ) {\n\t\t\t// Clog up the client: put many requests in the queue, to be executed asynchronously,\n\t\t\t// so that we're sure the next request will have to wait in the queue\n\t\t\t// for more that the configured timeout before it ends up being executed.\n\t\t\tfor ( int i = 0 ; i < 10 ; ++i ) {\n\t\t\t\tclient.submit( buildRequest( ElasticsearchRequest.post(), \"/long\", payload ) );\n\t\t\t}\n\n\t\t\tassertThatThrownBy( () -> {\n\t\t\t\t\tdoPost( client, \"/myIndex/myType\", payload );\n\t\t\t} )\n\t\t\t\t\t.isInstanceOf( AssertionFailure.class )\n\t\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t.isInstanceOf( CompletionException.class )\n\t\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t.hasMessageContainingAll( \"Request execution exceeded the timeout of 1s, 0ms and 0ns\",\n\t\t\t\t\t\t\t\"Request was POST /myIndex/myType with parameters {}\" );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2235\")\n\tpublic void multipleHosts() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1, wireMockRule2 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}\n\n\t@Test\n\tpublic void multipleURIs() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, httpsUrisFor( wireMockRule1, wireMockRule2 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2469\")\n\tpublic void multipleHosts_failover_serverError() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 503 ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1, wireMockRule2 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 1, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\n\t\t\twireMockRule1.resetRequests();\n\t\t\twireMockRule2.resetRequests();\n\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t// Must not use the failing node anymore\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 0, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2469\")\n\tpublic void multipleHosts_failover_timeout() {\n\t\tSubTest.expectSuccessAfterRetry(\n\t\t\t\t// This test is flaky, for some reason once in a while wiremock takes a very long time to answer\n\t\t\t\t// even though no delay was configured.\n\t\t\t\t// The exact reason is unknown though, so just try multiple times...\n\t\t\t\tthis::try_multipleHosts_failover_timeout\n\t\t);\n\t}\n\n\tprivate void try_multipleHosts_failover_timeout() throws Exception {\n\t\twireMockRule1.resetRequests();\n\t\twireMockRule2.resetRequests();\n\t\twireMockRule1.resetMappings();\n\t\twireMockRule2.resetMappings();\n\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withFixedDelay( 5_000 /* 5s => will time out */ ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1, wireMockRule2 ) );\n\t\t\t\t\t// Use a timeout much higher than 1s, because wiremock can be really slow...\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.READ_TIMEOUT, \"1000\" /* 1s */ );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 1, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\n\t\t\twireMockRule1.resetRequests();\n\t\t\twireMockRule2.resetRequests();\n\n\t\t\t/*\n\t\t\t * Remove the failure in the previously failing node,\n\t\t\t * so that we can detect if requests are sent to this node.\n\t\t\t */\n\t\t\twireMockRule2.resetMappings();\n\t\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t// Must not use the failing node anymore\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 0, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2469\")\n\tpublic void multipleHosts_failover_fault() {\n\t\tSubTest.expectSuccessAfterRetry(\n\t\t\t\t// This test is flaky, for some reason once in a while wiremock takes a very long time to answer\n\t\t\t\t// even though no delay was configured.\n\t\t\t\t// The exact reason is unknown though, so just try multiple times...\n\t\t\t\tthis::try_multipleHosts_failover_fault\n\t\t);\n\t}\n\n\tprivate void try_multipleHosts_failover_fault() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withFault( Fault.MALFORMED_RESPONSE_CHUNK ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1, wireMockRule2 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 1, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\n\t\t\twireMockRule1.resetRequests();\n\t\t\twireMockRule2.resetRequests();\n\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t// Must not use the failing node anymore\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 0, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2449\")\n\tpublic void discovery_http() throws Exception {\n\t\tString nodesInfoResult = dummyNodeInfoResponse( wireMockRule1.port(), wireMockRule2.port() );\n\n\t\twireMockRule1.stubFor( get( urlPathMatching( \"/_nodes.*\" ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withBody( nodesInfoResult ) ) );\n\t\twireMockRule2.stubFor( get( urlPathMatching( \"/_nodes.*\" ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withBody( nodesInfoResult ) ) );\n\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.DISCOVERY_ENABLED, \"true\" );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.DISCOVERY_REFRESH_INTERVAL, \"1\" );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t/*\n\t\t\t * Send requests repeatedly until both hosts have been targeted.\n\t\t\t * This should happen pretty early (as soon as we sent two requests, actually),\n\t\t\t * but there is always the risk that the sniffer would send a request\n\t\t\t * between our own requests, effectively making our own requests target the same host\n\t\t\t * (since the hosts are each targeted in turn).\n\t\t\t */\n\t\t\tawait().untilAsserted( () -> {\n\t\t\t\tElasticsearchResponse newResult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\t\tassertThat( newResult.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t\twireMockRule2.verify(\n\t\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t);\n\t\t\t\twireMockRule2.verify(\n\t\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t);\n\t\t\t} );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2736\")\n\tpublic void discovery_https() throws Exception {\n\t\tString nodesInfoResult = dummyNodeInfoResponse( wireMockRule1.httpsPort(), wireMockRule2.httpsPort() );\n\n\t\twireMockRule1.stubFor( get( urlPathMatching( \"/_nodes.*\" ) )\n\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withBody( nodesInfoResult ) ) );\n\t\twireMockRule2.stubFor( get( urlPathMatching( \"/_nodes.*\" ) )\n\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withBody( nodesInfoResult ) ) );\n\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpsHostAndPortFor( wireMockRule1 ) );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, \"https\" );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.DISCOVERY_ENABLED, \"true\" );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.DISCOVERY_REFRESH_INTERVAL, \"1\" );\n\t\t\t\t}\n\t\t) ) {\n\t\t\t/*\n\t\t\t * Send requests repeatedly until both hosts have been targeted.\n\t\t\t * This should happen pretty early (as soon as we sent two requests, actually),\n\t\t\t * but there is always the risk that the sniffer would send a request\n\t\t\t * between our own requests, effectively making our own requests target the same host\n\t\t\t * (since the hosts are each targeted in turn).\n\t\t\t */\n\t\t\tawait().untilAsserted( () -> {\n\t\t\t\tElasticsearchResponse newResult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\t\tassertThat( newResult.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t\twireMockRule2.verify(\n\t\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t);\n\t\t\t\twireMockRule2.verify(\n\t\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t);\n\t\t\t} );\n\t\t}\n\t}\n\n\tprivate static RequestMatcherExtension httpProtocol() {\n\t\treturn protocol( \"http\" );\n\t}\n\n\tprivate static RequestMatcherExtension httpsProtocol() {\n\t\treturn protocol( \"https\" );\n\t}\n\n\tprivate static RequestMatcherExtension protocol(String protocol) {\n\t\treturn new RequestMatcherExtension() {\n\t\t\t@Override\n\t\t\tpublic MatchResult match(Request request, Parameters parameters) {\n\t\t\t\treturn MatchResult.of( protocol.equals( request.getScheme() ) );\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic String getName() {\n\t\t\t\treturn \"expected protocol: \" + protocol;\n\t\t\t}\n\t\t};\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2453\")\n\t@Category(RequiresNoAutomaticAuthenticationHeader.class)\n\tpublic void authentication() throws Exception {\n\t\tString username = \"ironman\";\n\t\tString password = \"j@rV1s\";\n\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType/_search\" ) )\n\t\t\t\t.withBasicAuth( username, password )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.USERNAME, username );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PASSWORD, password );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType/_search\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2453\")\n\tpublic void authentication_error() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessageUnauthorized\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType/_search\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn(\n\t\t\t\t\t\telasticsearchResponse().withStatus( 401 /* Unauthorized */ )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient() ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType/_search\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 401 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2453\")\n\tpublic void authentication_http_password() throws Exception {\n\t\tString username = \"ironman\";\n\t\tString password = \"j@rV1s\";\n\n\t\tlogged.expectEvent( Level.WARN, \"The password will be sent in clear text over the network\" );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.USERNAME, username );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PASSWORD, password );\n\t\t\t\t}\n\t\t) ) {\n\t\t\t// Nothing to do here\n\t\t}\n\t}\n\n\t@Test\n\tpublic void uriAndProtocol() {\n\t\tConsumer<BiConsumer<String, Object>> additionalProperties = properties -> {\n\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null ); // remove HOSTS, keeping PROTOCOL\n\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, \"http://is-not-called:12345\" );\n\t\t};\n\n\t\tassertThatThrownBy( () -> createClient( additionalProperties ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\"Invalid target hosts configuration\",\n\t\t\t\t\t\t\"both the 'uris' property and the 'protocol' property are set\",\n\t\t\t\t\t\t\"Uris: '[http://is-not-called:12345]'\", \"Protocol: 'http'\",\n\t\t\t\t\t\t\"Either set the protocol and hosts simultaneously using the 'uris' property\",\n\t\t\t\t\t\t\"or set them separately using the 'protocol' property and the 'hosts' property\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void uriAndHosts() {\n\t\tConsumer<BiConsumer<String, Object>> additionalProperties = properties -> {\n\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null ); // remove PROTOCOL, keeping HOSTS\n\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, \"http://is-not-called:12345\" );\n\t\t};\n\n\t\tassertThatThrownBy( () -> createClient( additionalProperties ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\"Invalid target hosts configuration\",\n\t\t\t\t\t\t\"both the 'uris' property and the 'hosts' property are set\",\n\t\t\t\t\t\t\"Uris: '[http://is-not-called:12345]'\", \"Hosts: '[\", // host and port are dynamic\n\t\t\t\t\t\t\"Either set the protocol and hosts simultaneously using the 'uris' property\",\n\t\t\t\t\t\t\"or set them separately using the 'protocol' property and the 'hosts' property\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void differentProtocolsOnUris() {\n\t\tConsumer<BiConsumer<String, Object>> additionalProperties = properties -> {\n\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null );\n\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null );\n\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, \"http://is-not-called:12345, https://neather-is:12345\" );\n\t\t};\n\n\t\tassertThatThrownBy( () -> createClient( additionalProperties ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\"Invalid target hosts configuration: the 'uris' use different protocols (http, https)\",\n\t\t\t\t\t\t\"All URIs must use the same protocol\",\n\t\t\t\t\t\t\"Uris: '[http://is-not-called:12345, https://neather-is:12345]'\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void emptyListOfUris() {\n\t\tConsumer<BiConsumer<String, Object>> additionalProperties = properties -> {\n\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null );\n\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null );\n\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, Collections.emptyList() );\n\t\t};\n\n\t\tassertThatThrownBy( () -> createClient( additionalProperties ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining(\n\t\t\t\t\t\t\"Invalid target hosts configuration: the list of URIs must not be empty\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void emptyListOfHosts() {\n\t\tConsumer<BiConsumer<String, Object>> additionalProperties = properties -> {\n\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, Collections.emptyList() );\n\t\t};\n\n\t\tassertThatThrownBy( () -> createClient( additionalProperties ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining(\n\t\t\t\t\t\t\"Invalid target hosts configuration: the list of hosts must not be empty\"\n\t\t\t\t);\n\t}\n\n\tprivate ElasticsearchClientImplementor createClient() {\n\t\treturn createClient( ignored -> { } );\n\t}\n\n\tprivate ElasticsearchClientImplementor createClient(Consumer<BiConsumer<String, Object>> additionalProperties) {\n\t\tMap<String, ?> defaultBackendProperties =\n\t\t\t\tnew ElasticsearchTckBackendHelper().createDefaultBackendSetupStrategy()\n\t\t\t\t\t\t.createBackendConfigurationProperties( testConfigurationProvider );\n\n\t\tMap<String, Object> clientProperties = new HashMap<>( defaultBackendProperties );\n\t\t// Redirect requests to Wiremock (rule 1 only by default)\n\t\tclientProperties.put( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1 ) );\n\t\tclientProperties.put( ElasticsearchBackendSettings.PROTOCOL, \"http\" );\n\t\t// Per-test overrides\n\t\tadditionalProperties.accept( clientProperties::put );\n\t\tConfigurationPropertySource clientPropertySource = ConfigurationPropertySource.fromMap( clientProperties );\n\n\t\tMap<String, Object> beanResolverConfiguration = new HashMap<>();\n\t\t// Accept Wiremock's self-signed SSL certificates\n\t\tbeanResolverConfiguration.put(\n\t\t\t\tEngineSpiSettings.Radicals.BEAN_CONFIGURERS,\n\t\t\t\tCollections.singletonList( elasticsearchSslBeanConfigurer() )\n\t\t);\n\n\t\tBeanResolver beanResolver = testConfigurationProvider.createBeanResolverForTest(\n\t\t\t\tConfigurationPropertySource.fromMap( beanResolverConfiguration )\n\n\t\t);\n\t\ttry ( BeanHolder<ElasticsearchClientFactory> factoryHolder =\n\t\t\t\tbeanResolver.resolve( ElasticsearchClientFactoryImpl.REFERENCE ) ) {\n\t\t\treturn factoryHolder.get().create( beanResolver, clientPropertySource,\n\t\t\t\t\tthreadPoolProvider.threadProvider(), \"Client\",\n\t\t\t\t\ttimeoutExecutorService,\n\t\t\t\t\tGsonProvider.create( GsonBuilder::new, true ) );\n\t\t}\n\t}\n\n\tprivate ElasticsearchResponse doPost(ElasticsearchClient client, String path, String payload) {\n\t\ttry {\n\t\t\treturn client.submit( buildRequest( ElasticsearchRequest.post(), path, payload ) ).join();\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow new AssertionFailure( \"Unexpected exception during POST: \" + e.getMessage(), e );\n\t\t}\n\t}\n\n\tprivate ElasticsearchRequest buildRequest(ElasticsearchRequest.Builder builder, String path, String payload) {\n\t\tfor ( String pathComponent : path.split( \"/\" ) ) {\n\t\t\tif ( !pathComponent.isEmpty() ) {\n\t\t\t\tURLEncodedString fromString = URLEncodedString.fromString( pathComponent );\n\t\t\t\tbuilder = builder.pathComponent( fromString );\n\t\t\t}\n\t\t}\n\t\tif ( payload != null ) {\n\t\t\tbuilder = builder.body( JSON_PARSER.parse( payload ).getAsJsonObject() );\n\t\t}\n\t\treturn builder.build();\n\t}\n\n\tprivate static String httpHostAndPortFor(WireMockRule ... rules) {\n\t\treturn Arrays.stream( rules )\n\t\t\t\t.map( rule -> \"localhost:\" + rule.port() )\n\t\t\t\t.collect( Collectors.joining( \",\" ) );\n\t}\n\n\tprivate static String httpsHostAndPortFor(WireMockRule ... rules) {\n\t\treturn Arrays.stream( rules )\n\t\t\t\t.map( rule -> \"localhost:\" + rule.httpsPort() )\n\t\t\t\t.collect( Collectors.joining( \",\" ) );\n\t}\n\n\tprivate static String httpUrisFor(WireMockRule ... rules) {\n\t\treturn Arrays.stream( rules )\n\t\t\t\t.map( rule -> \"http://localhost:\" + rule.port() )\n\t\t\t\t.collect( Collectors.joining( \",\" ) );\n\t}\n\n\tprivate static String httpsUrisFor(WireMockRule ... rules) {\n\t\treturn Arrays.stream( rules )\n\t\t\t\t.map( rule -> \"https://localhost:\" + rule.httpsPort() )\n\t\t\t\t.collect( Collectors.joining( \",\" ) );\n\t}\n\n\tprivate static ResponseDefinitionBuilder elasticsearchResponse() {\n\t\treturn ResponseDefinitionBuilder.okForEmptyJson();\n\t}\n\n\tprivate String dummyNodeInfoResponse(int... ports) {\n\t\tJsonObject body = new JsonObject();\n\t\tbody.addProperty( \"cluster_name\", \"foo-cluster.local\" );\n\n\t\tJsonObject nodes = new JsonObject();\n\t\tbody.add( \"nodes\", nodes );\n\t\tint index = 1;\n\t\tfor ( int port : ports ) {\n\t\t\tnodes.add( \"hJLXmY_NTrCytiIMbX4_\" + index + \"g\", dummyNodeInfo( port ) );\n\t\t\t++index;\n\t\t}\n\n\t\treturn body.toString();\n\t}\n\n\tprivate JsonObject dummyNodeInfo(int port) {\n\t\tJsonObject node = new JsonObject();\n\t\tnode.addProperty( \"name\", \"nodeForPort\" + port );\n\t\tnode.addProperty( \"version\", ElasticsearchTestDialect.getClusterVersion() );\n\n\t\tJsonObject http = new JsonObject();\n\t\tnode.add( \"http\", http );\n\t\thttp.addProperty( \"publish_address\", \"127.0.0.1:\" + port );\n\t\tJsonArray boundAddresses = new JsonArray();\n\t\thttp.add( \"bound_address\", boundAddresses );\n\t\tboundAddresses.add( \"[::]:\" + port );\n\t\tboundAddresses.add( \"127.0.0.1:\" + port );\n\n\t\tJsonArray roles = new JsonArray();\n\t\tnode.add( \"roles\", roles );\n\t\troles.add( \"ingest\" );\n\t\troles.add( \"master\" );\n\t\troles.add( \"data\" );\n\t\troles.add( \"ml\" );\n\n\t\tnode.add( \"plugins\", new JsonObject() );\n\n\t\treturn node;\n\t}\n\n\tprivate static BeanConfigurer elasticsearchSslBeanConfigurer() {\n\t\treturn context -> {\n\t\t\tcontext.define(\n\t\t\t\t\tElasticsearchHttpClientConfigurer.class,\n\t\t\t\t\tBeanReference.ofInstance( new ElasticsearchHttpClientConfigurer() {\n\t\t\t\t\t\t@Override\n\t\t\t\t\t\tpublic void configure(ElasticsearchHttpClientConfigurationContext context) {\n\t\t\t\t\t\t\tcontext.clientBuilder().setSSLHostnameVerifier( NoopHostnameVerifier.INSTANCE );\n\t\t\t\t\t\t\tcontext.clientBuilder().setSSLContext( buildAllowAnythingSSLContext() );\n\t\t\t\t\t\t}\n\t\t\t\t\t} )\n\t\t\t);\n\t\t};\n\t}\n\n\tprivate static SSLContext buildAllowAnythingSSLContext() {\n\t\ttry {\n\t\t\treturn SSLContexts.custom().loadTrustMaterial( null, new TrustSelfSignedStrategy() ).build();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new AssertionFailure( \"Unexpected exception\", e );\n\t\t}\n\t}\n\n}\n",
        "filePathAfter": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/client/ElasticsearchClientFactoryImplIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.elasticsearch.client;\n\nimport static com.github.tomakehurst.wiremock.client.WireMock.equalToJson;\nimport static com.github.tomakehurst.wiremock.client.WireMock.get;\nimport static com.github.tomakehurst.wiremock.client.WireMock.post;\nimport static com.github.tomakehurst.wiremock.client.WireMock.postRequestedFor;\nimport static com.github.tomakehurst.wiremock.client.WireMock.urlPathMatching;\nimport static com.github.tomakehurst.wiremock.core.WireMockConfiguration.wireMockConfig;\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatThrownBy;\nimport static org.awaitility.Awaitility.await;\nimport static org.hibernate.search.util.impl.test.JsonHelper.assertJsonEquals;\n\nimport java.io.IOException;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.concurrent.CompletionException;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.function.BiConsumer;\nimport java.util.function.Consumer;\nimport java.util.stream.Collectors;\nimport javax.net.ssl.SSLContext;\n\nimport org.hibernate.search.backend.elasticsearch.cfg.ElasticsearchBackendSettings;\nimport org.hibernate.search.backend.elasticsearch.client.impl.ElasticsearchClientFactoryImpl;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchClient;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchClientFactory;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchClientImplementor;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchHttpClientConfigurationContext;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchHttpClientConfigurer;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchRequest;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchResponse;\nimport org.hibernate.search.backend.elasticsearch.gson.spi.GsonProvider;\nimport org.hibernate.search.backend.elasticsearch.util.spi.URLEncodedString;\nimport org.hibernate.search.engine.cfg.spi.ConfigurationPropertySource;\nimport org.hibernate.search.engine.cfg.spi.EngineSpiSettings;\nimport org.hibernate.search.engine.environment.bean.BeanHolder;\nimport org.hibernate.search.engine.environment.bean.BeanReference;\nimport org.hibernate.search.engine.environment.bean.BeanResolver;\nimport org.hibernate.search.engine.environment.bean.spi.BeanConfigurer;\nimport org.hibernate.search.engine.environment.thread.impl.EmbeddedThreadProvider;\nimport org.hibernate.search.engine.environment.thread.impl.ThreadPoolProviderImpl;\nimport org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.categories.RequiresNoAutomaticAuthenticationHeader;\nimport org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util.ElasticsearchTckBackendHelper;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.backend.elasticsearch.dialect.ElasticsearchTestDialect;\nimport org.hibernate.search.util.impl.integrationtest.common.TestConfigurationProvider;\nimport org.hibernate.search.util.impl.test.annotation.PortedFromSearch5;\nimport org.hibernate.search.util.impl.test.annotation.TestForIssue;\nimport org.hibernate.search.util.impl.test.rule.ExpectedLog4jLog;\nimport org.hibernate.search.util.impl.test.rule.Retry;\n\nimport org.junit.After;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.experimental.categories.Category;\n\nimport com.github.tomakehurst.wiremock.client.ResponseDefinitionBuilder;\nimport com.github.tomakehurst.wiremock.extension.Parameters;\nimport com.github.tomakehurst.wiremock.http.Fault;\nimport com.github.tomakehurst.wiremock.http.Request;\nimport com.github.tomakehurst.wiremock.junit.WireMockRule;\nimport com.github.tomakehurst.wiremock.matching.MatchResult;\nimport com.github.tomakehurst.wiremock.matching.RequestMatcherExtension;\nimport com.google.gson.GsonBuilder;\nimport com.google.gson.JsonArray;\nimport com.google.gson.JsonObject;\nimport com.google.gson.JsonParser;\nimport com.google.gson.JsonSyntaxException;\nimport org.apache.http.conn.ssl.NoopHostnameVerifier;\nimport org.apache.http.conn.ssl.TrustSelfSignedStrategy;\nimport org.apache.http.ssl.SSLContexts;\nimport org.apache.logging.log4j.Level;\nimport org.assertj.core.api.InstanceOfAssertFactories;\n\n@PortedFromSearch5(original = \"org.hibernate.search.elasticsearch.test.DefaultElasticsearchClientFactoryTest\")\npublic class ElasticsearchClientFactoryImplIT {\n\n\tprivate static final JsonParser JSON_PARSER = new JsonParser();\n\n\t// Some tests in here are flaky, for some reason once in a while wiremock takes a very long time to answer\n\t// even though no delay was configured.\n\t// The exact reason is unknown though, so just try multiple times...\n\t@Rule\n\tpublic Retry retry;\n\n\tprivate final ExpectedLog4jLog logged = ExpectedLog4jLog.create();\n\n\tprivate final WireMockRule wireMockRule1 = new WireMockRule( wireMockConfig().port( 0 )\n\t\t\t.httpsPort( 0 ) /* Automatic port selection */ );\n\n\tprivate final WireMockRule wireMockRule2 = new WireMockRule( wireMockConfig().port( 0 ).httpsPort( 0 ) /* Automatic port selection */ );\n\n\tprivate final TestConfigurationProvider testConfigurationProvider = new TestConfigurationProvider();\n\n\tprivate final ThreadPoolProviderImpl threadPoolProvider = new ThreadPoolProviderImpl(\n\t\t\tBeanHolder.of( new EmbeddedThreadProvider( ElasticsearchClientFactoryImplIT.class.getName() + \": \" ) )\n\t);\n\n\tprivate final ScheduledExecutorService timeoutExecutorService =\n\t\t\tthreadPoolProvider.newScheduledExecutor( 1, \"Timeout - \" );\n\n\tpublic ElasticsearchClientFactoryImplIT() {\n\t\tthis.retry = Retry.withOtherRules( logged, wireMockRule1, wireMockRule2, testConfigurationProvider );\n\t}\n\n\t@After\n\tpublic void cleanup() {\n\t\ttimeoutExecutorService.shutdownNow();\n\t\tthreadPoolProvider.close();\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2274\")\n\tpublic void simple_http() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessage\";\n\t\tString responseBody = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t\t\t.withBody( responseBody ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient() ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\n\t\t\twireMockRule1.verify(\n\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-4099\")\n\tpublic void uris_http() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessage\";\n\t\tString responseBody = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t\t\t.withBody( responseBody ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, httpUrisFor( wireMockRule1 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\n\t\t\twireMockRule1.verify(\n\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-4051\")\n\tpublic void pathPrefix_http() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessage\";\n\t\tString responseBody = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/bla/bla/bla/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t\t\t.withBody( responseBody ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PATH_PREFIX, \"bla/bla/bla\" );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\n\t\t\twireMockRule1.verify(\n\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/bla/bla/bla/myIndex/myType\" ) )\n\t\t\t\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-4099\")\n\tpublic void pathPrefix_uris() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessage\";\n\t\tString responseBody = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/bla/bla/bla/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t\t\t.withBody( responseBody ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PATH_PREFIX, \"bla/bla/bla\" );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, httpUrisFor( wireMockRule1 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\n\t\t\twireMockRule1.verify(\n\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/bla/bla/bla/myIndex/myType\" ) )\n\t\t\t\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2274\")\n\tpublic void simple_https() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessage\";\n\t\tString responseBody = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t\t\t.withBody( responseBody ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpsHostAndPortFor( wireMockRule1 ) );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, \"https\" );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\n\t\t\twireMockRule1.verify(\n\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2274\")\n\tpublic void uris_https() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessage\";\n\t\tString responseBody = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t\t\t.withBody( responseBody ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, httpsUrisFor( wireMockRule1 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\n\t\t\twireMockRule1.verify(\n\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\tpublic void error() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString responseBody = \"{ \\\"error\\\": \\\"ErrorMessageExplainingTheError\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn(\n\t\t\t\t\t\telasticsearchResponse().withStatus( 500 )\n\t\t\t\t\t\t.withBody( responseBody )\n\t\t\t\t) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient() ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 500 );\n\t\t\tassertJsonEquals( responseBody, result.body().toString() );\n\t\t}\n\t}\n\n\t@Test\n\tpublic void unparseable() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn(\n\t\t\t\t\t\telasticsearchResponse()\n\t\t\t\t\t\t.withBody( \"'unparseable\" )\n\t\t\t\t) );\n\n\t\tassertThatThrownBy( () -> {\n\t\t\ttry ( ElasticsearchClientImplementor client = createClient() ) {\n\t\t\t\tdoPost( client, \"/myIndex/myType\", payload );\n\t\t\t}\n\t\t} )\n\t\t\t\t.isInstanceOf( AssertionFailure.class )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( CompletionException.class )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"HSEARCH400089\" )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( JsonSyntaxException.class );\n\t}\n\n\t@Test\n\tpublic void timeout_read() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn(\n\t\t\t\t\t\telasticsearchResponse()\n\t\t\t\t\t\t.withFixedDelay( 2000 )\n\t\t\t\t) );\n\n\t\tassertThatThrownBy( () -> {\n\t\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\t\tproperties -> {\n\t\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.READ_TIMEOUT, \"1000\" );\n\t\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.REQUEST_TIMEOUT, \"99999\" );\n\t\t\t\t\t}\n\t\t\t) ) {\n\t\t\t\tdoPost( client, \"/myIndex/myType\", payload );\n\t\t\t}\n\t\t} )\n\t\t\t\t.isInstanceOf( AssertionFailure.class )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( CompletionException.class )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( IOException.class );\n\t}\n\n\t@Test\n\tpublic void timeout_request() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn(\n\t\t\t\t\t\telasticsearchResponse()\n\t\t\t\t\t\t.withFixedDelay( 2000 )\n\t\t\t\t) );\n\n\t\tassertThatThrownBy( () -> {\n\t\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\t\tproperties -> {\n\t\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.READ_TIMEOUT, \"99999\" );\n\t\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.REQUEST_TIMEOUT, \"1000\" );\n\t\t\t\t\t}\n\t\t\t) ) {\n\t\t\t\tdoPost( client, \"/myIndex/myType\", payload );\n\t\t\t}\n\t\t} )\n\t\t\t\t.isInstanceOf( AssertionFailure.class )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( CompletionException.class )\n\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll( \"Request execution exceeded the timeout of 1s, 0ms and 0ns\",\n\t\t\t\t\t\t\"Request was POST /myIndex/myType with parameters {}\" );\n\t}\n\n\t/**\n\t * Verify that by default, even when the client is clogged (many pending requests),\n\t * we don't trigger timeouts just because requests spend a long time waiting;\n\t * timeouts are only related to how long the *server* takes to answer.\n\t */\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2836\")\n\tpublic void cloggedClient_noTimeout_read() {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/long\" ) )\n\t\t\t\t.willReturn( elasticsearchResponse()\n\t\t\t\t\t\t.withFixedDelay( 300 /* 300ms => should not time out, but will still clog up the client */ ) ) );\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withFixedDelay( 100 /* 100ms => should not time out */ ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1 ) );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.MAX_CONNECTIONS_PER_ROUTE, \"1\" );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.READ_TIMEOUT, \"1000\" /* 1s */ );\n\t\t\t\t}\n\t\t) ) {\n\t\t\t// Clog up the client: put many requests in the queue, to be executed asynchronously,\n\t\t\t// so that we're sure the next request will have to wait in the queue\n\t\t\t// for more that the configured timeout before it ends up being executed.\n\t\t\tfor ( int i = 0 ; i < 10 ; ++i ) {\n\t\t\t\tclient.submit( buildRequest( ElasticsearchRequest.post(), \"/long\", payload ) );\n\t\t\t}\n\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( 1, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}\n\n\t/**\n\t * Verify that when a request timeout is set, and when the client is clogged (many pending requests),\n\t * we do trigger timeouts just because requests spend a long time waiting.\n\t */\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2836\")\n\tpublic void cloggedClient_timeout_request() {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/long\" ) )\n\t\t\t\t.willReturn( elasticsearchResponse()\n\t\t\t\t\t\t.withFixedDelay( 300 /* 300ms => should not time out, but will still clog up the client */ ) ) );\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 )\n\t\t\t\t\t\t.withFixedDelay( 100 /* 100ms => should not time out */ ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1 ) );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.MAX_CONNECTIONS_PER_ROUTE, \"1\" );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.REQUEST_TIMEOUT, \"1000\" /* 1s */ );\n\t\t\t\t}\n\t\t) ) {\n\t\t\t// Clog up the client: put many requests in the queue, to be executed asynchronously,\n\t\t\t// so that we're sure the next request will have to wait in the queue\n\t\t\t// for more that the configured timeout before it ends up being executed.\n\t\t\tfor ( int i = 0 ; i < 10 ; ++i ) {\n\t\t\t\tclient.submit( buildRequest( ElasticsearchRequest.post(), \"/long\", payload ) );\n\t\t\t}\n\n\t\t\tassertThatThrownBy( () -> {\n\t\t\t\t\tdoPost( client, \"/myIndex/myType\", payload );\n\t\t\t} )\n\t\t\t\t\t.isInstanceOf( AssertionFailure.class )\n\t\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t.isInstanceOf( CompletionException.class )\n\t\t\t\t\t.extracting( Throwable::getCause, InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t.hasMessageContainingAll( \"Request execution exceeded the timeout of 1s, 0ms and 0ns\",\n\t\t\t\t\t\t\t\"Request was POST /myIndex/myType with parameters {}\" );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2235\")\n\tpublic void multipleHosts() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1, wireMockRule2 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}\n\n\t@Test\n\tpublic void multipleURIs() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, httpsUrisFor( wireMockRule1, wireMockRule2 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2469\")\n\tpublic void multipleHosts_failover_serverError() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 503 ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1, wireMockRule2 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 1, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\n\t\t\twireMockRule1.resetRequests();\n\t\t\twireMockRule2.resetRequests();\n\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t// Must not use the failing node anymore\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 0, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2469\")\n\tpublic void multipleHosts_failover_timeout() {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withFixedDelay( 5_000 /* 5s => will time out */ ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1, wireMockRule2 ) );\n\t\t\t\t\t// Use a timeout much higher than 1s, because wiremock can be really slow...\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.READ_TIMEOUT, \"1000\" /* 1s */ );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 1, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\n\t\t\twireMockRule1.resetRequests();\n\t\t\twireMockRule2.resetRequests();\n\n\t\t\t/*\n\t\t\t * Remove the failure in the previously failing node,\n\t\t\t * so that we can detect if requests are sent to this node.\n\t\t\t */\n\t\t\twireMockRule2.resetMappings();\n\t\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t// Must not use the failing node anymore\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 0, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2469\")\n\tpublic void multipleHosts_failover_fault() {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withFault( Fault.MALFORMED_RESPONSE_CHUNK ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1, wireMockRule2 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 1, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\n\t\t\twireMockRule1.resetRequests();\n\t\t\twireMockRule2.resetRequests();\n\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t// Must not use the failing node anymore\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 0, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2449\")\n\tpublic void discovery_http() throws Exception {\n\t\tString nodesInfoResult = dummyNodeInfoResponse( wireMockRule1.port(), wireMockRule2.port() );\n\n\t\twireMockRule1.stubFor( get( urlPathMatching( \"/_nodes.*\" ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withBody( nodesInfoResult ) ) );\n\t\twireMockRule2.stubFor( get( urlPathMatching( \"/_nodes.*\" ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withBody( nodesInfoResult ) ) );\n\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.DISCOVERY_ENABLED, \"true\" );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.DISCOVERY_REFRESH_INTERVAL, \"1\" );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t/*\n\t\t\t * Send requests repeatedly until both hosts have been targeted.\n\t\t\t * This should happen pretty early (as soon as we sent two requests, actually),\n\t\t\t * but there is always the risk that the sniffer would send a request\n\t\t\t * between our own requests, effectively making our own requests target the same host\n\t\t\t * (since the hosts are each targeted in turn).\n\t\t\t */\n\t\t\tawait().untilAsserted( () -> {\n\t\t\t\tElasticsearchResponse newResult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\t\tassertThat( newResult.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t\twireMockRule2.verify(\n\t\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t);\n\t\t\t\twireMockRule2.verify(\n\t\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t\t.andMatching( httpProtocol() )\n\t\t\t\t);\n\t\t\t} );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2736\")\n\tpublic void discovery_https() throws Exception {\n\t\tString nodesInfoResult = dummyNodeInfoResponse( wireMockRule1.httpsPort(), wireMockRule2.httpsPort() );\n\n\t\twireMockRule1.stubFor( get( urlPathMatching( \"/_nodes.*\" ) )\n\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withBody( nodesInfoResult ) ) );\n\t\twireMockRule2.stubFor( get( urlPathMatching( \"/_nodes.*\" ) )\n\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withBody( nodesInfoResult ) ) );\n\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpsHostAndPortFor( wireMockRule1 ) );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, \"https\" );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.DISCOVERY_ENABLED, \"true\" );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.DISCOVERY_REFRESH_INTERVAL, \"1\" );\n\t\t\t\t}\n\t\t) ) {\n\t\t\t/*\n\t\t\t * Send requests repeatedly until both hosts have been targeted.\n\t\t\t * This should happen pretty early (as soon as we sent two requests, actually),\n\t\t\t * but there is always the risk that the sniffer would send a request\n\t\t\t * between our own requests, effectively making our own requests target the same host\n\t\t\t * (since the hosts are each targeted in turn).\n\t\t\t */\n\t\t\tawait().untilAsserted( () -> {\n\t\t\t\tElasticsearchResponse newResult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\t\tassertThat( newResult.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t\twireMockRule2.verify(\n\t\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t);\n\t\t\t\twireMockRule2.verify(\n\t\t\t\t\t\tpostRequestedFor( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t\t\t\t\t.andMatching( httpsProtocol() )\n\t\t\t\t);\n\t\t\t} );\n\t\t}\n\t}\n\n\tprivate static RequestMatcherExtension httpProtocol() {\n\t\treturn protocol( \"http\" );\n\t}\n\n\tprivate static RequestMatcherExtension httpsProtocol() {\n\t\treturn protocol( \"https\" );\n\t}\n\n\tprivate static RequestMatcherExtension protocol(String protocol) {\n\t\treturn new RequestMatcherExtension() {\n\t\t\t@Override\n\t\t\tpublic MatchResult match(Request request, Parameters parameters) {\n\t\t\t\treturn MatchResult.of( protocol.equals( request.getScheme() ) );\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic String getName() {\n\t\t\t\treturn \"expected protocol: \" + protocol;\n\t\t\t}\n\t\t};\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2453\")\n\t@Category(RequiresNoAutomaticAuthenticationHeader.class)\n\tpublic void authentication() throws Exception {\n\t\tString username = \"ironman\";\n\t\tString password = \"j@rV1s\";\n\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType/_search\" ) )\n\t\t\t\t.withBasicAuth( username, password )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.USERNAME, username );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PASSWORD, password );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType/_search\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2453\")\n\tpublic void authentication_error() throws Exception {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\tString statusMessage = \"StatusMessageUnauthorized\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType/_search\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn(\n\t\t\t\t\t\telasticsearchResponse().withStatus( 401 /* Unauthorized */ )\n\t\t\t\t\t\t.withStatusMessage( statusMessage )\n\t\t\t\t) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient() ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType/_search\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 401 );\n\t\t\tassertThat( result.statusMessage() ).as( \"status message\" ).isEqualTo( statusMessage );\n\t\t}\n\t}\n\n\t@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2453\")\n\tpublic void authentication_http_password() throws Exception {\n\t\tString username = \"ironman\";\n\t\tString password = \"j@rV1s\";\n\n\t\tlogged.expectEvent( Level.WARN, \"The password will be sent in clear text over the network\" );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.USERNAME, username );\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.PASSWORD, password );\n\t\t\t\t}\n\t\t) ) {\n\t\t\t// Nothing to do here\n\t\t}\n\t}\n\n\t@Test\n\tpublic void uriAndProtocol() {\n\t\tConsumer<BiConsumer<String, Object>> additionalProperties = properties -> {\n\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null ); // remove HOSTS, keeping PROTOCOL\n\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, \"http://is-not-called:12345\" );\n\t\t};\n\n\t\tassertThatThrownBy( () -> createClient( additionalProperties ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\"Invalid target hosts configuration\",\n\t\t\t\t\t\t\"both the 'uris' property and the 'protocol' property are set\",\n\t\t\t\t\t\t\"Uris: '[http://is-not-called:12345]'\", \"Protocol: 'http'\",\n\t\t\t\t\t\t\"Either set the protocol and hosts simultaneously using the 'uris' property\",\n\t\t\t\t\t\t\"or set them separately using the 'protocol' property and the 'hosts' property\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void uriAndHosts() {\n\t\tConsumer<BiConsumer<String, Object>> additionalProperties = properties -> {\n\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null ); // remove PROTOCOL, keeping HOSTS\n\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, \"http://is-not-called:12345\" );\n\t\t};\n\n\t\tassertThatThrownBy( () -> createClient( additionalProperties ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\"Invalid target hosts configuration\",\n\t\t\t\t\t\t\"both the 'uris' property and the 'hosts' property are set\",\n\t\t\t\t\t\t\"Uris: '[http://is-not-called:12345]'\", \"Hosts: '[\", // host and port are dynamic\n\t\t\t\t\t\t\"Either set the protocol and hosts simultaneously using the 'uris' property\",\n\t\t\t\t\t\t\"or set them separately using the 'protocol' property and the 'hosts' property\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void differentProtocolsOnUris() {\n\t\tConsumer<BiConsumer<String, Object>> additionalProperties = properties -> {\n\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null );\n\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null );\n\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, \"http://is-not-called:12345, https://neather-is:12345\" );\n\t\t};\n\n\t\tassertThatThrownBy( () -> createClient( additionalProperties ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\"Invalid target hosts configuration: the 'uris' use different protocols (http, https)\",\n\t\t\t\t\t\t\"All URIs must use the same protocol\",\n\t\t\t\t\t\t\"Uris: '[http://is-not-called:12345, https://neather-is:12345]'\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void emptyListOfUris() {\n\t\tConsumer<BiConsumer<String, Object>> additionalProperties = properties -> {\n\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, null );\n\t\t\tproperties.accept( ElasticsearchBackendSettings.PROTOCOL, null );\n\t\t\tproperties.accept( ElasticsearchBackendSettings.URIS, Collections.emptyList() );\n\t\t};\n\n\t\tassertThatThrownBy( () -> createClient( additionalProperties ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining(\n\t\t\t\t\t\t\"Invalid target hosts configuration: the list of URIs must not be empty\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void emptyListOfHosts() {\n\t\tConsumer<BiConsumer<String, Object>> additionalProperties = properties -> {\n\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, Collections.emptyList() );\n\t\t};\n\n\t\tassertThatThrownBy( () -> createClient( additionalProperties ) )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining(\n\t\t\t\t\t\t\"Invalid target hosts configuration: the list of hosts must not be empty\"\n\t\t\t\t);\n\t}\n\n\tprivate ElasticsearchClientImplementor createClient() {\n\t\treturn createClient( ignored -> { } );\n\t}\n\n\tprivate ElasticsearchClientImplementor createClient(Consumer<BiConsumer<String, Object>> additionalProperties) {\n\t\tMap<String, ?> defaultBackendProperties =\n\t\t\t\tnew ElasticsearchTckBackendHelper().createDefaultBackendSetupStrategy()\n\t\t\t\t\t\t.createBackendConfigurationProperties( testConfigurationProvider );\n\n\t\tMap<String, Object> clientProperties = new HashMap<>( defaultBackendProperties );\n\t\t// Redirect requests to Wiremock (rule 1 only by default)\n\t\tclientProperties.put( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1 ) );\n\t\tclientProperties.put( ElasticsearchBackendSettings.PROTOCOL, \"http\" );\n\t\t// Per-test overrides\n\t\tadditionalProperties.accept( clientProperties::put );\n\t\tConfigurationPropertySource clientPropertySource = ConfigurationPropertySource.fromMap( clientProperties );\n\n\t\tMap<String, Object> beanResolverConfiguration = new HashMap<>();\n\t\t// Accept Wiremock's self-signed SSL certificates\n\t\tbeanResolverConfiguration.put(\n\t\t\t\tEngineSpiSettings.Radicals.BEAN_CONFIGURERS,\n\t\t\t\tCollections.singletonList( elasticsearchSslBeanConfigurer() )\n\t\t);\n\n\t\tBeanResolver beanResolver = testConfigurationProvider.createBeanResolverForTest(\n\t\t\t\tConfigurationPropertySource.fromMap( beanResolverConfiguration )\n\n\t\t);\n\t\ttry ( BeanHolder<ElasticsearchClientFactory> factoryHolder =\n\t\t\t\tbeanResolver.resolve( ElasticsearchClientFactoryImpl.REFERENCE ) ) {\n\t\t\treturn factoryHolder.get().create( beanResolver, clientPropertySource,\n\t\t\t\t\tthreadPoolProvider.threadProvider(), \"Client\",\n\t\t\t\t\ttimeoutExecutorService,\n\t\t\t\t\tGsonProvider.create( GsonBuilder::new, true ) );\n\t\t}\n\t}\n\n\tprivate ElasticsearchResponse doPost(ElasticsearchClient client, String path, String payload) {\n\t\ttry {\n\t\t\treturn client.submit( buildRequest( ElasticsearchRequest.post(), path, payload ) ).join();\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow new AssertionFailure( \"Unexpected exception during POST: \" + e.getMessage(), e );\n\t\t}\n\t}\n\n\tprivate ElasticsearchRequest buildRequest(ElasticsearchRequest.Builder builder, String path, String payload) {\n\t\tfor ( String pathComponent : path.split( \"/\" ) ) {\n\t\t\tif ( !pathComponent.isEmpty() ) {\n\t\t\t\tURLEncodedString fromString = URLEncodedString.fromString( pathComponent );\n\t\t\t\tbuilder = builder.pathComponent( fromString );\n\t\t\t}\n\t\t}\n\t\tif ( payload != null ) {\n\t\t\tbuilder = builder.body( JSON_PARSER.parse( payload ).getAsJsonObject() );\n\t\t}\n\t\treturn builder.build();\n\t}\n\n\tprivate static String httpHostAndPortFor(WireMockRule ... rules) {\n\t\treturn Arrays.stream( rules )\n\t\t\t\t.map( rule -> \"localhost:\" + rule.port() )\n\t\t\t\t.collect( Collectors.joining( \",\" ) );\n\t}\n\n\tprivate static String httpsHostAndPortFor(WireMockRule ... rules) {\n\t\treturn Arrays.stream( rules )\n\t\t\t\t.map( rule -> \"localhost:\" + rule.httpsPort() )\n\t\t\t\t.collect( Collectors.joining( \",\" ) );\n\t}\n\n\tprivate static String httpUrisFor(WireMockRule ... rules) {\n\t\treturn Arrays.stream( rules )\n\t\t\t\t.map( rule -> \"http://localhost:\" + rule.port() )\n\t\t\t\t.collect( Collectors.joining( \",\" ) );\n\t}\n\n\tprivate static String httpsUrisFor(WireMockRule ... rules) {\n\t\treturn Arrays.stream( rules )\n\t\t\t\t.map( rule -> \"https://localhost:\" + rule.httpsPort() )\n\t\t\t\t.collect( Collectors.joining( \",\" ) );\n\t}\n\n\tprivate static ResponseDefinitionBuilder elasticsearchResponse() {\n\t\treturn ResponseDefinitionBuilder.okForEmptyJson();\n\t}\n\n\tprivate String dummyNodeInfoResponse(int... ports) {\n\t\tJsonObject body = new JsonObject();\n\t\tbody.addProperty( \"cluster_name\", \"foo-cluster.local\" );\n\n\t\tJsonObject nodes = new JsonObject();\n\t\tbody.add( \"nodes\", nodes );\n\t\tint index = 1;\n\t\tfor ( int port : ports ) {\n\t\t\tnodes.add( \"hJLXmY_NTrCytiIMbX4_\" + index + \"g\", dummyNodeInfo( port ) );\n\t\t\t++index;\n\t\t}\n\n\t\treturn body.toString();\n\t}\n\n\tprivate JsonObject dummyNodeInfo(int port) {\n\t\tJsonObject node = new JsonObject();\n\t\tnode.addProperty( \"name\", \"nodeForPort\" + port );\n\t\tnode.addProperty( \"version\", ElasticsearchTestDialect.getClusterVersion() );\n\n\t\tJsonObject http = new JsonObject();\n\t\tnode.add( \"http\", http );\n\t\thttp.addProperty( \"publish_address\", \"127.0.0.1:\" + port );\n\t\tJsonArray boundAddresses = new JsonArray();\n\t\thttp.add( \"bound_address\", boundAddresses );\n\t\tboundAddresses.add( \"[::]:\" + port );\n\t\tboundAddresses.add( \"127.0.0.1:\" + port );\n\n\t\tJsonArray roles = new JsonArray();\n\t\tnode.add( \"roles\", roles );\n\t\troles.add( \"ingest\" );\n\t\troles.add( \"master\" );\n\t\troles.add( \"data\" );\n\t\troles.add( \"ml\" );\n\n\t\tnode.add( \"plugins\", new JsonObject() );\n\n\t\treturn node;\n\t}\n\n\tprivate static BeanConfigurer elasticsearchSslBeanConfigurer() {\n\t\treturn context -> {\n\t\t\tcontext.define(\n\t\t\t\t\tElasticsearchHttpClientConfigurer.class,\n\t\t\t\t\tBeanReference.ofInstance( new ElasticsearchHttpClientConfigurer() {\n\t\t\t\t\t\t@Override\n\t\t\t\t\t\tpublic void configure(ElasticsearchHttpClientConfigurationContext context) {\n\t\t\t\t\t\t\tcontext.clientBuilder().setSSLHostnameVerifier( NoopHostnameVerifier.INSTANCE );\n\t\t\t\t\t\t\tcontext.clientBuilder().setSSLContext( buildAllowAnythingSSLContext() );\n\t\t\t\t\t\t}\n\t\t\t\t\t} )\n\t\t\t);\n\t\t};\n\t}\n\n\tprivate static SSLContext buildAllowAnythingSSLContext() {\n\t\ttry {\n\t\t\treturn SSLContexts.custom().loadTrustMaterial( null, new TrustSelfSignedStrategy() ).build();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new AssertionFailure( \"Unexpected exception\", e );\n\t\t}\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.client.ElasticsearchClientFactoryImplIT#doPost\n methodBody: private ElasticsearchResponse doPost(ElasticsearchClient client, String path, String payload) {\ntryreturn client.submit(buildRequest(ElasticsearchRequest.post(),path,payload)).join();\ncatch(RuntimeException e)throw new AssertionFailure(\"Unexpected exception during POST: \" + e.getMessage(),e);\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.client.ElasticsearchClientFactoryImplIT#elasticsearchResponse\n methodBody: private static ResponseDefinitionBuilder elasticsearchResponse() {\nreturn ResponseDefinitionBuilder.okForEmptyJson();\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.client.ElasticsearchClientFactoryImplIT#createClient\n methodBody: private ElasticsearchClientImplementor createClient(Consumer<BiConsumer<String, Object>> additionalProperties) {\nMap<String,?> defaultBackendProperties=new ElasticsearchTckBackendHelper().createDefaultBackendSetupStrategy().createBackendConfigurationProperties(testConfigurationProvider);\nMap<String,Object> clientProperties=new HashMap<>(defaultBackendProperties);\nclientProperties.put(ElasticsearchBackendSettings.HOSTS,httpHostAndPortFor(wireMockRule1));\nclientProperties.put(ElasticsearchBackendSettings.PROTOCOL,\"http\");\nadditionalProperties.accept(clientProperties::put);\nConfigurationPropertySource clientPropertySource=ConfigurationPropertySource.fromMap(clientProperties);\nMap<String,Object> beanResolverConfiguration=new HashMap<>();\nbeanResolverConfiguration.put(EngineSpiSettings.Radicals.BEAN_CONFIGURERS,Collections.singletonList(elasticsearchSslBeanConfigurer()));\nBeanResolver beanResolver=testConfigurationProvider.createBeanResolverForTest(ConfigurationPropertySource.fromMap(beanResolverConfiguration));\ntry(BeanHolder<ElasticsearchClientFactory> factoryHolder=beanResolver.resolve(ElasticsearchClientFactoryImpl.REFERENCE))return factoryHolder.get().create(beanResolver,clientPropertySource,threadPoolProvider.threadProvider(),\"Client\",timeoutExecutorService,GsonProvider.create(GsonBuilder::new,true));\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\t@TestForIssue(jiraKey = \"HSEARCH-2469\")\n\tpublic void multipleHosts_failover_fault() {\n\t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n\t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n\t\t\t\t.withRequestBody( equalToJson( payload ) )\n\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withFault( Fault.MALFORMED_RESPONSE_CHUNK ) ) );\n\n\t\ttry ( ElasticsearchClientImplementor client = createClient(\n\t\t\t\tproperties -> {\n\t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1, wireMockRule2 ) );\n\t\t\t\t}\n\t\t) ) {\n\t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 1, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\n\t\t\twireMockRule1.resetRequests();\n\t\t\twireMockRule2.resetRequests();\n\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n\t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n\n\t\t\t// Must not use the failing node anymore\n\t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t\twireMockRule2.verify( 0, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n\t\t}\n\t}",
        "diffSourceCode": "-  627: \t\t) ) {\n-  628: \t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n-  629: \t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n-  630: \t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n-  631: \t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n-  632: \n-  633: \t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n-  634: \t\t\twireMockRule2.verify( 1, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n-  635: \n-  636: \t\t\twireMockRule1.resetRequests();\n-  637: \t\t\twireMockRule2.resetRequests();\n-  638: \n-  639: \t\t\t/*\n-  640: \t\t\t * Remove the failure in the previously failing node,\n-  641: \t\t\t * so that we can detect if requests are sent to this node.\n-  642: \t\t\t */\n-  643: \t\t\twireMockRule2.resetMappings();\n-  644: \t\t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n-  645: \t\t\t\t\t.withRequestBody( equalToJson( payload ) )\n-  646: \t\t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n+  627: \t@Test\n+  628: \t@TestForIssue(jiraKey = \"HSEARCH-2469\")\n+  629: \tpublic void multipleHosts_failover_fault() {\n+  630: \t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n+  631: \t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n+  632: \t\t\t\t.withRequestBody( equalToJson( payload ) )\n+  633: \t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n+  634: \t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n+  635: \t\t\t\t.withRequestBody( equalToJson( payload ) )\n+  636: \t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withFault( Fault.MALFORMED_RESPONSE_CHUNK ) ) );\n+  637: \n+  638: \t\ttry ( ElasticsearchClientImplementor client = createClient(\n+  639: \t\t\t\tproperties -> {\n+  640: \t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1, wireMockRule2 ) );\n+  641: \t\t\t\t}\n+  642: \t\t) ) {\n+  643: \t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n+  644: \t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n+  645: \t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n+  646: \t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n   647: \n-  648: \t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n-  649: \t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n-  650: \t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n-  651: \t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n-  652: \n-  653: \t\t\t// Must not use the failing node anymore\n-  654: \t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n-  655: \t\t\twireMockRule2.verify( 0, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n-  656: \t\t}\n-  657: \t}\n+  648: \t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n+  649: \t\t\twireMockRule2.verify( 1, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n+  650: \n+  651: \t\t\twireMockRule1.resetRequests();\n+  652: \t\t\twireMockRule2.resetRequests();\n+  653: \n+  654: \t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n+  655: \t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n+  656: \t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n+  657: \t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n   658: \n-  659: \t@Test\n-  660: \t@TestForIssue(jiraKey = \"HSEARCH-2469\")\n-  661: \tpublic void multipleHosts_failover_fault() {\n-  662: \t\tSubTest.expectSuccessAfterRetry(\n-  663: \t\t\t\t// This test is flaky, for some reason once in a while wiremock takes a very long time to answer\n-  664: \t\t\t\t// even though no delay was configured.\n-  665: \t\t\t\t// The exact reason is unknown though, so just try multiple times...\n-  666: \t\t\t\tthis::try_multipleHosts_failover_fault\n-  667: \t\t);\n-  668: \t}\n-  670: \tprivate void try_multipleHosts_failover_fault() throws Exception {\n-  671: \t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n-  672: \t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n-  673: \t\t\t\t.withRequestBody( equalToJson( payload ) )\n-  674: \t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n-  675: \t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n-  676: \t\t\t\t.withRequestBody( equalToJson( payload ) )\n-  677: \t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withFault( Fault.MALFORMED_RESPONSE_CHUNK ) ) );\n-  678: \n-  679: \t\ttry ( ElasticsearchClientImplementor client = createClient(\n-  680: \t\t\t\tproperties -> {\n-  681: \t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.HOSTS, httpHostAndPortFor( wireMockRule1, wireMockRule2 ) );\n-  682: \t\t\t\t}\n-  683: \t\t) ) {\n-  684: \t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n-  685: \t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n-  686: \t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n-  687: \t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n-  688: \n-  689: \t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n-  690: \t\t\twireMockRule2.verify( 1, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n-  691: \n-  692: \t\t\twireMockRule1.resetRequests();\n-  693: \t\t\twireMockRule2.resetRequests();\n-  694: \n-  695: \t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n-  696: \t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n-  697: \t\t\tresult = doPost( client, \"/myIndex/myType\", payload );\n-  698: \t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n-  699: \n-  700: \t\t\t// Must not use the failing node anymore\n-  701: \t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n-  702: \t\t\twireMockRule2.verify( 0, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n-  703: \t\t}\n-  704: \t}\n+  659: \t\t\t// Must not use the failing node anymore\n+  660: \t\t\twireMockRule1.verify( 2, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n+  661: \t\t\twireMockRule2.verify( 0, postRequestedFor( urlPathMatching( \"/myIndex/myType\" ) ) );\n+  662: \t\t}\n+  663: \t}\n+  664: \n+  665: \t@Test\n+  666: \t@TestForIssue(jiraKey = \"HSEARCH-2449\")\n+  667: \tpublic void discovery_http() throws Exception {\n+  668: \t\tString nodesInfoResult = dummyNodeInfoResponse( wireMockRule1.port(), wireMockRule2.port() );\n+  670: \t\twireMockRule1.stubFor( get( urlPathMatching( \"/_nodes.*\" ) )\n+  671: \t\t\t\t.andMatching( httpProtocol() )\n+  672: \t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withBody( nodesInfoResult ) ) );\n+  673: \t\twireMockRule2.stubFor( get( urlPathMatching( \"/_nodes.*\" ) )\n+  674: \t\t\t\t.andMatching( httpProtocol() )\n+  675: \t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ).withBody( nodesInfoResult ) ) );\n+  676: \n+  677: \t\tString payload = \"{ \\\"foo\\\": \\\"bar\\\" }\";\n+  678: \t\twireMockRule1.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n+  679: \t\t\t\t.withRequestBody( equalToJson( payload ) )\n+  680: \t\t\t\t.andMatching( httpProtocol() )\n+  681: \t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n+  682: \t\twireMockRule2.stubFor( post( urlPathMatching( \"/myIndex/myType\" ) )\n+  683: \t\t\t\t.withRequestBody( equalToJson( payload ) )\n+  684: \t\t\t\t.andMatching( httpProtocol() )\n+  685: \t\t\t\t.willReturn( elasticsearchResponse().withStatus( 200 ) ) );\n+  686: \n+  687: \t\ttry ( ElasticsearchClientImplementor client = createClient(\n+  688: \t\t\t\tproperties -> {\n+  689: \t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.DISCOVERY_ENABLED, \"true\" );\n+  690: \t\t\t\t\tproperties.accept( ElasticsearchBackendSettings.DISCOVERY_REFRESH_INTERVAL, \"1\" );\n+  691: \t\t\t\t}\n+  692: \t\t) ) {\n+  693: \t\t\tElasticsearchResponse result = doPost( client, \"/myIndex/myType\", payload );\n+  694: \t\t\tassertThat( result.statusCode() ).as( \"status code\" ).isEqualTo( 200 );\n+  695: \n+  696: \t\t\t/*\n+  697: \t\t\t * Send requests repeatedly until both hosts have been targeted.\n+  698: \t\t\t * This should happen pretty early (as soon as we sent two requests, actually),\n+  699: \t\t\t * but there is always the risk that the sniffer would send a request\n+  700: \t\t\t * between our own requests, effectively making our own requests target the same host\n+  701: \t\t\t * (since the hosts are each targeted in turn).\n+  702: \t\t\t */\n+  703: \t\t\tawait().untilAsserted( () -> {\n+  704: \t\t\t\tElasticsearchResponse newResult = doPost( client, \"/myIndex/myType\", payload );\n",
        "uniqueId": "280cdd4a54bba61ac2dd9e927d0599ae0a263e8b_659_668__627_663_670_704",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11
    },
    {
        "type": "Move And Rename Method",
        "description": "Move And Rename Method\tpublic forType(typeReference Class<T>) : BeanHolder<T> from class org.hibernate.search.engine.environment.bean.impl.ReflectionBeanProvider to public retrieveUsingConstructor(typeReference Class<T>) : BeanHolder<T> from class org.hibernate.search.engine.environment.bean.impl.BeanResolverImpl",
        "diffLocations": [
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/environment/bean/impl/ReflectionBeanProvider.java",
                "startLine": 35,
                "endLine": 43,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/environment/bean/impl/BeanResolverImpl.java",
                "startLine": 215,
                "endLine": 222,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Override\n\tpublic <T> BeanHolder<T> forType(Class<T> typeReference) {\n\t\ttry {\n\t\t\treturn BeanHolder.of( ClassLoaderHelper.untypedInstanceFromClass( typeReference ) );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow log.unableToCreateBeanUsingReflection( e.getMessage(), e );\n\t\t}\n\t}",
        "filePathBefore": "engine/src/main/java/org/hibernate/search/engine/environment/bean/impl/ReflectionBeanProvider.java",
        "isPureRefactoring": true,
        "commitId": "14e6370a9ab5e7fca20e508f6413bd11ed242cde",
        "packageNameBefore": "org.hibernate.search.engine.environment.bean.impl",
        "classNameBefore": "org.hibernate.search.engine.environment.bean.impl.ReflectionBeanProvider",
        "methodNameBefore": "org.hibernate.search.engine.environment.bean.impl.ReflectionBeanProvider#forType",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.environment.bean.BeanReference#of\n methodBody: static <T> BeanReference<T> of(Class<T> type, String name) {\nif(StringHelper.isNotEmpty(name)){return new TypeAndNameBeanReference<>(type,name);\n}{return new TypeBeanReference<>(type);\n}}\nmethodSignature: org.hibernate.search.engine.logging.impl.Log#unableToCreateBeanUsingReflection\n methodBody: BeanNotFoundException unableToCreateBeanUsingReflection(String causeMessage, @Cause Exception e);",
        "classSignatureBefore": "final class ReflectionBeanProvider implements BeanProvider ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.environment.bean.impl.ReflectionBeanProvider#forType"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.environment.bean.impl.ReflectionBeanProvider"
        ],
        "classSignatureBeforeSet": [
            "final class ReflectionBeanProvider implements BeanProvider "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.environment.bean.impl;\n\n\nimport java.lang.invoke.MethodHandles;\n\nimport org.hibernate.search.engine.environment.bean.BeanHolder;\nimport org.hibernate.search.engine.environment.bean.spi.BeanProvider;\nimport org.hibernate.search.engine.environment.classpath.spi.ClassLoaderHelper;\nimport org.hibernate.search.engine.environment.classpath.spi.ClassResolver;\nimport org.hibernate.search.engine.logging.impl.Log;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n\nfinal class ReflectionBeanProvider implements BeanProvider {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate final ClassResolver classResolver;\n\n\tReflectionBeanProvider(ClassResolver classResolver) {\n\t\tthis.classResolver = classResolver;\n\t}\n\n\t@Override\n\tpublic void close() {\n\t\t// Nothing to do\n\t}\n\n\t@Override\n\tpublic <T> BeanHolder<T> forType(Class<T> typeReference) {\n\t\ttry {\n\t\t\treturn BeanHolder.of( ClassLoaderHelper.untypedInstanceFromClass( typeReference ) );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow log.unableToCreateBeanUsingReflection( e.getMessage(), e );\n\t\t}\n\t}\n\n\t@Override\n\tpublic <T> BeanHolder<T> forTypeAndName(Class<T> typeReference, String implementationFullyQualifiedClassName) {\n\t\ttry {\n\t\t\treturn BeanHolder.of( ClassLoaderHelper.instanceFromName(\n\t\t\t\t\ttypeReference, implementationFullyQualifiedClassName, classResolver\n\t\t\t) );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow log.unableToCreateBeanUsingReflection( e.getMessage(), e );\n\t\t}\n\t}\n\n}\n",
        "filePathAfter": "engine/src/main/java/org/hibernate/search/engine/environment/bean/impl/BeanResolverImpl.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.environment.bean.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Map;\n\nimport org.hibernate.search.engine.cfg.spi.ConfigurationProperty;\nimport org.hibernate.search.engine.cfg.spi.ConfigurationPropertySource;\nimport org.hibernate.search.engine.cfg.spi.EngineSpiSettings;\nimport org.hibernate.search.engine.environment.bean.BeanHolder;\nimport org.hibernate.search.engine.environment.bean.BeanReference;\nimport org.hibernate.search.engine.environment.bean.BeanResolver;\nimport org.hibernate.search.engine.environment.bean.BeanRetrieval;\nimport org.hibernate.search.engine.environment.bean.spi.BeanConfigurer;\nimport org.hibernate.search.engine.environment.bean.spi.BeanNotFoundException;\nimport org.hibernate.search.engine.environment.bean.spi.BeanProvider;\nimport org.hibernate.search.engine.environment.classpath.spi.ClassLoaderHelper;\nimport org.hibernate.search.engine.environment.classpath.spi.ClassResolver;\nimport org.hibernate.search.engine.environment.classpath.spi.ServiceResolver;\nimport org.hibernate.search.engine.logging.impl.Log;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.impl.Contracts;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\npublic final class BeanResolverImpl implements BeanResolver {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate static final ConfigurationProperty<List<BeanReference<? extends BeanConfigurer>>> BEAN_CONFIGURERS =\n\t\t\tConfigurationProperty.forKey( EngineSpiSettings.Radicals.BEAN_CONFIGURERS )\n\t\t\t\t\t.asBeanReference( BeanConfigurer.class )\n\t\t\t\t\t.multivalued()\n\t\t\t\t\t.withDefault( EngineSpiSettings.Defaults.BEAN_CONFIGURERS )\n\t\t\t\t\t.build();\n\n\tpublic static BeanResolverImpl create(ClassResolver classResolver, ServiceResolver serviceResolver,\n\t\t\tBeanProvider beanManagerBeanProvider, ConfigurationPropertySource configurationPropertySource) {\n\t\tif ( beanManagerBeanProvider == null ) {\n\t\t\tbeanManagerBeanProvider = new NoConfiguredBeanManagerBeanProvider();\n\t\t}\n\n\t\tBeanConfigurationContextImpl configurationContext = new BeanConfigurationContextImpl();\n\t\tfor ( BeanConfigurer beanConfigurer : serviceResolver.loadJavaServices( BeanConfigurer.class ) ) {\n\t\t\tbeanConfigurer.configure( configurationContext );\n\t\t}\n\n\t\tConfigurationBeanRegistry emptyBeanRegistry = new ConfigurationBeanRegistry( Collections.emptyMap() );\n\t\tBeanResolverImpl beanResolverForConfigurers =\n\t\t\t\tnew BeanResolverImpl( classResolver, emptyBeanRegistry, beanManagerBeanProvider );\n\t\ttry ( BeanHolder<List<BeanConfigurer>> beanConfigurersFromConfigurationProperties =\n\t\t\t\tBEAN_CONFIGURERS.getAndTransform( configurationPropertySource, beanResolverForConfigurers::resolve ) ) {\n\t\t\tfor ( BeanConfigurer beanConfigurer : beanConfigurersFromConfigurationProperties.get() ) {\n\t\t\t\tbeanConfigurer.configure( configurationContext );\n\t\t\t}\n\t\t}\n\n\t\treturn new BeanResolverImpl( classResolver, configurationContext.buildRegistry(), beanManagerBeanProvider );\n\t}\n\n\tprivate final ClassResolver classResolver;\n\tprivate final ConfigurationBeanRegistry configurationBeanRegistry;\n\tprivate final BeanProvider beanManagerBeanProvider;\n\n\tprivate BeanResolverImpl(ClassResolver classResolver,\n\t\t\tConfigurationBeanRegistry configurationBeanRegistry,\n\t\t\tBeanProvider beanManagerBeanProvider) {\n\t\tthis.classResolver = classResolver;\n\t\tthis.configurationBeanRegistry = configurationBeanRegistry;\n\t\tthis.beanManagerBeanProvider = beanManagerBeanProvider;\n\t}\n\n\t@Override\n\tpublic <T> BeanHolder<T> resolve(Class<T> typeReference, BeanRetrieval retrieval) {\n\t\tContracts.assertNotNull( typeReference, \"typeReference\" );\n\t\treturn resolveFromFirstSuccessfulSource( typeReference, retrieval );\n\t}\n\n\t@Override\n\tpublic <T> BeanHolder<T> resolve(Class<T> typeReference, String nameReference, BeanRetrieval retrieval) {\n\t\tContracts.assertNotNull( typeReference, \"typeReference\" );\n\t\tContracts.assertNotNullNorEmpty( nameReference, \"nameReference\" );\n\t\treturn resolveFromFirstSuccessfulSource( typeReference, nameReference, retrieval );\n\t}\n\n\t@Override\n\tpublic <T> List<BeanReference<T>> allConfiguredForRole(Class<T> role) {\n\t\tContracts.assertNotNull( role, \"role\" );\n\t\tBeanReferenceRegistryForType<T> registry = configurationBeanRegistry.explicitlyConfiguredBeans( role );\n\t\tif ( registry == null ) {\n\t\t\treturn Collections.emptyList();\n\t\t}\n\t\treturn registry.all();\n\t}\n\n\t@Override\n\tpublic <T> Map<String, BeanReference<T>> namedConfiguredForRole(Class<T> role) {\n\t\tContracts.assertNotNull( role, \"role\" );\n\t\tBeanReferenceRegistryForType<T> registry = configurationBeanRegistry.explicitlyConfiguredBeans( role );\n\t\tif ( registry == null ) {\n\t\t\treturn Collections.emptyMap();\n\t\t}\n\t\treturn registry.named();\n\t}\n\n\tprivate List<BeanSource> toSources(BeanRetrieval retrieval, boolean hasName) {\n\t\tswitch ( retrieval ) {\n\t\t\tcase BUILTIN:\n\t\t\t\treturn Collections.singletonList( BeanSource.CONFIGURATION );\n\t\t\tcase BEAN:\n\t\t\t\treturn Collections.singletonList( BeanSource.BEAN_MANAGER );\n\t\t\tcase CLASS:\n\t\t\t\treturn Arrays.asList( BeanSource.BEAN_MANAGER_ASSUME_CLASS_NAME, BeanSource.REFLECTION );\n\t\t\tcase CONSTRUCTOR:\n\t\t\t\treturn Collections.singletonList( BeanSource.REFLECTION );\n\t\t\tcase ANY:\n\t\t\t\treturn hasName\n\t\t\t\t\t\t? Arrays.asList( BeanSource.CONFIGURATION, BeanSource.BEAN_MANAGER,\n\t\t\t\t\t\t\t\tBeanSource.BEAN_MANAGER_ASSUME_CLASS_NAME, BeanSource.REFLECTION )\n\t\t\t\t\t\t: Arrays.asList( BeanSource.CONFIGURATION, BeanSource.BEAN_MANAGER, BeanSource.REFLECTION );\n\t\t\tdefault:\n\t\t\t\tthrow new AssertionFailure( \"Unknown bean retrieval: \" + retrieval );\n\t\t}\n\t}\n\n\tprivate <T> BeanHolder<T> resolveFromFirstSuccessfulSource(Class<T> typeReference, BeanRetrieval retrieval) {\n\t\tList<BeanSource> sources = toSources( retrieval, false );\n\t\tBeanNotFoundException firstFailure = null;\n\t\tList<BeanNotFoundException> otherFailures = new ArrayList<>();\n\t\tfor ( BeanSource source : sources ) {\n\t\t\ttry {\n\t\t\t\treturn tryResolve( typeReference, source );\n\t\t\t}\n\t\t\tcatch (BeanNotFoundException e) {\n\t\t\t\tif ( firstFailure == null ) {\n\t\t\t\t\tfirstFailure = e;\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\totherFailures.add( e );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tthrow log.cannotResolveBeanReference( typeReference,\n\t\t\t\tbuildFailureMessage( sources, firstFailure, otherFailures ), firstFailure, otherFailures );\n\t}\n\n\tprivate <T> BeanHolder<T> resolveFromFirstSuccessfulSource(Class<T> typeReference, String nameReference,\n\t\t\tBeanRetrieval retrieval) {\n\t\tList<BeanSource> sources = toSources( retrieval, true );\n\t\tBeanNotFoundException firstFailure = null;\n\t\tList<BeanNotFoundException> otherFailures = new ArrayList<>();\n\t\tfor ( BeanSource source : sources ) {\n\t\t\ttry {\n\t\t\t\treturn tryResolve( typeReference, nameReference, source );\n\t\t\t}\n\t\t\tcatch (BeanNotFoundException e) {\n\t\t\t\tif ( firstFailure == null ) {\n\t\t\t\t\tfirstFailure = e;\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\totherFailures.add( e );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tthrow log.cannotResolveBeanReference( typeReference, nameReference,\n\t\t\t\tbuildFailureMessage( sources, firstFailure, otherFailures ), firstFailure, otherFailures );\n\t}\n\n\tprivate <T> BeanHolder<T> tryResolve(Class<T> typeReference, BeanSource source) {\n\t\tswitch ( source ) {\n\t\t\tcase CONFIGURATION:\n\t\t\t\treturn configurationBeanRegistry.resolve( typeReference, this );\n\t\t\tcase BEAN_MANAGER:\n\t\t\tcase BEAN_MANAGER_ASSUME_CLASS_NAME:\n\t\t\t\treturn beanManagerBeanProvider.forType( typeReference );\n\t\t\tcase REFLECTION:\n\t\t\t\treturn retrieveUsingConstructor( typeReference );\n\t\t\tdefault:\n\t\t\t\tthrow unknownBeanSource( source );\n\t\t}\n\t}\n\n\tprivate <T> BeanHolder<T> tryResolve(Class<T> typeReference, String nameReference, BeanSource source) {\n\t\tswitch ( source ) {\n\t\t\tcase CONFIGURATION:\n\t\t\t\treturn configurationBeanRegistry.resolve( typeReference, nameReference, this );\n\t\t\tcase BEAN_MANAGER:\n\t\t\t\treturn beanManagerBeanProvider.forTypeAndName( typeReference, nameReference );\n\t\t\tcase BEAN_MANAGER_ASSUME_CLASS_NAME:\n\t\t\t\treturn beanManagerBeanProvider.forType( toClass( typeReference, nameReference ) );\n\t\t\tcase REFLECTION:\n\t\t\t\treturn retrieveUsingConstructor( toClass( typeReference, nameReference ) );\n\t\t\tdefault:\n\t\t\t\tthrow unknownBeanSource( source );\n\t\t}\n\t}\n\n\tpublic <T> Class<? extends T> toClass(Class<T> typeReference, String nameReference) {\n\t\ttry {\n\t\t\treturn ClassLoaderHelper.classForName( typeReference, nameReference, classResolver );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow log.unableToResolveToClassName( typeReference, nameReference, e.getMessage(), e );\n\t\t}\n\t}\n\n\tpublic <T> BeanHolder<T> retrieveUsingConstructor(Class<T> typeReference) {\n\t\ttry {\n\t\t\treturn BeanHolder.of( ClassLoaderHelper.untypedInstanceFromClass( typeReference ) );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow log.unableToCreateBeanUsingReflection( e.getMessage(), e );\n\t\t}\n\t}\n\n\tprivate String buildFailureMessage(List<BeanSource> sources, BeanNotFoundException firstFailure,\n\t\t\tList<BeanNotFoundException> otherFailures) {\n\t\tStringBuilder builder = new StringBuilder();\n\t\tbuilder.append( renderFailure( sources.get( 0 ), firstFailure ) );\n\t\tfor ( int i = 0; i < otherFailures.size(); i++ ) {\n\t\t\tRuntimeException failure = otherFailures.get( i );\n\t\t\tbuilder.append( \" \" );\n\t\t\tbuilder.append( renderFailure( sources.get( i + 1 ), failure ) );\n\t\t}\n\t\treturn builder.toString();\n\t}\n\n\tprivate String renderFailure(BeanSource source, RuntimeException failure) {\n\t\tswitch ( source ) {\n\t\t\tcase CONFIGURATION:\n\t\t\t\treturn log.failedToResolveBeanUsingInternalRegistry( failure.getMessage() );\n\t\t\tcase BEAN_MANAGER:\n\t\t\t\treturn log.failedToResolveBeanUsingBeanManager( failure.getMessage() );\n\t\t\tcase BEAN_MANAGER_ASSUME_CLASS_NAME:\n\t\t\t\treturn log.failedToResolveBeanUsingBeanManager( failure.getMessage() );\n\t\t\tcase REFLECTION:\n\t\t\t\treturn log.failedToResolveBeanUsingReflection( failure.getMessage() );\n\t\t\tdefault:\n\t\t\t\tthrow unknownBeanSource( source );\n\t\t}\n\t}\n\n\tprivate AssertionFailure unknownBeanSource(BeanSource source) {\n\t\treturn new AssertionFailure( \"Unknown bean source: \" + source );\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.environment.bean.BeanReference#of\n methodBody: static <T> BeanReference<T> of(Class<T> type, String name) {\nif(StringHelper.isNotEmpty(name)){return new TypeAndNameBeanReference<>(type,name);\n}{return new TypeBeanReference<>(type);\n}}",
            "methodSignature: org.hibernate.search.engine.logging.impl.Log#unableToCreateBeanUsingReflection\n methodBody: BeanNotFoundException unableToCreateBeanUsingReflection(String causeMessage, @Cause Exception e);"
        ],
        "sourceCodeAfterRefactoring": "public <T> BeanHolder<T> retrieveUsingConstructor(Class<T> typeReference) {\n\t\ttry {\n\t\t\treturn BeanHolder.of( ClassLoaderHelper.untypedInstanceFromClass( typeReference ) );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow log.unableToCreateBeanUsingReflection( e.getMessage(), e );\n\t\t}\n\t}",
        "diffSourceCode": "-   35: \t@Override\n-   36: \tpublic <T> BeanHolder<T> forType(Class<T> typeReference) {\n-   37: \t\ttry {\n-   38: \t\t\treturn BeanHolder.of( ClassLoaderHelper.untypedInstanceFromClass( typeReference ) );\n-   39: \t\t}\n-   40: \t\tcatch (RuntimeException e) {\n-   41: \t\t\tthrow log.unableToCreateBeanUsingReflection( e.getMessage(), e );\n-   42: \t\t}\n-   43: \t}\n+   35: \n+   36: \tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n+   37: \n+   38: \tprivate static final ConfigurationProperty<List<BeanReference<? extends BeanConfigurer>>> BEAN_CONFIGURERS =\n+   39: \t\t\tConfigurationProperty.forKey( EngineSpiSettings.Radicals.BEAN_CONFIGURERS )\n+   40: \t\t\t\t\t.asBeanReference( BeanConfigurer.class )\n+   41: \t\t\t\t\t.multivalued()\n+   42: \t\t\t\t\t.withDefault( EngineSpiSettings.Defaults.BEAN_CONFIGURERS )\n+   43: \t\t\t\t\t.build();\n+  215: \tpublic <T> BeanHolder<T> retrieveUsingConstructor(Class<T> typeReference) {\n+  216: \t\ttry {\n+  217: \t\t\treturn BeanHolder.of( ClassLoaderHelper.untypedInstanceFromClass( typeReference ) );\n+  218: \t\t}\n+  219: \t\tcatch (RuntimeException e) {\n+  220: \t\t\tthrow log.unableToCreateBeanUsingReflection( e.getMessage(), e );\n+  221: \t\t}\n+  222: \t}\n",
        "uniqueId": "14e6370a9ab5e7fca20e508f6413bd11ed242cde_35_43__215_222",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 11
            },
            "LINE": {
                "missed": 0,
                "covered": 3
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate createProtocolDialectV6(version ElasticsearchVersion, minor int) : ElasticsearchProtocolDialect extracted from public createProtocolDialect(version ElasticsearchVersion) : ElasticsearchProtocolDialect in class org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory",
        "diffLocations": [
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
                "startLine": 60,
                "endLine": 107,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
                "startLine": 52,
                "endLine": 79,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
                "startLine": 104,
                "endLine": 119,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n\t\t\t);\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\tif ( minor < 6 ) {\n\t\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t\t}\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( minor != 6 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch56ProtocolDialect();\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\tif ( minor < 3 ) {\n\t\t\t\treturn new Elasticsearch60ProtocolDialect();\n\t\t\t}\n\t\t\tif ( minor < 4 ) {\n\t\t\t\treturn new Elasticsearch63ProtocolDialect();\n\t\t\t}\n\t\t\tif ( minor < 7 ) {\n\t\t\t\treturn new Elasticsearch64ProtocolDialect();\n\t\t\t}\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( minor > 8 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch67ProtocolDialect();\n\t\t}\n\t\telse {\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( major != 7 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch70ProtocolDialect();\n\t\t}\n\t}",
        "filePathBefore": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
        "isPureRefactoring": true,
        "commitId": "b77e41600e4d9e32170dbb10d77e5f676f429d42",
        "packageNameBefore": "org.hibernate.search.backend.elasticsearch.dialect.impl",
        "classNameBefore": "org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory",
        "methodNameBefore": "org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory#createProtocolDialect",
        "classSignatureBefore": "public class ElasticsearchDialectFactory ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory#createProtocolDialect"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory"
        ],
        "classSignatureBeforeSet": [
            "public class ElasticsearchDialectFactory "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.dialect.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.OptionalInt;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchVersion;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch56ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch6ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch7ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.ElasticsearchModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch56ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch60ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch63ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch64ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch67ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch70ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.ElasticsearchProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * Allows to create an Elasticsearch dialect by detecting the version of a remote cluster.\n */\npublic class ElasticsearchDialectFactory {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tpublic ElasticsearchModelDialect createModelDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\t\tOptionalInt minorOptional = version.minor();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\tif ( !minorOptional.isPresent() ) {\n\t\t\t\tthrow log.ambiguousElasticsearchVersion( version );\n\t\t\t}\n\t\t\tint minor = minorOptional.getAsInt();\n\t\t\tif ( minor < 6 ) {\n\t\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch56ModelDialect();\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn new Elasticsearch6ModelDialect();\n\t\t}\n\t\telse {\n\t\t\treturn new Elasticsearch7ModelDialect();\n\t\t}\n\t}\n\n\tpublic ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n\t\t\t);\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\tif ( minor < 6 ) {\n\t\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t\t}\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( minor != 6 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch56ProtocolDialect();\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\tif ( minor < 3 ) {\n\t\t\t\treturn new Elasticsearch60ProtocolDialect();\n\t\t\t}\n\t\t\tif ( minor < 4 ) {\n\t\t\t\treturn new Elasticsearch63ProtocolDialect();\n\t\t\t}\n\t\t\tif ( minor < 7 ) {\n\t\t\t\treturn new Elasticsearch64ProtocolDialect();\n\t\t\t}\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( minor > 8 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch67ProtocolDialect();\n\t\t}\n\t\telse {\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( major != 7 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch70ProtocolDialect();\n\t\t}\n\t}\n\n}\n",
        "filePathAfter": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.dialect.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.OptionalInt;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchVersion;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch56ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch6ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch7ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.ElasticsearchModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch56ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch60ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch63ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch64ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch67ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch70ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.ElasticsearchProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * Allows to create an Elasticsearch dialect by detecting the version of a remote cluster.\n */\npublic class ElasticsearchDialectFactory {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tpublic ElasticsearchModelDialect createModelDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\treturn createModelDialectV5( version );\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn new Elasticsearch6ModelDialect();\n\t\t}\n\t\telse {\n\t\t\treturn new Elasticsearch7ModelDialect();\n\t\t}\n\t}\n\n\tpublic ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n\t\t\t);\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\treturn createProtocolDialectV5( version, minor );\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn createProtocolDialectV6( version, minor );\n\t\t}\n\t\telse {\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( major != 7 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch70ProtocolDialect();\n\t\t}\n\t}\n\n\tprivate ElasticsearchModelDialect createModelDialectV5(ElasticsearchVersion version) {\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\tthrow log.ambiguousElasticsearchVersion( version );\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\t\tif ( minor < 6 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch56ModelDialect();\n\t}\n\n\tprivate ElasticsearchProtocolDialect createProtocolDialectV5(ElasticsearchVersion version, int minor) {\n\t\tif ( minor < 6 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\t// Either the latest supported version, or a newer/unknown one\n\t\tif ( minor != 6 ) {\n\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch56ProtocolDialect();\n\t}\n\n\tprivate ElasticsearchProtocolDialect createProtocolDialectV6(ElasticsearchVersion version, int minor) {\n\t\tif ( minor < 3 ) {\n\t\t\treturn new Elasticsearch60ProtocolDialect();\n\t\t}\n\t\tif ( minor < 4 ) {\n\t\t\treturn new Elasticsearch63ProtocolDialect();\n\t\t}\n\t\tif ( minor < 7 ) {\n\t\t\treturn new Elasticsearch64ProtocolDialect();\n\t\t}\n\t\t// Either the latest supported version, or a newer/unknown one\n\t\tif ( minor > 8 ) {\n\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch67ProtocolDialect();\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "private ElasticsearchProtocolDialect createProtocolDialectV6(ElasticsearchVersion version, int minor) {\n\t\tif ( minor < 3 ) {\n\t\t\treturn new Elasticsearch60ProtocolDialect();\n\t\t}\n\t\tif ( minor < 4 ) {\n\t\t\treturn new Elasticsearch63ProtocolDialect();\n\t\t}\n\t\tif ( minor < 7 ) {\n\t\t\treturn new Elasticsearch64ProtocolDialect();\n\t\t}\n\t\t// Either the latest supported version, or a newer/unknown one\n\t\tif ( minor > 8 ) {\n\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch67ProtocolDialect();\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "public ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n\t\t\t);\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\treturn createProtocolDialectV5( version, minor );\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn createProtocolDialectV6( version, minor );\n\t\t}\n\t\telse {\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( major != 7 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch70ProtocolDialect();\n\t\t}\n\t}\nprivate ElasticsearchProtocolDialect createProtocolDialectV6(ElasticsearchVersion version, int minor) {\n\t\tif ( minor < 3 ) {\n\t\t\treturn new Elasticsearch60ProtocolDialect();\n\t\t}\n\t\tif ( minor < 4 ) {\n\t\t\treturn new Elasticsearch63ProtocolDialect();\n\t\t}\n\t\tif ( minor < 7 ) {\n\t\t\treturn new Elasticsearch64ProtocolDialect();\n\t\t}\n\t\t// Either the latest supported version, or a newer/unknown one\n\t\tif ( minor > 8 ) {\n\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch67ProtocolDialect();\n\t}",
        "diffSourceCode": "-   52: \t\telse if ( major == 6 ) {\n-   53: \t\t\treturn new Elasticsearch6ModelDialect();\n-   54: \t\t}\n-   55: \t\telse {\n-   56: \t\t\treturn new Elasticsearch7ModelDialect();\n-   57: \t\t}\n-   58: \t}\n-   59: \n-   60: \tpublic ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n-   61: \t\tint major = version.major();\n-   62: \t\tOptionalInt minorOptional = version.minor();\n-   63: \t\tif ( !minorOptional.isPresent() ) {\n-   64: \t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n-   65: \t\t\tthrow new AssertionFailure(\n-   66: \t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n-   67: \t\t\t);\n+   52: \tpublic ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n+   53: \t\tint major = version.major();\n+   54: \t\tOptionalInt minorOptional = version.minor();\n+   55: \t\tif ( !minorOptional.isPresent() ) {\n+   56: \t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n+   57: \t\t\tthrow new AssertionFailure(\n+   58: \t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n+   59: \t\t\t);\n+   60: \t\t}\n+   61: \t\tint minor = minorOptional.getAsInt();\n+   62: \n+   63: \t\tif ( major < 5 ) {\n+   64: \t\t\tthrow log.unsupportedElasticsearchVersion( version );\n+   65: \t\t}\n+   66: \t\telse if ( major == 5 ) {\n+   67: \t\t\treturn createProtocolDialectV5( version, minor );\n    68: \t\t}\n-   69: \t\tint minor = minorOptional.getAsInt();\n-   70: \n-   71: \t\tif ( major < 5 ) {\n-   72: \t\t\tthrow log.unsupportedElasticsearchVersion( version );\n-   73: \t\t}\n-   74: \t\telse if ( major == 5 ) {\n-   75: \t\t\tif ( minor < 6 ) {\n-   76: \t\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n-   77: \t\t\t}\n-   78: \t\t\t// Either the latest supported version, or a newer/unknown one\n-   79: \t\t\tif ( minor != 6 ) {\n-   80: \t\t\t\tlog.unknownElasticsearchVersion( version );\n-   81: \t\t\t}\n-   82: \t\t\treturn new Elasticsearch56ProtocolDialect();\n-   83: \t\t}\n-   84: \t\telse if ( major == 6 ) {\n-   85: \t\t\tif ( minor < 3 ) {\n-   86: \t\t\t\treturn new Elasticsearch60ProtocolDialect();\n-   87: \t\t\t}\n-   88: \t\t\tif ( minor < 4 ) {\n-   89: \t\t\t\treturn new Elasticsearch63ProtocolDialect();\n-   90: \t\t\t}\n-   91: \t\t\tif ( minor < 7 ) {\n-   92: \t\t\t\treturn new Elasticsearch64ProtocolDialect();\n-   93: \t\t\t}\n-   94: \t\t\t// Either the latest supported version, or a newer/unknown one\n-   95: \t\t\tif ( minor > 8 ) {\n-   96: \t\t\t\tlog.unknownElasticsearchVersion( version );\n-   97: \t\t\t}\n-   98: \t\t\treturn new Elasticsearch67ProtocolDialect();\n-   99: \t\t}\n-  100: \t\telse {\n-  101: \t\t\t// Either the latest supported version, or a newer/unknown one\n-  102: \t\t\tif ( major != 7 ) {\n-  103: \t\t\t\tlog.unknownElasticsearchVersion( version );\n-  104: \t\t\t}\n-  105: \t\t\treturn new Elasticsearch70ProtocolDialect();\n-  106: \t\t}\n-  107: \t}\n-  108: \n-  109: }\n+   69: \t\telse if ( major == 6 ) {\n+   70: \t\t\treturn createProtocolDialectV6( version, minor );\n+   71: \t\t}\n+   72: \t\telse {\n+   73: \t\t\t// Either the latest supported version, or a newer/unknown one\n+   74: \t\t\tif ( major != 7 ) {\n+   75: \t\t\t\tlog.unknownElasticsearchVersion( version );\n+   76: \t\t\t}\n+   77: \t\t\treturn new Elasticsearch70ProtocolDialect();\n+   78: \t\t}\n+   79: \t}\n+   80: \n+   81: \tprivate ElasticsearchModelDialect createModelDialectV5(ElasticsearchVersion version) {\n+   82: \t\tOptionalInt minorOptional = version.minor();\n+   83: \t\tif ( !minorOptional.isPresent() ) {\n+   84: \t\t\tthrow log.ambiguousElasticsearchVersion( version );\n+   85: \t\t}\n+   86: \t\tint minor = minorOptional.getAsInt();\n+   87: \t\tif ( minor < 6 ) {\n+   88: \t\t\tthrow log.unsupportedElasticsearchVersion( version );\n+   89: \t\t}\n+   90: \t\treturn new Elasticsearch56ModelDialect();\n+   91: \t}\n+   92: \n+   93: \tprivate ElasticsearchProtocolDialect createProtocolDialectV5(ElasticsearchVersion version, int minor) {\n+   94: \t\tif ( minor < 6 ) {\n+   95: \t\t\tthrow log.unsupportedElasticsearchVersion( version );\n+   96: \t\t}\n+   97: \t\t// Either the latest supported version, or a newer/unknown one\n+   98: \t\tif ( minor != 6 ) {\n+   99: \t\t\tlog.unknownElasticsearchVersion( version );\n+  100: \t\t}\n+  101: \t\treturn new Elasticsearch56ProtocolDialect();\n+  102: \t}\n+  103: \n+  104: \tprivate ElasticsearchProtocolDialect createProtocolDialectV6(ElasticsearchVersion version, int minor) {\n+  105: \t\tif ( minor < 3 ) {\n+  106: \t\t\treturn new Elasticsearch60ProtocolDialect();\n+  107: \t\t}\n+  108: \t\tif ( minor < 4 ) {\n+  109: \t\t\treturn new Elasticsearch63ProtocolDialect();\n+  110: \t\t}\n+  111: \t\tif ( minor < 7 ) {\n+  112: \t\t\treturn new Elasticsearch64ProtocolDialect();\n+  113: \t\t}\n+  114: \t\t// Either the latest supported version, or a newer/unknown one\n+  115: \t\tif ( minor > 8 ) {\n+  116: \t\t\tlog.unknownElasticsearchVersion( version );\n+  117: \t\t}\n+  118: \t\treturn new Elasticsearch67ProtocolDialect();\n+  119: \t}\n",
        "uniqueId": "b77e41600e4d9e32170dbb10d77e5f676f429d42_60_107_104_119_52_79",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 9,
                "covered": 41
            },
            "BRANCH": {
                "missed": 2,
                "covered": 8
            },
            "LINE": {
                "missed": 2,
                "covered": 12
            },
            "COMPLEXITY": {
                "missed": 2,
                "covered": 4
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate createModelDialectElastic(version ElasticsearchVersion) : ElasticsearchModelDialect extracted from public createModelDialect(version ElasticsearchVersion) : ElasticsearchModelDialect in class org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory",
        "diffLocations": [
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
                "startLine": 35,
                "endLine": 50,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
                "startLine": 35,
                "endLine": 44,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
                "startLine": 46,
                "endLine": 61,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public ElasticsearchModelDialect createModelDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\treturn createModelDialectV5( version );\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn new Elasticsearch6ModelDialect();\n\t\t}\n\t\telse {\n\t\t\treturn new Elasticsearch7ModelDialect();\n\t\t}\n\t}",
        "filePathBefore": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
        "isPureRefactoring": true,
        "commitId": "63bb8c546de700b3e8009d3e0ea5806c0826ac22",
        "packageNameBefore": "org.hibernate.search.backend.elasticsearch.dialect.impl",
        "classNameBefore": "org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory",
        "methodNameBefore": "org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory#createModelDialect",
        "invokedMethod": "methodSignature: org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory#createModelDialectV5\n methodBody: private ElasticsearchModelDialect createModelDialectV5(ElasticsearchVersion version) {\nOptionalInt minorOptional=version.minor();\nif(!minorOptional.isPresent()){throw log.ambiguousElasticsearchVersion(version);\n}int minor=minorOptional.getAsInt();\nif(minor < 6){throw log.unsupportedElasticsearchVersion(version);\n}return new Elasticsearch56ModelDialect();\n}",
        "classSignatureBefore": "public class ElasticsearchDialectFactory ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory#createModelDialect"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory"
        ],
        "classSignatureBeforeSet": [
            "public class ElasticsearchDialectFactory "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Tolerable changes in the body\n",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.dialect.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.OptionalInt;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchVersion;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch56ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch6ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch7ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.ElasticsearchModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch56ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch60ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch63ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch64ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch67ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch70ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.ElasticsearchProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * Allows to create an Elasticsearch dialect by detecting the version of a remote cluster.\n */\npublic class ElasticsearchDialectFactory {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tpublic ElasticsearchModelDialect createModelDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\treturn createModelDialectV5( version );\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn new Elasticsearch6ModelDialect();\n\t\t}\n\t\telse {\n\t\t\treturn new Elasticsearch7ModelDialect();\n\t\t}\n\t}\n\n\tpublic ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n\t\t\t);\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\treturn createProtocolDialectV5( version, minor );\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn createProtocolDialectV6( version, minor );\n\t\t}\n\t\telse {\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( major != 7 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch70ProtocolDialect();\n\t\t}\n\t}\n\n\tprivate ElasticsearchModelDialect createModelDialectV5(ElasticsearchVersion version) {\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\tthrow log.ambiguousElasticsearchVersion( version );\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\t\tif ( minor < 6 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch56ModelDialect();\n\t}\n\n\tprivate ElasticsearchProtocolDialect createProtocolDialectV5(ElasticsearchVersion version, int minor) {\n\t\tif ( minor < 6 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\t// Either the latest supported version, or a newer/unknown one\n\t\tif ( minor != 6 ) {\n\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch56ProtocolDialect();\n\t}\n\n\tprivate ElasticsearchProtocolDialect createProtocolDialectV6(ElasticsearchVersion version, int minor) {\n\t\tif ( minor < 3 ) {\n\t\t\treturn new Elasticsearch60ProtocolDialect();\n\t\t}\n\t\tif ( minor < 4 ) {\n\t\t\treturn new Elasticsearch63ProtocolDialect();\n\t\t}\n\t\tif ( minor < 7 ) {\n\t\t\treturn new Elasticsearch64ProtocolDialect();\n\t\t}\n\t\t// Either the latest supported version, or a newer/unknown one\n\t\tif ( minor > 8 ) {\n\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch67ProtocolDialect();\n\t}\n\n}\n",
        "filePathAfter": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.dialect.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.OptionalInt;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchVersion;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch56ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch6ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch7ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.ElasticsearchModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch56ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch60ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch63ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch64ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch67ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch70ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.ElasticsearchProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * Creates an Elasticsearch dialect for a given Elasticsearch version.\n */\npublic class ElasticsearchDialectFactory {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tpublic ElasticsearchModelDialect createModelDialect(ElasticsearchVersion version) {\n\t\tswitch ( version.distribution() ) {\n\t\t\tcase ELASTIC:\n\t\t\t\treturn createModelDialectElastic( version );\n\t\t\tcase OPENSEARCH:\n\t\t\t\treturn createModelDialectOpenSearch( version );\n\t\t\tdefault:\n\t\t\t\tthrow new AssertionFailure( \"Unrecognized Elasticsearch distribution: \" + version.distribution() );\n\t\t}\n\t}\n\n\tprivate ElasticsearchModelDialect createModelDialectElastic(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\treturn createModelDialectElasticV5( version );\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn new Elasticsearch6ModelDialect();\n\t\t}\n\t\telse {\n\t\t\treturn new Elasticsearch7ModelDialect();\n\t\t}\n\t}\n\n\tprivate ElasticsearchModelDialect createModelDialectElasticV5(ElasticsearchVersion version) {\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\tthrow log.ambiguousElasticsearchVersion( version );\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\t\tif ( minor < 6 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch56ModelDialect();\n\t}\n\n\tprivate ElasticsearchModelDialect createModelDialectOpenSearch(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\n\t\tif ( major < 1 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse {\n\t\t\treturn new Elasticsearch7ModelDialect();\n\t\t}\n\t}\n\n\tpublic ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n\t\tswitch ( version.distribution() ) {\n\t\t\tcase ELASTIC:\n\t\t\t\treturn createProtocolDialectElastic( version );\n\t\t\tcase OPENSEARCH:\n\t\t\t\treturn createProtocolDialectOpenSearch( version );\n\t\t\tdefault:\n\t\t\t\tthrow new AssertionFailure( \"Unrecognized Elasticsearch distribution: \" + version.distribution() );\n\t\t}\n\t}\n\n\tprivate ElasticsearchProtocolDialect createProtocolDialectElastic(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n\t\t\tthrow new AssertionFailure( \"The Elasticsearch version is incomplete when creating the protocol dialect.\" );\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\treturn createProtocolDialectElasticV5( version, minor );\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn createProtocolDialectElasticV6( version, minor );\n\t\t}\n\t\telse {\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( major != 7 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch70ProtocolDialect();\n\t\t}\n\t}\n\n\tprivate ElasticsearchProtocolDialect createProtocolDialectElasticV5(ElasticsearchVersion version, int minor) {\n\t\tif ( minor < 6 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\t// Either the latest supported version, or a newer/unknown one\n\t\tif ( minor != 6 ) {\n\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch56ProtocolDialect();\n\t}\n\n\tprivate ElasticsearchProtocolDialect createProtocolDialectElasticV6(ElasticsearchVersion version, int minor) {\n\t\tif ( minor < 3 ) {\n\t\t\treturn new Elasticsearch60ProtocolDialect();\n\t\t}\n\t\tif ( minor < 4 ) {\n\t\t\treturn new Elasticsearch63ProtocolDialect();\n\t\t}\n\t\tif ( minor < 7 ) {\n\t\t\treturn new Elasticsearch64ProtocolDialect();\n\t\t}\n\t\t// Either the latest supported version, or a newer/unknown one\n\t\tif ( minor > 8 ) {\n\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch67ProtocolDialect();\n\t}\n\n\tprivate ElasticsearchProtocolDialect createProtocolDialectOpenSearch(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\n\t\tif ( major < 1 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse {\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( major != 1 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch70ProtocolDialect();\n\t\t}\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "private ElasticsearchModelDialect createModelDialectElastic(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\treturn createModelDialectElasticV5( version );\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn new Elasticsearch6ModelDialect();\n\t\t}\n\t\telse {\n\t\t\treturn new Elasticsearch7ModelDialect();\n\t\t}\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory#createModelDialectV5\n methodBody: private ElasticsearchModelDialect createModelDialectV5(ElasticsearchVersion version) {\nOptionalInt minorOptional=version.minor();\nif(!minorOptional.isPresent()){throw log.ambiguousElasticsearchVersion(version);\n}int minor=minorOptional.getAsInt();\nif(minor < 6){throw log.unsupportedElasticsearchVersion(version);\n}return new Elasticsearch56ModelDialect();\n}"
        ],
        "sourceCodeAfterRefactoring": "public ElasticsearchModelDialect createModelDialect(ElasticsearchVersion version) {\n\t\tswitch ( version.distribution() ) {\n\t\t\tcase ELASTIC:\n\t\t\t\treturn createModelDialectElastic( version );\n\t\t\tcase OPENSEARCH:\n\t\t\t\treturn createModelDialectOpenSearch( version );\n\t\t\tdefault:\n\t\t\t\tthrow new AssertionFailure( \"Unrecognized Elasticsearch distribution: \" + version.distribution() );\n\t\t}\n\t}\nprivate ElasticsearchModelDialect createModelDialectElastic(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\treturn createModelDialectElasticV5( version );\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn new Elasticsearch6ModelDialect();\n\t\t}\n\t\telse {\n\t\t\treturn new Elasticsearch7ModelDialect();\n\t\t}\n\t}",
        "diffSourceCode": "    35: \tpublic ElasticsearchModelDialect createModelDialect(ElasticsearchVersion version) {\n-   36: \t\tint major = version.major();\n-   37: \n-   38: \t\tif ( major < 5 ) {\n-   39: \t\t\tthrow log.unsupportedElasticsearchVersion( version );\n-   40: \t\t}\n-   41: \t\telse if ( major == 5 ) {\n-   42: \t\t\treturn createModelDialectV5( version );\n+   36: \t\tswitch ( version.distribution() ) {\n+   37: \t\t\tcase ELASTIC:\n+   38: \t\t\t\treturn createModelDialectElastic( version );\n+   39: \t\t\tcase OPENSEARCH:\n+   40: \t\t\t\treturn createModelDialectOpenSearch( version );\n+   41: \t\t\tdefault:\n+   42: \t\t\t\tthrow new AssertionFailure( \"Unrecognized Elasticsearch distribution: \" + version.distribution() );\n    43: \t\t}\n-   44: \t\telse if ( major == 6 ) {\n-   45: \t\t\treturn new Elasticsearch6ModelDialect();\n-   46: \t\t}\n-   47: \t\telse {\n-   48: \t\t\treturn new Elasticsearch7ModelDialect();\n-   49: \t\t}\n-   50: \t}\n-   51: \n-   52: \tpublic ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n-   53: \t\tint major = version.major();\n-   54: \t\tOptionalInt minorOptional = version.minor();\n-   55: \t\tif ( !minorOptional.isPresent() ) {\n-   56: \t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n-   57: \t\t\tthrow new AssertionFailure(\n-   58: \t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n-   59: \t\t\t);\n+   44: \t}\n+   45: \n+   46: \tprivate ElasticsearchModelDialect createModelDialectElastic(ElasticsearchVersion version) {\n+   47: \t\tint major = version.major();\n+   48: \n+   49: \t\tif ( major < 5 ) {\n+   50: \t\t\tthrow log.unsupportedElasticsearchVersion( version );\n+   51: \t\t}\n+   52: \t\telse if ( major == 5 ) {\n+   53: \t\t\treturn createModelDialectElasticV5( version );\n+   54: \t\t}\n+   55: \t\telse if ( major == 6 ) {\n+   56: \t\t\treturn new Elasticsearch6ModelDialect();\n+   57: \t\t}\n+   58: \t\telse {\n+   59: \t\t\treturn new Elasticsearch7ModelDialect();\n    60: \t\t}\n-   61: \t\tint minor = minorOptional.getAsInt();\n+   61: \t}\n",
        "uniqueId": "63bb8c546de700b3e8009d3e0ea5806c0826ac22_35_50_46_61_35_44",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 28
            },
            "BRANCH": {
                "missed": 0,
                "covered": 6
            },
            "LINE": {
                "missed": 0,
                "covered": 8
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 4
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tpackage init(model SingleTypeLoadingModel<T>, mapping SingleTypeLoadingMapping) : void extracted from package AbstractSearchQueryEntityLoadingSingleTypeIT(model SingleTypeLoadingModel<T>, mapping SingleTypeLoadingMapping) in class org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingSingleTypeIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/search/loading/AbstractSearchQueryEntityLoadingSingleTypeIT.java",
                "startLine": 38,
                "endLine": 41,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/search/loading/AbstractSearchQueryEntityLoadingSingleTypeIT.java",
                "startLine": 38,
                "endLine": 40,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/search/loading/AbstractSearchQueryEntityLoadingSingleTypeIT.java",
                "startLine": 42,
                "endLine": 45,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "AbstractSearchQueryEntityLoadingSingleTypeIT(SingleTypeLoadingModel<T> model, SingleTypeLoadingMapping mapping) {\n\t\tthis.model = model;\n\t\tthis.mapping = mapping;\n\t}",
        "filePathBefore": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/search/loading/AbstractSearchQueryEntityLoadingSingleTypeIT.java",
        "isPureRefactoring": true,
        "commitId": "94c571c53c35a92257bede06edfb7f4bc3dd50f3",
        "packageNameBefore": "org.hibernate.search.integrationtest.mapper.orm.search.loading",
        "classNameBefore": "org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingSingleTypeIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingSingleTypeIT#AbstractSearchQueryEntityLoadingSingleTypeIT",
        "classSignatureBefore": "public abstract class AbstractSearchQueryEntityLoadingSingleTypeIT<T> extends AbstractSearchQueryEntityLoadingIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingSingleTypeIT#AbstractSearchQueryEntityLoadingSingleTypeIT"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingSingleTypeIT"
        ],
        "classSignatureBeforeSet": [
            "public abstract class AbstractSearchQueryEntityLoadingSingleTypeIT<T> extends AbstractSearchQueryEntityLoadingIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.search.loading;\n\nimport static org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils.with;\n\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.BiConsumer;\nimport java.util.function.Consumer;\n\nimport org.hibernate.Session;\nimport org.hibernate.search.integrationtest.mapper.orm.search.loading.model.singletype.SingleTypeLoadingMapping;\nimport org.hibernate.search.integrationtest.mapper.orm.search.loading.model.singletype.SingleTypeLoadingModel;\nimport org.hibernate.search.mapper.orm.search.loading.dsl.SearchLoadingOptionsStep;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSoftAssertions;\n\npublic abstract class AbstractSearchQueryEntityLoadingSingleTypeIT<T> extends AbstractSearchQueryEntityLoadingIT {\n\n\tprotected static void forAllModelMappingCombinations(\n\t\t\tBiConsumer<SingleTypeLoadingModel<?>, SingleTypeLoadingMapping> consumer) {\n\t\tfor ( SingleTypeLoadingModel<?> model : SingleTypeLoadingModel.all() ) {\n\t\t\tfor ( SingleTypeLoadingMapping mapping : SingleTypeLoadingMapping.all() ) {\n\t\t\t\tconsumer.accept( model, mapping );\n\t\t\t}\n\t\t}\n\t}\n\n\tprotected final SingleTypeLoadingModel<T> model;\n\n\tprotected final SingleTypeLoadingMapping mapping;\n\n\tAbstractSearchQueryEntityLoadingSingleTypeIT(SingleTypeLoadingModel<T> model, SingleTypeLoadingMapping mapping) {\n\t\tthis.model = model;\n\t\tthis.mapping = mapping;\n\t}\n\n\tprotected final void persistThatManyEntities(int entityCount) {\n\t\t// We don't care about what is indexed exactly, so use the lenient mode\n\t\tbackendMock().inLenientMode( () -> with( sessionFactory() ).runInTransaction( session -> {\n\t\t\tfor ( int i = 0; i < entityCount; i++ ) {\n\t\t\t\tsession.persist( model.newIndexed( i, mapping ) );\n\t\t\t}\n\t\t} ) );\n\t}\n\n\tprotected final void testLoadingThatManyEntities(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tint entityCount,\n\t\t\tConsumer<OrmSoftAssertions> assertionsContributor) {\n\t\ttestLoadingThatManyEntities( sessionSetup, loadingOptionsContributor, entityCount, assertionsContributor,\n\t\t\t\tnull, null );\n\t}\n\n\tprotected final void testLoadingThatManyEntities(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tint entityCount,\n\t\t\tConsumer<OrmSoftAssertions> assertionsContributor,\n\t\t\tInteger timeout, TimeUnit timeUnit) {\n\t\ttestLoading(\n\t\t\t\tsessionSetup,\n\t\t\t\tCollections.singletonList( model.getIndexedClass() ),\n\t\t\t\tCollections.singletonList( model.getIndexName() ),\n\t\t\t\tloadingOptionsContributor,\n\t\t\t\tc -> {\n\t\t\t\t\tfor ( int i = 0; i < entityCount; i++ ) {\n\t\t\t\t\t\tc.doc( model.getIndexName(), mapping.getDocumentIdForEntityId( i ) );\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\tc -> {\n\t\t\t\t\tfor ( int i = 0; i < entityCount; i++ ) {\n\t\t\t\t\t\tc.entity( model.getIndexedClass(), i );\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t(assertions, ignored) -> assertionsContributor.accept( assertions ),\n\t\t\t\ttimeout, timeUnit\n\t\t);\n\t}\n\n\tprotected final void testLoading(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tConsumer<DocumentReferenceCollector> hitDocumentReferencesContributor,\n\t\t\tConsumer<EntityCollector<T>> expectedLoadedEntitiesContributor,\n\t\t\tConsumer<OrmSoftAssertions> assertionsContributor) {\n\t\ttestLoading(\n\t\t\t\tsessionSetup,\n\t\t\t\tCollections.singletonList( model.getIndexedClass() ),\n\t\t\t\tCollections.singletonList( model.getIndexName() ),\n\t\t\t\tloadingOptionsContributor,\n\t\t\t\thitDocumentReferencesContributor,\n\t\t\t\texpectedLoadedEntitiesContributor,\n\t\t\t\tassertionsContributor\n\t\t);\n\t}\n\n\tprotected final void testLoading(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tConsumer<DocumentReferenceCollector> hitDocumentReferencesContributor,\n\t\t\tConsumer<EntityCollector<T>> expectedLoadedEntitiesContributor,\n\t\t\tBiConsumer<OrmSoftAssertions, List<T>> assertionsContributor) {\n\t\ttestLoading(\n\t\t\t\tsessionSetup,\n\t\t\t\tCollections.singletonList( model.getIndexedClass() ),\n\t\t\t\tCollections.singletonList( model.getIndexName() ),\n\t\t\t\tloadingOptionsContributor,\n\t\t\t\thitDocumentReferencesContributor,\n\t\t\t\texpectedLoadedEntitiesContributor,\n\t\t\t\tassertionsContributor,\n\t\t\t\tnull, null\n\t\t);\n\t}\n\n}\n",
        "filePathAfter": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/search/loading/AbstractSearchQueryEntityLoadingSingleTypeIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.search.loading;\n\nimport static org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils.with;\n\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.concurrent.TimeUnit;\nimport java.util.function.BiConsumer;\nimport java.util.function.Consumer;\n\nimport org.hibernate.Session;\nimport org.hibernate.search.integrationtest.mapper.orm.search.loading.model.singletype.SingleTypeLoadingMapping;\nimport org.hibernate.search.integrationtest.mapper.orm.search.loading.model.singletype.SingleTypeLoadingModel;\nimport org.hibernate.search.mapper.orm.search.loading.dsl.SearchLoadingOptionsStep;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSoftAssertions;\n\npublic abstract class AbstractSearchQueryEntityLoadingSingleTypeIT<T> extends AbstractSearchQueryEntityLoadingIT {\n\n\tprotected static void forAllModelMappingCombinations(\n\t\t\tBiConsumer<SingleTypeLoadingModel<?>, SingleTypeLoadingMapping> consumer) {\n\t\tfor ( SingleTypeLoadingModel<?> model : SingleTypeLoadingModel.all() ) {\n\t\t\tfor ( SingleTypeLoadingMapping mapping : SingleTypeLoadingMapping.all() ) {\n\t\t\t\tconsumer.accept( model, mapping );\n\t\t\t}\n\t\t}\n\t}\n\n\tprotected SingleTypeLoadingModel<T> model;\n\n\tprotected SingleTypeLoadingMapping mapping;\n\n\tAbstractSearchQueryEntityLoadingSingleTypeIT(SingleTypeLoadingModel<T> model, SingleTypeLoadingMapping mapping) {\n\t\tinit( model, mapping );\n\t}\n\n\tvoid init(SingleTypeLoadingModel<T> model, SingleTypeLoadingMapping mapping) {\n\t\tthis.model = model;\n\t\tthis.mapping = mapping;\n\t}\n\n\tprotected final void persistThatManyEntities(int entityCount) {\n\t\t// We don't care about what is indexed exactly, so use the lenient mode\n\t\tbackendMock().inLenientMode( () -> with( sessionFactory() ).runInTransaction( session -> {\n\t\t\tfor ( int i = 0; i < entityCount; i++ ) {\n\t\t\t\tsession.persist( model.newIndexed( i, mapping ) );\n\t\t\t}\n\t\t} ) );\n\t}\n\n\tprotected final void testLoadingThatManyEntities(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tint entityCount,\n\t\t\tConsumer<OrmSoftAssertions> assertionsContributor) {\n\t\ttestLoadingThatManyEntities( sessionSetup, loadingOptionsContributor, entityCount, assertionsContributor,\n\t\t\t\tnull, null );\n\t}\n\n\tprotected final void testLoadingThatManyEntities(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tint entityCount,\n\t\t\tConsumer<OrmSoftAssertions> assertionsContributor,\n\t\t\tInteger timeout, TimeUnit timeUnit) {\n\t\ttestLoading(\n\t\t\t\tsessionSetup,\n\t\t\t\tCollections.singletonList( model.getIndexedClass() ),\n\t\t\t\tCollections.singletonList( model.getIndexName() ),\n\t\t\t\tloadingOptionsContributor,\n\t\t\t\tc -> {\n\t\t\t\t\tfor ( int i = 0; i < entityCount; i++ ) {\n\t\t\t\t\t\tc.doc( model.getIndexName(), mapping.getDocumentIdForEntityId( i ) );\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\tc -> {\n\t\t\t\t\tfor ( int i = 0; i < entityCount; i++ ) {\n\t\t\t\t\t\tc.entity( model.getIndexedClass(), i );\n\t\t\t\t\t}\n\t\t\t\t},\n\t\t\t\t(assertions, ignored) -> assertionsContributor.accept( assertions ),\n\t\t\t\ttimeout, timeUnit\n\t\t);\n\t}\n\n\tprotected final void testLoading(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tConsumer<DocumentReferenceCollector> hitDocumentReferencesContributor,\n\t\t\tConsumer<EntityCollector<T>> expectedLoadedEntitiesContributor,\n\t\t\tConsumer<OrmSoftAssertions> assertionsContributor) {\n\t\ttestLoading(\n\t\t\t\tsessionSetup,\n\t\t\t\tCollections.singletonList( model.getIndexedClass() ),\n\t\t\t\tCollections.singletonList( model.getIndexName() ),\n\t\t\t\tloadingOptionsContributor,\n\t\t\t\thitDocumentReferencesContributor,\n\t\t\t\texpectedLoadedEntitiesContributor,\n\t\t\t\tassertionsContributor\n\t\t);\n\t}\n\n\tprotected final void testLoading(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tConsumer<DocumentReferenceCollector> hitDocumentReferencesContributor,\n\t\t\tConsumer<EntityCollector<T>> expectedLoadedEntitiesContributor,\n\t\t\tBiConsumer<OrmSoftAssertions, List<T>> assertionsContributor) {\n\t\ttestLoading(\n\t\t\t\tsessionSetup,\n\t\t\t\tCollections.singletonList( model.getIndexedClass() ),\n\t\t\t\tCollections.singletonList( model.getIndexName() ),\n\t\t\t\tloadingOptionsContributor,\n\t\t\t\thitDocumentReferencesContributor,\n\t\t\t\texpectedLoadedEntitiesContributor,\n\t\t\t\tassertionsContributor,\n\t\t\t\tnull, null\n\t\t);\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "void init(SingleTypeLoadingModel<T> model, SingleTypeLoadingMapping mapping) {\n\t\tthis.model = model;\n\t\tthis.mapping = mapping;\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "AbstractSearchQueryEntityLoadingSingleTypeIT(SingleTypeLoadingModel<T> model, SingleTypeLoadingMapping mapping) {\n\t\tinit( model, mapping );\n\t}\nvoid init(SingleTypeLoadingModel<T> model, SingleTypeLoadingMapping mapping) {\n\t\tthis.model = model;\n\t\tthis.mapping = mapping;\n\t}",
        "diffSourceCode": "    38: \tAbstractSearchQueryEntityLoadingSingleTypeIT(SingleTypeLoadingModel<T> model, SingleTypeLoadingMapping mapping) {\n-   39: \t\tthis.model = model;\n-   40: \t\tthis.mapping = mapping;\n-   41: \t}\n-   42: \n-   43: \tprotected final void persistThatManyEntities(int entityCount) {\n-   44: \t\t// We don't care about what is indexed exactly, so use the lenient mode\n-   45: \t\tbackendMock().inLenientMode( () -> with( sessionFactory() ).runInTransaction( session -> {\n+   39: \t\tinit( model, mapping );\n+   40: \t}\n+   41: \n+   42: \tvoid init(SingleTypeLoadingModel<T> model, SingleTypeLoadingMapping mapping) {\n+   43: \t\tthis.model = model;\n+   44: \t\tthis.mapping = mapping;\n+   45: \t}\n",
        "uniqueId": "94c571c53c35a92257bede06edfb7f4bc3dd50f3_38_41_42_45_38_40",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Inline Method",
        "description": "Inline Method\tpublic testParamsForBothAnnotationsAndProgrammatic(defaultBackendConfiguration BackendConfiguration, namedBackendConfigurations Map<String,BackendConfiguration>, programmaticMappingContributor Consumer<ProgrammaticMappingConfigurationContext>) : List<DocumentationSetupHelper> inlined to public testParamsForBothAnnotationsAndProgrammatic(programmaticMappingContributor Consumer<ProgrammaticMappingConfigurationContext>) : List<? extends Arguments> in class org.hibernate.search.documentation.testsupport.DocumentationSetupHelper",
        "diffLocations": [
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
                "startLine": 40,
                "endLine": 45,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
                "startLine": 42,
                "endLine": 46,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
                "startLine": 56,
                "endLine": 62,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendConfiguration defaultBackendConfiguration,\n\t\t\tMap<String, BackendConfiguration> namedBackendConfigurations,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\treturn testParamsForBothAnnotationsAndProgrammatic( defaultBackendConfiguration, namedBackendConfigurations,\n\t\t\t\tCollections.emptySet(), programmaticMappingContributor );\n\t}",
        "filePathBefore": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
        "isPureRefactoring": true,
        "commitId": "94c571c53c35a92257bede06edfb7f4bc3dd50f3",
        "packageNameBefore": "org.hibernate.search.documentation.testsupport",
        "classNameBefore": "org.hibernate.search.documentation.testsupport.DocumentationSetupHelper",
        "methodNameBefore": "org.hibernate.search.documentation.testsupport.DocumentationSetupHelper#testParamsForBothAnnotationsAndProgrammatic",
        "invokedMethod": "methodSignature: org.hibernate.search.documentation.testsupport.DocumentationSetupHelper#testParamsForBothAnnotationsAndProgrammatic\n methodBody: public static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendSetupStrategy backendSetupStrategy,\n\t\t\tSet<Class<?>> additionalAnnotatedClasses,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\nList<DocumentationSetupHelper> result=new ArrayList<>();\nHibernateOrmSearchMappingConfigurer annotationMappingConfigurer=additionalAnnotatedClasses.isEmpty() ? null : context -> context.annotationMapping().add(additionalAnnotatedClasses);\nresult.add(new DocumentationSetupHelper(backendSetupStrategy,null,annotationMappingConfigurer));\nHibernateOrmSearchMappingConfigurer programmaticMappingConfigurer=context -> programmaticMappingContributor.accept(context.programmaticMapping());\nresult.add(new DocumentationSetupHelper(backendSetupStrategy,false,programmaticMappingConfigurer));\nreturn result;\n}",
        "classSignatureBefore": "public final class DocumentationSetupHelper\n\t\textends\n\t\tMappingSetupHelper<DocumentationSetupHelper.SetupContext,\n\t\t\t\tSimpleSessionFactoryBuilder,\n\t\t\t\tSimpleSessionFactoryBuilder,\n\t\t\t\tSessionFactory> ",
        "methodNameBeforeSet": [
            "org.hibernate.search.documentation.testsupport.DocumentationSetupHelper#testParamsForBothAnnotationsAndProgrammatic"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.documentation.testsupport.DocumentationSetupHelper"
        ],
        "classSignatureBeforeSet": [
            "public final class DocumentationSetupHelper\n\t\textends\n\t\tMappingSetupHelper<DocumentationSetupHelper.SetupContext,\n\t\t\t\tSimpleSessionFactoryBuilder,\n\t\t\t\tSimpleSessionFactoryBuilder,\n\t\t\t\tSessionFactory> "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Overlapped refactoring - can be identical by undoing the overlapped refactoring\n- Remove Parameter-",
                "description": "Remove Parameter refactoring on top the inlined method - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.documentation.testsupport;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.LinkedHashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.function.Consumer;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.mapping.HibernateOrmSearchMappingConfigurer;\nimport org.hibernate.search.mapper.orm.schema.management.SchemaManagementStrategyName;\nimport org.hibernate.search.mapper.pojo.mapping.definition.programmatic.ProgrammaticMappingConfigurationContext;\nimport org.hibernate.search.mapper.pojo.work.IndexingPlanSynchronizationStrategyNames;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendConfiguration;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendSetupStrategy;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.MappingSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.BackendMappingHandle;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.HibernateOrmMappingHandle;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmAssertionHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.SimpleSessionFactoryBuilder;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.multitenancy.impl.MultitenancyTestHelper;\n\npublic final class DocumentationSetupHelper\n\t\textends\n\t\tMappingSetupHelper<DocumentationSetupHelper.SetupContext,\n\t\t\t\tSimpleSessionFactoryBuilder,\n\t\t\t\tSimpleSessionFactoryBuilder,\n\t\t\t\tSessionFactory> {\n\n\tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendConfiguration backendConfiguration,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\treturn testParamsForBothAnnotationsAndProgrammatic( backendConfiguration,\n\t\t\t\tCollections.emptySet(), programmaticMappingContributor );\n\t}\n\n\tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendConfiguration backendConfiguration,\n\t\t\tSet<Class<?>> additionalAnnotatedClasses,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\treturn testParamsForBothAnnotationsAndProgrammatic(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n\t\t\t\tadditionalAnnotatedClasses, programmaticMappingContributor );\n\t}\n\n\tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendConfiguration defaultBackendConfiguration,\n\t\t\tMap<String, BackendConfiguration> namedBackendConfigurations,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\treturn testParamsForBothAnnotationsAndProgrammatic( defaultBackendConfiguration, namedBackendConfigurations,\n\t\t\t\tCollections.emptySet(), programmaticMappingContributor );\n\t}\n\n\tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendConfiguration defaultBackendConfiguration,\n\t\t\tMap<String, BackendConfiguration> namedBackendConfigurations,\n\t\t\tSet<Class<?>> additionalAnnotatedClasses,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\treturn testParamsForBothAnnotationsAndProgrammatic(\n\t\t\t\tBackendSetupStrategy.withMultipleBackends( defaultBackendConfiguration, namedBackendConfigurations ),\n\t\t\t\tadditionalAnnotatedClasses, programmaticMappingContributor );\n\t}\n\n\tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendSetupStrategy backendSetupStrategy,\n\t\t\tSet<Class<?>> additionalAnnotatedClasses,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\tList<DocumentationSetupHelper> result = new ArrayList<>();\n\t\t// Annotation-based mapping\n\t\tHibernateOrmSearchMappingConfigurer annotationMappingConfigurer =\n\t\t\t\tadditionalAnnotatedClasses.isEmpty()\n\t\t\t\t\t\t? null\n\t\t\t\t\t\t: context -> context.annotationMapping().add( additionalAnnotatedClasses );\n\t\tresult.add( new DocumentationSetupHelper( backendSetupStrategy,\n\t\t\t\tnull, annotationMappingConfigurer ) );\n\t\t// Programmatic mapping\n\t\tHibernateOrmSearchMappingConfigurer programmaticMappingConfigurer =\n\t\t\t\tcontext -> programmaticMappingContributor.accept( context.programmaticMapping() );\n\t\tresult.add( new DocumentationSetupHelper( backendSetupStrategy,\n\t\t\t\tfalse, programmaticMappingConfigurer ) );\n\t\treturn result;\n\t}\n\n\tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n\t\t\t\tnull, null\n\t\t);\n\t}\n\n\tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration,\n\t\t\tBoolean annotationProcessingEnabled, HibernateOrmSearchMappingConfigurer defaultMappingConfigurer) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n\t\t\t\tannotationProcessingEnabled, defaultMappingConfigurer\n\t\t);\n\t}\n\n\tprivate final Boolean annotationProcessingEnabled;\n\n\tprivate final HibernateOrmSearchMappingConfigurer defaultMappingConfigurer;\n\tprivate final OrmAssertionHelper assertionHelper;\n\n\tprivate DocumentationSetupHelper(BackendSetupStrategy backendSetupStrategy,\n\t\t\tBoolean annotationProcessingEnabled,\n\t\t\tHibernateOrmSearchMappingConfigurer defaultMappingConfigurer) {\n\t\tsuper( backendSetupStrategy );\n\t\tthis.annotationProcessingEnabled = annotationProcessingEnabled;\n\t\tthis.defaultMappingConfigurer = defaultMappingConfigurer;\n\t\tthis.assertionHelper = new OrmAssertionHelper( backendSetupStrategy );\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn super.toString()\n\t\t\t\t+ ( annotationProcessingEnabled == Boolean.FALSE ? \" - programmatic mapping\" : \"\" );\n\t}\n\n\t@Override\n\tpublic OrmAssertionHelper assertions() {\n\t\treturn assertionHelper;\n\t}\n\n\t@Override\n\tprotected SetupContext createSetupContext() {\n\t\treturn new SetupContext( annotationProcessingEnabled, defaultMappingConfigurer );\n\t}\n\n\t@Override\n\tprotected void close(SessionFactory toClose) {\n\t\ttoClose.close();\n\t}\n\n\tpublic final class SetupContext\n\t\t\textends\n\t\t\tMappingSetupHelper<SetupContext,\n\t\t\t\t\tSimpleSessionFactoryBuilder,\n\t\t\t\t\tSimpleSessionFactoryBuilder,\n\t\t\t\t\tSessionFactory>.AbstractSetupContext {\n\n\t\t// Use a LinkedHashMap for deterministic iteration\n\t\tprivate final Map<String, Object> overriddenProperties = new LinkedHashMap<>();\n\n\t\tSetupContext(Boolean annotationProcessingEnabled, HibernateOrmSearchMappingConfigurer defaultMappingConfigurer) {\n\t\t\t// Real backend => ensure we clean up everything before and after the tests\n\t\t\twithProperty( HibernateOrmMapperSettings.SCHEMA_MANAGEMENT_STRATEGY,\n\t\t\t\t\tSchemaManagementStrategyName.DROP_AND_CREATE_AND_DROP );\n\t\t\t// Override the indexing plan synchronization strategy according to our needs for testing\n\t\t\twithProperty( HibernateOrmMapperSettings.INDEXING_PLAN_SYNCHRONIZATION_STRATEGY,\n\t\t\t\t\tIndexingPlanSynchronizationStrategyNames.SYNC );\n\t\t\t// Set up default mapping if necessary\n\t\t\tif ( annotationProcessingEnabled != null ) {\n\t\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_PROCESS_ANNOTATIONS, annotationProcessingEnabled );\n\t\t\t}\n\t\t\tif ( defaultMappingConfigurer != null ) {\n\t\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_CONFIGURER, defaultMappingConfigurer );\n\t\t\t}\n\t\t\t// Ensure we don't build Jandex indexes needlessly:\n\t\t\t// discovery based on Jandex ought to be tested in real projects that don't use this setup helper.\n\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_BUILD_MISSING_DISCOVERED_JANDEX_INDEXES, false );\n\t\t\t// Ensure overridden properties will be applied\n\t\t\twithConfiguration( builder -> overriddenProperties.forEach( builder::setProperty ) );\n\t\t}\n\n\t\t@Override\n\t\tpublic SetupContext withProperty(String key, Object value) {\n\t\t\toverriddenProperties.put( key, value );\n\t\t\treturn thisAsC();\n\t\t}\n\n\t\tpublic SetupContext tenants(String... tenants) {\n\t\t\twithConfiguration( b -> MultitenancyTestHelper.enable( b, tenants ) );\n\t\t\treturn thisAsC();\n\t\t}\n\n\t\tpublic SessionFactory setup(Class<?>... annotatedTypes) {\n\t\t\treturn withConfiguration( builder -> builder.addAnnotatedClasses( Arrays.asList( annotatedTypes ) ) )\n\t\t\t\t\t.setup();\n\t\t}\n\n\t\t@Override\n\t\tprotected SimpleSessionFactoryBuilder createBuilder() {\n\t\t\treturn new SimpleSessionFactoryBuilder();\n\t\t}\n\n\t\t@Override\n\t\tprotected void consumeBeforeBuildConfigurations(SimpleSessionFactoryBuilder builder,\n\t\t\t\tList<Consumer<SimpleSessionFactoryBuilder>> consumers) {\n\t\t\tconsumers.forEach( c -> c.accept( builder ) );\n\t\t}\n\n\t\t@Override\n\t\tprotected SessionFactory build(SimpleSessionFactoryBuilder builder) {\n\t\t\treturn builder.build();\n\t\t}\n\n\t\t@Override\n\t\tprotected BackendMappingHandle toBackendMappingHandle(SessionFactory result) {\n\t\t\treturn new HibernateOrmMappingHandle( result );\n\t\t}\n\n\t\t@Override\n\t\tprotected SetupContext thisAsC() {\n\t\t\treturn this;\n\t\t}\n\t}\n\n}\n",
        "filePathAfter": "documentation/src/test/java/org/hibernate/search/documentation/testsupport/DocumentationSetupHelper.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.documentation.testsupport;\n\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.LinkedHashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.function.Consumer;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.mapping.HibernateOrmSearchMappingConfigurer;\nimport org.hibernate.search.mapper.orm.schema.management.SchemaManagementStrategyName;\nimport org.hibernate.search.mapper.pojo.mapping.definition.programmatic.ProgrammaticMappingConfigurationContext;\nimport org.hibernate.search.mapper.pojo.work.IndexingPlanSynchronizationStrategyNames;\nimport org.hibernate.search.util.impl.integrationtest.common.extension.BackendConfiguration;\nimport org.hibernate.search.util.impl.integrationtest.common.extension.BackendSetupStrategy;\nimport org.hibernate.search.util.impl.integrationtest.common.extension.MappingSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.BackendMappingHandle;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.HibernateOrmMappingHandle;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmAssertionHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.SimpleSessionFactoryBuilder;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.multitenancy.impl.MultitenancyTestHelper;\n\nimport org.junit.jupiter.params.provider.Arguments;\n\npublic final class DocumentationSetupHelper\n\t\textends\n\t\tMappingSetupHelper<DocumentationSetupHelper.SetupContext,\n\t\t\t\tSimpleSessionFactoryBuilder,\n\t\t\t\tSimpleSessionFactoryBuilder,\n\t\t\t\tSessionFactory> {\n\n\tpublic static List<? extends Arguments> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor\n\t) {\n\t\treturn testParamsForBothAnnotationsAndProgrammatic( Collections.emptySet(), programmaticMappingContributor );\n\t}\n\n\tpublic static List<? extends Arguments> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tSet<Class<?>> additionalAnnotatedClasses,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n\t\tList<Arguments> result = new ArrayList<>();\n\t\t// Annotation-based mapping\n\t\tHibernateOrmSearchMappingConfigurer annotationMappingConfigurer =\n\t\t\t\tadditionalAnnotatedClasses.isEmpty()\n\t\t\t\t\t\t? null\n\t\t\t\t\t\t: context -> context.annotationMapping().add( additionalAnnotatedClasses );\n\t\tresult.add( Arguments.of( null, annotationMappingConfigurer ) );\n\t\t// Programmatic mapping\n\t\tHibernateOrmSearchMappingConfigurer programmaticMappingConfigurer =\n\t\t\t\tcontext -> programmaticMappingContributor.accept( context.programmaticMapping() );\n\t\tresult.add( Arguments.of( false, programmaticMappingConfigurer ) );\n\t\treturn result;\n\t}\n\n\tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n\t\t\t\tnull, null\n\t\t);\n\t}\n\n\tpublic static DocumentationSetupHelper withSingleBackend(BackendConfiguration backendConfiguration,\n\t\t\tBoolean annotationProcessingEnabled, HibernateOrmSearchMappingConfigurer defaultMappingConfigurer) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withSingleBackend( backendConfiguration ),\n\t\t\t\tannotationProcessingEnabled, defaultMappingConfigurer\n\t\t);\n\t}\n\n\tpublic static DocumentationSetupHelper withMultipleBackends(BackendConfiguration defaultBackendConfiguration,\n\t\t\tMap<String, BackendConfiguration> namedBackendConfigurations) {\n\t\treturn new DocumentationSetupHelper(\n\t\t\t\tBackendSetupStrategy.withMultipleBackends( defaultBackendConfiguration, namedBackendConfigurations ),\n\t\t\t\tnull, null\n\t\t);\n\t}\n\n\tprivate Boolean annotationProcessingEnabled;\n\tprivate HibernateOrmSearchMappingConfigurer defaultMappingConfigurer;\n\tprivate OrmAssertionHelper assertionHelper;\n\n\tprivate DocumentationSetupHelper(BackendSetupStrategy backendSetupStrategy,\n\t\t\tBoolean annotationProcessingEnabled,\n\t\t\tHibernateOrmSearchMappingConfigurer defaultMappingConfigurer) {\n\t\tsuper( backendSetupStrategy, Type.METHOD );\n\t\tthis.annotationProcessingEnabled = annotationProcessingEnabled;\n\t\tthis.defaultMappingConfigurer = defaultMappingConfigurer;\n\t\tthis.assertionHelper = new OrmAssertionHelper( backendSetupStrategy );\n\t}\n\n\tpublic DocumentationSetupHelper withMappingConfigurer(HibernateOrmSearchMappingConfigurer defaultMappingConfigurer) {\n\t\tif ( this.defaultMappingConfigurer != null ) {\n\t\t\tthrow new IllegalStateException();\n\t\t}\n\t\tthis.defaultMappingConfigurer = defaultMappingConfigurer;\n\n\t\treturn this;\n\t}\n\n\tpublic DocumentationSetupHelper withAnnotationProcessingEnabled(Boolean annotationProcessingEnabled) {\n\t\tthis.annotationProcessingEnabled = annotationProcessingEnabled;\n\n\t\treturn this;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn super.toString()\n\t\t\t\t+ ( annotationProcessingEnabled == Boolean.FALSE ? \" - programmatic mapping\" : \"\" );\n\t}\n\n\t@Override\n\tpublic OrmAssertionHelper assertions() {\n\t\treturn assertionHelper;\n\t}\n\n\t@Override\n\tprotected SetupContext createSetupContext() {\n\t\treturn new SetupContext( annotationProcessingEnabled, defaultMappingConfigurer );\n\t}\n\n\t@Override\n\tprotected void close(SessionFactory toClose) {\n\t\ttoClose.close();\n\t}\n\n\tpublic final class SetupContext\n\t\t\textends\n\t\t\tMappingSetupHelper<SetupContext,\n\t\t\t\t\tSimpleSessionFactoryBuilder,\n\t\t\t\t\tSimpleSessionFactoryBuilder,\n\t\t\t\t\tSessionFactory>.AbstractSetupContext {\n\n\t\t// Use a LinkedHashMap for deterministic iteration\n\t\tprivate final Map<String, Object> overriddenProperties = new LinkedHashMap<>();\n\n\t\tSetupContext(Boolean annotationProcessingEnabled, HibernateOrmSearchMappingConfigurer defaultMappingConfigurer) {\n\t\t\t// Real backend => ensure we clean up everything before and after the tests\n\t\t\twithProperty( HibernateOrmMapperSettings.SCHEMA_MANAGEMENT_STRATEGY,\n\t\t\t\t\tSchemaManagementStrategyName.DROP_AND_CREATE_AND_DROP );\n\t\t\t// Override the indexing plan synchronization strategy according to our needs for testing\n\t\t\twithProperty( HibernateOrmMapperSettings.INDEXING_PLAN_SYNCHRONIZATION_STRATEGY,\n\t\t\t\t\tIndexingPlanSynchronizationStrategyNames.SYNC );\n\t\t\t// Set up default mapping if necessary\n\t\t\tif ( annotationProcessingEnabled != null ) {\n\t\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_PROCESS_ANNOTATIONS, annotationProcessingEnabled );\n\t\t\t}\n\t\t\tif ( defaultMappingConfigurer != null ) {\n\t\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_CONFIGURER, defaultMappingConfigurer );\n\t\t\t}\n\t\t\t// Ensure we don't build Jandex indexes needlessly:\n\t\t\t// discovery based on Jandex ought to be tested in real projects that don't use this setup helper.\n\t\t\twithProperty( HibernateOrmMapperSettings.MAPPING_BUILD_MISSING_DISCOVERED_JANDEX_INDEXES, false );\n\t\t\t// Ensure overridden properties will be applied\n\t\t\twithConfiguration( builder -> overriddenProperties.forEach( builder::setProperty ) );\n\t\t}\n\n\t\t@Override\n\t\tpublic SetupContext withProperty(String key, Object value) {\n\t\t\toverriddenProperties.put( key, value );\n\t\t\treturn thisAsC();\n\t\t}\n\n\t\tpublic SetupContext tenants(String... tenants) {\n\t\t\twithConfiguration( b -> MultitenancyTestHelper.enable( b, tenants ) );\n\t\t\treturn thisAsC();\n\t\t}\n\n\t\tpublic SessionFactory setup(Class<?>... annotatedTypes) {\n\t\t\treturn withConfiguration( builder -> builder.addAnnotatedClasses( Arrays.asList( annotatedTypes ) ) )\n\t\t\t\t\t.setup();\n\t\t}\n\n\t\t@Override\n\t\tprotected SimpleSessionFactoryBuilder createBuilder() {\n\t\t\treturn new SimpleSessionFactoryBuilder();\n\t\t}\n\n\t\t@Override\n\t\tprotected void consumeBeforeBuildConfigurations(SimpleSessionFactoryBuilder builder,\n\t\t\t\tList<Consumer<SimpleSessionFactoryBuilder>> consumers) {\n\t\t\tconsumers.forEach( c -> c.accept( builder ) );\n\t\t}\n\n\t\t@Override\n\t\tprotected SessionFactory build(SimpleSessionFactoryBuilder builder) {\n\t\t\treturn builder.build();\n\t\t}\n\n\t\t@Override\n\t\tprotected BackendMappingHandle toBackendMappingHandle(SessionFactory result) {\n\t\t\treturn new HibernateOrmMappingHandle( result );\n\t\t}\n\n\t\t@Override\n\t\tprotected SetupContext thisAsC() {\n\t\t\treturn this;\n\t\t}\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.documentation.testsupport.DocumentationSetupHelper#testParamsForBothAnnotationsAndProgrammatic\n methodBody: public static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tBackendSetupStrategy backendSetupStrategy,\n\t\t\tSet<Class<?>> additionalAnnotatedClasses,\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\nList<DocumentationSetupHelper> result=new ArrayList<>();\nHibernateOrmSearchMappingConfigurer annotationMappingConfigurer=additionalAnnotatedClasses.isEmpty() ? null : context -> context.annotationMapping().add(additionalAnnotatedClasses);\nresult.add(new DocumentationSetupHelper(backendSetupStrategy,null,annotationMappingConfigurer));\nHibernateOrmSearchMappingConfigurer programmaticMappingConfigurer=context -> programmaticMappingContributor.accept(context.programmaticMapping());\nresult.add(new DocumentationSetupHelper(backendSetupStrategy,false,programmaticMappingConfigurer));\nreturn result;\n}"
        ],
        "sourceCodeAfterRefactoring": "public static List<? extends Arguments> testParamsForBothAnnotationsAndProgrammatic(\n\t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor\n\t) {\n\t\treturn testParamsForBothAnnotationsAndProgrammatic( Collections.emptySet(), programmaticMappingContributor );\n\t}",
        "diffSourceCode": "-   40: \tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n-   41: \t\t\tBackendConfiguration backendConfiguration,\n-   42: \t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n-   43: \t\treturn testParamsForBothAnnotationsAndProgrammatic( backendConfiguration,\n-   44: \t\t\t\tCollections.emptySet(), programmaticMappingContributor );\n-   45: \t}\n-   46: \n-   56: \tpublic static List<DocumentationSetupHelper> testParamsForBothAnnotationsAndProgrammatic(\n-   57: \t\t\tBackendConfiguration defaultBackendConfiguration,\n-   58: \t\t\tMap<String, BackendConfiguration> namedBackendConfigurations,\n-   59: \t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor) {\n-   60: \t\treturn testParamsForBothAnnotationsAndProgrammatic( defaultBackendConfiguration, namedBackendConfigurations,\n-   61: \t\t\t\tCollections.emptySet(), programmaticMappingContributor );\n-   62: \t}\n+   40: \t\t\t\tSessionFactory> {\n+   41: \n+   42: \tpublic static List<? extends Arguments> testParamsForBothAnnotationsAndProgrammatic(\n+   43: \t\t\tConsumer<ProgrammaticMappingConfigurationContext> programmaticMappingContributor\n+   44: \t) {\n+   45: \t\treturn testParamsForBothAnnotationsAndProgrammatic( Collections.emptySet(), programmaticMappingContributor );\n+   46: \t}\n+   56: \t\t\t\t\t\t: context -> context.annotationMapping().add( additionalAnnotatedClasses );\n+   57: \t\tresult.add( Arguments.of( null, annotationMappingConfigurer ) );\n+   58: \t\t// Programmatic mapping\n+   59: \t\tHibernateOrmSearchMappingConfigurer programmaticMappingConfigurer =\n+   60: \t\t\t\tcontext -> programmaticMappingContributor.accept( context.programmaticMapping() );\n+   61: \t\tresult.add( Arguments.of( false, programmaticMappingConfigurer ) );\n+   62: \t\treturn result;\n",
        "uniqueId": "94c571c53c35a92257bede06edfb7f4bc3dd50f3_40_45__42_46_56_62",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate loadDocument() : Document extracted from public verifyIndexFieldTypes() : void in class org.hibernate.search.integrationtest.backend.lucene.mapping.LuceneFieldAttributesIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/mapping/LuceneFieldAttributesIT.java",
                "startLine": 55,
                "endLine": 74,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/mapping/LuceneFieldAttributesIT.java",
                "startLine": 58,
                "endLine": 69,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/mapping/LuceneFieldAttributesIT.java",
                "startLine": 96,
                "endLine": 108,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void verifyIndexFieldTypes() {\n\t\tSearchQuery<Document> query = indexManager.createSearchScope().query()\n\t\t\t\t.asProjection(\n\t\t\t\t\t\tf -> f.extension( LuceneExtension.get() ).document()\n\t\t\t\t)\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\n\t\tList<Document> result = query.fetch().getHits();\n\n\t\tAssertions.assertThat( result ).hasSize( 1 );\n\t\tDocument document = result.get( 0 );\n\n\t\t// norms false => omit-norms true\n\t\tAssertions.assertThat( document.getField( \"keyword\" ).fieldType().omitNorms() ).isTrue();\n\n\t\t// norms true => omit-norms false\n\t\tAssertions.assertThat( document.getField( \"text\" ).fieldType().omitNorms() ).isFalse();\n\t}",
        "filePathBefore": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/mapping/LuceneFieldAttributesIT.java",
        "isPureRefactoring": true,
        "commitId": "be1c35f6dba9c0ad5f4b65bc47950783ae1c7b32",
        "packageNameBefore": "org.hibernate.search.integrationtest.backend.lucene.mapping",
        "classNameBefore": "org.hibernate.search.integrationtest.backend.lucene.mapping.LuceneFieldAttributesIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.backend.lucene.mapping.LuceneFieldAttributesIT#verifyIndexFieldTypes",
        "classSignatureBefore": "public class LuceneFieldAttributesIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.lucene.mapping.LuceneFieldAttributesIT#verifyIndexFieldTypes"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.lucene.mapping.LuceneFieldAttributesIT"
        ],
        "classSignatureBeforeSet": [
            "public class LuceneFieldAttributesIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.lucene.mapping;\n\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMapperUtils.referenceProvider;\n\nimport java.util.List;\n\nimport org.hibernate.search.backend.lucene.LuceneExtension;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldReference;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\nimport org.hibernate.search.engine.backend.types.Projectable;\nimport org.hibernate.search.engine.search.query.SearchQuery;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.configuration.DefaultAnalysisDefinitions;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.rule.SearchSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMappingIndexManager;\n\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.apache.lucene.document.Document;\nimport org.assertj.core.api.Assertions;\n\npublic class LuceneFieldAttributesIT {\n\n\tprivate static final String INDEX_NAME = \"my-index\";\n\tprivate static final String TEXT = \"This is a text containing things. Red house with a blue carpet on the road...\";\n\n\t@Rule\n\tpublic SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\tprivate IndexMapping indexMapping;\n\tprivate StubMappingIndexManager indexManager;\n\n\t@Before\n\tpublic void setup() {\n\t\tsetupHelper.withDefaultConfiguration( \"myLuceneBackend\" )\n\t\t\t\t.withIndex(\n\t\t\t\t\t\tINDEX_NAME,\n\t\t\t\t\t\tctx -> this.indexMapping = new IndexMapping( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.indexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.setup();\n\n\t\tinitData();\n\t}\n\n\t@Test\n\tpublic void verifyIndexFieldTypes() {\n\t\tSearchQuery<Document> query = indexManager.createSearchScope().query()\n\t\t\t\t.asProjection(\n\t\t\t\t\t\tf -> f.extension( LuceneExtension.get() ).document()\n\t\t\t\t)\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\n\t\tList<Document> result = query.fetch().getHits();\n\n\t\tAssertions.assertThat( result ).hasSize( 1 );\n\t\tDocument document = result.get( 0 );\n\n\t\t// norms false => omit-norms true\n\t\tAssertions.assertThat( document.getField( \"keyword\" ).fieldType().omitNorms() ).isTrue();\n\n\t\t// norms true => omit-norms false\n\t\tAssertions.assertThat( document.getField( \"text\" ).fieldType().omitNorms() ).isFalse();\n\t}\n\n\tprivate void initData() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan();\n\n\t\tworkPlan.add( referenceProvider( \"ID:1\" ), document -> {\n\t\t\tdocument.addValue( indexMapping.string, \"keyword\" );\n\t\t\tdocument.addValue( indexMapping.text, TEXT );\n\t\t} );\n\n\t\tworkPlan.execute().join();\n\t}\n\n\tprivate static class IndexMapping {\n\n\t\tfinal IndexFieldReference<String> string;\n\t\tfinal IndexFieldReference<String> text;\n\n\t\tIndexMapping(IndexSchemaElement root) {\n\t\t\tstring = root.field( \"keyword\", f -> f.asString().projectable( Projectable.YES ) ).toReference();\n\t\t\ttext = root.field( \"text\", f -> f.asString().analyzer( DefaultAnalysisDefinitions.ANALYZER_STANDARD_ENGLISH.name ).projectable( Projectable.YES ) ).toReference();\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/mapping/LuceneFieldAttributesIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.lucene.mapping;\n\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMapperUtils.referenceProvider;\n\nimport java.util.List;\n\nimport org.hibernate.search.backend.lucene.LuceneExtension;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldReference;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.index.spi.IndexWorkPlan;\nimport org.hibernate.search.engine.backend.types.Norms;\nimport org.hibernate.search.engine.backend.types.Projectable;\nimport org.hibernate.search.engine.backend.types.TermVector;\nimport org.hibernate.search.engine.search.query.SearchQuery;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.configuration.DefaultAnalysisDefinitions;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.rule.SearchSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMappingIndexManager;\n\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.index.IndexableField;\nimport org.assertj.core.api.Assertions;\n\npublic class LuceneFieldAttributesIT {\n\n\tprivate static final String INDEX_NAME = \"my-index\";\n\tprivate static final String TEXT = \"This is a text containing things. Red house with a blue carpet on the road...\";\n\n\t@Rule\n\tpublic SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\tprivate IndexMapping indexMapping;\n\tprivate StubMappingIndexManager indexManager;\n\n\t@Before\n\tpublic void setup() {\n\t\tsetupHelper.withDefaultConfiguration( \"myLuceneBackend\" )\n\t\t\t\t.withIndex(\n\t\t\t\t\t\tINDEX_NAME,\n\t\t\t\t\t\tctx -> this.indexMapping = new IndexMapping( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.indexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.setup();\n\n\t\tinitData();\n\t}\n\n\t@Test\n\tpublic void verifyNorms() {\n\t\tDocument document = loadDocument();\n\n\t\t// norms false => omit-norms true\n\t\tAssertions.assertThat( document.getField( \"keyword\" ).fieldType().omitNorms() ).isTrue();\n\t\tAssertions.assertThat( document.getField( \"noNorms\" ).fieldType().omitNorms() ).isTrue();\n\n\t\t// norms true => omit-norms false\n\t\tAssertions.assertThat( document.getField( \"text\" ).fieldType().omitNorms() ).isFalse();\n\t\tAssertions.assertThat( document.getField( \"norms\" ).fieldType().omitNorms() ).isFalse();\n\t}\n\n\t@Test\n\tpublic void verifyTermVector() {\n\t\tDocument document = loadDocument();\n\n\t\tIndexableField field = document.getField( \"text\" );\n\t\t// default no term vector stored\n\t\tAssertions.assertThat( field.fieldType().storeTermVectors() ).isFalse();\n\t\tAssertions.assertThat( field.fieldType().storeTermVectorPositions() ).isFalse();\n\t\tAssertions.assertThat( field.fieldType().storeTermVectorOffsets() ).isFalse();\n\t\tAssertions.assertThat( field.fieldType().storeTermVectorPayloads() ).isFalse();\n\n\t\tfield = document.getField( \"termVector\" );\n\t\tAssertions.assertThat( field.fieldType().storeTermVectors() ).isTrue();\n\t\tAssertions.assertThat( field.fieldType().storeTermVectorPositions() ).isFalse();\n\t\tAssertions.assertThat( field.fieldType().storeTermVectorOffsets() ).isFalse();\n\t\tAssertions.assertThat( field.fieldType().storeTermVectorPayloads() ).isFalse();\n\n\t\tfield = document.getField( \"moreOptions\" );\n\t\tAssertions.assertThat( field.fieldType().storeTermVectors() ).isTrue();\n\t\t// TODO these are not true:\n\t\t// Assertions.assertThat( field.fieldType().storeTermVectorPositions() ).isTrue();\n\t\t// Assertions.assertThat( field.fieldType().storeTermVectorOffsets() ).isTrue();\n\t\t// Assertions.assertThat( field.fieldType().storeTermVectorPayloads() ).isTrue();\n\t}\n\n\tprivate Document loadDocument() {\n\t\tSearchQuery<Document> query = indexManager.createSearchScope().query()\n\t\t\t\t.asProjection(\n\t\t\t\t\t\tf -> f.extension( LuceneExtension.get() ).document()\n\t\t\t\t)\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\n\t\tList<Document> result = query.fetch().getHits();\n\n\t\tAssertions.assertThat( result ).hasSize( 1 );\n\t\treturn result.get( 0 );\n\t}\n\n\tprivate void initData() {\n\t\tIndexWorkPlan<? extends DocumentElement> workPlan = indexManager.createWorkPlan();\n\t\tworkPlan.add( referenceProvider( \"ID:1\" ), document -> {\n\t\t\tdocument.addValue( indexMapping.string, \"keyword\" );\n\t\t\tdocument.addValue( indexMapping.text, TEXT );\n\t\t\tdocument.addValue( indexMapping.norms, TEXT );\n\t\t\tdocument.addValue( indexMapping.noNorms, TEXT );\n\t\t\tdocument.addValue( indexMapping.termVector, TEXT );\n\t\t\tdocument.addValue( indexMapping.moreOptions, \"Search 6 groundwork - Add the missing common field type options compared to Search 5\" );\n\t\t} );\n\n\t\tworkPlan.execute().join();\n\t}\n\n\tprivate static class IndexMapping {\n\t\tfinal IndexFieldReference<String> string;\n\t\tfinal IndexFieldReference<String> text;\n\t\tfinal IndexFieldReference<String> norms;\n\t\tfinal IndexFieldReference<String> noNorms;\n\t\tfinal IndexFieldReference<String> termVector;\n\t\tfinal IndexFieldReference<String> moreOptions;\n\n\t\tIndexMapping(IndexSchemaElement root) {\n\t\t\tstring = root.field( \"keyword\", f -> f.asString().projectable( Projectable.YES ) ).toReference();\n\t\t\ttext = root.field( \"text\", f -> f.asString().analyzer( DefaultAnalysisDefinitions.ANALYZER_STANDARD_ENGLISH.name ).projectable( Projectable.YES ) ).toReference();\n\n\t\t\tnorms = root.field( \"norms\", f -> f.asString().analyzer( DefaultAnalysisDefinitions.ANALYZER_STANDARD_ENGLISH.name ).projectable( Projectable.YES )\n\t\t\t\t\t.norms( Norms.YES )\n\t\t\t).toReference();\n\n\t\t\tnoNorms = root.field( \"noNorms\", f -> f.asString().analyzer( DefaultAnalysisDefinitions.ANALYZER_STANDARD_ENGLISH.name ).projectable( Projectable.YES )\n\t\t\t\t\t.norms( Norms.NO )\n\t\t\t).toReference();\n\n\t\t\ttermVector = root.field( \"termVector\", f -> f.asString().analyzer( DefaultAnalysisDefinitions.ANALYZER_STANDARD_ENGLISH.name ).projectable( Projectable.YES )\n\t\t\t\t\t.termVector( TermVector.YES )\n\t\t\t).toReference();\n\n\t\t\tmoreOptions = root.field( \"moreOptions\", f -> f.asString().analyzer( DefaultAnalysisDefinitions.ANALYZER_STANDARD_ENGLISH.name ).projectable( Projectable.YES )\n\t\t\t\t\t.termVector( TermVector.WITH_POSITIONS_OFFSETS_PAYLOADS )\n\t\t\t).toReference();\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private Document loadDocument() {\n\t\tSearchQuery<Document> query = indexManager.createSearchScope().query()\n\t\t\t\t.asProjection(\n\t\t\t\t\t\tf -> f.extension( LuceneExtension.get() ).document()\n\t\t\t\t)\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\n\t\tList<Document> result = query.fetch().getHits();\n\n\t\tAssertions.assertThat( result ).hasSize( 1 );\n\t\treturn result.get( 0 );\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void verifyNorms() {\n\t\tDocument document = loadDocument();\n\n\t\t// norms false => omit-norms true\n\t\tAssertions.assertThat( document.getField( \"keyword\" ).fieldType().omitNorms() ).isTrue();\n\t\tAssertions.assertThat( document.getField( \"noNorms\" ).fieldType().omitNorms() ).isTrue();\n\n\t\t// norms true => omit-norms false\n\t\tAssertions.assertThat( document.getField( \"text\" ).fieldType().omitNorms() ).isFalse();\n\t\tAssertions.assertThat( document.getField( \"norms\" ).fieldType().omitNorms() ).isFalse();\n\t}\nprivate Document loadDocument() {\n\t\tSearchQuery<Document> query = indexManager.createSearchScope().query()\n\t\t\t\t.asProjection(\n\t\t\t\t\t\tf -> f.extension( LuceneExtension.get() ).document()\n\t\t\t\t)\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\n\t\tList<Document> result = query.fetch().getHits();\n\n\t\tAssertions.assertThat( result ).hasSize( 1 );\n\t\treturn result.get( 0 );\n\t}",
        "diffSourceCode": "-   55: \t@Test\n-   56: \tpublic void verifyIndexFieldTypes() {\n-   57: \t\tSearchQuery<Document> query = indexManager.createSearchScope().query()\n-   58: \t\t\t\t.asProjection(\n-   59: \t\t\t\t\t\tf -> f.extension( LuceneExtension.get() ).document()\n-   60: \t\t\t\t)\n-   61: \t\t\t\t.predicate( f -> f.matchAll() )\n-   62: \t\t\t\t.toQuery();\n-   63: \n-   64: \t\tList<Document> result = query.fetch().getHits();\n+   55: \t\tinitData();\n+   56: \t}\n+   57: \n+   58: \t@Test\n+   59: \tpublic void verifyNorms() {\n+   60: \t\tDocument document = loadDocument();\n+   61: \n+   62: \t\t// norms false => omit-norms true\n+   63: \t\tAssertions.assertThat( document.getField( \"keyword\" ).fieldType().omitNorms() ).isTrue();\n+   64: \t\tAssertions.assertThat( document.getField( \"noNorms\" ).fieldType().omitNorms() ).isTrue();\n    65: \n-   66: \t\tAssertions.assertThat( result ).hasSize( 1 );\n-   67: \t\tDocument document = result.get( 0 );\n-   68: \n-   69: \t\t// norms false => omit-norms true\n-   70: \t\tAssertions.assertThat( document.getField( \"keyword\" ).fieldType().omitNorms() ).isTrue();\n-   71: \n-   72: \t\t// norms true => omit-norms false\n-   73: \t\tAssertions.assertThat( document.getField( \"text\" ).fieldType().omitNorms() ).isFalse();\n-   74: \t}\n-   96: \t}\n-   97: }\n+   66: \t\t// norms true => omit-norms false\n+   67: \t\tAssertions.assertThat( document.getField( \"text\" ).fieldType().omitNorms() ).isFalse();\n+   68: \t\tAssertions.assertThat( document.getField( \"norms\" ).fieldType().omitNorms() ).isFalse();\n+   69: \t}\n+   70: \n+   71: \t@Test\n+   72: \tpublic void verifyTermVector() {\n+   73: \t\tDocument document = loadDocument();\n+   74: \n+   96: \tprivate Document loadDocument() {\n+   97: \t\tSearchQuery<Document> query = indexManager.createSearchScope().query()\n+   98: \t\t\t\t.asProjection(\n+   99: \t\t\t\t\t\tf -> f.extension( LuceneExtension.get() ).document()\n+  100: \t\t\t\t)\n+  101: \t\t\t\t.predicate( f -> f.matchAll() )\n+  102: \t\t\t\t.toQuery();\n+  103: \n+  104: \t\tList<Document> result = query.fetch().getHits();\n+  105: \n+  106: \t\tAssertions.assertThat( result ).hasSize( 1 );\n+  107: \t\treturn result.get( 0 );\n+  108: \t}\n",
        "uniqueId": "be1c35f6dba9c0ad5f4b65bc47950783ae1c7b32_55_74_96_108_58_69",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Move And Rename Method",
        "description": "Move And Rename Method\tpublic indexNullAs_noParsing() : void from class org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.ValueBridgeBaseIT to public error_indexNullAs_noParsing() : void from class org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.FieldBaseIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/pojo/src/test/java/org/hibernate/search/integrationtest/mapper/pojo/mapping/definition/ValueBridgeBaseIT.java",
                "startLine": 79,
                "endLine": 98,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/pojo/src/test/java/org/hibernate/search/integrationtest/mapper/pojo/mapping/definition/FieldBaseIT.java",
                "startLine": 325,
                "endLine": 344,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void indexNullAs_noParsing() {\n\t\t@Indexed(index = INDEX_NAME)\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tInteger integer;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = NoParsingValueBridge.class), indexNullAs = \"7\")\n\t\t\tpublic Integer getInteger() { return integer; }\n\t\t}\n\n\t\tSubTest.expectException( () -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class ) )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"does not support parsing a value from a String\" )\n\t\t\t\t.hasMessageContaining( \"integer\" );\n\t}",
        "filePathBefore": "integrationtest/mapper/pojo/src/test/java/org/hibernate/search/integrationtest/mapper/pojo/mapping/definition/ValueBridgeBaseIT.java",
        "isPureRefactoring": true,
        "commitId": "60720a3723b836213b0e663b71fcbdbb3ce58d36",
        "packageNameBefore": "org.hibernate.search.integrationtest.mapper.pojo.mapping.definition",
        "classNameBefore": "org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.ValueBridgeBaseIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.ValueBridgeBaseIT#indexNullAs_noParsing",
        "classSignatureBefore": "public class ValueBridgeBaseIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.ValueBridgeBaseIT#indexNullAs_noParsing"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.ValueBridgeBaseIT"
        ],
        "classSignatureBeforeSet": [
            "public class ValueBridgeBaseIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.pojo.mapping.definition;\n\nimport java.lang.invoke.MethodHandles;\n\nimport org.hibernate.search.integrationtest.mapper.pojo.testsupport.util.rule.JavaBeanMappingSetupHelper;\nimport org.hibernate.search.mapper.javabean.JavaBeanMapping;\nimport org.hibernate.search.mapper.javabean.session.SearchSession;\nimport org.hibernate.search.mapper.pojo.bridge.ValueBridge;\nimport org.hibernate.search.mapper.pojo.bridge.runtime.ValueBridgeToIndexedValueContext;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.DocumentId;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.ValueBridgeRef;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.test.SubTest;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\n/**\n * Test common use cases of (custom) value bridges.\n * <p>\n * Does not test reindexing in depth; this is tested in {@code AutomaticIndexing*} tests in the ORM mapper.\n * <p>\n * Does not test field annotations in depth; this is tested in {@link FieldBaseIT}.\n */\n@SuppressWarnings(\"unused\")\npublic class ValueBridgeBaseIT {\n\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic JavaBeanMappingSetupHelper setupHelper = new JavaBeanMappingSetupHelper( MethodHandles.lookup() );\n\n\t@Test\n\tpublic void indexNullAs() {\n\t\t@Indexed(index = INDEX_NAME)\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tInteger integer;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = ParsingValueBridge.class), indexNullAs = \"7\")\n\t\t\tpublic Integer getInteger() { return integer; }\n\t\t}\n\n\t\tbackendMock.expectSchema( INDEX_NAME, b -> b\n\t\t\t\t.field( \"integer\", Integer.class, f -> f.indexNullAs( 7 ) )\n\t\t);\n\n\t\tJavaBeanMapping mapping = setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\ttry ( SearchSession session = mapping.createSession() ) {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.id = 1;\n\t\t\tsession.getMainWorkPlan().add( entity );\n\n\t\t\tbackendMock.expectWorks( INDEX_NAME )\n\t\t\t\t\t// Stub backend is not supposed to use 'indexNullAs' option\n\t\t\t\t\t.add( \"1\", b -> b.field( \"integer\", null ) )\n\t\t\t\t\t.preparedThenExecuted();\n\t\t}\n\t\tbackendMock.verifyExpectationsMet();\n\t}\n\n\t@Test\n\tpublic void indexNullAs_noParsing() {\n\t\t@Indexed(index = INDEX_NAME)\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tInteger integer;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = NoParsingValueBridge.class), indexNullAs = \"7\")\n\t\t\tpublic Integer getInteger() { return integer; }\n\t\t}\n\n\t\tSubTest.expectException( () -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class ) )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"does not support parsing a value from a String\" )\n\t\t\t\t.hasMessageContaining( \"integer\" );\n\t}\n\n\tpublic static class NoParsingValueBridge implements ValueBridge<Integer, Integer> {\n\n\t\tpublic NoParsingValueBridge() {\n\t\t}\n\n\t\t@Override\n\t\tpublic Integer toIndexedValue(Integer value, ValueBridgeToIndexedValueContext context) {\n\t\t\treturn value;\n\t\t}\n\n\t\t@Override\n\t\tpublic Integer cast(Object value) {\n\t\t\treturn (Integer) value;\n\t\t}\n\t}\n\n\tpublic static class ParsingValueBridge extends NoParsingValueBridge {\n\n\t\tpublic ParsingValueBridge() {\n\t\t}\n\n\t\t@Override\n\t\tpublic Integer parse(String value) {\n\t\t\treturn Integer.parseInt( value );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/pojo/src/test/java/org/hibernate/search/integrationtest/mapper/pojo/mapping/definition/FieldBaseIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.pojo.mapping.definition;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.List;\n\nimport org.hibernate.search.integrationtest.mapper.pojo.testsupport.util.rule.JavaBeanMappingSetupHelper;\nimport org.hibernate.search.mapper.javabean.JavaBeanMapping;\nimport org.hibernate.search.mapper.javabean.session.SearchSession;\nimport org.hibernate.search.mapper.pojo.bridge.ValueBridge;\nimport org.hibernate.search.mapper.pojo.bridge.runtime.ValueBridgeToIndexedValueContext;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.DocumentId;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.ValueBridgeRef;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.FailureReportUtils;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.test.SubTest;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\n/**\n * Test common use cases of the {@code @GenericField} annotation.\n * <p>\n * Does not test default bridges, which are tested in {@link FieldDefaultBridgeIT}.\n * <p>\n * Does not test uses of container value extractors, which are tested in {@link FieldContainerExtractorBaseIT}\n * (and others, see javadoc on that class).\n */\npublic class FieldBaseIT {\n\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic JavaBeanMappingSetupHelper setupHelper = new JavaBeanMappingSetupHelper( MethodHandles.lookup() );\n\n\t@Test\n\tpublic void error_unableToResolveDefaultValueBridgeFromSourceType() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tObject myProperty;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField\n\t\t\tpublic Object getMyProperty() {\n\t\t\t\treturn myProperty;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".myProperty\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Unable to find a default value bridge implementation for type '\"\n\t\t\t\t\t\t\t\t\t\t+ Object.class.getName() + \"'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\t@Test\n\t@SuppressWarnings(\"rawtypes\")\n\tpublic void error_unableToResolveDefaultValueBridgeFromSourceType_enumSuperClassRaw() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tEnum myProperty;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField\n\t\t\tpublic Enum getMyProperty() {\n\t\t\t\treturn myProperty;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".myProperty\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Unable to find a default value bridge implementation for type 'java.lang.Enum'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void error_unableToResolveDefaultValueBridgeFromSourceType_enumSuperClassWithWildcard() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tEnum<?> myProperty;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField\n\t\t\tpublic Enum<?> getMyProperty() {\n\t\t\t\treturn myProperty;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".myProperty\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Unable to find a default value bridge implementation for type 'java.lang.Enum<?>'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void error_unableToResolveDefaultValueBridgeFromSourceType_enumSuperClassWithParameters() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tEnum<EnumForEnumSuperClassTest> myProperty;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField\n\t\t\tpublic Enum<EnumForEnumSuperClassTest> getMyProperty() {\n\t\t\t\treturn myProperty;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".myProperty\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Unable to find a default value bridge implementation for type 'java.lang.Enum<\"\n\t\t\t\t\t\t\t\t\t\t+ EnumForEnumSuperClassTest.class.getName() + \">'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\tenum EnumForEnumSuperClassTest {\n\t\tVALUE1,\n\t\tVALUE2\n\t}\n\n\t@Test\n\tpublic void error_invalidInputTypeForValueBridge() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\t@DocumentId\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = MyStringBridge.class))\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".id\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Value bridge '\" + MyStringBridge.TOSTRING + \"' cannot be applied to input type '\"\n\t\t\t\t\t\t\t\t\t\t+ Integer.class.getName() + \"'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void error_invalidInputTypeForValueBridge_implicitContainerExtractor() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tList<Integer> numbers;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = MyStringBridge.class))\n\t\t\tpublic List<Integer> getNumbers() {\n\t\t\t\treturn numbers;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".numbers\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Value bridge '\" + MyStringBridge.TOSTRING + \"' cannot be applied to input type '\"\n\t\t\t\t\t\t\t\t\t\t+ Integer.class.getName() + \"'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\tpublic static class MyStringBridge implements ValueBridge<String, String> {\n\t\tprivate static String TOSTRING = \"<MyStringBridge toString() result>\";\n\t\t@Override\n\t\tpublic String cast(Object value) {\n\t\t\tthrow new UnsupportedOperationException( \"Should not be called\" );\n\t\t}\n\t\t@Override\n\t\tpublic String toIndexedValue(String value,\n\t\t\t\tValueBridgeToIndexedValueContext context) {\n\t\t\tthrow new UnsupportedOperationException( \"Should not be called\" );\n\t\t}\n\t\t@Override\n\t\tpublic String toString() {\n\t\t\treturn TOSTRING;\n\t\t}\n\t}\n\n\t@Test\n\tpublic void error_definingBothBridgeReferenceAndBridgeBuilderReference() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\t@DocumentId\n\t\t\t@GenericField(\n\t\t\t\t\tvalueBridge = @ValueBridgeRef(name = \"foo\", builderName = \"bar\")\n\t\t\t)\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".id\" )\n\t\t\t\t\t\t.annotationContextAnyParameters( GenericField.class )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Annotation @GenericField on property 'id' defines both valueBridge and valueBridgeBuilder.\"\n\t\t\t\t\t\t\t\t\t\t+ \" Only one of those can be defined, not both.\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void indexNullAs() {\n\t\t@Indexed(index = INDEX_NAME)\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tInteger integer;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = ParsingValueBridge.class), indexNullAs = \"7\")\n\t\t\tpublic Integer getInteger() { return integer; }\n\t\t}\n\n\t\tbackendMock.expectSchema( INDEX_NAME, b -> b\n\t\t\t\t.field( \"integer\", Integer.class, f -> f.indexNullAs( 7 ) )\n\t\t);\n\n\t\tJavaBeanMapping mapping = setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\ttry ( SearchSession session = mapping.createSession() ) {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.id = 1;\n\t\t\tsession.getMainWorkPlan().add( entity );\n\n\t\t\tbackendMock.expectWorks( INDEX_NAME )\n\t\t\t\t\t// Stub backend is not supposed to use 'indexNullAs' option\n\t\t\t\t\t.add( \"1\", b -> b.field( \"integer\", null ) )\n\t\t\t\t\t.preparedThenExecuted();\n\t\t}\n\t\tbackendMock.verifyExpectationsMet();\n\t}\n\n\tpublic static class ParsingValueBridge extends NoParsingValueBridge {\n\n\t\tpublic ParsingValueBridge() {\n\t\t}\n\n\t\t@Override\n\t\tpublic Integer parse(String value) {\n\t\t\treturn Integer.parseInt( value );\n\t\t}\n\t}\n\n\t@Test\n\tpublic void error_indexNullAs_noParsing() {\n\t\t@Indexed(index = INDEX_NAME)\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tInteger integer;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = NoParsingValueBridge.class), indexNullAs = \"7\")\n\t\t\tpublic Integer getInteger() { return integer; }\n\t\t}\n\n\t\tSubTest.expectException( () -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class ) )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"does not support parsing a value from a String\" )\n\t\t\t\t.hasMessageContaining( \"integer\" );\n\t}\n\n\tpublic static class NoParsingValueBridge implements ValueBridge<Integer, Integer> {\n\n\t\tpublic NoParsingValueBridge() {\n\t\t}\n\n\t\t@Override\n\t\tpublic Integer toIndexedValue(Integer value, ValueBridgeToIndexedValueContext context) {\n\t\t\treturn value;\n\t\t}\n\n\t\t@Override\n\t\tpublic Integer cast(Object value) {\n\t\t\treturn (Integer) value;\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void error_indexNullAs_noParsing() {\n\t\t@Indexed(index = INDEX_NAME)\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tInteger integer;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = NoParsingValueBridge.class), indexNullAs = \"7\")\n\t\t\tpublic Integer getInteger() { return integer; }\n\t\t}\n\n\t\tSubTest.expectException( () -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class ) )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"does not support parsing a value from a String\" )\n\t\t\t\t.hasMessageContaining( \"integer\" );\n\t}",
        "diffSourceCode": "-   79: \t@Test\n-   80: \tpublic void indexNullAs_noParsing() {\n-   81: \t\t@Indexed(index = INDEX_NAME)\n+   79: \t@SuppressWarnings(\"rawtypes\")\n+   80: \tpublic void error_unableToResolveDefaultValueBridgeFromSourceType_enumSuperClassRaw() {\n+   81: \t\t@Indexed\n    82: \t\tclass IndexedEntity {\n    83: \t\t\tInteger id;\n-   84: \t\t\tInteger integer;\n+   84: \t\t\tEnum myProperty;\n    85: \t\t\t@DocumentId\n    86: \t\t\tpublic Integer getId() {\n    87: \t\t\t\treturn id;\n    88: \t\t\t}\n-   89: \t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = NoParsingValueBridge.class), indexNullAs = \"7\")\n-   90: \t\t\tpublic Integer getInteger() { return integer; }\n-   91: \t\t}\n-   92: \n-   93: \t\tSubTest.expectException( () -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class ) )\n-   94: \t\t\t\t.assertThrown()\n-   95: \t\t\t\t.isInstanceOf( SearchException.class )\n-   96: \t\t\t\t.hasMessageContaining( \"does not support parsing a value from a String\" )\n-   97: \t\t\t\t.hasMessageContaining( \"integer\" );\n-   98: \t}\n+   89: \t\t\t@GenericField\n+   90: \t\t\tpublic Enum getMyProperty() {\n+   91: \t\t\t\treturn myProperty;\n+   92: \t\t\t}\n+   93: \t\t}\n+   94: \t\tSubTest.expectException(\n+   95: \t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n+   96: \t\t)\n+   97: \t\t\t\t.assertThrown()\n+   98: \t\t\t\t.isInstanceOf( SearchException.class )\n+  325: \t@Test\n+  326: \tpublic void error_indexNullAs_noParsing() {\n+  327: \t\t@Indexed(index = INDEX_NAME)\n+  328: \t\tclass IndexedEntity {\n+  329: \t\t\tInteger id;\n+  330: \t\t\tInteger integer;\n+  331: \t\t\t@DocumentId\n+  332: \t\t\tpublic Integer getId() {\n+  333: \t\t\t\treturn id;\n+  334: \t\t\t}\n+  335: \t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = NoParsingValueBridge.class), indexNullAs = \"7\")\n+  336: \t\t\tpublic Integer getInteger() { return integer; }\n+  337: \t\t}\n+  338: \n+  339: \t\tSubTest.expectException( () -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class ) )\n+  340: \t\t\t\t.assertThrown()\n+  341: \t\t\t\t.isInstanceOf( SearchException.class )\n+  342: \t\t\t\t.hasMessageContaining( \"does not support parsing a value from a String\" )\n+  343: \t\t\t\t.hasMessageContaining( \"integer\" );\n+  344: \t}\n",
        "uniqueId": "60720a3723b836213b0e663b71fcbdbb3ce58d36_79_98__325_344",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Move Method",
        "description": "Move Method\tpublic indexNullAs() : void from class org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.ValueBridgeBaseIT to public indexNullAs() : void from class org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.FieldBaseIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/pojo/src/test/java/org/hibernate/search/integrationtest/mapper/pojo/mapping/definition/ValueBridgeBaseIT.java",
                "startLine": 45,
                "endLine": 77,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/pojo/src/test/java/org/hibernate/search/integrationtest/mapper/pojo/mapping/definition/FieldBaseIT.java",
                "startLine": 280,
                "endLine": 312,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void indexNullAs() {\n\t\t@Indexed(index = INDEX_NAME)\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tInteger integer;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = ParsingValueBridge.class), indexNullAs = \"7\")\n\t\t\tpublic Integer getInteger() { return integer; }\n\t\t}\n\n\t\tbackendMock.expectSchema( INDEX_NAME, b -> b\n\t\t\t\t.field( \"integer\", Integer.class, f -> f.indexNullAs( 7 ) )\n\t\t);\n\n\t\tJavaBeanMapping mapping = setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\ttry ( SearchSession session = mapping.createSession() ) {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.id = 1;\n\t\t\tsession.getMainWorkPlan().add( entity );\n\n\t\t\tbackendMock.expectWorks( INDEX_NAME )\n\t\t\t\t\t// Stub backend is not supposed to use 'indexNullAs' option\n\t\t\t\t\t.add( \"1\", b -> b.field( \"integer\", null ) )\n\t\t\t\t\t.preparedThenExecuted();\n\t\t}\n\t\tbackendMock.verifyExpectationsMet();\n\t}",
        "filePathBefore": "integrationtest/mapper/pojo/src/test/java/org/hibernate/search/integrationtest/mapper/pojo/mapping/definition/ValueBridgeBaseIT.java",
        "isPureRefactoring": true,
        "commitId": "60720a3723b836213b0e663b71fcbdbb3ce58d36",
        "packageNameBefore": "org.hibernate.search.integrationtest.mapper.pojo.mapping.definition",
        "classNameBefore": "org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.ValueBridgeBaseIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.ValueBridgeBaseIT#indexNullAs",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.ValueBridgeBaseIT#indexNullAs\n methodBody: public void indexNullAs() {\nbackendMock.expectSchema(INDEX_NAME,b -> b.field(\"integer\",Integer.class,f -> f.indexNullAs(7)));\nJavaBeanMapping mapping=setupHelper.withBackendMock(backendMock).setup(IndexedEntity.class);\nbackendMock.verifyExpectationsMet();\ntry(SearchSession session=mapping.createSession())IndexedEntity entity=new IndexedEntity();\nentity.id=1;\nsession.getMainWorkPlan().add(entity);\nbackendMock.expectWorks(INDEX_NAME).add(\"1\",b -> b.field(\"integer\",null)).preparedThenExecuted();\nbackendMock.verifyExpectationsMet();\n}",
        "classSignatureBefore": "public class ValueBridgeBaseIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.ValueBridgeBaseIT#indexNullAs"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.ValueBridgeBaseIT"
        ],
        "classSignatureBeforeSet": [
            "public class ValueBridgeBaseIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.pojo.mapping.definition;\n\nimport java.lang.invoke.MethodHandles;\n\nimport org.hibernate.search.integrationtest.mapper.pojo.testsupport.util.rule.JavaBeanMappingSetupHelper;\nimport org.hibernate.search.mapper.javabean.JavaBeanMapping;\nimport org.hibernate.search.mapper.javabean.session.SearchSession;\nimport org.hibernate.search.mapper.pojo.bridge.ValueBridge;\nimport org.hibernate.search.mapper.pojo.bridge.runtime.ValueBridgeToIndexedValueContext;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.DocumentId;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.ValueBridgeRef;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.test.SubTest;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\n/**\n * Test common use cases of (custom) value bridges.\n * <p>\n * Does not test reindexing in depth; this is tested in {@code AutomaticIndexing*} tests in the ORM mapper.\n * <p>\n * Does not test field annotations in depth; this is tested in {@link FieldBaseIT}.\n */\n@SuppressWarnings(\"unused\")\npublic class ValueBridgeBaseIT {\n\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic JavaBeanMappingSetupHelper setupHelper = new JavaBeanMappingSetupHelper( MethodHandles.lookup() );\n\n\t@Test\n\tpublic void indexNullAs() {\n\t\t@Indexed(index = INDEX_NAME)\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tInteger integer;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = ParsingValueBridge.class), indexNullAs = \"7\")\n\t\t\tpublic Integer getInteger() { return integer; }\n\t\t}\n\n\t\tbackendMock.expectSchema( INDEX_NAME, b -> b\n\t\t\t\t.field( \"integer\", Integer.class, f -> f.indexNullAs( 7 ) )\n\t\t);\n\n\t\tJavaBeanMapping mapping = setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\ttry ( SearchSession session = mapping.createSession() ) {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.id = 1;\n\t\t\tsession.getMainWorkPlan().add( entity );\n\n\t\t\tbackendMock.expectWorks( INDEX_NAME )\n\t\t\t\t\t// Stub backend is not supposed to use 'indexNullAs' option\n\t\t\t\t\t.add( \"1\", b -> b.field( \"integer\", null ) )\n\t\t\t\t\t.preparedThenExecuted();\n\t\t}\n\t\tbackendMock.verifyExpectationsMet();\n\t}\n\n\t@Test\n\tpublic void indexNullAs_noParsing() {\n\t\t@Indexed(index = INDEX_NAME)\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tInteger integer;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = NoParsingValueBridge.class), indexNullAs = \"7\")\n\t\t\tpublic Integer getInteger() { return integer; }\n\t\t}\n\n\t\tSubTest.expectException( () -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class ) )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"does not support parsing a value from a String\" )\n\t\t\t\t.hasMessageContaining( \"integer\" );\n\t}\n\n\tpublic static class NoParsingValueBridge implements ValueBridge<Integer, Integer> {\n\n\t\tpublic NoParsingValueBridge() {\n\t\t}\n\n\t\t@Override\n\t\tpublic Integer toIndexedValue(Integer value, ValueBridgeToIndexedValueContext context) {\n\t\t\treturn value;\n\t\t}\n\n\t\t@Override\n\t\tpublic Integer cast(Object value) {\n\t\t\treturn (Integer) value;\n\t\t}\n\t}\n\n\tpublic static class ParsingValueBridge extends NoParsingValueBridge {\n\n\t\tpublic ParsingValueBridge() {\n\t\t}\n\n\t\t@Override\n\t\tpublic Integer parse(String value) {\n\t\t\treturn Integer.parseInt( value );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/pojo/src/test/java/org/hibernate/search/integrationtest/mapper/pojo/mapping/definition/FieldBaseIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.pojo.mapping.definition;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.List;\n\nimport org.hibernate.search.integrationtest.mapper.pojo.testsupport.util.rule.JavaBeanMappingSetupHelper;\nimport org.hibernate.search.mapper.javabean.JavaBeanMapping;\nimport org.hibernate.search.mapper.javabean.session.SearchSession;\nimport org.hibernate.search.mapper.pojo.bridge.ValueBridge;\nimport org.hibernate.search.mapper.pojo.bridge.runtime.ValueBridgeToIndexedValueContext;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.DocumentId;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.ValueBridgeRef;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.FailureReportUtils;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.test.SubTest;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\n/**\n * Test common use cases of the {@code @GenericField} annotation.\n * <p>\n * Does not test default bridges, which are tested in {@link FieldDefaultBridgeIT}.\n * <p>\n * Does not test uses of container value extractors, which are tested in {@link FieldContainerExtractorBaseIT}\n * (and others, see javadoc on that class).\n */\npublic class FieldBaseIT {\n\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic JavaBeanMappingSetupHelper setupHelper = new JavaBeanMappingSetupHelper( MethodHandles.lookup() );\n\n\t@Test\n\tpublic void error_unableToResolveDefaultValueBridgeFromSourceType() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tObject myProperty;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField\n\t\t\tpublic Object getMyProperty() {\n\t\t\t\treturn myProperty;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".myProperty\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Unable to find a default value bridge implementation for type '\"\n\t\t\t\t\t\t\t\t\t\t+ Object.class.getName() + \"'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\t@Test\n\t@SuppressWarnings(\"rawtypes\")\n\tpublic void error_unableToResolveDefaultValueBridgeFromSourceType_enumSuperClassRaw() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tEnum myProperty;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField\n\t\t\tpublic Enum getMyProperty() {\n\t\t\t\treturn myProperty;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".myProperty\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Unable to find a default value bridge implementation for type 'java.lang.Enum'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void error_unableToResolveDefaultValueBridgeFromSourceType_enumSuperClassWithWildcard() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tEnum<?> myProperty;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField\n\t\t\tpublic Enum<?> getMyProperty() {\n\t\t\t\treturn myProperty;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".myProperty\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Unable to find a default value bridge implementation for type 'java.lang.Enum<?>'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void error_unableToResolveDefaultValueBridgeFromSourceType_enumSuperClassWithParameters() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tEnum<EnumForEnumSuperClassTest> myProperty;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField\n\t\t\tpublic Enum<EnumForEnumSuperClassTest> getMyProperty() {\n\t\t\t\treturn myProperty;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".myProperty\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Unable to find a default value bridge implementation for type 'java.lang.Enum<\"\n\t\t\t\t\t\t\t\t\t\t+ EnumForEnumSuperClassTest.class.getName() + \">'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\tenum EnumForEnumSuperClassTest {\n\t\tVALUE1,\n\t\tVALUE2\n\t}\n\n\t@Test\n\tpublic void error_invalidInputTypeForValueBridge() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\t@DocumentId\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = MyStringBridge.class))\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".id\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Value bridge '\" + MyStringBridge.TOSTRING + \"' cannot be applied to input type '\"\n\t\t\t\t\t\t\t\t\t\t+ Integer.class.getName() + \"'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void error_invalidInputTypeForValueBridge_implicitContainerExtractor() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tList<Integer> numbers;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = MyStringBridge.class))\n\t\t\tpublic List<Integer> getNumbers() {\n\t\t\t\treturn numbers;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".numbers\" )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Value bridge '\" + MyStringBridge.TOSTRING + \"' cannot be applied to input type '\"\n\t\t\t\t\t\t\t\t\t\t+ Integer.class.getName() + \"'\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\tpublic static class MyStringBridge implements ValueBridge<String, String> {\n\t\tprivate static String TOSTRING = \"<MyStringBridge toString() result>\";\n\t\t@Override\n\t\tpublic String cast(Object value) {\n\t\t\tthrow new UnsupportedOperationException( \"Should not be called\" );\n\t\t}\n\t\t@Override\n\t\tpublic String toIndexedValue(String value,\n\t\t\t\tValueBridgeToIndexedValueContext context) {\n\t\t\tthrow new UnsupportedOperationException( \"Should not be called\" );\n\t\t}\n\t\t@Override\n\t\tpublic String toString() {\n\t\t\treturn TOSTRING;\n\t\t}\n\t}\n\n\t@Test\n\tpublic void error_definingBothBridgeReferenceAndBridgeBuilderReference() {\n\t\t@Indexed\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\t@DocumentId\n\t\t\t@GenericField(\n\t\t\t\t\tvalueBridge = @ValueBridgeRef(name = \"foo\", builderName = \"bar\")\n\t\t\t)\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t}\n\t\tSubTest.expectException(\n\t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n\t\t\t\t\t\t.pathContext( \".id\" )\n\t\t\t\t\t\t.annotationContextAnyParameters( GenericField.class )\n\t\t\t\t\t\t.failure(\n\t\t\t\t\t\t\t\t\"Annotation @GenericField on property 'id' defines both valueBridge and valueBridgeBuilder.\"\n\t\t\t\t\t\t\t\t\t\t+ \" Only one of those can be defined, not both.\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void indexNullAs() {\n\t\t@Indexed(index = INDEX_NAME)\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tInteger integer;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = ParsingValueBridge.class), indexNullAs = \"7\")\n\t\t\tpublic Integer getInteger() { return integer; }\n\t\t}\n\n\t\tbackendMock.expectSchema( INDEX_NAME, b -> b\n\t\t\t\t.field( \"integer\", Integer.class, f -> f.indexNullAs( 7 ) )\n\t\t);\n\n\t\tJavaBeanMapping mapping = setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\ttry ( SearchSession session = mapping.createSession() ) {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.id = 1;\n\t\t\tsession.getMainWorkPlan().add( entity );\n\n\t\t\tbackendMock.expectWorks( INDEX_NAME )\n\t\t\t\t\t// Stub backend is not supposed to use 'indexNullAs' option\n\t\t\t\t\t.add( \"1\", b -> b.field( \"integer\", null ) )\n\t\t\t\t\t.preparedThenExecuted();\n\t\t}\n\t\tbackendMock.verifyExpectationsMet();\n\t}\n\n\tpublic static class ParsingValueBridge extends NoParsingValueBridge {\n\n\t\tpublic ParsingValueBridge() {\n\t\t}\n\n\t\t@Override\n\t\tpublic Integer parse(String value) {\n\t\t\treturn Integer.parseInt( value );\n\t\t}\n\t}\n\n\t@Test\n\tpublic void error_indexNullAs_noParsing() {\n\t\t@Indexed(index = INDEX_NAME)\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tInteger integer;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = NoParsingValueBridge.class), indexNullAs = \"7\")\n\t\t\tpublic Integer getInteger() { return integer; }\n\t\t}\n\n\t\tSubTest.expectException( () -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class ) )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"does not support parsing a value from a String\" )\n\t\t\t\t.hasMessageContaining( \"integer\" );\n\t}\n\n\tpublic static class NoParsingValueBridge implements ValueBridge<Integer, Integer> {\n\n\t\tpublic NoParsingValueBridge() {\n\t\t}\n\n\t\t@Override\n\t\tpublic Integer toIndexedValue(Integer value, ValueBridgeToIndexedValueContext context) {\n\t\t\treturn value;\n\t\t}\n\n\t\t@Override\n\t\tpublic Integer cast(Object value) {\n\t\t\treturn (Integer) value;\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.mapper.pojo.mapping.definition.ValueBridgeBaseIT#indexNullAs\n methodBody: public void indexNullAs() {\nbackendMock.expectSchema(INDEX_NAME,b -> b.field(\"integer\",Integer.class,f -> f.indexNullAs(7)));\nJavaBeanMapping mapping=setupHelper.withBackendMock(backendMock).setup(IndexedEntity.class);\nbackendMock.verifyExpectationsMet();\ntry(SearchSession session=mapping.createSession())IndexedEntity entity=new IndexedEntity();\nentity.id=1;\nsession.getMainWorkPlan().add(entity);\nbackendMock.expectWorks(INDEX_NAME).add(\"1\",b -> b.field(\"integer\",null)).preparedThenExecuted();\nbackendMock.verifyExpectationsMet();\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void indexNullAs() {\n\t\t@Indexed(index = INDEX_NAME)\n\t\tclass IndexedEntity {\n\t\t\tInteger id;\n\t\t\tInteger integer;\n\t\t\t@DocumentId\n\t\t\tpublic Integer getId() {\n\t\t\t\treturn id;\n\t\t\t}\n\t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = ParsingValueBridge.class), indexNullAs = \"7\")\n\t\t\tpublic Integer getInteger() { return integer; }\n\t\t}\n\n\t\tbackendMock.expectSchema( INDEX_NAME, b -> b\n\t\t\t\t.field( \"integer\", Integer.class, f -> f.indexNullAs( 7 ) )\n\t\t);\n\n\t\tJavaBeanMapping mapping = setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\ttry ( SearchSession session = mapping.createSession() ) {\n\t\t\tIndexedEntity entity = new IndexedEntity();\n\t\t\tentity.id = 1;\n\t\t\tsession.getMainWorkPlan().add( entity );\n\n\t\t\tbackendMock.expectWorks( INDEX_NAME )\n\t\t\t\t\t// Stub backend is not supposed to use 'indexNullAs' option\n\t\t\t\t\t.add( \"1\", b -> b.field( \"integer\", null ) )\n\t\t\t\t\t.preparedThenExecuted();\n\t\t}\n\t\tbackendMock.verifyExpectationsMet();\n\t}",
        "diffSourceCode": "-   45: \t@Test\n-   46: \tpublic void indexNullAs() {\n-   47: \t\t@Indexed(index = INDEX_NAME)\n-   48: \t\tclass IndexedEntity {\n-   49: \t\t\tInteger id;\n-   50: \t\t\tInteger integer;\n-   51: \t\t\t@DocumentId\n-   52: \t\t\tpublic Integer getId() {\n-   53: \t\t\t\treturn id;\n-   54: \t\t\t}\n-   55: \t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = ParsingValueBridge.class), indexNullAs = \"7\")\n-   56: \t\t\tpublic Integer getInteger() { return integer; }\n-   57: \t\t}\n-   58: \n-   59: \t\tbackendMock.expectSchema( INDEX_NAME, b -> b\n-   60: \t\t\t\t.field( \"integer\", Integer.class, f -> f.indexNullAs( 7 ) )\n-   61: \t\t);\n-   62: \n-   63: \t\tJavaBeanMapping mapping = setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class );\n-   64: \t\tbackendMock.verifyExpectationsMet();\n-   65: \n-   66: \t\ttry ( SearchSession session = mapping.createSession() ) {\n-   67: \t\t\tIndexedEntity entity = new IndexedEntity();\n-   68: \t\t\tentity.id = 1;\n-   69: \t\t\tsession.getMainWorkPlan().add( entity );\n-   70: \n-   71: \t\t\tbackendMock.expectWorks( INDEX_NAME )\n-   72: \t\t\t\t\t// Stub backend is not supposed to use 'indexNullAs' option\n-   73: \t\t\t\t\t.add( \"1\", b -> b.field( \"integer\", null ) )\n-   74: \t\t\t\t\t.preparedThenExecuted();\n-   75: \t\t}\n-   76: \t\tbackendMock.verifyExpectationsMet();\n-   77: \t}\n+   45: \tpublic JavaBeanMappingSetupHelper setupHelper = new JavaBeanMappingSetupHelper( MethodHandles.lookup() );\n+   46: \n+   47: \t@Test\n+   48: \tpublic void error_unableToResolveDefaultValueBridgeFromSourceType() {\n+   49: \t\t@Indexed\n+   50: \t\tclass IndexedEntity {\n+   51: \t\t\tInteger id;\n+   52: \t\t\tObject myProperty;\n+   53: \t\t\t@DocumentId\n+   54: \t\t\tpublic Integer getId() {\n+   55: \t\t\t\treturn id;\n+   56: \t\t\t}\n+   57: \t\t\t@GenericField\n+   58: \t\t\tpublic Object getMyProperty() {\n+   59: \t\t\t\treturn myProperty;\n+   60: \t\t\t}\n+   61: \t\t}\n+   62: \t\tSubTest.expectException(\n+   63: \t\t\t\t() -> setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class )\n+   64: \t\t)\n+   65: \t\t\t\t.assertThrown()\n+   66: \t\t\t\t.isInstanceOf( SearchException.class )\n+   67: \t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n+   68: \t\t\t\t\t\t.typeContext( IndexedEntity.class.getName() )\n+   69: \t\t\t\t\t\t.pathContext( \".myProperty\" )\n+   70: \t\t\t\t\t\t.failure(\n+   71: \t\t\t\t\t\t\t\t\"Unable to find a default value bridge implementation for type '\"\n+   72: \t\t\t\t\t\t\t\t\t\t+ Object.class.getName() + \"'\"\n+   73: \t\t\t\t\t\t)\n+   74: \t\t\t\t\t\t.build()\n+   75: \t\t\t\t);\n+   76: \t}\n+   77: \n+  280: \t@Test\n+  281: \tpublic void indexNullAs() {\n+  282: \t\t@Indexed(index = INDEX_NAME)\n+  283: \t\tclass IndexedEntity {\n+  284: \t\t\tInteger id;\n+  285: \t\t\tInteger integer;\n+  286: \t\t\t@DocumentId\n+  287: \t\t\tpublic Integer getId() {\n+  288: \t\t\t\treturn id;\n+  289: \t\t\t}\n+  290: \t\t\t@GenericField(valueBridge = @ValueBridgeRef(type = ParsingValueBridge.class), indexNullAs = \"7\")\n+  291: \t\t\tpublic Integer getInteger() { return integer; }\n+  292: \t\t}\n+  293: \n+  294: \t\tbackendMock.expectSchema( INDEX_NAME, b -> b\n+  295: \t\t\t\t.field( \"integer\", Integer.class, f -> f.indexNullAs( 7 ) )\n+  296: \t\t);\n+  297: \n+  298: \t\tJavaBeanMapping mapping = setupHelper.withBackendMock( backendMock ).setup( IndexedEntity.class );\n+  299: \t\tbackendMock.verifyExpectationsMet();\n+  300: \n+  301: \t\ttry ( SearchSession session = mapping.createSession() ) {\n+  302: \t\t\tIndexedEntity entity = new IndexedEntity();\n+  303: \t\t\tentity.id = 1;\n+  304: \t\t\tsession.getMainWorkPlan().add( entity );\n+  305: \n+  306: \t\t\tbackendMock.expectWorks( INDEX_NAME )\n+  307: \t\t\t\t\t// Stub backend is not supposed to use 'indexNullAs' option\n+  308: \t\t\t\t\t.add( \"1\", b -> b.field( \"integer\", null ) )\n+  309: \t\t\t\t\t.preparedThenExecuted();\n+  310: \t\t}\n+  311: \t\tbackendMock.verifyExpectationsMet();\n+  312: \t}\n",
        "uniqueId": "60720a3723b836213b0e663b71fcbdbb3ce58d36_45_77__280_312",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tpublic create(templateString String, templateOrder int, settings String) : TemplateClient extracted from public create(templateString String, settings String) : TemplateClient in class org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util.TestElasticsearchClient.TemplateClient",
        "diffLocations": [
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/testsupport/util/TestElasticsearchClient.java",
                "startLine": 216,
                "endLine": 218,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/testsupport/util/TestElasticsearchClient.java",
                "startLine": 216,
                "endLine": 218,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/testsupport/util/TestElasticsearchClient.java",
                "startLine": 220,
                "endLine": 222,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public TemplateClient create(String templateString, String settings) {\n\t\t\treturn create( templateString, toJsonElement( settings ).getAsJsonObject() );\n\t\t}",
        "filePathBefore": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/testsupport/util/TestElasticsearchClient.java",
        "isPureRefactoring": true,
        "commitId": "769be0fb9220c3a46d4e2a1562295c55eca40539",
        "packageNameBefore": "org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util.TestElasticsearchClient",
        "classNameBefore": "org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util.TestElasticsearchClient.TemplateClient",
        "methodNameBefore": "org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util.TestElasticsearchClient.TemplateClient#create",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util.TestElasticsearchClient.TemplateClient#create\n methodBody: public TemplateClient create(String templateString, JsonObject settings) {\nTestElasticsearchClient.this.createTemplate(templateName,templateString,settings);\nreturn this;\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util.TestElasticsearchClient#toJsonElement\n methodBody: private JsonElement toJsonElement(String jsonAsString) {\nreturn new JsonParser().parse(jsonAsString);\n}",
        "classSignatureBefore": "public class TemplateClient ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util.TestElasticsearchClient.TemplateClient#create"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util.TestElasticsearchClient.TemplateClient"
        ],
        "classSignatureBeforeSet": [
            "public class TemplateClient "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Overlapped refactoring - can be identical by undoing the overlapped refactoring\n- Add Parameter-",
                "description": "Parametrization or Add Parameter on top of the extract method - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Optional;\n\nimport org.hibernate.search.backend.elasticsearch.cfg.ElasticsearchIndexSettings;\nimport org.hibernate.search.backend.elasticsearch.cfg.ElasticsearchIndexStatus;\nimport org.hibernate.search.backend.elasticsearch.client.impl.ElasticsearchClientFactoryImpl;\nimport org.hibernate.search.backend.elasticsearch.client.impl.ElasticsearchClientUtils;\nimport org.hibernate.search.backend.elasticsearch.client.impl.Paths;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchClientFactory;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchClientImplementor;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchRequest;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchResponse;\nimport org.hibernate.search.backend.elasticsearch.gson.impl.DefaultGsonProvider;\nimport org.hibernate.search.backend.elasticsearch.impl.ElasticsearchIndexNameNormalizer;\nimport org.hibernate.search.backend.elasticsearch.logging.impl.ElasticsearchRequestFormatter;\nimport org.hibernate.search.backend.elasticsearch.logging.impl.ElasticsearchResponseFormatter;\nimport org.hibernate.search.backend.elasticsearch.util.spi.URLEncodedString;\nimport org.hibernate.search.engine.cfg.spi.ConfigurationPropertySource;\nimport org.hibernate.search.engine.environment.bean.BeanHolder;\nimport org.hibernate.search.engine.environment.bean.BeanResolver;\nimport org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.dialect.ElasticsearchTestDialect;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.TckConfiguration;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.impl.Closer;\nimport org.hibernate.search.util.impl.integrationtest.common.TestConfigurationProvider;\n\nimport org.junit.rules.TestRule;\nimport org.junit.runner.Description;\nimport org.junit.runners.model.Statement;\n\nimport com.google.gson.GsonBuilder;\nimport com.google.gson.JsonElement;\nimport com.google.gson.JsonObject;\nimport com.google.gson.JsonParser;\n\npublic class TestElasticsearchClient implements TestRule {\n\n\tprivate final ElasticsearchTestDialect dialect = ElasticsearchTestDialect.get();\n\n\tprivate final TestConfigurationProvider configurationProvider = new TestConfigurationProvider();\n\n\tprivate ElasticsearchClientImplementor client;\n\n\tprivate final List<URLEncodedString> createdIndicesNames = new ArrayList<>();\n\n\tprivate final List<String> createdTemplatesNames = new ArrayList<>();\n\n\tpublic ElasticsearchTestDialect getDialect() {\n\t\treturn dialect;\n\t}\n\n\tpublic IndexClient index(String indexName) {\n\t\treturn new IndexClient( URLEncodedString.fromString( ElasticsearchIndexNameNormalizer.normalize( indexName ) ) );\n\t}\n\n\tpublic class IndexClient {\n\n\t\tprivate final URLEncodedString indexName;\n\n\t\tpublic IndexClient(URLEncodedString indexName) {\n\t\t\tthis.indexName = indexName;\n\t\t}\n\n\t\tpublic void waitForRequiredIndexStatus() {\n\t\t\tTestElasticsearchClient.this.waitForRequiredIndexStatus( indexName );\n\t\t}\n\n\t\tpublic IndexClient deleteAndCreate() {\n\t\t\tTestElasticsearchClient.this.deleteAndCreateIndex( indexName );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic IndexClient deleteAndCreate(String settingsPath, String settings) {\n\t\t\tJsonObject settingsAsJsonObject = buildSettings( settingsPath, settings );\n\t\t\tTestElasticsearchClient.this.deleteAndCreateIndex( indexName, settingsAsJsonObject );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic IndexClient ensureDoesNotExist() {\n\t\t\tTestElasticsearchClient.this.ensureIndexDoesNotExist( indexName );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic IndexClient registerForCleanup() {\n\t\t\tTestElasticsearchClient.this.registerIndexForCleanup( indexName );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic TypeClient type() {\n\t\t\treturn new TypeClient( this );\n\t\t}\n\n\t\tpublic SettingsClient settings() {\n\t\t\treturn settings( \"\" );\n\t\t}\n\n\t\tpublic SettingsClient settings(String settingsPath) {\n\t\t\treturn new SettingsClient( this, settingsPath );\n\t\t}\n\t}\n\n\tpublic class TypeClient {\n\n\t\tprivate final IndexClient indexClient;\n\n\t\tpublic TypeClient(IndexClient indexClient) {\n\t\t\tthis.indexClient = indexClient;\n\t\t}\n\n\t\tpublic TypeClient putMapping(String mappingJson) {\n\t\t\tTestElasticsearchClient.this.putMapping( indexClient.indexName, mappingJson );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic String getMapping() {\n\t\t\treturn TestElasticsearchClient.this.getMapping( indexClient.indexName );\n\t\t}\n\n\t\tpublic TypeClient index(URLEncodedString id, String jsonDocument) {\n\t\t\tURLEncodedString indexName = indexClient.indexName;\n\t\t\tTestElasticsearchClient.this.index( indexName, id, jsonDocument );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic DocumentClient document(String id) {\n\t\t\treturn new DocumentClient( this, id );\n\t\t}\n\t}\n\n\tpublic class SettingsClient {\n\n\t\tprivate final IndexClient indexClient;\n\n\t\tprivate final String settingsPath;\n\n\t\tpublic SettingsClient(IndexClient indexClient, String settingsPath) {\n\t\t\tthis.indexClient = indexClient;\n\t\t\tthis.settingsPath = settingsPath;\n\t\t}\n\n\t\tpublic String get() {\n\t\t\tURLEncodedString indexName = indexClient.indexName;\n\t\t\treturn TestElasticsearchClient.this.getSettings( indexName, settingsPath );\n\t\t}\n\n\t\t/**\n\t\t * Put settings without closing the index first.\n\t\t *\n\t\t * @param settings The settings value to put\n\t\t * @throws IOException\n\t\t */\n\t\tpublic void putDynamic(String settings) {\n\t\t\tURLEncodedString indexName = indexClient.indexName;\n\t\t\tJsonObject settingsAsJsonObject = buildSettings( settingsPath, settings );\n\t\t\tTestElasticsearchClient.this.putDynamicSettings( indexName, settingsAsJsonObject );\n\t\t}\n\n\t\t/**\n\t\t * Put settings, closing the index first and reopening the index afterwards.\n\t\t *\n\t\t * @param settings The settings value to put\n\t\t * @throws IOException\n\t\t */\n\t\tpublic void putNonDynamic(String settings) {\n\t\t\tURLEncodedString indexName = indexClient.indexName;\n\t\t\tJsonObject settingsAsJsonObject = buildSettings( settingsPath, settings );\n\t\t\tTestElasticsearchClient.this.putNonDynamicSettings( indexName, settingsAsJsonObject );\n\t\t}\n\t}\n\n\tpublic class DocumentClient {\n\n\t\tprivate final TypeClient typeClient;\n\n\t\tprivate final URLEncodedString id;\n\n\t\tpublic DocumentClient(TypeClient typeClient, String id) {\n\t\t\tthis.typeClient = typeClient;\n\t\t\tthis.id = URLEncodedString.fromString( id );\n\t\t}\n\n\t\tpublic JsonObject getSource() {\n\t\t\treturn TestElasticsearchClient.this.getDocumentSource( typeClient.indexClient.indexName, id );\n\t\t}\n\n\t\tpublic JsonElement getStoredField(String fieldName) {\n\t\t\treturn TestElasticsearchClient.this.getDocumentField( typeClient.indexClient.indexName, id, fieldName );\n\t\t}\n\t}\n\n\tpublic TemplateClient template(String templateName) {\n\t\treturn new TemplateClient( templateName );\n\t}\n\n\tpublic class TemplateClient {\n\n\t\tprivate final String templateName;\n\n\t\tpublic TemplateClient(String templateName) {\n\t\t\tthis.templateName = templateName;\n\t\t}\n\n\t\tpublic TemplateClient create(String templateString, String settings) {\n\t\t\treturn create( templateString, toJsonElement( settings ).getAsJsonObject() );\n\t\t}\n\n\t\tpublic TemplateClient create(String templateString, JsonObject settings) {\n\t\t\tTestElasticsearchClient.this.createTemplate( templateName, templateString, settings );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic TemplateClient registerForCleanup() {\n\t\t\tTestElasticsearchClient.this.registerTemplateForCleanup( templateName );\n\t\t\treturn this;\n\t\t}\n\t}\n\n\tprivate void deleteAndCreateIndex(URLEncodedString indexName) {\n\t\tdeleteAndCreateIndex( indexName, null );\n\t}\n\n\tprivate void deleteAndCreateIndex(URLEncodedString indexName, JsonObject settingsAsJsonObject) {\n\t\tElasticsearchRequest.Builder builder = ElasticsearchRequest.put()\n\t\t\t\t.pathComponent( indexName );\n\n\t\tif ( settingsAsJsonObject != null ) {\n\t\t\tJsonObject payload = new JsonObject();\n\t\t\tpayload.add( \"settings\", settingsAsJsonObject );\n\t\t\tbuilder.body( payload );\n\t\t}\n\n\t\tBoolean includeTypeName = dialect.getIncludeTypeNameParameterForMappingApi();\n\t\tif ( includeTypeName != null ) {\n\t\t\tbuilder.param( \"include_type_name\", includeTypeName );\n\t\t}\n\n\t\tdoDeleteAndCreateIndex(\n\t\t\t\tindexName,\n\t\t\t\tbuilder.build()\n\t\t);\n\t}\n\n\tprivate void doDeleteAndCreateIndex(URLEncodedString indexName, ElasticsearchRequest createRequest) {\n\t\t// Ignore the result: if the deletion fails, we don't care unless the creation just after also fails\n\t\ttryDeleteESIndex( indexName );\n\n\t\tregisterIndexForCleanup( indexName );\n\t\tperformRequest( createRequest );\n\n\t\twaitForRequiredIndexStatus( indexName );\n\t}\n\n\tprivate void createTemplate(String templateName, String templateString, JsonObject settings) {\n\t\tJsonObject source = new JsonObject();\n\t\tdialect.setTemplatePattern( source, templateString );\n\t\tsource.add( \"settings\", settings );\n\n\t\tregisterTemplateForCleanup( templateName );\n\n\t\tElasticsearchRequest.Builder builder = ElasticsearchRequest.put()\n\t\t\t\t.pathComponent( Paths._TEMPLATE ).pathComponent( URLEncodedString.fromString( templateName ) )\n\t\t\t\t.body( source );\n\n\t\tBoolean includeTypeName = dialect.getIncludeTypeNameParameterForMappingApi();\n\t\tif ( includeTypeName != null ) {\n\t\t\tbuilder.param( \"include_type_name\", includeTypeName );\n\t\t}\n\n\t\tperformRequest( builder.build() );\n\t}\n\n\tprivate void ensureIndexDoesNotExist(URLEncodedString indexName) {\n\t\tperformRequestIgnore404( ElasticsearchRequest.delete()\n\t\t\t\t.pathComponent( indexName )\n\t\t\t\t.build() );\n\t}\n\n\tprivate void registerIndexForCleanup(URLEncodedString indexName) {\n\t\tcreatedIndicesNames.add( indexName );\n\t}\n\n\tprivate void registerTemplateForCleanup(String templateName) {\n\t\tcreatedTemplatesNames.add( templateName );\n\t}\n\n\tprivate void waitForRequiredIndexStatus(final URLEncodedString indexName) {\n\t\tperformRequest( ElasticsearchRequest.get()\n\t\t\t\t.pathComponent( Paths._CLUSTER ).pathComponent( Paths.HEALTH ).pathComponent( indexName )\n\t\t\t\t/*\n\t\t\t\t * We only wait for YELLOW: it's perfectly fine, and some tests actually expect\n\t\t\t\t * the indexes to never reach a green status\n\t\t\t\t */\n\t\t\t\t.param( \"wait_for_status\", ElasticsearchIndexStatus.YELLOW.getElasticsearchString() )\n\t\t\t\t.param( \"timeout\", ElasticsearchIndexSettings.Defaults.LIFECYCLE_MINIMAL_REQUIRED_STATUS_WAIT_TIMEOUT + \"ms\" )\n\t\t\t\t.build() );\n\t}\n\n\tprivate void putMapping(URLEncodedString indexName, String mappingJson) {\n\t\tJsonObject mappingJsonObject = toJsonElement( mappingJson ).getAsJsonObject();\n\n\t\tElasticsearchRequest.Builder builder = ElasticsearchRequest.put()\n\t\t\t\t.pathComponent( indexName ).pathComponent( Paths._MAPPING );\n\t\tdialect.getTypeNameForMappingApi().ifPresent( builder::pathComponent );\n\t\tbuilder.body( mappingJsonObject );\n\n\t\tBoolean includeTypeName = dialect.getIncludeTypeNameParameterForMappingApi();\n\t\tif ( includeTypeName != null ) {\n\t\t\tbuilder.param( \"include_type_name\", includeTypeName );\n\t\t}\n\n\t\tperformRequest( builder.build() );\n\t}\n\n\tprivate String getMapping(URLEncodedString indexName) {\n\n\t\tElasticsearchRequest.Builder builder = ElasticsearchRequest.get()\n\t\t\t\t.pathComponent( indexName ).pathComponent( Paths._MAPPING );\n\t\tdialect.getTypeNameForMappingApi().ifPresent( builder::pathComponent );\n\n\t\tBoolean includeTypeName = dialect.getIncludeTypeNameParameterForMappingApi();\n\t\tif ( includeTypeName != null ) {\n\t\t\tbuilder.param( \"include_type_name\", includeTypeName );\n\t\t}\n\n\t\t/*\n\t\t * Elasticsearch 5.5+ triggers a 404 error when mappings are missing,\n\t\t * while 5.4 and below just return an empty mapping.\n\t\t * In our case, an empty mapping is fine, so we'll just ignore 404.\n\t\t */\n\t\tElasticsearchResponse response = performRequestIgnore404( builder.build() );\n\t\tJsonObject result = response.getBody();\n\t\tJsonElement index = result.get( indexName.original );\n\t\tif ( index == null ) {\n\t\t\treturn new JsonObject().toString();\n\t\t}\n\t\tJsonElement mappings = index.getAsJsonObject().get( \"mappings\" );\n\t\tif ( mappings == null ) {\n\t\t\treturn new JsonObject().toString();\n\t\t}\n\t\tOptional<URLEncodedString> typeName = dialect.getTypeNameForMappingApi();\n\t\tif ( typeName.isPresent() ) {\n\t\t\tJsonElement mapping = mappings.getAsJsonObject().get( typeName.get().original );\n\t\t\tif ( mapping == null ) {\n\t\t\t\treturn new JsonObject().toString();\n\t\t\t}\n\t\t\treturn mapping.toString();\n\t\t}\n\t\telse {\n\t\t\treturn mappings.toString();\n\t\t}\n\t}\n\n\tprivate void putDynamicSettings(URLEncodedString indexName, JsonObject settingsJsonObject) {\n\t\tperformRequest( ElasticsearchRequest.put()\n\t\t\t\t.pathComponent( indexName ).pathComponent( Paths._SETTINGS )\n\t\t\t\t.body( settingsJsonObject )\n\t\t\t\t.build() );\n\t}\n\n\tprivate void putNonDynamicSettings(URLEncodedString indexName, JsonObject settingsJsonObject) {\n\t\tperformRequest( ElasticsearchRequest.post()\n\t\t\t\t.pathComponent( indexName )\n\t\t\t\t.pathComponent( Paths._CLOSE )\n\t\t\t\t.build() );\n\n\t\tperformRequest( ElasticsearchRequest.put()\n\t\t\t\t.pathComponent( indexName ).pathComponent( Paths._SETTINGS )\n\t\t\t\t.body( settingsJsonObject )\n\t\t\t\t.build() );\n\n\t\tperformRequest( ElasticsearchRequest.post()\n\t\t\t\t.pathComponent( indexName )\n\t\t\t\t.pathComponent( Paths._OPEN )\n\t\t\t\t.build() );\n\t}\n\n\tprivate JsonObject buildSettings(String settingsPath, String settings) {\n\t\tJsonElement settingsJsonElement = toJsonElement( settings );\n\n\t\tList<String> components = Arrays.asList( settingsPath.split( \"\\\\.\" ) );\n\t\tCollections.reverse( components );\n\t\tfor ( String property : components ) {\n\t\t\tJsonObject parent = new JsonObject();\n\t\t\tparent.add( property, settingsJsonElement );\n\t\t\tsettingsJsonElement = parent;\n\t\t}\n\n\t\treturn settingsJsonElement.getAsJsonObject();\n\t}\n\n\tprivate String getSettings(URLEncodedString indexName, String path) {\n\t\tElasticsearchResponse response = performRequest( ElasticsearchRequest.get()\n\t\t\t\t.pathComponent( indexName ).pathComponent( Paths._SETTINGS )\n\t\t\t\t.build() );\n\t\tJsonObject result = response.getBody();\n\t\tJsonElement index = result.get( indexName.original );\n\t\tif ( index == null ) {\n\t\t\treturn new JsonObject().toString();\n\t\t}\n\t\tJsonElement settings = index.getAsJsonObject().get( \"settings\" );\n\t\tfor ( String property : path.split( \"\\\\.\" ) ) {\n\t\t\tif ( settings == null ) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsettings = settings.getAsJsonObject().get( property );\n\t\t}\n\t\tif ( settings == null ) {\n\t\t\treturn new JsonObject().toString();\n\t\t}\n\t\treturn settings.toString();\n\t}\n\n\tprivate void index(URLEncodedString indexName, URLEncodedString id, String jsonDocument) {\n\t\tJsonObject documentJsonObject = toJsonElement( jsonDocument ).getAsJsonObject();\n\t\tperformRequest( ElasticsearchRequest.put()\n\t\t\t\t.pathComponent( indexName ).pathComponent( dialect.getTypeKeywordForNonMappingApi() ).pathComponent( id )\n\t\t\t\t.body( documentJsonObject )\n\t\t\t\t.param( \"refresh\", true )\n\t\t\t\t.build() );\n\t}\n\n\tprivate JsonObject getDocumentSource(URLEncodedString indexName, URLEncodedString id) {\n\t\tElasticsearchResponse response = performRequest( ElasticsearchRequest.get()\n\t\t\t\t.pathComponent( indexName ).pathComponent( dialect.getTypeKeywordForNonMappingApi() ).pathComponent( id )\n\t\t\t\t.build() );\n\t\tJsonObject result = response.getBody();\n\t\treturn result.get( \"_source\" ).getAsJsonObject();\n\t}\n\n\tprotected JsonElement getDocumentField(URLEncodedString indexName, URLEncodedString id, String fieldName) {\n\t\tElasticsearchResponse response = performRequest( ElasticsearchRequest.get()\n\t\t\t\t.pathComponent( indexName ).pathComponent( dialect.getTypeKeywordForNonMappingApi() ).pathComponent( id )\n\t\t\t\t.param( \"stored_fields\", fieldName )\n\t\t\t\t.build() );\n\t\tJsonObject result = response.getBody();\n\t\treturn result.get( \"fields\" ).getAsJsonObject().get( fieldName );\n\t}\n\n\t@Override\n\tpublic Statement apply(Statement base, Description description) {\n\t\tStatement wrapped = new Statement() {\n\t\t\t@Override\n\t\t\tpublic void evaluate() throws Throwable {\n\t\t\t\ttry ( Closer<IOException> closer = new Closer<>() ) {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tbefore();\n\t\t\t\t\t\tbase.evaluate();\n\t\t\t\t\t}\n\t\t\t\t\tfinally {\n\t\t\t\t\t\tafter( closer );\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\treturn configurationProvider.apply( wrapped, description );\n\t}\n\n\tprivate void before() {\n\t\tConfigurationPropertySource backendProperties =\n\t\t\t\tTckConfiguration.get().getBackendProperties( configurationProvider, null );\n\n\t\tBeanResolver beanResolver = configurationProvider.createBeanResolverForTest();\n\t\t/*\n\t\t * We use a {@link ElasticsearchClientFactoryImpl} to create our low-level client.\n\t\t *\n\t\t * The main advantage is that we ensure we connect to Elasticsearch exactly the same way\n\t\t * as any test-created SearchFactory, allowing to support things like testing on AWS\n\t\t * (using the hibernate-search-elasticsearch-aws module).\n\t\t */\n\t\ttry ( BeanHolder<ElasticsearchClientFactory> factoryHolder =\n\t\t\t\tbeanResolver.resolve( ElasticsearchClientFactoryImpl.REFERENCE ) ) {\n\t\t\tclient = factoryHolder.get().create(\n\t\t\t\t\tbackendProperties, DefaultGsonProvider.create( GsonBuilder::new, true )\n\t\t\t);\n\t\t}\n\t}\n\n\tprivate void after(Closer<IOException> closer) {\n\t\tcloser.pushAll( this::tryDeleteESIndex, createdIndicesNames );\n\t\tcreatedIndicesNames.clear();\n\t\tcloser.pushAll( this::tryDeleteESTemplate, createdTemplatesNames );\n\t\tcreatedTemplatesNames.clear();\n\t\tcloser.push( this::tryCloseClient, client );\n\t\tclient = null;\n\t}\n\n\tprivate void tryDeleteESIndex(URLEncodedString indexName) {\n\t\ttry {\n\t\t\tperformRequestIgnore404( ElasticsearchRequest.delete()\n\t\t\t\t\t.pathComponent( indexName )\n\t\t\t\t\t.build() );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\tString.format( Locale.ROOT, \"Error while trying to delete index '%s' as part of test cleanup\", indexName ),\n\t\t\t\t\te\n\t\t\t);\n\t\t}\n\t}\n\n\tprivate void tryDeleteESTemplate(String templateName) {\n\t\ttry {\n\t\t\tperformRequestIgnore404( ElasticsearchRequest.delete()\n\t\t\t\t\t.pathComponent( Paths._TEMPLATE ).pathComponent( URLEncodedString.fromString( templateName ) )\n\t\t\t\t\t.build() );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\tString.format( Locale.ROOT, \"Error while trying to delete template '%s' as part of test cleanup\", templateName ),\n\t\t\t\t\te\n\t\t\t);\n\t\t}\n\t}\n\n\tprivate void tryCloseClient(ElasticsearchClientImplementor client) {\n\t\ttry {\n\t\t\tclient.close();\n\t\t}\n\t\tcatch (RuntimeException | IOException e) {\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"Unexpected exception when closing the ElasticsearchClient used in \"\n\t\t\t\t\t\t\t+ TestElasticsearchClient.class.getSimpleName(),\n\t\t\t\t\te\n\t\t\t);\n\t\t}\n\t}\n\n\tprotected ElasticsearchResponse performRequest(ElasticsearchRequest request) {\n\t\tElasticsearchResponse response;\n\t\ttry {\n\t\t\tresponse = client.submit( request ).join();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow requestFailed( request, e );\n\t\t}\n\t\tint statusCode = response.getStatusCode();\n\t\tif ( !ElasticsearchClientUtils.isSuccessCode( statusCode ) ) {\n\t\t\tthrow requestFailed( request, response );\n\t\t}\n\t\treturn response;\n\t}\n\n\tprotected ElasticsearchResponse performRequestIgnore404(ElasticsearchRequest request) {\n\t\tElasticsearchResponse response;\n\t\ttry {\n\t\t\tresponse = client.submit( request ).join();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow requestFailed( request, e );\n\t\t}\n\t\tint statusCode = response.getStatusCode();\n\t\tif ( !ElasticsearchClientUtils.isSuccessCode( statusCode ) && 404 != statusCode ) {\n\t\t\tthrow requestFailed( request, response );\n\t\t}\n\t\treturn response;\n\t}\n\n\tprivate AssertionFailure requestFailed(ElasticsearchRequest request, Exception e) {\n\t\treturn new AssertionFailure( \"Elasticsearch request in TestElasticsearchClient failed:\"\n\t\t\t\t+ \"Request:\\n\"\n\t\t\t\t+ \"========\\n\"\n\t\t\t\t+ new ElasticsearchRequestFormatter( request ),\n\t\t\t\te );\n\t}\n\n\tprivate AssertionFailure requestFailed(ElasticsearchRequest request, ElasticsearchResponse response) {\n\t\treturn new AssertionFailure( \"Elasticsearch request in TestElasticsearchClient failed:\\n\"\n\t\t\t\t+ \"Request:\\n\"\n\t\t\t\t+ \"========\\n\"\n\t\t\t\t+ new ElasticsearchRequestFormatter( request )\n\t\t\t\t+ \"\\nResponse:\\n\"\n\t\t\t\t+ \"========\\n\"\n\t\t\t\t+ new ElasticsearchResponseFormatter( response )\n\t\t\t\t);\n\t}\n\n\t/*\n\t * Convert provided JSON to JsonElement, so that some Elasticsearch peculiarities (such as the fact that\n\t * single quotes are not accepted as a substitute for single quotes) can be worked around.\n\t * In tests, single quotes are way easier to include in JSON strings, because we don't have to escape them.\n\t */\n\tprivate JsonElement toJsonElement(String jsonAsString) {\n\t\treturn new JsonParser().parse( jsonAsString );\n\t}\n\n}\n",
        "filePathAfter": "integrationtest/backend/elasticsearch/src/test/java/org/hibernate/search/integrationtest/backend/elasticsearch/testsupport/util/TestElasticsearchClient.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util;\n\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Optional;\n\nimport org.hibernate.search.backend.elasticsearch.cfg.ElasticsearchIndexSettings;\nimport org.hibernate.search.backend.elasticsearch.cfg.ElasticsearchIndexStatus;\nimport org.hibernate.search.backend.elasticsearch.client.impl.ElasticsearchClientFactoryImpl;\nimport org.hibernate.search.backend.elasticsearch.client.impl.ElasticsearchClientUtils;\nimport org.hibernate.search.backend.elasticsearch.client.impl.Paths;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchClientFactory;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchClientImplementor;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchRequest;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchResponse;\nimport org.hibernate.search.backend.elasticsearch.gson.impl.DefaultGsonProvider;\nimport org.hibernate.search.backend.elasticsearch.impl.ElasticsearchIndexNameNormalizer;\nimport org.hibernate.search.backend.elasticsearch.logging.impl.ElasticsearchRequestFormatter;\nimport org.hibernate.search.backend.elasticsearch.logging.impl.ElasticsearchResponseFormatter;\nimport org.hibernate.search.backend.elasticsearch.util.spi.URLEncodedString;\nimport org.hibernate.search.engine.cfg.spi.ConfigurationPropertySource;\nimport org.hibernate.search.engine.environment.bean.BeanHolder;\nimport org.hibernate.search.engine.environment.bean.BeanResolver;\nimport org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.dialect.ElasticsearchTestDialect;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.TckConfiguration;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.impl.Closer;\nimport org.hibernate.search.util.impl.integrationtest.common.TestConfigurationProvider;\n\nimport org.junit.rules.TestRule;\nimport org.junit.runner.Description;\nimport org.junit.runners.model.Statement;\n\nimport com.google.gson.GsonBuilder;\nimport com.google.gson.JsonElement;\nimport com.google.gson.JsonObject;\nimport com.google.gson.JsonParser;\n\npublic class TestElasticsearchClient implements TestRule {\n\n\tprivate final ElasticsearchTestDialect dialect = ElasticsearchTestDialect.get();\n\n\tprivate final TestConfigurationProvider configurationProvider = new TestConfigurationProvider();\n\n\tprivate ElasticsearchClientImplementor client;\n\n\tprivate final List<URLEncodedString> createdIndicesNames = new ArrayList<>();\n\n\tprivate final List<String> createdTemplatesNames = new ArrayList<>();\n\n\tpublic ElasticsearchTestDialect getDialect() {\n\t\treturn dialect;\n\t}\n\n\tpublic IndexClient index(String indexName) {\n\t\treturn new IndexClient( URLEncodedString.fromString( ElasticsearchIndexNameNormalizer.normalize( indexName ) ) );\n\t}\n\n\tpublic class IndexClient {\n\n\t\tprivate final URLEncodedString indexName;\n\n\t\tpublic IndexClient(URLEncodedString indexName) {\n\t\t\tthis.indexName = indexName;\n\t\t}\n\n\t\tpublic void waitForRequiredIndexStatus() {\n\t\t\tTestElasticsearchClient.this.waitForRequiredIndexStatus( indexName );\n\t\t}\n\n\t\tpublic IndexClient deleteAndCreate() {\n\t\t\tTestElasticsearchClient.this.deleteAndCreateIndex( indexName );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic IndexClient deleteAndCreate(String settingsPath, String settings) {\n\t\t\tJsonObject settingsAsJsonObject = buildSettings( settingsPath, settings );\n\t\t\tTestElasticsearchClient.this.deleteAndCreateIndex( indexName, settingsAsJsonObject );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic IndexClient ensureDoesNotExist() {\n\t\t\tTestElasticsearchClient.this.ensureIndexDoesNotExist( indexName );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic IndexClient registerForCleanup() {\n\t\t\tTestElasticsearchClient.this.registerIndexForCleanup( indexName );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic TypeClient type() {\n\t\t\treturn new TypeClient( this );\n\t\t}\n\n\t\tpublic SettingsClient settings() {\n\t\t\treturn settings( \"\" );\n\t\t}\n\n\t\tpublic SettingsClient settings(String settingsPath) {\n\t\t\treturn new SettingsClient( this, settingsPath );\n\t\t}\n\t}\n\n\tpublic class TypeClient {\n\n\t\tprivate final IndexClient indexClient;\n\n\t\tpublic TypeClient(IndexClient indexClient) {\n\t\t\tthis.indexClient = indexClient;\n\t\t}\n\n\t\tpublic TypeClient putMapping(String mappingJson) {\n\t\t\tTestElasticsearchClient.this.putMapping( indexClient.indexName, mappingJson );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic String getMapping() {\n\t\t\treturn TestElasticsearchClient.this.getMapping( indexClient.indexName );\n\t\t}\n\n\t\tpublic TypeClient index(URLEncodedString id, String jsonDocument) {\n\t\t\tURLEncodedString indexName = indexClient.indexName;\n\t\t\tTestElasticsearchClient.this.index( indexName, id, jsonDocument );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic DocumentClient document(String id) {\n\t\t\treturn new DocumentClient( this, id );\n\t\t}\n\t}\n\n\tpublic class SettingsClient {\n\n\t\tprivate final IndexClient indexClient;\n\n\t\tprivate final String settingsPath;\n\n\t\tpublic SettingsClient(IndexClient indexClient, String settingsPath) {\n\t\t\tthis.indexClient = indexClient;\n\t\t\tthis.settingsPath = settingsPath;\n\t\t}\n\n\t\tpublic String get() {\n\t\t\tURLEncodedString indexName = indexClient.indexName;\n\t\t\treturn TestElasticsearchClient.this.getSettings( indexName, settingsPath );\n\t\t}\n\n\t\t/**\n\t\t * Put settings without closing the index first.\n\t\t *\n\t\t * @param settings The settings value to put\n\t\t * @throws IOException\n\t\t */\n\t\tpublic void putDynamic(String settings) {\n\t\t\tURLEncodedString indexName = indexClient.indexName;\n\t\t\tJsonObject settingsAsJsonObject = buildSettings( settingsPath, settings );\n\t\t\tTestElasticsearchClient.this.putDynamicSettings( indexName, settingsAsJsonObject );\n\t\t}\n\n\t\t/**\n\t\t * Put settings, closing the index first and reopening the index afterwards.\n\t\t *\n\t\t * @param settings The settings value to put\n\t\t * @throws IOException\n\t\t */\n\t\tpublic void putNonDynamic(String settings) {\n\t\t\tURLEncodedString indexName = indexClient.indexName;\n\t\t\tJsonObject settingsAsJsonObject = buildSettings( settingsPath, settings );\n\t\t\tTestElasticsearchClient.this.putNonDynamicSettings( indexName, settingsAsJsonObject );\n\t\t}\n\t}\n\n\tpublic class DocumentClient {\n\n\t\tprivate final TypeClient typeClient;\n\n\t\tprivate final URLEncodedString id;\n\n\t\tpublic DocumentClient(TypeClient typeClient, String id) {\n\t\t\tthis.typeClient = typeClient;\n\t\t\tthis.id = URLEncodedString.fromString( id );\n\t\t}\n\n\t\tpublic JsonObject getSource() {\n\t\t\treturn TestElasticsearchClient.this.getDocumentSource( typeClient.indexClient.indexName, id );\n\t\t}\n\n\t\tpublic JsonElement getStoredField(String fieldName) {\n\t\t\treturn TestElasticsearchClient.this.getDocumentField( typeClient.indexClient.indexName, id, fieldName );\n\t\t}\n\t}\n\n\tpublic TemplateClient template(String templateName) {\n\t\treturn new TemplateClient( templateName );\n\t}\n\n\tpublic class TemplateClient {\n\n\t\tprivate final String templateName;\n\n\t\tpublic TemplateClient(String templateName) {\n\t\t\tthis.templateName = templateName;\n\t\t}\n\n\t\tpublic TemplateClient create(String templateString, String settings) {\n\t\t\treturn create( templateString, 0, settings );\n\t\t}\n\n\t\tpublic TemplateClient create(String templateString, int templateOrder, String settings) {\n\t\t\treturn create( templateString, templateOrder, toJsonElement( settings ).getAsJsonObject() );\n\t\t}\n\n\t\tpublic TemplateClient create(String templateString, int templateOrder, JsonObject settings) {\n\t\t\tTestElasticsearchClient.this.createTemplate( templateName, templateString, templateOrder, settings );\n\t\t\treturn this;\n\t\t}\n\n\t\tpublic TemplateClient registerForCleanup() {\n\t\t\tTestElasticsearchClient.this.registerTemplateForCleanup( templateName );\n\t\t\treturn this;\n\t\t}\n\t}\n\n\tprivate void deleteAndCreateIndex(URLEncodedString indexName) {\n\t\tdeleteAndCreateIndex( indexName, null );\n\t}\n\n\tprivate void deleteAndCreateIndex(URLEncodedString indexName, JsonObject settingsAsJsonObject) {\n\t\tElasticsearchRequest.Builder builder = ElasticsearchRequest.put()\n\t\t\t\t.pathComponent( indexName );\n\n\t\tif ( settingsAsJsonObject != null ) {\n\t\t\tJsonObject payload = new JsonObject();\n\t\t\tpayload.add( \"settings\", settingsAsJsonObject );\n\t\t\tbuilder.body( payload );\n\t\t}\n\n\t\tBoolean includeTypeName = dialect.getIncludeTypeNameParameterForMappingApi();\n\t\tif ( includeTypeName != null ) {\n\t\t\tbuilder.param( \"include_type_name\", includeTypeName );\n\t\t}\n\n\t\tdoDeleteAndCreateIndex(\n\t\t\t\tindexName,\n\t\t\t\tbuilder.build()\n\t\t);\n\t}\n\n\tprivate void doDeleteAndCreateIndex(URLEncodedString indexName, ElasticsearchRequest createRequest) {\n\t\t// Ignore the result: if the deletion fails, we don't care unless the creation just after also fails\n\t\ttryDeleteESIndex( indexName );\n\n\t\tregisterIndexForCleanup( indexName );\n\t\tperformRequest( createRequest );\n\n\t\twaitForRequiredIndexStatus( indexName );\n\t}\n\n\tprivate void createTemplate(String templateName, String templateString, int templateOrder, JsonObject settings) {\n\t\tJsonObject source = new JsonObject();\n\t\tdialect.setTemplatePattern( source, templateString );\n\t\tsource.addProperty( \"order\", templateOrder );\n\t\tsource.add( \"settings\", settings );\n\n\t\tregisterTemplateForCleanup( templateName );\n\n\t\tElasticsearchRequest.Builder builder = ElasticsearchRequest.put()\n\t\t\t\t.pathComponent( Paths._TEMPLATE ).pathComponent( URLEncodedString.fromString( templateName ) )\n\t\t\t\t.body( source );\n\n\t\tBoolean includeTypeName = dialect.getIncludeTypeNameParameterForMappingApi();\n\t\tif ( includeTypeName != null ) {\n\t\t\tbuilder.param( \"include_type_name\", includeTypeName );\n\t\t}\n\n\t\tperformRequest( builder.build() );\n\t}\n\n\tprivate void ensureIndexDoesNotExist(URLEncodedString indexName) {\n\t\tperformRequestIgnore404( ElasticsearchRequest.delete()\n\t\t\t\t.pathComponent( indexName )\n\t\t\t\t.build() );\n\t}\n\n\tprivate void registerIndexForCleanup(URLEncodedString indexName) {\n\t\tcreatedIndicesNames.add( indexName );\n\t}\n\n\tprivate void registerTemplateForCleanup(String templateName) {\n\t\tcreatedTemplatesNames.add( templateName );\n\t}\n\n\tprivate void waitForRequiredIndexStatus(final URLEncodedString indexName) {\n\t\tperformRequest( ElasticsearchRequest.get()\n\t\t\t\t.pathComponent( Paths._CLUSTER ).pathComponent( Paths.HEALTH ).pathComponent( indexName )\n\t\t\t\t/*\n\t\t\t\t * We only wait for YELLOW: it's perfectly fine, and some tests actually expect\n\t\t\t\t * the indexes to never reach a green status\n\t\t\t\t */\n\t\t\t\t.param( \"wait_for_status\", ElasticsearchIndexStatus.YELLOW.getElasticsearchString() )\n\t\t\t\t.param( \"timeout\", ElasticsearchIndexSettings.Defaults.LIFECYCLE_MINIMAL_REQUIRED_STATUS_WAIT_TIMEOUT + \"ms\" )\n\t\t\t\t.build() );\n\t}\n\n\tprivate void putMapping(URLEncodedString indexName, String mappingJson) {\n\t\tJsonObject mappingJsonObject = toJsonElement( mappingJson ).getAsJsonObject();\n\n\t\tElasticsearchRequest.Builder builder = ElasticsearchRequest.put()\n\t\t\t\t.pathComponent( indexName ).pathComponent( Paths._MAPPING );\n\t\tdialect.getTypeNameForMappingApi().ifPresent( builder::pathComponent );\n\t\tbuilder.body( mappingJsonObject );\n\n\t\tBoolean includeTypeName = dialect.getIncludeTypeNameParameterForMappingApi();\n\t\tif ( includeTypeName != null ) {\n\t\t\tbuilder.param( \"include_type_name\", includeTypeName );\n\t\t}\n\n\t\tperformRequest( builder.build() );\n\t}\n\n\tprivate String getMapping(URLEncodedString indexName) {\n\n\t\tElasticsearchRequest.Builder builder = ElasticsearchRequest.get()\n\t\t\t\t.pathComponent( indexName ).pathComponent( Paths._MAPPING );\n\t\tdialect.getTypeNameForMappingApi().ifPresent( builder::pathComponent );\n\n\t\tBoolean includeTypeName = dialect.getIncludeTypeNameParameterForMappingApi();\n\t\tif ( includeTypeName != null ) {\n\t\t\tbuilder.param( \"include_type_name\", includeTypeName );\n\t\t}\n\n\t\t/*\n\t\t * Elasticsearch 5.5+ triggers a 404 error when mappings are missing,\n\t\t * while 5.4 and below just return an empty mapping.\n\t\t * In our case, an empty mapping is fine, so we'll just ignore 404.\n\t\t */\n\t\tElasticsearchResponse response = performRequestIgnore404( builder.build() );\n\t\tJsonObject result = response.getBody();\n\t\tJsonElement index = result.get( indexName.original );\n\t\tif ( index == null ) {\n\t\t\treturn new JsonObject().toString();\n\t\t}\n\t\tJsonElement mappings = index.getAsJsonObject().get( \"mappings\" );\n\t\tif ( mappings == null ) {\n\t\t\treturn new JsonObject().toString();\n\t\t}\n\t\tOptional<URLEncodedString> typeName = dialect.getTypeNameForMappingApi();\n\t\tif ( typeName.isPresent() ) {\n\t\t\tJsonElement mapping = mappings.getAsJsonObject().get( typeName.get().original );\n\t\t\tif ( mapping == null ) {\n\t\t\t\treturn new JsonObject().toString();\n\t\t\t}\n\t\t\treturn mapping.toString();\n\t\t}\n\t\telse {\n\t\t\treturn mappings.toString();\n\t\t}\n\t}\n\n\tprivate void putDynamicSettings(URLEncodedString indexName, JsonObject settingsJsonObject) {\n\t\tperformRequest( ElasticsearchRequest.put()\n\t\t\t\t.pathComponent( indexName ).pathComponent( Paths._SETTINGS )\n\t\t\t\t.body( settingsJsonObject )\n\t\t\t\t.build() );\n\t}\n\n\tprivate void putNonDynamicSettings(URLEncodedString indexName, JsonObject settingsJsonObject) {\n\t\tperformRequest( ElasticsearchRequest.post()\n\t\t\t\t.pathComponent( indexName )\n\t\t\t\t.pathComponent( Paths._CLOSE )\n\t\t\t\t.build() );\n\n\t\tperformRequest( ElasticsearchRequest.put()\n\t\t\t\t.pathComponent( indexName ).pathComponent( Paths._SETTINGS )\n\t\t\t\t.body( settingsJsonObject )\n\t\t\t\t.build() );\n\n\t\tperformRequest( ElasticsearchRequest.post()\n\t\t\t\t.pathComponent( indexName )\n\t\t\t\t.pathComponent( Paths._OPEN )\n\t\t\t\t.build() );\n\t}\n\n\tprivate JsonObject buildSettings(String settingsPath, String settings) {\n\t\tJsonElement settingsJsonElement = toJsonElement( settings );\n\n\t\tList<String> components = Arrays.asList( settingsPath.split( \"\\\\.\" ) );\n\t\tCollections.reverse( components );\n\t\tfor ( String property : components ) {\n\t\t\tJsonObject parent = new JsonObject();\n\t\t\tparent.add( property, settingsJsonElement );\n\t\t\tsettingsJsonElement = parent;\n\t\t}\n\n\t\treturn settingsJsonElement.getAsJsonObject();\n\t}\n\n\tprivate String getSettings(URLEncodedString indexName, String path) {\n\t\tElasticsearchResponse response = performRequest( ElasticsearchRequest.get()\n\t\t\t\t.pathComponent( indexName ).pathComponent( Paths._SETTINGS )\n\t\t\t\t.build() );\n\t\tJsonObject result = response.getBody();\n\t\tJsonElement index = result.get( indexName.original );\n\t\tif ( index == null ) {\n\t\t\treturn new JsonObject().toString();\n\t\t}\n\t\tJsonElement settings = index.getAsJsonObject().get( \"settings\" );\n\t\tfor ( String property : path.split( \"\\\\.\" ) ) {\n\t\t\tif ( settings == null ) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsettings = settings.getAsJsonObject().get( property );\n\t\t}\n\t\tif ( settings == null ) {\n\t\t\treturn new JsonObject().toString();\n\t\t}\n\t\treturn settings.toString();\n\t}\n\n\tprivate void index(URLEncodedString indexName, URLEncodedString id, String jsonDocument) {\n\t\tJsonObject documentJsonObject = toJsonElement( jsonDocument ).getAsJsonObject();\n\t\tperformRequest( ElasticsearchRequest.put()\n\t\t\t\t.pathComponent( indexName ).pathComponent( dialect.getTypeKeywordForNonMappingApi() ).pathComponent( id )\n\t\t\t\t.body( documentJsonObject )\n\t\t\t\t.param( \"refresh\", true )\n\t\t\t\t.build() );\n\t}\n\n\tprivate JsonObject getDocumentSource(URLEncodedString indexName, URLEncodedString id) {\n\t\tElasticsearchResponse response = performRequest( ElasticsearchRequest.get()\n\t\t\t\t.pathComponent( indexName ).pathComponent( dialect.getTypeKeywordForNonMappingApi() ).pathComponent( id )\n\t\t\t\t.build() );\n\t\tJsonObject result = response.getBody();\n\t\treturn result.get( \"_source\" ).getAsJsonObject();\n\t}\n\n\tprotected JsonElement getDocumentField(URLEncodedString indexName, URLEncodedString id, String fieldName) {\n\t\tElasticsearchResponse response = performRequest( ElasticsearchRequest.get()\n\t\t\t\t.pathComponent( indexName ).pathComponent( dialect.getTypeKeywordForNonMappingApi() ).pathComponent( id )\n\t\t\t\t.param( \"stored_fields\", fieldName )\n\t\t\t\t.build() );\n\t\tJsonObject result = response.getBody();\n\t\treturn result.get( \"fields\" ).getAsJsonObject().get( fieldName );\n\t}\n\n\t@Override\n\tpublic Statement apply(Statement base, Description description) {\n\t\tStatement wrapped = new Statement() {\n\t\t\t@Override\n\t\t\tpublic void evaluate() throws Throwable {\n\t\t\t\ttry ( Closer<IOException> closer = new Closer<>() ) {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tbefore();\n\t\t\t\t\t\tbase.evaluate();\n\t\t\t\t\t}\n\t\t\t\t\tfinally {\n\t\t\t\t\t\tafter( closer );\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t\treturn configurationProvider.apply( wrapped, description );\n\t}\n\n\tprivate void before() {\n\t\tConfigurationPropertySource backendProperties =\n\t\t\t\tTckConfiguration.get().getBackendProperties( configurationProvider, null );\n\n\t\tBeanResolver beanResolver = configurationProvider.createBeanResolverForTest();\n\t\t/*\n\t\t * We use a {@link ElasticsearchClientFactoryImpl} to create our low-level client.\n\t\t *\n\t\t * The main advantage is that we ensure we connect to Elasticsearch exactly the same way\n\t\t * as any test-created SearchFactory, allowing to support things like testing on AWS\n\t\t * (using the hibernate-search-elasticsearch-aws module).\n\t\t */\n\t\ttry ( BeanHolder<ElasticsearchClientFactory> factoryHolder =\n\t\t\t\tbeanResolver.resolve( ElasticsearchClientFactoryImpl.REFERENCE ) ) {\n\t\t\tclient = factoryHolder.get().create(\n\t\t\t\t\tbackendProperties, DefaultGsonProvider.create( GsonBuilder::new, true )\n\t\t\t);\n\t\t}\n\t}\n\n\tprivate void after(Closer<IOException> closer) {\n\t\tcloser.pushAll( this::tryDeleteESIndex, createdIndicesNames );\n\t\tcreatedIndicesNames.clear();\n\t\tcloser.pushAll( this::tryDeleteESTemplate, createdTemplatesNames );\n\t\tcreatedTemplatesNames.clear();\n\t\tcloser.push( this::tryCloseClient, client );\n\t\tclient = null;\n\t}\n\n\tprivate void tryDeleteESIndex(URLEncodedString indexName) {\n\t\ttry {\n\t\t\tperformRequestIgnore404( ElasticsearchRequest.delete()\n\t\t\t\t\t.pathComponent( indexName )\n\t\t\t\t\t.build() );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\tString.format( Locale.ROOT, \"Error while trying to delete index '%s' as part of test cleanup\", indexName ),\n\t\t\t\t\te\n\t\t\t);\n\t\t}\n\t}\n\n\tprivate void tryDeleteESTemplate(String templateName) {\n\t\ttry {\n\t\t\tperformRequestIgnore404( ElasticsearchRequest.delete()\n\t\t\t\t\t.pathComponent( Paths._TEMPLATE ).pathComponent( URLEncodedString.fromString( templateName ) )\n\t\t\t\t\t.build() );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\tString.format( Locale.ROOT, \"Error while trying to delete template '%s' as part of test cleanup\", templateName ),\n\t\t\t\t\te\n\t\t\t);\n\t\t}\n\t}\n\n\tprivate void tryCloseClient(ElasticsearchClientImplementor client) {\n\t\ttry {\n\t\t\tclient.close();\n\t\t}\n\t\tcatch (RuntimeException | IOException e) {\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"Unexpected exception when closing the ElasticsearchClient used in \"\n\t\t\t\t\t\t\t+ TestElasticsearchClient.class.getSimpleName(),\n\t\t\t\t\te\n\t\t\t);\n\t\t}\n\t}\n\n\tprotected ElasticsearchResponse performRequest(ElasticsearchRequest request) {\n\t\tElasticsearchResponse response;\n\t\ttry {\n\t\t\tresponse = client.submit( request ).join();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow requestFailed( request, e );\n\t\t}\n\t\tint statusCode = response.getStatusCode();\n\t\tif ( !ElasticsearchClientUtils.isSuccessCode( statusCode ) ) {\n\t\t\tthrow requestFailed( request, response );\n\t\t}\n\t\treturn response;\n\t}\n\n\tprotected ElasticsearchResponse performRequestIgnore404(ElasticsearchRequest request) {\n\t\tElasticsearchResponse response;\n\t\ttry {\n\t\t\tresponse = client.submit( request ).join();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow requestFailed( request, e );\n\t\t}\n\t\tint statusCode = response.getStatusCode();\n\t\tif ( !ElasticsearchClientUtils.isSuccessCode( statusCode ) && 404 != statusCode ) {\n\t\t\tthrow requestFailed( request, response );\n\t\t}\n\t\treturn response;\n\t}\n\n\tprivate AssertionFailure requestFailed(ElasticsearchRequest request, Exception e) {\n\t\treturn new AssertionFailure( \"Elasticsearch request in TestElasticsearchClient failed:\"\n\t\t\t\t+ \"Request:\\n\"\n\t\t\t\t+ \"========\\n\"\n\t\t\t\t+ new ElasticsearchRequestFormatter( request ),\n\t\t\t\te );\n\t}\n\n\tprivate AssertionFailure requestFailed(ElasticsearchRequest request, ElasticsearchResponse response) {\n\t\treturn new AssertionFailure( \"Elasticsearch request in TestElasticsearchClient failed:\\n\"\n\t\t\t\t+ \"Request:\\n\"\n\t\t\t\t+ \"========\\n\"\n\t\t\t\t+ new ElasticsearchRequestFormatter( request )\n\t\t\t\t+ \"\\nResponse:\\n\"\n\t\t\t\t+ \"========\\n\"\n\t\t\t\t+ new ElasticsearchResponseFormatter( response )\n\t\t\t\t);\n\t}\n\n\t/*\n\t * Convert provided JSON to JsonElement, so that some Elasticsearch peculiarities (such as the fact that\n\t * single quotes are not accepted as a substitute for single quotes) can be worked around.\n\t * In tests, single quotes are way easier to include in JSON strings, because we don't have to escape them.\n\t */\n\tprivate JsonElement toJsonElement(String jsonAsString) {\n\t\treturn new JsonParser().parse( jsonAsString );\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "public TemplateClient create(String templateString, int templateOrder, String settings) {\n\t\t\treturn create( templateString, templateOrder, toJsonElement( settings ).getAsJsonObject() );\n\t\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util.TestElasticsearchClient.TemplateClient#create\n methodBody: public TemplateClient create(String templateString, JsonObject settings) {\nTestElasticsearchClient.this.createTemplate(templateName,templateString,settings);\nreturn this;\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.elasticsearch.testsupport.util.TestElasticsearchClient#toJsonElement\n methodBody: private JsonElement toJsonElement(String jsonAsString) {\nreturn new JsonParser().parse(jsonAsString);\n}"
        ],
        "sourceCodeAfterRefactoring": "public TemplateClient create(String templateString, String settings) {\n\t\t\treturn create( templateString, 0, settings );\n\t\t}\npublic TemplateClient create(String templateString, int templateOrder, String settings) {\n\t\t\treturn create( templateString, templateOrder, toJsonElement( settings ).getAsJsonObject() );\n\t\t}",
        "diffSourceCode": "   216: \t\tpublic TemplateClient create(String templateString, String settings) {\n-  217: \t\t\treturn create( templateString, toJsonElement( settings ).getAsJsonObject() );\n+  217: \t\t\treturn create( templateString, 0, settings );\n   218: \t\t}\n-  220: \t\tpublic TemplateClient create(String templateString, JsonObject settings) {\n-  221: \t\t\tTestElasticsearchClient.this.createTemplate( templateName, templateString, settings );\n-  222: \t\t\treturn this;\n+  220: \t\tpublic TemplateClient create(String templateString, int templateOrder, String settings) {\n+  221: \t\t\treturn create( templateString, templateOrder, toJsonElement( settings ).getAsJsonObject() );\n+  222: \t\t}\n",
        "uniqueId": "769be0fb9220c3a46d4e2a1562295c55eca40539_216_218_220_222_216_218",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Move Method",
        "description": "Move Method\tpublic getBean(typeReference Class<T>, nameReference String) : BeanHolder<T> from class org.hibernate.search.engine.environment.bean.BeanResolver to public getBean(typeReference Class<T>, nameReference String) : BeanHolder<T> from class org.hibernate.search.engine.environment.bean.spi.BeanProvider",
        "diffLocations": [
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/environment/bean/BeanResolver.java",
                "startLine": 44,
                "endLine": 52,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/environment/bean/spi/BeanProvider.java",
                "startLine": 45,
                "endLine": 53,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "/**\n\t * Retrieve a bean referenced by its type and name.\n\t * @param <T> The expected return type.\n\t * @param typeReference The type used as a reference to the bean to retrieve. Must be non-null.\n\t * @param nameReference The name used as a reference to the bean to retrieve. Must be non-null and non-empty.\n\t * @return A {@link BeanHolder} containing the resolved bean.\n\t * @throws SearchException if the reference is invalid (null or empty) or the bean cannot be resolved.\n\t */\n\t<T> BeanHolder<T> getBean(Class<T> typeReference, String nameReference);",
        "filePathBefore": "engine/src/main/java/org/hibernate/search/engine/environment/bean/BeanResolver.java",
        "isPureRefactoring": true,
        "commitId": "eb7e2caf23d4cf385d1f6b4cd99a9f2195297a3f",
        "packageNameBefore": "org.hibernate.search.engine.environment.bean",
        "classNameBefore": "org.hibernate.search.engine.environment.bean.BeanResolver",
        "methodNameBefore": "org.hibernate.search.engine.environment.bean.BeanResolver#getBean",
        "classSignatureBefore": "public interface BeanResolver ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.environment.bean.BeanResolver#getBean"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.environment.bean.BeanResolver"
        ],
        "classSignatureBeforeSet": [
            "public interface BeanResolver "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.environment.bean;\n\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.function.Function;\n\nimport org.hibernate.search.engine.environment.bean.spi.BeanProvider;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.common.impl.Contracts;\nimport org.hibernate.search.util.common.impl.SuppressingCloser;\n\n/**\n * The main entry point for components looking to retrieve user-provided beans.\n * <p>\n * Depending on the integration, beans may be resolved using reflection (expecting a no-argument constructor),\n * or using a more advanced dependency injection context (CDI, Spring DI).\n * <p>\n * Regardless of the implementations, this interface is used to retrieve the beans,\n * referenced either by their name, by their type, or both.\n * <p>\n * This interface may be used by any Hibernate Search module,\n * but should only be implemented by the Hibernate Search engine itself;\n * if you are looking for implementing your own bean resolver,\n * you should implement {@link BeanProvider} instead.\n */\npublic interface BeanResolver {\n\n\t/**\n\t * Retrieve a bean referenced by its type.\n\t * @param <T> The expected return type.\n\t * @param typeReference The type used as a reference to the bean to retrieve. Must be non-null.\n\t * @return A {@link BeanHolder} containing the resolved bean.\n\t * @throws SearchException if the reference is invalid (null) or the bean cannot be resolved.\n\t */\n\t<T> BeanHolder<T> getBean(Class<T> typeReference);\n\n\t/**\n\t * Retrieve a bean referenced by its type and name.\n\t * @param <T> The expected return type.\n\t * @param typeReference The type used as a reference to the bean to retrieve. Must be non-null.\n\t * @param nameReference The name used as a reference to the bean to retrieve. Must be non-null and non-empty.\n\t * @return A {@link BeanHolder} containing the resolved bean.\n\t * @throws SearchException if the reference is invalid (null or empty) or the bean cannot be resolved.\n\t */\n\t<T> BeanHolder<T> getBean(Class<T> typeReference, String nameReference);\n\n\t/**\n\t * Retrieve a bean from a {@link BeanReference}.\n\t * <p>\n\t * This method is just syntactic sugar to allow to write {@code bridgeProvider::getBean}\n\t * and get a {@code Function<BeanReference<T>, T>} that can be used in {@link java.util.Optional#map(Function)}\n\t * for instance.\n\t *\n\t * @param <T> The expected return type.\n\t * @param reference The reference to the bean to retrieve. Must be non-null.\n\t * @return A {@link BeanHolder} containing the resolved bean.\n\t * @throws SearchException if the reference is invalid (null or empty) or the bean cannot be resolved.\n\t */\n\tdefault <T> BeanHolder<T> getBean(BeanReference<T> reference) {\n\t\tContracts.assertNotNull( reference, \"reference\" );\n\t\treturn reference.getBean( this );\n\t}\n\n\t/**\n\t * Retrieve a list of beans from a list of {@link BeanReference}s.\n\t * <p>\n\t * The main advantage of calling this method over looping and calling {@link #getBean(BeanReference)} repeatedly\n\t * is that errors are handled correctly: if a bean was already instantiated, and getting the next one fails,\n\t * then the first bean will be properly {@link BeanHolder#close() closed} before the exception is propagated.\n\t * Also, this method returns a {@code BeanHolder<List<T>>} instead of a {@code List<BeanHolder<T>>},\n\t * so its result is easier to use in a try-with-resources.\n\t * <p>\n\t * This method is also syntactic sugar to allow to write {@code bridgeProvider::getBeans}\n\t * and get a {@code Function<BeanReference<T>, T>} that can be used in {@link java.util.Optional#map(Function)}\n\t * for instance.\n\t *\n\t * @param <T> The expected bean type.\n\t * @param references The references to the beans to retrieve. Must be non-null.\n\t * @return A {@link BeanHolder} containing a {@link List} containing the resolved beans,\n\t * in the same order as the {@code references}.\n\t * @throws SearchException if one reference is invalid (null or empty) or the corresponding bean cannot be resolved.\n\t */\n\tdefault <T> BeanHolder<List<T>> getBeans(List<? extends BeanReference<? extends T>> references) {\n\t\tList<BeanHolder<? extends T>> beanHolders = new ArrayList<>();\n\t\ttry {\n\t\t\tfor ( BeanReference<? extends T> reference : references ) {\n\t\t\t\tbeanHolders.add( reference.getBean( this ) );\n\t\t\t}\n\t\t\treturn BeanHolder.of( beanHolders );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tnew SuppressingCloser( e ).pushAll( BeanHolder::close, beanHolders );\n\t\t\tthrow e;\n\t\t}\n\t}\n\n\t/**\n\t * Retrieve a list of beans with the given role.\n\t * <p>\n\t * <strong>WARNING:</strong> this does not just return all the beans that implement {@code role}.\n\t * Beans are assigned a role explicitly during\n\t * {@link org.hibernate.search.engine.environment.bean.spi.BeanConfigurer bean configuration}\n\t * by calling\n\t * {@link org.hibernate.search.engine.environment.bean.spi.BeanConfigurationContext#assignRole(Class, BeanReference)}.\n\t *\n\t * @param <T> The expected bean type.\n\t * @param role The role that must have been assigned to the retrieved beans. Must be non-null and non-empty.\n\t * @return A {@link BeanHolder} containing a {@link List} containing the resolved beans.\n\t * @throws SearchException if one of the references assigned to the role cannot be resolved.\n\t */\n\t<T> BeanHolder<List<T>> getBeansWithRole(Class<T> role);\n\n}\n",
        "filePathAfter": "engine/src/main/java/org/hibernate/search/engine/environment/bean/spi/BeanProvider.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.environment.bean.spi;\n\n\nimport org.hibernate.search.engine.environment.bean.BeanHolder;\nimport org.hibernate.search.engine.environment.bean.BeanResolver;\nimport org.hibernate.search.util.common.SearchException;\n\n/**\n * The interface to be implemented by components providing beans to Hibernate Search.\n * <p>\n * This interface should only be called by Hibernate Search itself;\n * if you are looking to retrieve beans,\n * you should use {@link BeanResolver} instead.\n */\npublic interface BeanProvider extends AutoCloseable {\n\n\t/**\n\t * Release any internal resource created to support provided beans.\n\t * <p>\n\t * Provided beans will not be usable after a call to this method.\n\t * <p>\n\t * This may not release all resources that were allocated for each {@link BeanHolder};\n\t * {@link BeanHolder#close()} still needs to be called consistently for each created bean.\n\t *\n\t * @see AutoCloseable#close()\n\t */\n\t@Override\n\tvoid close();\n\n\t/**\n\t * Provide a bean referenced by its type.\n\t * @param <T> The expected return type.\n\t * @param typeReference The type used as a reference to the bean to retrieve. Must be non-null.\n\t * @return A {@link BeanHolder} containing the resolved bean.\n\t * @throws SearchException if the reference is invalid (null) or the bean does not exist.\n\t */\n\t<T> BeanHolder<T> getBean(Class<T> typeReference);\n\n\t/**\n\t * Provide a bean referenced by its type and name.\n\t * @param <T> The expected return type.\n\t * @param typeReference The type used as a reference to the bean to retrieve. Must be non-null.\n\t * @param nameReference The name used as a reference to the bean to retrieve. Must be non-null and non-empty.\n\t * @return A {@link BeanHolder} containing the resolved bean.\n\t * @throws SearchException if a reference is invalid (null or empty) or the bean does not exist.\n\t */\n\t<T> BeanHolder<T> getBean(Class<T> typeReference, String nameReference);\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "/**\n\t * Provide a bean referenced by its type and name.\n\t * @param <T> The expected return type.\n\t * @param typeReference The type used as a reference to the bean to retrieve. Must be non-null.\n\t * @param nameReference The name used as a reference to the bean to retrieve. Must be non-null and non-empty.\n\t * @return A {@link BeanHolder} containing the resolved bean.\n\t * @throws SearchException if a reference is invalid (null or empty) or the bean does not exist.\n\t */\n\t<T> BeanHolder<T> getBean(Class<T> typeReference, String nameReference);",
        "diffSourceCode": "-   44: \t/**\n-   45: \t * Retrieve a bean referenced by its type and name.\n-   46: \t * @param <T> The expected return type.\n-   47: \t * @param typeReference The type used as a reference to the bean to retrieve. Must be non-null.\n-   48: \t * @param nameReference The name used as a reference to the bean to retrieve. Must be non-null and non-empty.\n-   49: \t * @return A {@link BeanHolder} containing the resolved bean.\n-   50: \t * @throws SearchException if the reference is invalid (null or empty) or the bean cannot be resolved.\n-   51: \t */\n-   52: \t<T> BeanHolder<T> getBean(Class<T> typeReference, String nameReference);\n-   53: \n+   44: \n+   45: \t/**\n+   46: \t * Provide a bean referenced by its type and name.\n+   47: \t * @param <T> The expected return type.\n+   48: \t * @param typeReference The type used as a reference to the bean to retrieve. Must be non-null.\n+   49: \t * @param nameReference The name used as a reference to the bean to retrieve. Must be non-null and non-empty.\n+   50: \t * @return A {@link BeanHolder} containing the resolved bean.\n+   51: \t * @throws SearchException if a reference is invalid (null or empty) or the bean does not exist.\n+   52: \t */\n+   53: \t<T> BeanHolder<T> getBean(Class<T> typeReference, String nameReference);\n",
        "uniqueId": "eb7e2caf23d4cf385d1f6b4cd99a9f2195297a3f_44_52__45_53",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 7
            },
            "LINE": {
                "missed": 0,
                "covered": 2
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate tryGetElasticsearchVersion(client ElasticsearchClient) : ElasticsearchVersion extracted from public getElasticsearchVersion(client ElasticsearchClient) : ElasticsearchVersion in class org.hibernate.search.backend.elasticsearch.client.impl.ElasticsearchClientUtils",
        "diffLocations": [
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/client/impl/ElasticsearchClientUtils.java",
                "startLine": 51,
                "endLine": 73,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/client/impl/ElasticsearchClientUtils.java",
                "startLine": 51,
                "endLine": 58,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/client/impl/ElasticsearchClientUtils.java",
                "startLine": 60,
                "endLine": 77,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public static ElasticsearchVersion getElasticsearchVersion(ElasticsearchClient client) {\n\t\ttry {\n\t\t\tElasticsearchRequest request = ElasticsearchRequest.get().build();\n\t\t\tElasticsearchResponse response = null;\n\t\t\ttry {\n\t\t\t\tresponse = Futures.unwrappedExceptionJoin( client.submit( request ) );\n\n\t\t\t\tif ( !ElasticsearchClientUtils.isSuccessCode( response.getStatusCode() ) ) {\n\t\t\t\t\tthrow log.elasticsearchResponseIndicatesFailure();\n\t\t\t\t}\n\n\t\t\t\treturn VERSION_ACCESSOR.get( response.getBody() )\n\t\t\t\t\t\t.map( ElasticsearchVersion::of )\n\t\t\t\t\t\t.orElseThrow( () -> new AssertionFailure( \"Missing version number in JSON response\" ) );\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\tthrow log.elasticsearchRequestFailed( request, response, e.getMessage(), e );\n\t\t\t}\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow log.failedToDetectElasticsearchVersion( e.getMessage(), e );\n\t\t}\n\t}",
        "filePathBefore": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/client/impl/ElasticsearchClientUtils.java",
        "isPureRefactoring": true,
        "commitId": "3853f803c46d0ffc3dfe6f337aeaf2786adc0865",
        "packageNameBefore": "org.hibernate.search.backend.elasticsearch.client.impl",
        "classNameBefore": "org.hibernate.search.backend.elasticsearch.client.impl.ElasticsearchClientUtils",
        "methodNameBefore": "org.hibernate.search.backend.elasticsearch.client.impl.ElasticsearchClientUtils#getElasticsearchVersion",
        "invokedMethod": "methodSignature: org.hibernate.search.backend.elasticsearch.client.impl.ElasticsearchClientUtils#isSuccessCode\n methodBody: public static boolean isSuccessCode(int code) {\nreturn 200 <= code && code < 300;\n}",
        "classSignatureBefore": "public class ElasticsearchClientUtils ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.client.impl.ElasticsearchClientUtils#getElasticsearchVersion"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.client.impl.ElasticsearchClientUtils"
        ],
        "classSignatureBeforeSet": [
            "public class ElasticsearchClientUtils "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.client.impl;\n\nimport java.io.IOException;\nimport java.lang.invoke.MethodHandles;\nimport java.util.List;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchVersion;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchClient;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchRequest;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchResponse;\nimport org.hibernate.search.backend.elasticsearch.gson.impl.JsonAccessor;\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\nimport com.google.gson.Gson;\nimport com.google.gson.JsonObject;\nimport org.apache.http.HttpEntity;\n\n\npublic class ElasticsearchClientUtils {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate static final JsonAccessor<String> VERSION_ACCESSOR =\n\t\t\tJsonAccessor.root().property( \"version\" ).property( \"number\" ).asString();\n\n\tprivate ElasticsearchClientUtils() {\n\t\t// Private constructor\n\t}\n\n\tpublic static boolean isSuccessCode(int code) {\n\t\treturn 200 <= code && code < 300;\n\t}\n\n\tpublic static HttpEntity toEntity(Gson gson, ElasticsearchRequest request) throws IOException {\n\t\tfinal List<JsonObject> bodyParts = request.getBodyParts();\n\t\tif ( bodyParts.isEmpty() ) {\n\t\t\treturn null;\n\t\t}\n\t\treturn new GsonHttpEntity( gson, bodyParts );\n\t}\n\n\tpublic static ElasticsearchVersion getElasticsearchVersion(ElasticsearchClient client) {\n\t\ttry {\n\t\t\tElasticsearchRequest request = ElasticsearchRequest.get().build();\n\t\t\tElasticsearchResponse response = null;\n\t\t\ttry {\n\t\t\t\tresponse = Futures.unwrappedExceptionJoin( client.submit( request ) );\n\n\t\t\t\tif ( !ElasticsearchClientUtils.isSuccessCode( response.getStatusCode() ) ) {\n\t\t\t\t\tthrow log.elasticsearchResponseIndicatesFailure();\n\t\t\t\t}\n\n\t\t\t\treturn VERSION_ACCESSOR.get( response.getBody() )\n\t\t\t\t\t\t.map( ElasticsearchVersion::of )\n\t\t\t\t\t\t.orElseThrow( () -> new AssertionFailure( \"Missing version number in JSON response\" ) );\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\tthrow log.elasticsearchRequestFailed( request, response, e.getMessage(), e );\n\t\t\t}\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow log.failedToDetectElasticsearchVersion( e.getMessage(), e );\n\t\t}\n\t}\n\n}\n",
        "filePathAfter": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/client/impl/ElasticsearchClientUtils.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.client.impl;\n\nimport java.io.IOException;\nimport java.lang.invoke.MethodHandles;\nimport java.util.List;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchVersion;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchClient;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchRequest;\nimport org.hibernate.search.backend.elasticsearch.client.spi.ElasticsearchResponse;\nimport org.hibernate.search.backend.elasticsearch.gson.impl.JsonAccessor;\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\nimport com.google.gson.Gson;\nimport com.google.gson.JsonObject;\nimport org.apache.http.HttpEntity;\n\n\npublic class ElasticsearchClientUtils {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate static final JsonAccessor<String> VERSION_ACCESSOR =\n\t\t\tJsonAccessor.root().property( \"version\" ).property( \"number\" ).asString();\n\n\tprivate ElasticsearchClientUtils() {\n\t\t// Private constructor\n\t}\n\n\tpublic static boolean isSuccessCode(int code) {\n\t\treturn 200 <= code && code < 300;\n\t}\n\n\tpublic static HttpEntity toEntity(Gson gson, ElasticsearchRequest request) throws IOException {\n\t\tfinal List<JsonObject> bodyParts = request.getBodyParts();\n\t\tif ( bodyParts.isEmpty() ) {\n\t\t\treturn null;\n\t\t}\n\t\treturn new GsonHttpEntity( gson, bodyParts );\n\t}\n\n\tpublic static ElasticsearchVersion getElasticsearchVersion(ElasticsearchClient client) {\n\t\ttry {\n\t\t\treturn tryGetElasticsearchVersion( client );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow log.failedToDetectElasticsearchVersion( e.getMessage(), e );\n\t\t}\n\t}\n\n\tprivate static ElasticsearchVersion tryGetElasticsearchVersion(ElasticsearchClient client) {\n\t\tElasticsearchRequest request = ElasticsearchRequest.get().build();\n\t\tElasticsearchResponse response = null;\n\t\ttry {\n\t\t\tresponse = Futures.unwrappedExceptionJoin( client.submit( request ) );\n\n\t\t\tif ( !ElasticsearchClientUtils.isSuccessCode( response.getStatusCode() ) ) {\n\t\t\t\tthrow log.elasticsearchResponseIndicatesFailure();\n\t\t\t}\n\n\t\t\treturn VERSION_ACCESSOR.get( response.getBody() )\n\t\t\t\t\t.map( ElasticsearchVersion::of )\n\t\t\t\t\t.orElseThrow( () -> new AssertionFailure( \"Missing version number in JSON response\" ) );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow log.elasticsearchRequestFailed( request, response, e.getMessage(), e );\n\t\t}\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "private static ElasticsearchVersion tryGetElasticsearchVersion(ElasticsearchClient client) {\n\t\tElasticsearchRequest request = ElasticsearchRequest.get().build();\n\t\tElasticsearchResponse response = null;\n\t\ttry {\n\t\t\tresponse = Futures.unwrappedExceptionJoin( client.submit( request ) );\n\n\t\t\tif ( !ElasticsearchClientUtils.isSuccessCode( response.getStatusCode() ) ) {\n\t\t\t\tthrow log.elasticsearchResponseIndicatesFailure();\n\t\t\t}\n\n\t\t\treturn VERSION_ACCESSOR.get( response.getBody() )\n\t\t\t\t\t.map( ElasticsearchVersion::of )\n\t\t\t\t\t.orElseThrow( () -> new AssertionFailure( \"Missing version number in JSON response\" ) );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow log.elasticsearchRequestFailed( request, response, e.getMessage(), e );\n\t\t}\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.backend.elasticsearch.client.impl.ElasticsearchClientUtils#isSuccessCode\n methodBody: public static boolean isSuccessCode(int code) {\nreturn 200 <= code && code < 300;\n}"
        ],
        "sourceCodeAfterRefactoring": "public static ElasticsearchVersion getElasticsearchVersion(ElasticsearchClient client) {\n\t\ttry {\n\t\t\treturn tryGetElasticsearchVersion( client );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow log.failedToDetectElasticsearchVersion( e.getMessage(), e );\n\t\t}\n\t}\nprivate static ElasticsearchVersion tryGetElasticsearchVersion(ElasticsearchClient client) {\n\t\tElasticsearchRequest request = ElasticsearchRequest.get().build();\n\t\tElasticsearchResponse response = null;\n\t\ttry {\n\t\t\tresponse = Futures.unwrappedExceptionJoin( client.submit( request ) );\n\n\t\t\tif ( !ElasticsearchClientUtils.isSuccessCode( response.getStatusCode() ) ) {\n\t\t\t\tthrow log.elasticsearchResponseIndicatesFailure();\n\t\t\t}\n\n\t\t\treturn VERSION_ACCESSOR.get( response.getBody() )\n\t\t\t\t\t.map( ElasticsearchVersion::of )\n\t\t\t\t\t.orElseThrow( () -> new AssertionFailure( \"Missing version number in JSON response\" ) );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tthrow log.elasticsearchRequestFailed( request, response, e.getMessage(), e );\n\t\t}\n\t}",
        "diffSourceCode": "    51: \tpublic static ElasticsearchVersion getElasticsearchVersion(ElasticsearchClient client) {\n    52: \t\ttry {\n-   53: \t\t\tElasticsearchRequest request = ElasticsearchRequest.get().build();\n-   54: \t\t\tElasticsearchResponse response = null;\n-   55: \t\t\ttry {\n-   56: \t\t\t\tresponse = Futures.unwrappedExceptionJoin( client.submit( request ) );\n-   57: \n-   58: \t\t\t\tif ( !ElasticsearchClientUtils.isSuccessCode( response.getStatusCode() ) ) {\n-   59: \t\t\t\t\tthrow log.elasticsearchResponseIndicatesFailure();\n-   60: \t\t\t\t}\n-   61: \n-   62: \t\t\t\treturn VERSION_ACCESSOR.get( response.getBody() )\n-   63: \t\t\t\t\t\t.map( ElasticsearchVersion::of )\n-   64: \t\t\t\t\t\t.orElseThrow( () -> new AssertionFailure( \"Missing version number in JSON response\" ) );\n-   65: \t\t\t}\n-   66: \t\t\tcatch (RuntimeException e) {\n-   67: \t\t\t\tthrow log.elasticsearchRequestFailed( request, response, e.getMessage(), e );\n+   53: \t\t\treturn tryGetElasticsearchVersion( client );\n+   54: \t\t}\n+   55: \t\tcatch (RuntimeException e) {\n+   56: \t\t\tthrow log.failedToDetectElasticsearchVersion( e.getMessage(), e );\n+   57: \t\t}\n+   58: \t}\n+   59: \n+   60: \tprivate static ElasticsearchVersion tryGetElasticsearchVersion(ElasticsearchClient client) {\n+   61: \t\tElasticsearchRequest request = ElasticsearchRequest.get().build();\n+   62: \t\tElasticsearchResponse response = null;\n+   63: \t\ttry {\n+   64: \t\t\tresponse = Futures.unwrappedExceptionJoin( client.submit( request ) );\n+   65: \n+   66: \t\t\tif ( !ElasticsearchClientUtils.isSuccessCode( response.getStatusCode() ) ) {\n+   67: \t\t\t\tthrow log.elasticsearchResponseIndicatesFailure();\n    68: \t\t\t}\n-   69: \t\t}\n-   70: \t\tcatch (RuntimeException e) {\n-   71: \t\t\tthrow log.failedToDetectElasticsearchVersion( e.getMessage(), e );\n-   72: \t\t}\n-   73: \t}\n-   74: \n-   75: }\n+   69: \n+   70: \t\t\treturn VERSION_ACCESSOR.get( response.getBody() )\n+   71: \t\t\t\t\t.map( ElasticsearchVersion::of )\n+   72: \t\t\t\t\t.orElseThrow( () -> new AssertionFailure( \"Missing version number in JSON response\" ) );\n+   73: \t\t}\n+   74: \t\tcatch (RuntimeException e) {\n+   75: \t\t\tthrow log.elasticsearchRequestFailed( request, response, e.getMessage(), e );\n+   76: \t\t}\n+   77: \t}\n",
        "uniqueId": "3853f803c46d0ffc3dfe6f337aeaf2786adc0865_51_73_60_77_51_58",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 3,
                "covered": 41
            },
            "BRANCH": {
                "missed": 1,
                "covered": 1
            },
            "LINE": {
                "missed": 1,
                "covered": 11
            },
            "COMPLEXITY": {
                "missed": 1,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate appendIndentIfNecessary(grandChild StructureType, child StructureType, current StructureType, hasParent boolean) : void extracted from private appendIndent() : void in class org.hibernate.search.util.common.impl.ToStringTreeBuilder",
        "diffLocations": [
            {
                "filePath": "util/common/src/main/java/org/hibernate/search/util/common/impl/ToStringTreeBuilder.java",
                "startLine": 171,
                "endLine": 214,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/common/src/main/java/org/hibernate/search/util/common/impl/ToStringTreeBuilder.java",
                "startLine": 171,
                "endLine": 192,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/common/src/main/java/org/hibernate/search/util/common/impl/ToStringTreeBuilder.java",
                "startLine": 194,
                "endLine": 219,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void appendIndent() {\n\t\tif ( structureTypeStack.isEmpty() ) {\n\t\t\treturn;\n\t\t}\n\n\t\tIterator<StructureType> iterator = structureTypeStack.descendingIterator();\n\t\tStructureType grandParent = null;\n\t\tStructureType parent = null;\n\t\tStructureType current = iterator.next();\n\t\tStructureType child = iterator.hasNext() ? iterator.next() : null;\n\t\tStructureType grandChild;\n\t\twhile ( current != null ) {\n\t\t\tgrandChild = iterator.hasNext() ? iterator.next() : null;\n\t\t\tif ( !shouldSqueeze( current, parent, grandParent ) ) {\n\t\t\t\tswitch ( current ) {\n\t\t\t\t\tcase OBJECT:\n\t\t\t\t\t\tbuilder.append( style.indentInObject );\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase LIST:\n\t\t\t\t\t\t// Display a bullet point if:\n\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t\t// We are adding a element directly to the list\n\t\t\t\t\t\t\t\tchild == null\n\t\t\t\t\t\t\t\t// OR we are adding the first element to a squeezed element in the list\n\t\t\t\t\t\t\t\t|| shouldSqueeze( grandChild, child, current ) && !iterator.hasNext() && first\n\t\t\t\t\t\t) {\n\t\t\t\t\t\t\tbuilder.append( style.indentInListBulletPoint );\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\tbuilder.append( style.indentInListNoBulletPoint );\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase UNNAMED_ENTRY:\n\t\t\t\t\tcase NAMED_ENTRY:\n\t\t\t\t\t\t// No indent for these\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tgrandParent = parent;\n\t\t\tparent = current;\n\t\t\tcurrent = child;\n\t\t\tchild = grandChild;\n\t\t}\n\t}",
        "filePathBefore": "util/common/src/main/java/org/hibernate/search/util/common/impl/ToStringTreeBuilder.java",
        "isPureRefactoring": true,
        "commitId": "a4d2e5b39bb53fefd2c31be404a6682af6c3a5bb",
        "packageNameBefore": "org.hibernate.search.util.common.impl",
        "classNameBefore": "org.hibernate.search.util.common.impl.ToStringTreeBuilder",
        "methodNameBefore": "org.hibernate.search.util.common.impl.ToStringTreeBuilder#appendIndent",
        "invokedMethod": "methodSignature: org.hibernate.search.util.common.impl.ToStringTreeBuilder#shouldSqueeze\n methodBody: private boolean shouldSqueeze(StructureType structureType, StructureType parentStructureType,\n\t\t\tStructureType grandParentStructureType) {\nreturn style.squeezeObjectsInList && StructureType.LIST.equals(grandParentStructureType) && StructureType.UNNAMED_ENTRY.equals(parentStructureType) && StructureType.OBJECT.equals(structureType);\n}",
        "classSignatureBefore": "public class ToStringTreeBuilder ",
        "methodNameBeforeSet": [
            "org.hibernate.search.util.common.impl.ToStringTreeBuilder#appendIndent"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.util.common.impl.ToStringTreeBuilder"
        ],
        "classSignatureBeforeSet": [
            "public class ToStringTreeBuilder "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.common.impl;\n\nimport java.util.ArrayDeque;\nimport java.util.Deque;\nimport java.util.Iterator;\n\nimport org.hibernate.search.util.common.AssertionFailure;\n\npublic class ToStringTreeBuilder {\n\n\tprivate final ToStringStyle style;\n\tprivate final StringBuilder builder = new StringBuilder();\n\n\tprivate final Deque<StructureType> structureTypeStack = new ArrayDeque<>();\n\tprivate boolean first = true;\n\n\tpublic ToStringTreeBuilder() {\n\t\tthis( ToStringStyle.inlineDelimiterStructure() );\n\t}\n\n\tpublic ToStringTreeBuilder(ToStringStyle style) {\n\t\tthis.style = style;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn builder.toString();\n\t}\n\n\tpublic ToStringTreeBuilder attribute(String name, Object value) {\n\t\tif ( value instanceof ToStringTreeAppendable ) {\n\t\t\tToStringTreeAppendable appendable = ( (ToStringTreeAppendable) value );\n\t\t\tstartEntry( name, StructureType.OBJECT );\n\t\t\tstartStructure( StructureType.OBJECT, style.startObject );\n\t\t\tappendable.appendTo( this );\n\t\t\tendStructure( StructureType.OBJECT, style.endObject );\n\t\t\tendEntry();\n\t\t}\n\t\telse {\n\t\t\tstartEntry( name, null );\n\t\t\tbuilder.append( value );\n\t\t\tendEntry();\n\t\t}\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder value(Object value) {\n\t\treturn attribute( null, value );\n\t}\n\n\tpublic ToStringTreeBuilder startObject() {\n\t\treturn startObject( null );\n\t}\n\n\tpublic ToStringTreeBuilder startObject(String name) {\n\t\tstartEntry( name, StructureType.OBJECT );\n\t\tstartStructure( StructureType.OBJECT, style.startObject );\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder endObject() {\n\t\tendStructure( StructureType.OBJECT, style.endObject );\n\t\tendEntry();\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder startList() {\n\t\treturn startList( null );\n\t}\n\n\tpublic ToStringTreeBuilder startList(String name) {\n\t\tstartEntry( name, StructureType.LIST );\n\t\tstartStructure( StructureType.LIST, style.startList );\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder endList() {\n\t\tendStructure( StructureType.LIST, style.endList );\n\t\tendEntry();\n\t\treturn this;\n\t}\n\n\tprivate void startEntry(String name, StructureType containedStructureType) {\n\t\tif ( !first ) {\n\t\t\tbuilder.append( style.entrySeparator );\n\t\t}\n\n\t\tStructureType entryType =\n\t\t\t\tStringHelper.isEmpty( name ) ? StructureType.UNNAMED_ENTRY : StructureType.NAMED_ENTRY;\n\n\t\t// Add a new line\n\t\tif (\n\t\t\t\t// ... except for the very first element at the root\n\t\t\t\t!( first && structureTypeStack.isEmpty() )\n\t\t\t\t// ... or for entries containing a squeezed structure\n\t\t\t\t&& !shouldSqueeze( containedStructureType, entryType, structureTypeStack.peek() )\n\t\t\t\t// ... or for structures without a name nor a start delimiter\n\t\t\t\t&& !(\n\t\t\t\t\t\tStructureType.UNNAMED_ENTRY.equals( entryType )\n\t\t\t\t\t\t&& StructureType.OBJECT.equals( containedStructureType )\n\t\t\t\t\t\t&& StringHelper.isEmpty( style.startObject )\n\t\t\t\t)\n\t\t\t\t&& !(\n\t\t\t\t\t\tStructureType.UNNAMED_ENTRY.equals( entryType )\n\t\t\t\t\t\t&& StructureType.LIST.equals( containedStructureType )\n\t\t\t\t\t\t&& StringHelper.isEmpty( style.startList )\n\t\t\t\t)\n\t\t) {\n\t\t\tappendNewline();\n\t\t\tappendIndent();\n\t\t}\n\n\t\tif ( StringHelper.isNotEmpty( name ) ) {\n\t\t\tbuilder.append( name );\n\t\t\tbuilder.append( style.nameValueSeparator );\n\t\t}\n\n\t\tstructureTypeStack.push( entryType );\n\t}\n\n\tprivate void endEntry() {\n\t\tStructureType lastType = structureTypeStack.peek();\n\t\tif ( lastType == null ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, already at root\" );\n\t\t}\n\t\telse if ( !StructureType.UNNAMED_ENTRY.equals( lastType )\n\t\t\t\t&& !StructureType.NAMED_ENTRY.equals( lastType ) ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, not inside an entry\" );\n\t\t}\n\t\tstructureTypeStack.pop();\n\t\tfirst = false;\n\t}\n\n\tprivate void startStructure(StructureType structureType, String startDelimiter) {\n\t\tif ( StringHelper.isNotEmpty( startDelimiter ) ) {\n\t\t\tbuilder.append( startDelimiter );\n\t\t}\n\n\t\tstructureTypeStack.push( structureType );\n\t\tfirst = true;\n\t}\n\n\tprivate void endStructure(StructureType structureType, String endDelimiter) {\n\t\tStructureType lastType = structureTypeStack.peek();\n\t\tif ( lastType == null ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, already at root\" );\n\t\t}\n\t\telse if ( lastType != structureType ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, not inside a \" + structureType );\n\t\t}\n\t\tstructureTypeStack.pop();\n\n\t\tif ( StringHelper.isNotEmpty( endDelimiter ) ) {\n\t\t\tappendNewline();\n\t\t\tappendIndent();\n\t\t\tbuilder.append( endDelimiter );\n\t\t}\n\t\tfirst = false;\n\t}\n\n\tprivate void appendNewline() {\n\t\tbuilder.append( style.newline );\n\t}\n\n\tprivate void appendIndent() {\n\t\tif ( structureTypeStack.isEmpty() ) {\n\t\t\treturn;\n\t\t}\n\n\t\tIterator<StructureType> iterator = structureTypeStack.descendingIterator();\n\t\tStructureType grandParent = null;\n\t\tStructureType parent = null;\n\t\tStructureType current = iterator.next();\n\t\tStructureType child = iterator.hasNext() ? iterator.next() : null;\n\t\tStructureType grandChild;\n\t\twhile ( current != null ) {\n\t\t\tgrandChild = iterator.hasNext() ? iterator.next() : null;\n\t\t\tif ( !shouldSqueeze( current, parent, grandParent ) ) {\n\t\t\t\tswitch ( current ) {\n\t\t\t\t\tcase OBJECT:\n\t\t\t\t\t\tbuilder.append( style.indentInObject );\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase LIST:\n\t\t\t\t\t\t// Display a bullet point if:\n\t\t\t\t\t\tif (\n\t\t\t\t\t\t\t\t// We are adding a element directly to the list\n\t\t\t\t\t\t\t\tchild == null\n\t\t\t\t\t\t\t\t// OR we are adding the first element to a squeezed element in the list\n\t\t\t\t\t\t\t\t|| shouldSqueeze( grandChild, child, current ) && !iterator.hasNext() && first\n\t\t\t\t\t\t) {\n\t\t\t\t\t\t\tbuilder.append( style.indentInListBulletPoint );\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\tbuilder.append( style.indentInListNoBulletPoint );\n\t\t\t\t\t\t}\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase UNNAMED_ENTRY:\n\t\t\t\t\tcase NAMED_ENTRY:\n\t\t\t\t\t\t// No indent for these\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tgrandParent = parent;\n\t\t\tparent = current;\n\t\t\tcurrent = child;\n\t\t\tchild = grandChild;\n\t\t}\n\t}\n\n\t/**\n\t * @param structureType The type of the potentially squeezed structure\n\t * @param parentStructureType The type of the closest containing structure\n\t * @param grandParentStructureType The type of the second closest containing structure\n\t * @return {@code true} if the child structure should be squeezed,\n\t * i.e. displayed on the same line as its parent if it's the first element,\n\t * and have its indenting ignored.\n\t */\n\tprivate boolean shouldSqueeze(StructureType structureType, StructureType parentStructureType,\n\t\t\tStructureType grandParentStructureType) {\n\t\treturn style.squeezeObjectsInList\n\t\t\t\t&& StructureType.LIST.equals( grandParentStructureType )\n\t\t\t\t&& StructureType.UNNAMED_ENTRY.equals( parentStructureType )\n\t\t\t\t&& StructureType.OBJECT.equals( structureType );\n\t}\n\n\tprivate enum StructureType {\n\t\tOBJECT,\n\t\tLIST,\n\t\tNAMED_ENTRY,\n\t\tUNNAMED_ENTRY\n\t}\n\n}\n",
        "filePathAfter": "util/common/src/main/java/org/hibernate/search/util/common/impl/ToStringTreeBuilder.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.common.impl;\n\nimport java.util.ArrayDeque;\nimport java.util.Deque;\nimport java.util.Iterator;\n\nimport org.hibernate.search.util.common.AssertionFailure;\n\npublic class ToStringTreeBuilder {\n\n\tprivate final ToStringStyle style;\n\tprivate final StringBuilder builder = new StringBuilder();\n\n\tprivate final Deque<StructureType> structureTypeStack = new ArrayDeque<>();\n\tprivate boolean first = true;\n\n\tpublic ToStringTreeBuilder() {\n\t\tthis( ToStringStyle.inlineDelimiterStructure() );\n\t}\n\n\tpublic ToStringTreeBuilder(ToStringStyle style) {\n\t\tthis.style = style;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn builder.toString();\n\t}\n\n\tpublic ToStringTreeBuilder attribute(String name, Object value) {\n\t\tif ( value instanceof ToStringTreeAppendable ) {\n\t\t\tToStringTreeAppendable appendable = ( (ToStringTreeAppendable) value );\n\t\t\tstartEntry( name, StructureType.OBJECT );\n\t\t\tstartStructure( StructureType.OBJECT, style.startObject );\n\t\t\tappendable.appendTo( this );\n\t\t\tendStructure( StructureType.OBJECT, style.endObject );\n\t\t\tendEntry();\n\t\t}\n\t\telse {\n\t\t\tstartEntry( name, null );\n\t\t\tbuilder.append( value );\n\t\t\tendEntry();\n\t\t}\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder value(Object value) {\n\t\treturn attribute( null, value );\n\t}\n\n\tpublic ToStringTreeBuilder startObject() {\n\t\treturn startObject( null );\n\t}\n\n\tpublic ToStringTreeBuilder startObject(String name) {\n\t\tstartEntry( name, StructureType.OBJECT );\n\t\tstartStructure( StructureType.OBJECT, style.startObject );\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder endObject() {\n\t\tendStructure( StructureType.OBJECT, style.endObject );\n\t\tendEntry();\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder startList() {\n\t\treturn startList( null );\n\t}\n\n\tpublic ToStringTreeBuilder startList(String name) {\n\t\tstartEntry( name, StructureType.LIST );\n\t\tstartStructure( StructureType.LIST, style.startList );\n\t\treturn this;\n\t}\n\n\tpublic ToStringTreeBuilder endList() {\n\t\tendStructure( StructureType.LIST, style.endList );\n\t\tendEntry();\n\t\treturn this;\n\t}\n\n\tprivate void startEntry(String name, StructureType containedStructureType) {\n\t\tif ( !first ) {\n\t\t\tbuilder.append( style.entrySeparator );\n\t\t}\n\n\t\tStructureType entryType =\n\t\t\t\tStringHelper.isEmpty( name ) ? StructureType.UNNAMED_ENTRY : StructureType.NAMED_ENTRY;\n\n\t\t// Add a new line\n\t\tif (\n\t\t\t\t// ... except for the very first element at the root\n\t\t\t\t!( first && structureTypeStack.isEmpty() )\n\t\t\t\t// ... or for entries containing a squeezed structure\n\t\t\t\t&& !shouldSqueeze( containedStructureType, entryType, structureTypeStack.peek() )\n\t\t\t\t// ... or for structures without a name nor a start delimiter\n\t\t\t\t&& !(\n\t\t\t\t\t\tStructureType.UNNAMED_ENTRY.equals( entryType )\n\t\t\t\t\t\t&& StructureType.OBJECT.equals( containedStructureType )\n\t\t\t\t\t\t&& StringHelper.isEmpty( style.startObject )\n\t\t\t\t)\n\t\t\t\t&& !(\n\t\t\t\t\t\tStructureType.UNNAMED_ENTRY.equals( entryType )\n\t\t\t\t\t\t&& StructureType.LIST.equals( containedStructureType )\n\t\t\t\t\t\t&& StringHelper.isEmpty( style.startList )\n\t\t\t\t)\n\t\t) {\n\t\t\tappendNewline();\n\t\t\tappendIndentIfNecessary();\n\t\t}\n\n\t\tif ( StringHelper.isNotEmpty( name ) ) {\n\t\t\tbuilder.append( name );\n\t\t\tbuilder.append( style.nameValueSeparator );\n\t\t}\n\n\t\tstructureTypeStack.push( entryType );\n\t}\n\n\tprivate void endEntry() {\n\t\tStructureType lastType = structureTypeStack.peek();\n\t\tif ( lastType == null ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, already at root\" );\n\t\t}\n\t\telse if ( !StructureType.UNNAMED_ENTRY.equals( lastType )\n\t\t\t\t&& !StructureType.NAMED_ENTRY.equals( lastType ) ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, not inside an entry\" );\n\t\t}\n\t\tstructureTypeStack.pop();\n\t\tfirst = false;\n\t}\n\n\tprivate void startStructure(StructureType structureType, String startDelimiter) {\n\t\tif ( StringHelper.isNotEmpty( startDelimiter ) ) {\n\t\t\tbuilder.append( startDelimiter );\n\t\t}\n\n\t\tstructureTypeStack.push( structureType );\n\t\tfirst = true;\n\t}\n\n\tprivate void endStructure(StructureType structureType, String endDelimiter) {\n\t\tStructureType lastType = structureTypeStack.peek();\n\t\tif ( lastType == null ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, already at root\" );\n\t\t}\n\t\telse if ( lastType != structureType ) {\n\t\t\tthrow new AssertionFailure( \"Cannot pop, not inside a \" + structureType );\n\t\t}\n\t\tstructureTypeStack.pop();\n\n\t\tif ( StringHelper.isNotEmpty( endDelimiter ) ) {\n\t\t\tappendNewline();\n\t\t\tappendIndentIfNecessary();\n\t\t\tbuilder.append( endDelimiter );\n\t\t}\n\t\tfirst = false;\n\t}\n\n\tprivate void appendNewline() {\n\t\tbuilder.append( style.newline );\n\t}\n\n\tprivate void appendIndentIfNecessary() {\n\t\tif ( structureTypeStack.isEmpty() ) {\n\t\t\treturn;\n\t\t}\n\n\t\tIterator<StructureType> iterator = structureTypeStack.descendingIterator();\n\t\tStructureType grandParent = null;\n\t\tStructureType parent = null;\n\t\tStructureType current = iterator.next();\n\t\tStructureType child = iterator.hasNext() ? iterator.next() : null;\n\t\tStructureType grandChild;\n\t\twhile ( current != null ) {\n\t\t\tgrandChild = iterator.hasNext() ? iterator.next() : null;\n\t\t\tif ( !shouldSqueeze( current, parent, grandParent ) ) {\n\t\t\t\tappendIndentIfNecessary( grandChild, child, current, iterator.hasNext() );\n\t\t\t}\n\t\t\tgrandParent = parent;\n\t\t\tparent = current;\n\t\t\tcurrent = child;\n\t\t\tchild = grandChild;\n\t\t}\n\t}\n\n\tprivate void appendIndentIfNecessary(StructureType grandChild, StructureType child, StructureType current,\n\t\t\tboolean hasParent) {\n\t\tswitch ( current ) {\n\t\t\tcase OBJECT:\n\t\t\t\tbuilder.append( style.indentInObject );\n\t\t\t\tbreak;\n\t\t\tcase LIST:\n\t\t\t\t// Display a bullet point if:\n\t\t\t\tif (\n\t\t\t\t\t\t// We are adding an element directly to the list\n\t\t\t\t\t\tchild == null\n\t\t\t\t\t\t// OR we are adding the first element to a squeezed element in the list\n\t\t\t\t\t\t|| shouldSqueeze( grandChild, child, current ) && !hasParent && first\n\t\t\t\t) {\n\t\t\t\t\tbuilder.append( style.indentInListBulletPoint );\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tbuilder.append( style.indentInListNoBulletPoint );\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase UNNAMED_ENTRY:\n\t\t\tcase NAMED_ENTRY:\n\t\t\t\t// No indent for these\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\t/**\n\t * @param structureType The type of the potentially squeezed structure\n\t * @param parentStructureType The type of the closest containing structure\n\t * @param grandParentStructureType The type of the second closest containing structure\n\t * @return {@code true} if the child structure should be squeezed,\n\t * i.e. displayed on the same line as its parent if it's the first element,\n\t * and have its indenting ignored.\n\t */\n\tprivate boolean shouldSqueeze(StructureType structureType, StructureType parentStructureType,\n\t\t\tStructureType grandParentStructureType) {\n\t\treturn style.squeezeObjectsInList\n\t\t\t\t&& StructureType.LIST.equals( grandParentStructureType )\n\t\t\t\t&& StructureType.UNNAMED_ENTRY.equals( parentStructureType )\n\t\t\t\t&& StructureType.OBJECT.equals( structureType );\n\t}\n\n\tprivate enum StructureType {\n\t\tOBJECT,\n\t\tLIST,\n\t\tNAMED_ENTRY,\n\t\tUNNAMED_ENTRY\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "private void appendIndentIfNecessary(StructureType grandChild, StructureType child, StructureType current,\n\t\t\tboolean hasParent) {\n\t\tswitch ( current ) {\n\t\t\tcase OBJECT:\n\t\t\t\tbuilder.append( style.indentInObject );\n\t\t\t\tbreak;\n\t\t\tcase LIST:\n\t\t\t\t// Display a bullet point if:\n\t\t\t\tif (\n\t\t\t\t\t\t// We are adding an element directly to the list\n\t\t\t\t\t\tchild == null\n\t\t\t\t\t\t// OR we are adding the first element to a squeezed element in the list\n\t\t\t\t\t\t|| shouldSqueeze( grandChild, child, current ) && !hasParent && first\n\t\t\t\t) {\n\t\t\t\t\tbuilder.append( style.indentInListBulletPoint );\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tbuilder.append( style.indentInListNoBulletPoint );\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase UNNAMED_ENTRY:\n\t\t\tcase NAMED_ENTRY:\n\t\t\t\t// No indent for these\n\t\t\t\tbreak;\n\t\t}\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.util.common.impl.ToStringTreeBuilder#shouldSqueeze\n methodBody: private boolean shouldSqueeze(StructureType structureType, StructureType parentStructureType,\n\t\t\tStructureType grandParentStructureType) {\nreturn style.squeezeObjectsInList && StructureType.LIST.equals(grandParentStructureType) && StructureType.UNNAMED_ENTRY.equals(parentStructureType) && StructureType.OBJECT.equals(structureType);\n}"
        ],
        "sourceCodeAfterRefactoring": "private void appendIndentIfNecessary() {\n\t\tif ( structureTypeStack.isEmpty() ) {\n\t\t\treturn;\n\t\t}\n\n\t\tIterator<StructureType> iterator = structureTypeStack.descendingIterator();\n\t\tStructureType grandParent = null;\n\t\tStructureType parent = null;\n\t\tStructureType current = iterator.next();\n\t\tStructureType child = iterator.hasNext() ? iterator.next() : null;\n\t\tStructureType grandChild;\n\t\twhile ( current != null ) {\n\t\t\tgrandChild = iterator.hasNext() ? iterator.next() : null;\n\t\t\tif ( !shouldSqueeze( current, parent, grandParent ) ) {\n\t\t\t\tappendIndentIfNecessary( grandChild, child, current, iterator.hasNext() );\n\t\t\t}\n\t\t\tgrandParent = parent;\n\t\t\tparent = current;\n\t\t\tcurrent = child;\n\t\t\tchild = grandChild;\n\t\t}\n\t}\nprivate void appendIndentIfNecessary(StructureType grandChild, StructureType child, StructureType current,\n\t\t\tboolean hasParent) {\n\t\tswitch ( current ) {\n\t\t\tcase OBJECT:\n\t\t\t\tbuilder.append( style.indentInObject );\n\t\t\t\tbreak;\n\t\t\tcase LIST:\n\t\t\t\t// Display a bullet point if:\n\t\t\t\tif (\n\t\t\t\t\t\t// We are adding an element directly to the list\n\t\t\t\t\t\tchild == null\n\t\t\t\t\t\t// OR we are adding the first element to a squeezed element in the list\n\t\t\t\t\t\t|| shouldSqueeze( grandChild, child, current ) && !hasParent && first\n\t\t\t\t) {\n\t\t\t\t\tbuilder.append( style.indentInListBulletPoint );\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tbuilder.append( style.indentInListNoBulletPoint );\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase UNNAMED_ENTRY:\n\t\t\tcase NAMED_ENTRY:\n\t\t\t\t// No indent for these\n\t\t\t\tbreak;\n\t\t}\n\t}",
        "diffSourceCode": "-  171: \tprivate void appendIndent() {\n+  171: \tprivate void appendIndentIfNecessary() {\n   172: \t\tif ( structureTypeStack.isEmpty() ) {\n   173: \t\t\treturn;\n   174: \t\t}\n   175: \n   176: \t\tIterator<StructureType> iterator = structureTypeStack.descendingIterator();\n   177: \t\tStructureType grandParent = null;\n   178: \t\tStructureType parent = null;\n   179: \t\tStructureType current = iterator.next();\n   180: \t\tStructureType child = iterator.hasNext() ? iterator.next() : null;\n   181: \t\tStructureType grandChild;\n   182: \t\twhile ( current != null ) {\n   183: \t\t\tgrandChild = iterator.hasNext() ? iterator.next() : null;\n   184: \t\t\tif ( !shouldSqueeze( current, parent, grandParent ) ) {\n-  185: \t\t\t\tswitch ( current ) {\n-  186: \t\t\t\t\tcase OBJECT:\n-  187: \t\t\t\t\t\tbuilder.append( style.indentInObject );\n-  188: \t\t\t\t\t\tbreak;\n-  189: \t\t\t\t\tcase LIST:\n-  190: \t\t\t\t\t\t// Display a bullet point if:\n-  191: \t\t\t\t\t\tif (\n-  192: \t\t\t\t\t\t\t\t// We are adding a element directly to the list\n-  193: \t\t\t\t\t\t\t\tchild == null\n-  194: \t\t\t\t\t\t\t\t// OR we are adding the first element to a squeezed element in the list\n-  195: \t\t\t\t\t\t\t\t|| shouldSqueeze( grandChild, child, current ) && !iterator.hasNext() && first\n-  196: \t\t\t\t\t\t) {\n-  197: \t\t\t\t\t\t\tbuilder.append( style.indentInListBulletPoint );\n-  198: \t\t\t\t\t\t}\n-  199: \t\t\t\t\t\telse {\n-  200: \t\t\t\t\t\t\tbuilder.append( style.indentInListNoBulletPoint );\n-  201: \t\t\t\t\t\t}\n-  202: \t\t\t\t\t\tbreak;\n-  203: \t\t\t\t\tcase UNNAMED_ENTRY:\n-  204: \t\t\t\t\tcase NAMED_ENTRY:\n-  205: \t\t\t\t\t\t// No indent for these\n-  206: \t\t\t\t\t\tbreak;\n-  207: \t\t\t\t}\n-  208: \t\t\t}\n-  209: \t\t\tgrandParent = parent;\n-  210: \t\t\tparent = current;\n-  211: \t\t\tcurrent = child;\n-  212: \t\t\tchild = grandChild;\n-  213: \t\t}\n-  214: \t}\n-  215: \n-  216: \t/**\n-  217: \t * @param structureType The type of the potentially squeezed structure\n-  218: \t * @param parentStructureType The type of the closest containing structure\n-  219: \t * @param grandParentStructureType The type of the second closest containing structure\n+  185: \t\t\t\tappendIndentIfNecessary( grandChild, child, current, iterator.hasNext() );\n+  186: \t\t\t}\n+  187: \t\t\tgrandParent = parent;\n+  188: \t\t\tparent = current;\n+  189: \t\t\tcurrent = child;\n+  190: \t\t\tchild = grandChild;\n+  191: \t\t}\n+  192: \t}\n+  193: \n+  194: \tprivate void appendIndentIfNecessary(StructureType grandChild, StructureType child, StructureType current,\n+  195: \t\t\tboolean hasParent) {\n+  196: \t\tswitch ( current ) {\n+  197: \t\t\tcase OBJECT:\n+  198: \t\t\t\tbuilder.append( style.indentInObject );\n+  199: \t\t\t\tbreak;\n+  200: \t\t\tcase LIST:\n+  201: \t\t\t\t// Display a bullet point if:\n+  202: \t\t\t\tif (\n+  203: \t\t\t\t\t\t// We are adding an element directly to the list\n+  204: \t\t\t\t\t\tchild == null\n+  205: \t\t\t\t\t\t// OR we are adding the first element to a squeezed element in the list\n+  206: \t\t\t\t\t\t|| shouldSqueeze( grandChild, child, current ) && !hasParent && first\n+  207: \t\t\t\t) {\n+  208: \t\t\t\t\tbuilder.append( style.indentInListBulletPoint );\n+  209: \t\t\t\t}\n+  210: \t\t\t\telse {\n+  211: \t\t\t\t\tbuilder.append( style.indentInListNoBulletPoint );\n+  212: \t\t\t\t}\n+  213: \t\t\t\tbreak;\n+  214: \t\t\tcase UNNAMED_ENTRY:\n+  215: \t\t\tcase NAMED_ENTRY:\n+  216: \t\t\t\t// No indent for these\n+  217: \t\t\t\tbreak;\n+  218: \t\t}\n+  219: \t}\n",
        "uniqueId": "a4d2e5b39bb53fefd2c31be404a6682af6c3a5bb_171_214_194_219_171_192",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 94
            },
            "BRANCH": {
                "missed": 0,
                "covered": 21
            },
            "LINE": {
                "missed": 0,
                "covered": 23
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 12
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate createExtractingParameterizedTypeMatcher(typePattern ParameterizedType, typeToExtract Type) : ExtractingTypePatternMatcher extracted from public createExtractingMatcher(typePattern Type, typeToExtract Type) : ExtractingTypePatternMatcher in class org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory",
        "diffLocations": [
            {
                "filePath": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
                "startLine": 42,
                "endLine": 149,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
                "startLine": 42,
                "endLine": 73,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
                "startLine": 109,
                "endLine": 165,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "/**\n\t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n\t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n\t * @return A type pattern matcher matching subtypes of {@code typePattern}\n\t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n\t * in the even of a match.\n\t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n\t * for the given types.\n\t */\n\tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n\t\tif ( typePattern instanceof TypeVariable ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof WildcardType ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof ParameterizedType ) {\n\t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n\t\t\tClass<?> rawTypePattern = (Class<?>) parameterizedTypePattern.getRawType();\n\t\t\tType[] typeArguments = parameterizedTypePattern.getActualTypeArguments();\n\n\t\t\tInteger typeVariableIndex = null;\n\t\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n\t\t\t\tType typeArgument = typeArguments[i];\n\t\t\t\tif ( typeArgument instanceof TypeVariable ) {\n\t\t\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\t\t\ttypeVariableIndex = i;\n\t\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n\t\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n\t\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n\t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n\t\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n\t\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse if ( typeArgument instanceof WildcardType ) {\n\t\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n\t\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n\t\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n\t\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n\t\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n\t\t}\n\t\telse if ( typePattern instanceof Class ) {\n\t\t\tif ( !( typeToExtract instanceof Class ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( (Class<?>) typePattern );\n\t\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n\t\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n\t\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n\t\t\t\t\ttypeToExtractModel\n\t\t\t);\n\t\t}\n\t\telse if ( typePattern instanceof GenericArrayType ) {\n\t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n\t\t\tif ( !( typeToExtract instanceof TypeVariable )\n\t\t\t\t\t|| !arrayTypePattern.getGenericComponentType().equals( typeToExtract ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n\t\t\tType[] upperBounds = resultTypeVariable.getBounds();\n\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new ArrayElementTypeMatcher();\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n\t\t}\n\t}",
        "filePathBefore": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
        "isPureRefactoring": true,
        "commitId": "a4d2e5b39bb53fefd2c31be404a6682af6c3a5bb",
        "packageNameBefore": "org.hibernate.search.mapper.pojo.model.typepattern.impl",
        "classNameBefore": "org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory",
        "methodNameBefore": "org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory#createExtractingMatcher",
        "classSignatureBefore": "public class TypePatternMatcherFactory ",
        "methodNameBeforeSet": [
            "org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory#createExtractingMatcher"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory"
        ],
        "classSignatureBeforeSet": [
            "public class TypePatternMatcherFactory "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.pojo.model.typepattern.impl;\n\nimport java.lang.reflect.GenericArrayType;\nimport java.lang.reflect.ParameterizedType;\nimport java.lang.reflect.Type;\nimport java.lang.reflect.TypeVariable;\nimport java.lang.reflect.WildcardType;\n\nimport org.hibernate.search.mapper.pojo.model.spi.PojoBootstrapIntrospector;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoGenericTypeModel;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeModel;\nimport org.hibernate.search.util.common.AssertionFailure;\n\npublic class TypePatternMatcherFactory {\n\n\tprivate final PojoBootstrapIntrospector introspector;\n\n\t/**\n\t * @param introspector An introspector to use for reflection,\n\t * mainly for {@link PojoBootstrapIntrospector#getGenericTypeModel(Class)}\n\t */\n\tpublic TypePatternMatcherFactory(PojoBootstrapIntrospector introspector) {\n\t\tthis.introspector = introspector;\n\t}\n\n\tpublic TypePatternMatcher createExactRawTypeMatcher(Class<?> exactTypeToMatch) {\n\t\tPojoRawTypeModel<?> exactTypeToMatchModel = introspector.getTypeModel( exactTypeToMatch );\n\t\treturn new ExactRawTypeMatcher( exactTypeToMatchModel );\n\t}\n\n\tpublic TypePatternMatcher createRawSuperTypeMatcher(Class<?> superTypeToMatch) {\n\t\tPojoRawTypeModel<?> superTypeToMatchModel = introspector.getTypeModel( superTypeToMatch );\n\t\treturn new RawSuperTypeMatcher( superTypeToMatchModel );\n\t}\n\n\t/**\n\t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n\t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n\t * @return A type pattern matcher matching subtypes of {@code typePattern}\n\t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n\t * in the even of a match.\n\t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n\t * for the given types.\n\t */\n\tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n\t\tif ( typePattern instanceof TypeVariable ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof WildcardType ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof ParameterizedType ) {\n\t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n\t\t\tClass<?> rawTypePattern = (Class<?>) parameterizedTypePattern.getRawType();\n\t\t\tType[] typeArguments = parameterizedTypePattern.getActualTypeArguments();\n\n\t\t\tInteger typeVariableIndex = null;\n\t\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n\t\t\t\tType typeArgument = typeArguments[i];\n\t\t\t\tif ( typeArgument instanceof TypeVariable ) {\n\t\t\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\t\t\ttypeVariableIndex = i;\n\t\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n\t\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n\t\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n\t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n\t\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n\t\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse if ( typeArgument instanceof WildcardType ) {\n\t\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n\t\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n\t\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n\t\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n\t\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n\t\t}\n\t\telse if ( typePattern instanceof Class ) {\n\t\t\tif ( !( typeToExtract instanceof Class ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( (Class<?>) typePattern );\n\t\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n\t\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n\t\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n\t\t\t\t\ttypeToExtractModel\n\t\t\t);\n\t\t}\n\t\telse if ( typePattern instanceof GenericArrayType ) {\n\t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n\t\t\tif ( !( typeToExtract instanceof TypeVariable )\n\t\t\t\t\t|| !arrayTypePattern.getGenericComponentType().equals( typeToExtract ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n\t\t\tType[] upperBounds = resultTypeVariable.getBounds();\n\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new ArrayElementTypeMatcher();\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.pojo.model.typepattern.impl;\n\nimport java.lang.reflect.GenericArrayType;\nimport java.lang.reflect.ParameterizedType;\nimport java.lang.reflect.Type;\nimport java.lang.reflect.TypeVariable;\nimport java.lang.reflect.WildcardType;\n\nimport org.hibernate.search.mapper.pojo.model.spi.PojoBootstrapIntrospector;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoGenericTypeModel;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeModel;\nimport org.hibernate.search.util.common.AssertionFailure;\n\npublic class TypePatternMatcherFactory {\n\n\tprivate final PojoBootstrapIntrospector introspector;\n\n\t/**\n\t * @param introspector An introspector to use for reflection,\n\t * mainly for {@link PojoBootstrapIntrospector#getGenericTypeModel(Class)}\n\t */\n\tpublic TypePatternMatcherFactory(PojoBootstrapIntrospector introspector) {\n\t\tthis.introspector = introspector;\n\t}\n\n\tpublic TypePatternMatcher createExactRawTypeMatcher(Class<?> exactTypeToMatch) {\n\t\tPojoRawTypeModel<?> exactTypeToMatchModel = introspector.getTypeModel( exactTypeToMatch );\n\t\treturn new ExactRawTypeMatcher( exactTypeToMatchModel );\n\t}\n\n\tpublic TypePatternMatcher createRawSuperTypeMatcher(Class<?> superTypeToMatch) {\n\t\tPojoRawTypeModel<?> superTypeToMatchModel = introspector.getTypeModel( superTypeToMatch );\n\t\treturn new RawSuperTypeMatcher( superTypeToMatchModel );\n\t}\n\n\t/**\n\t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n\t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n\t * @return A type pattern matcher matching subtypes of {@code typePattern}\n\t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n\t * in the even of a match.\n\t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n\t * for the given types.\n\t */\n\tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n\t\tif ( typePattern instanceof TypeVariable ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof WildcardType ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof ParameterizedType ) {\n\t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n\t\t\treturn createExtractingParameterizedTypeMatcher( parameterizedTypePattern, typeToExtract );\n\t\t}\n\t\telse if ( typePattern instanceof Class ) {\n\t\t\tClass<?> classTypePattern = (Class<?>) typePattern;\n\t\t\treturn createExtractingClassTypeMatcher( classTypePattern, typeToExtract );\n\t\t}\n\t\telse if ( typePattern instanceof GenericArrayType ) {\n\t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n\t\t\treturn createExtractingGenericArrayTypeMatcher( arrayTypePattern, typeToExtract );\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n\t\t}\n\t}\n\n\tprivate ExtractingTypePatternMatcher createExtractingGenericArrayTypeMatcher(GenericArrayType typePattern,\n\t\t\tType typeToExtract) {\n\t\tif ( !( typeToExtract instanceof TypeVariable )\n\t\t\t\t|| !typePattern.getGenericComponentType().equals( typeToExtract ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t);\n\t\t}\n\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n\t\tType[] upperBounds = resultTypeVariable.getBounds();\n\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n\t\t\t);\n\t\t}\n\t\treturn new ArrayElementTypeMatcher();\n\t}\n\n\tprivate ExtractingTypePatternMatcher createExtractingClassTypeMatcher(Class<?> typePattern,\n\t\t\tType typeToExtract) {\n\t\tif ( !( typeToExtract instanceof Class ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n\t\t\t);\n\t\t}\n\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( typePattern );\n\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n\t\t\t\ttypeToExtractModel\n\t\t);\n\t}\n\n\tprivate ExtractingTypePatternMatcher createExtractingParameterizedTypeMatcher(ParameterizedType typePattern,\n\t\t\tType typeToExtract) {\n\t\tClass<?> rawTypePattern = (Class<?>) typePattern.getRawType();\n\t\tType[] typeArguments = typePattern.getActualTypeArguments();\n\n\t\tInteger typeVariableIndex = null;\n\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n\t\t\tType typeArgument = typeArguments[i];\n\t\t\tif ( typeArgument instanceof TypeVariable ) {\n\t\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\t\ttypeVariableIndex = i;\n\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if ( typeArgument instanceof WildcardType ) {\n\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n\t\t\t\t);\n\t\t\t}\n\t\t}\n\t\tif ( typeVariableIndex == null ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n\t\t\t);\n\t\t}\n\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private ExtractingTypePatternMatcher createExtractingParameterizedTypeMatcher(ParameterizedType typePattern,\n\t\t\tType typeToExtract) {\n\t\tClass<?> rawTypePattern = (Class<?>) typePattern.getRawType();\n\t\tType[] typeArguments = typePattern.getActualTypeArguments();\n\n\t\tInteger typeVariableIndex = null;\n\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n\t\t\tType typeArgument = typeArguments[i];\n\t\t\tif ( typeArgument instanceof TypeVariable ) {\n\t\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\t\ttypeVariableIndex = i;\n\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if ( typeArgument instanceof WildcardType ) {\n\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n\t\t\t\t);\n\t\t\t}\n\t\t}\n\t\tif ( typeVariableIndex == null ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n\t\t\t);\n\t\t}\n\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "/**\n\t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n\t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n\t * @return A type pattern matcher matching subtypes of {@code typePattern}\n\t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n\t * in the even of a match.\n\t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n\t * for the given types.\n\t */\n\tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n\t\tif ( typePattern instanceof TypeVariable ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof WildcardType ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof ParameterizedType ) {\n\t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n\t\t\treturn createExtractingParameterizedTypeMatcher( parameterizedTypePattern, typeToExtract );\n\t\t}\n\t\telse if ( typePattern instanceof Class ) {\n\t\t\tClass<?> classTypePattern = (Class<?>) typePattern;\n\t\t\treturn createExtractingClassTypeMatcher( classTypePattern, typeToExtract );\n\t\t}\n\t\telse if ( typePattern instanceof GenericArrayType ) {\n\t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n\t\t\treturn createExtractingGenericArrayTypeMatcher( arrayTypePattern, typeToExtract );\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n\t\t}\n\t}\nprivate ExtractingTypePatternMatcher createExtractingParameterizedTypeMatcher(ParameterizedType typePattern,\n\t\t\tType typeToExtract) {\n\t\tClass<?> rawTypePattern = (Class<?>) typePattern.getRawType();\n\t\tType[] typeArguments = typePattern.getActualTypeArguments();\n\n\t\tInteger typeVariableIndex = null;\n\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n\t\t\tType typeArgument = typeArguments[i];\n\t\t\tif ( typeArgument instanceof TypeVariable ) {\n\t\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\t\ttypeVariableIndex = i;\n\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if ( typeArgument instanceof WildcardType ) {\n\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n\t\t\t\t);\n\t\t\t}\n\t\t}\n\t\tif ( typeVariableIndex == null ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n\t\t\t);\n\t\t}\n\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n\t}",
        "diffSourceCode": "    42: \t/**\n    43: \t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n    44: \t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n    45: \t * @return A type pattern matcher matching subtypes of {@code typePattern}\n    46: \t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n    47: \t * in the even of a match.\n    48: \t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n    49: \t * for the given types.\n    50: \t */\n    51: \tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n    52: \t\tif ( typePattern instanceof TypeVariable ) {\n    53: \t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n    54: \t\t}\n    55: \t\telse if ( typePattern instanceof WildcardType ) {\n    56: \t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n    57: \t\t}\n    58: \t\telse if ( typePattern instanceof ParameterizedType ) {\n    59: \t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n-   60: \t\t\tClass<?> rawTypePattern = (Class<?>) parameterizedTypePattern.getRawType();\n-   61: \t\t\tType[] typeArguments = parameterizedTypePattern.getActualTypeArguments();\n-   62: \n-   63: \t\t\tInteger typeVariableIndex = null;\n-   64: \t\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n-   65: \t\t\t\tType typeArgument = typeArguments[i];\n-   66: \t\t\t\tif ( typeArgument instanceof TypeVariable ) {\n-   67: \t\t\t\t\tif ( typeVariableIndex == null ) {\n-   68: \t\t\t\t\t\ttypeVariableIndex = i;\n-   69: \t\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n-   70: \t\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n-   71: \t\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n-   72: \t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n-   73: \t\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n-   74: \t\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n-   75: \t\t\t\t\t\t\t);\n-   76: \t\t\t\t\t\t}\n-   77: \t\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n-   78: \t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n-   79: \t\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n-   80: \t\t\t\t\t\t\t);\n-   81: \t\t\t\t\t\t}\n-   82: \t\t\t\t\t}\n-   83: \t\t\t\t\telse {\n-   84: \t\t\t\t\t\tthrow new UnsupportedOperationException(\n-   85: \t\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n-   86: \t\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n-   87: \t\t\t\t\t\t);\n-   88: \t\t\t\t\t}\n-   89: \t\t\t\t}\n-   90: \t\t\t\telse if ( typeArgument instanceof WildcardType ) {\n-   91: \t\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n-   92: \t\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n-   93: \t\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n-   94: \t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n-   95: \t\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n-   96: \t\t\t\t\t\tthrow new UnsupportedOperationException(\n-   97: \t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n-   98: \t\t\t\t\t\t);\n-   99: \t\t\t\t\t}\n-  100: \t\t\t\t}\n-  101: \t\t\t\telse {\n-  102: \t\t\t\t\tthrow new UnsupportedOperationException(\n-  103: \t\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n-  104: \t\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n-  105: \t\t\t\t\t);\n-  106: \t\t\t\t}\n-  107: \t\t\t}\n-  108: \t\t\tif ( typeVariableIndex == null ) {\n-  109: \t\t\t\tthrow new UnsupportedOperationException(\n-  110: \t\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n-  111: \t\t\t\t);\n-  112: \t\t\t}\n-  113: \t\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n-  114: \t\t}\n-  115: \t\telse if ( typePattern instanceof Class ) {\n-  116: \t\t\tif ( !( typeToExtract instanceof Class ) ) {\n-  117: \t\t\t\tthrow new UnsupportedOperationException(\n-  118: \t\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n-  119: \t\t\t\t);\n-  120: \t\t\t}\n-  121: \t\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( (Class<?>) typePattern );\n-  122: \t\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n-  123: \t\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n-  124: \t\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n-  125: \t\t\t\t\ttypeToExtractModel\n-  126: \t\t\t);\n-  127: \t\t}\n-  128: \t\telse if ( typePattern instanceof GenericArrayType ) {\n-  129: \t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n-  130: \t\t\tif ( !( typeToExtract instanceof TypeVariable )\n-  131: \t\t\t\t\t|| !arrayTypePattern.getGenericComponentType().equals( typeToExtract ) ) {\n-  132: \t\t\t\tthrow new UnsupportedOperationException(\n-  133: \t\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n-  134: \t\t\t\t\t\t\t\t+ \" is not supported\"\n-  135: \t\t\t\t);\n-  136: \t\t\t}\n-  137: \t\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n-  138: \t\t\tType[] upperBounds = resultTypeVariable.getBounds();\n-  139: \t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n-  140: \t\t\t\tthrow new UnsupportedOperationException(\n-  141: \t\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n-  142: \t\t\t\t);\n-  143: \t\t\t}\n-  144: \t\t\treturn new ArrayElementTypeMatcher();\n-  145: \t\t}\n-  146: \t\telse {\n-  147: \t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n-  148: \t\t}\n-  149: \t}\n-  150: }\n+   60: \t\t\treturn createExtractingParameterizedTypeMatcher( parameterizedTypePattern, typeToExtract );\n+   61: \t\t}\n+   62: \t\telse if ( typePattern instanceof Class ) {\n+   63: \t\t\tClass<?> classTypePattern = (Class<?>) typePattern;\n+   64: \t\t\treturn createExtractingClassTypeMatcher( classTypePattern, typeToExtract );\n+   65: \t\t}\n+   66: \t\telse if ( typePattern instanceof GenericArrayType ) {\n+   67: \t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n+   68: \t\t\treturn createExtractingGenericArrayTypeMatcher( arrayTypePattern, typeToExtract );\n+   69: \t\t}\n+   70: \t\telse {\n+   71: \t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n+   72: \t\t}\n+   73: \t}\n+   74: \n+   75: \tprivate ExtractingTypePatternMatcher createExtractingGenericArrayTypeMatcher(GenericArrayType typePattern,\n+   76: \t\t\tType typeToExtract) {\n+   77: \t\tif ( !( typeToExtract instanceof TypeVariable )\n+   78: \t\t\t\t|| !typePattern.getGenericComponentType().equals( typeToExtract ) ) {\n+   79: \t\t\tthrow new UnsupportedOperationException(\n+   80: \t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n+   81: \t\t\t\t\t\t\t+ \" is not supported\"\n+   82: \t\t\t);\n+   83: \t\t}\n+   84: \t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n+   85: \t\tType[] upperBounds = resultTypeVariable.getBounds();\n+   86: \t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n+   87: \t\t\tthrow new UnsupportedOperationException(\n+   88: \t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n+   89: \t\t\t);\n+   90: \t\t}\n+   91: \t\treturn new ArrayElementTypeMatcher();\n+   92: \t}\n+   93: \n+   94: \tprivate ExtractingTypePatternMatcher createExtractingClassTypeMatcher(Class<?> typePattern,\n+   95: \t\t\tType typeToExtract) {\n+   96: \t\tif ( !( typeToExtract instanceof Class ) ) {\n+   97: \t\t\tthrow new UnsupportedOperationException(\n+   98: \t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n+   99: \t\t\t);\n+  100: \t\t}\n+  101: \t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( typePattern );\n+  102: \t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n+  103: \t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n+  104: \t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n+  105: \t\t\t\ttypeToExtractModel\n+  106: \t\t);\n+  107: \t}\n+  108: \n+  109: \tprivate ExtractingTypePatternMatcher createExtractingParameterizedTypeMatcher(ParameterizedType typePattern,\n+  110: \t\t\tType typeToExtract) {\n+  111: \t\tClass<?> rawTypePattern = (Class<?>) typePattern.getRawType();\n+  112: \t\tType[] typeArguments = typePattern.getActualTypeArguments();\n+  113: \n+  114: \t\tInteger typeVariableIndex = null;\n+  115: \t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n+  116: \t\t\tType typeArgument = typeArguments[i];\n+  117: \t\t\tif ( typeArgument instanceof TypeVariable ) {\n+  118: \t\t\t\tif ( typeVariableIndex == null ) {\n+  119: \t\t\t\t\ttypeVariableIndex = i;\n+  120: \t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n+  121: \t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n+  122: \t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n+  123: \t\t\t\t\t\tthrow new UnsupportedOperationException(\n+  124: \t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n+  125: \t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n+  126: \t\t\t\t\t\t);\n+  127: \t\t\t\t\t}\n+  128: \t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n+  129: \t\t\t\t\t\tthrow new UnsupportedOperationException(\n+  130: \t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n+  131: \t\t\t\t\t\t);\n+  132: \t\t\t\t\t}\n+  133: \t\t\t\t}\n+  134: \t\t\t\telse {\n+  135: \t\t\t\t\tthrow new UnsupportedOperationException(\n+  136: \t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n+  137: \t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n+  138: \t\t\t\t\t);\n+  139: \t\t\t\t}\n+  140: \t\t\t}\n+  141: \t\t\telse if ( typeArgument instanceof WildcardType ) {\n+  142: \t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n+  143: \t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n+  144: \t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n+  145: \t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n+  146: \t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n+  147: \t\t\t\t\tthrow new UnsupportedOperationException(\n+  148: \t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n+  149: \t\t\t\t\t);\n+  150: \t\t\t\t}\n+  151: \t\t\t}\n+  152: \t\t\telse {\n+  153: \t\t\t\tthrow new UnsupportedOperationException(\n+  154: \t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n+  155: \t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n+  156: \t\t\t\t);\n+  157: \t\t\t}\n+  158: \t\t}\n+  159: \t\tif ( typeVariableIndex == null ) {\n+  160: \t\t\tthrow new UnsupportedOperationException(\n+  161: \t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n+  162: \t\t\t);\n+  163: \t\t}\n+  164: \t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n+  165: \t}\n",
        "uniqueId": "a4d2e5b39bb53fefd2c31be404a6682af6c3a5bb_42_149_109_165_42_73",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 13,
                "covered": 49
            },
            "BRANCH": {
                "missed": 1,
                "covered": 9
            },
            "LINE": {
                "missed": 1,
                "covered": 13
            },
            "COMPLEXITY": {
                "missed": 1,
                "covered": 5
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate createExtractingClassTypeMatcher(typePattern Class<?>, typeToExtract Type) : ExtractingTypePatternMatcher extracted from public createExtractingMatcher(typePattern Type, typeToExtract Type) : ExtractingTypePatternMatcher in class org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory",
        "diffLocations": [
            {
                "filePath": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
                "startLine": 42,
                "endLine": 149,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
                "startLine": 42,
                "endLine": 73,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
                "startLine": 94,
                "endLine": 107,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "/**\n\t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n\t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n\t * @return A type pattern matcher matching subtypes of {@code typePattern}\n\t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n\t * in the even of a match.\n\t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n\t * for the given types.\n\t */\n\tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n\t\tif ( typePattern instanceof TypeVariable ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof WildcardType ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof ParameterizedType ) {\n\t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n\t\t\tClass<?> rawTypePattern = (Class<?>) parameterizedTypePattern.getRawType();\n\t\t\tType[] typeArguments = parameterizedTypePattern.getActualTypeArguments();\n\n\t\t\tInteger typeVariableIndex = null;\n\t\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n\t\t\t\tType typeArgument = typeArguments[i];\n\t\t\t\tif ( typeArgument instanceof TypeVariable ) {\n\t\t\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\t\t\ttypeVariableIndex = i;\n\t\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n\t\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n\t\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n\t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n\t\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n\t\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse if ( typeArgument instanceof WildcardType ) {\n\t\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n\t\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n\t\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n\t\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n\t\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n\t\t}\n\t\telse if ( typePattern instanceof Class ) {\n\t\t\tif ( !( typeToExtract instanceof Class ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( (Class<?>) typePattern );\n\t\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n\t\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n\t\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n\t\t\t\t\ttypeToExtractModel\n\t\t\t);\n\t\t}\n\t\telse if ( typePattern instanceof GenericArrayType ) {\n\t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n\t\t\tif ( !( typeToExtract instanceof TypeVariable )\n\t\t\t\t\t|| !arrayTypePattern.getGenericComponentType().equals( typeToExtract ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n\t\t\tType[] upperBounds = resultTypeVariable.getBounds();\n\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new ArrayElementTypeMatcher();\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n\t\t}\n\t}",
        "filePathBefore": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
        "isPureRefactoring": true,
        "commitId": "a4d2e5b39bb53fefd2c31be404a6682af6c3a5bb",
        "packageNameBefore": "org.hibernate.search.mapper.pojo.model.typepattern.impl",
        "classNameBefore": "org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory",
        "methodNameBefore": "org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory#createExtractingMatcher",
        "classSignatureBefore": "public class TypePatternMatcherFactory ",
        "methodNameBeforeSet": [
            "org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory#createExtractingMatcher"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory"
        ],
        "classSignatureBeforeSet": [
            "public class TypePatternMatcherFactory "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Overlapped refactoring - can be identical by undoing the overlapped refactoring\n- Extract Variable-",
                "description": "Extract variable on the top of the extract method - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.pojo.model.typepattern.impl;\n\nimport java.lang.reflect.GenericArrayType;\nimport java.lang.reflect.ParameterizedType;\nimport java.lang.reflect.Type;\nimport java.lang.reflect.TypeVariable;\nimport java.lang.reflect.WildcardType;\n\nimport org.hibernate.search.mapper.pojo.model.spi.PojoBootstrapIntrospector;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoGenericTypeModel;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeModel;\nimport org.hibernate.search.util.common.AssertionFailure;\n\npublic class TypePatternMatcherFactory {\n\n\tprivate final PojoBootstrapIntrospector introspector;\n\n\t/**\n\t * @param introspector An introspector to use for reflection,\n\t * mainly for {@link PojoBootstrapIntrospector#getGenericTypeModel(Class)}\n\t */\n\tpublic TypePatternMatcherFactory(PojoBootstrapIntrospector introspector) {\n\t\tthis.introspector = introspector;\n\t}\n\n\tpublic TypePatternMatcher createExactRawTypeMatcher(Class<?> exactTypeToMatch) {\n\t\tPojoRawTypeModel<?> exactTypeToMatchModel = introspector.getTypeModel( exactTypeToMatch );\n\t\treturn new ExactRawTypeMatcher( exactTypeToMatchModel );\n\t}\n\n\tpublic TypePatternMatcher createRawSuperTypeMatcher(Class<?> superTypeToMatch) {\n\t\tPojoRawTypeModel<?> superTypeToMatchModel = introspector.getTypeModel( superTypeToMatch );\n\t\treturn new RawSuperTypeMatcher( superTypeToMatchModel );\n\t}\n\n\t/**\n\t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n\t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n\t * @return A type pattern matcher matching subtypes of {@code typePattern}\n\t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n\t * in the even of a match.\n\t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n\t * for the given types.\n\t */\n\tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n\t\tif ( typePattern instanceof TypeVariable ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof WildcardType ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof ParameterizedType ) {\n\t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n\t\t\tClass<?> rawTypePattern = (Class<?>) parameterizedTypePattern.getRawType();\n\t\t\tType[] typeArguments = parameterizedTypePattern.getActualTypeArguments();\n\n\t\t\tInteger typeVariableIndex = null;\n\t\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n\t\t\t\tType typeArgument = typeArguments[i];\n\t\t\t\tif ( typeArgument instanceof TypeVariable ) {\n\t\t\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\t\t\ttypeVariableIndex = i;\n\t\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n\t\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n\t\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n\t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n\t\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n\t\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse if ( typeArgument instanceof WildcardType ) {\n\t\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n\t\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n\t\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n\t\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n\t\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n\t\t}\n\t\telse if ( typePattern instanceof Class ) {\n\t\t\tif ( !( typeToExtract instanceof Class ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( (Class<?>) typePattern );\n\t\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n\t\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n\t\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n\t\t\t\t\ttypeToExtractModel\n\t\t\t);\n\t\t}\n\t\telse if ( typePattern instanceof GenericArrayType ) {\n\t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n\t\t\tif ( !( typeToExtract instanceof TypeVariable )\n\t\t\t\t\t|| !arrayTypePattern.getGenericComponentType().equals( typeToExtract ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n\t\t\tType[] upperBounds = resultTypeVariable.getBounds();\n\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new ArrayElementTypeMatcher();\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.pojo.model.typepattern.impl;\n\nimport java.lang.reflect.GenericArrayType;\nimport java.lang.reflect.ParameterizedType;\nimport java.lang.reflect.Type;\nimport java.lang.reflect.TypeVariable;\nimport java.lang.reflect.WildcardType;\n\nimport org.hibernate.search.mapper.pojo.model.spi.PojoBootstrapIntrospector;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoGenericTypeModel;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeModel;\nimport org.hibernate.search.util.common.AssertionFailure;\n\npublic class TypePatternMatcherFactory {\n\n\tprivate final PojoBootstrapIntrospector introspector;\n\n\t/**\n\t * @param introspector An introspector to use for reflection,\n\t * mainly for {@link PojoBootstrapIntrospector#getGenericTypeModel(Class)}\n\t */\n\tpublic TypePatternMatcherFactory(PojoBootstrapIntrospector introspector) {\n\t\tthis.introspector = introspector;\n\t}\n\n\tpublic TypePatternMatcher createExactRawTypeMatcher(Class<?> exactTypeToMatch) {\n\t\tPojoRawTypeModel<?> exactTypeToMatchModel = introspector.getTypeModel( exactTypeToMatch );\n\t\treturn new ExactRawTypeMatcher( exactTypeToMatchModel );\n\t}\n\n\tpublic TypePatternMatcher createRawSuperTypeMatcher(Class<?> superTypeToMatch) {\n\t\tPojoRawTypeModel<?> superTypeToMatchModel = introspector.getTypeModel( superTypeToMatch );\n\t\treturn new RawSuperTypeMatcher( superTypeToMatchModel );\n\t}\n\n\t/**\n\t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n\t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n\t * @return A type pattern matcher matching subtypes of {@code typePattern}\n\t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n\t * in the even of a match.\n\t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n\t * for the given types.\n\t */\n\tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n\t\tif ( typePattern instanceof TypeVariable ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof WildcardType ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof ParameterizedType ) {\n\t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n\t\t\treturn createExtractingParameterizedTypeMatcher( parameterizedTypePattern, typeToExtract );\n\t\t}\n\t\telse if ( typePattern instanceof Class ) {\n\t\t\tClass<?> classTypePattern = (Class<?>) typePattern;\n\t\t\treturn createExtractingClassTypeMatcher( classTypePattern, typeToExtract );\n\t\t}\n\t\telse if ( typePattern instanceof GenericArrayType ) {\n\t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n\t\t\treturn createExtractingGenericArrayTypeMatcher( arrayTypePattern, typeToExtract );\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n\t\t}\n\t}\n\n\tprivate ExtractingTypePatternMatcher createExtractingGenericArrayTypeMatcher(GenericArrayType typePattern,\n\t\t\tType typeToExtract) {\n\t\tif ( !( typeToExtract instanceof TypeVariable )\n\t\t\t\t|| !typePattern.getGenericComponentType().equals( typeToExtract ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t);\n\t\t}\n\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n\t\tType[] upperBounds = resultTypeVariable.getBounds();\n\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n\t\t\t);\n\t\t}\n\t\treturn new ArrayElementTypeMatcher();\n\t}\n\n\tprivate ExtractingTypePatternMatcher createExtractingClassTypeMatcher(Class<?> typePattern,\n\t\t\tType typeToExtract) {\n\t\tif ( !( typeToExtract instanceof Class ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n\t\t\t);\n\t\t}\n\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( typePattern );\n\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n\t\t\t\ttypeToExtractModel\n\t\t);\n\t}\n\n\tprivate ExtractingTypePatternMatcher createExtractingParameterizedTypeMatcher(ParameterizedType typePattern,\n\t\t\tType typeToExtract) {\n\t\tClass<?> rawTypePattern = (Class<?>) typePattern.getRawType();\n\t\tType[] typeArguments = typePattern.getActualTypeArguments();\n\n\t\tInteger typeVariableIndex = null;\n\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n\t\t\tType typeArgument = typeArguments[i];\n\t\t\tif ( typeArgument instanceof TypeVariable ) {\n\t\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\t\ttypeVariableIndex = i;\n\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if ( typeArgument instanceof WildcardType ) {\n\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n\t\t\t\t);\n\t\t\t}\n\t\t}\n\t\tif ( typeVariableIndex == null ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n\t\t\t);\n\t\t}\n\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private ExtractingTypePatternMatcher createExtractingClassTypeMatcher(Class<?> typePattern,\n\t\t\tType typeToExtract) {\n\t\tif ( !( typeToExtract instanceof Class ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n\t\t\t);\n\t\t}\n\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( typePattern );\n\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n\t\t\t\ttypeToExtractModel\n\t\t);\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "/**\n\t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n\t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n\t * @return A type pattern matcher matching subtypes of {@code typePattern}\n\t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n\t * in the even of a match.\n\t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n\t * for the given types.\n\t */\n\tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n\t\tif ( typePattern instanceof TypeVariable ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof WildcardType ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof ParameterizedType ) {\n\t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n\t\t\treturn createExtractingParameterizedTypeMatcher( parameterizedTypePattern, typeToExtract );\n\t\t}\n\t\telse if ( typePattern instanceof Class ) {\n\t\t\tClass<?> classTypePattern = (Class<?>) typePattern;\n\t\t\treturn createExtractingClassTypeMatcher( classTypePattern, typeToExtract );\n\t\t}\n\t\telse if ( typePattern instanceof GenericArrayType ) {\n\t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n\t\t\treturn createExtractingGenericArrayTypeMatcher( arrayTypePattern, typeToExtract );\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n\t\t}\n\t}\nprivate ExtractingTypePatternMatcher createExtractingClassTypeMatcher(Class<?> typePattern,\n\t\t\tType typeToExtract) {\n\t\tif ( !( typeToExtract instanceof Class ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n\t\t\t);\n\t\t}\n\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( typePattern );\n\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n\t\t\t\ttypeToExtractModel\n\t\t);\n\t}",
        "diffSourceCode": "    42: \t/**\n    43: \t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n    44: \t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n    45: \t * @return A type pattern matcher matching subtypes of {@code typePattern}\n    46: \t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n    47: \t * in the even of a match.\n    48: \t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n    49: \t * for the given types.\n    50: \t */\n    51: \tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n    52: \t\tif ( typePattern instanceof TypeVariable ) {\n    53: \t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n    54: \t\t}\n    55: \t\telse if ( typePattern instanceof WildcardType ) {\n    56: \t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n    57: \t\t}\n    58: \t\telse if ( typePattern instanceof ParameterizedType ) {\n    59: \t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n-   60: \t\t\tClass<?> rawTypePattern = (Class<?>) parameterizedTypePattern.getRawType();\n-   61: \t\t\tType[] typeArguments = parameterizedTypePattern.getActualTypeArguments();\n-   62: \n-   63: \t\t\tInteger typeVariableIndex = null;\n-   64: \t\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n-   65: \t\t\t\tType typeArgument = typeArguments[i];\n-   66: \t\t\t\tif ( typeArgument instanceof TypeVariable ) {\n-   67: \t\t\t\t\tif ( typeVariableIndex == null ) {\n-   68: \t\t\t\t\t\ttypeVariableIndex = i;\n-   69: \t\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n-   70: \t\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n-   71: \t\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n-   72: \t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n-   73: \t\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n-   74: \t\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n-   75: \t\t\t\t\t\t\t);\n-   76: \t\t\t\t\t\t}\n-   77: \t\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n-   78: \t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n-   79: \t\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n-   80: \t\t\t\t\t\t\t);\n-   81: \t\t\t\t\t\t}\n-   82: \t\t\t\t\t}\n-   83: \t\t\t\t\telse {\n-   84: \t\t\t\t\t\tthrow new UnsupportedOperationException(\n-   85: \t\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n-   86: \t\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n-   87: \t\t\t\t\t\t);\n-   88: \t\t\t\t\t}\n-   89: \t\t\t\t}\n-   90: \t\t\t\telse if ( typeArgument instanceof WildcardType ) {\n-   91: \t\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n-   92: \t\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n-   93: \t\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n-   94: \t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n-   95: \t\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n-   96: \t\t\t\t\t\tthrow new UnsupportedOperationException(\n-   97: \t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n-   98: \t\t\t\t\t\t);\n-   99: \t\t\t\t\t}\n-  100: \t\t\t\t}\n-  101: \t\t\t\telse {\n-  102: \t\t\t\t\tthrow new UnsupportedOperationException(\n-  103: \t\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n-  104: \t\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n-  105: \t\t\t\t\t);\n-  106: \t\t\t\t}\n-  107: \t\t\t}\n-  108: \t\t\tif ( typeVariableIndex == null ) {\n-  109: \t\t\t\tthrow new UnsupportedOperationException(\n-  110: \t\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n-  111: \t\t\t\t);\n-  112: \t\t\t}\n-  113: \t\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n-  114: \t\t}\n-  115: \t\telse if ( typePattern instanceof Class ) {\n-  116: \t\t\tif ( !( typeToExtract instanceof Class ) ) {\n-  117: \t\t\t\tthrow new UnsupportedOperationException(\n-  118: \t\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n-  119: \t\t\t\t);\n-  120: \t\t\t}\n-  121: \t\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( (Class<?>) typePattern );\n-  122: \t\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n-  123: \t\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n-  124: \t\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n-  125: \t\t\t\t\ttypeToExtractModel\n-  126: \t\t\t);\n-  127: \t\t}\n-  128: \t\telse if ( typePattern instanceof GenericArrayType ) {\n-  129: \t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n-  130: \t\t\tif ( !( typeToExtract instanceof TypeVariable )\n-  131: \t\t\t\t\t|| !arrayTypePattern.getGenericComponentType().equals( typeToExtract ) ) {\n-  132: \t\t\t\tthrow new UnsupportedOperationException(\n-  133: \t\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n-  134: \t\t\t\t\t\t\t\t+ \" is not supported\"\n-  135: \t\t\t\t);\n-  136: \t\t\t}\n-  137: \t\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n-  138: \t\t\tType[] upperBounds = resultTypeVariable.getBounds();\n-  139: \t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n-  140: \t\t\t\tthrow new UnsupportedOperationException(\n-  141: \t\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n-  142: \t\t\t\t);\n-  143: \t\t\t}\n-  144: \t\t\treturn new ArrayElementTypeMatcher();\n-  145: \t\t}\n-  146: \t\telse {\n-  147: \t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n-  148: \t\t}\n-  149: \t}\n+   60: \t\t\treturn createExtractingParameterizedTypeMatcher( parameterizedTypePattern, typeToExtract );\n+   61: \t\t}\n+   62: \t\telse if ( typePattern instanceof Class ) {\n+   63: \t\t\tClass<?> classTypePattern = (Class<?>) typePattern;\n+   64: \t\t\treturn createExtractingClassTypeMatcher( classTypePattern, typeToExtract );\n+   65: \t\t}\n+   66: \t\telse if ( typePattern instanceof GenericArrayType ) {\n+   67: \t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n+   68: \t\t\treturn createExtractingGenericArrayTypeMatcher( arrayTypePattern, typeToExtract );\n+   69: \t\t}\n+   70: \t\telse {\n+   71: \t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n+   72: \t\t}\n+   73: \t}\n+   74: \n+   75: \tprivate ExtractingTypePatternMatcher createExtractingGenericArrayTypeMatcher(GenericArrayType typePattern,\n+   76: \t\t\tType typeToExtract) {\n+   77: \t\tif ( !( typeToExtract instanceof TypeVariable )\n+   78: \t\t\t\t|| !typePattern.getGenericComponentType().equals( typeToExtract ) ) {\n+   79: \t\t\tthrow new UnsupportedOperationException(\n+   80: \t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n+   81: \t\t\t\t\t\t\t+ \" is not supported\"\n+   82: \t\t\t);\n+   83: \t\t}\n+   84: \t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n+   85: \t\tType[] upperBounds = resultTypeVariable.getBounds();\n+   86: \t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n+   87: \t\t\tthrow new UnsupportedOperationException(\n+   88: \t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n+   89: \t\t\t);\n+   90: \t\t}\n+   91: \t\treturn new ArrayElementTypeMatcher();\n+   92: \t}\n+   93: \n+   94: \tprivate ExtractingTypePatternMatcher createExtractingClassTypeMatcher(Class<?> typePattern,\n+   95: \t\t\tType typeToExtract) {\n+   96: \t\tif ( !( typeToExtract instanceof Class ) ) {\n+   97: \t\t\tthrow new UnsupportedOperationException(\n+   98: \t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n+   99: \t\t\t);\n+  100: \t\t}\n+  101: \t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( typePattern );\n+  102: \t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n+  103: \t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n+  104: \t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n+  105: \t\t\t\ttypeToExtractModel\n+  106: \t\t);\n+  107: \t}\n+  108: \n+  109: \tprivate ExtractingTypePatternMatcher createExtractingParameterizedTypeMatcher(ParameterizedType typePattern,\n+  110: \t\t\tType typeToExtract) {\n+  111: \t\tClass<?> rawTypePattern = (Class<?>) typePattern.getRawType();\n+  112: \t\tType[] typeArguments = typePattern.getActualTypeArguments();\n+  113: \n+  114: \t\tInteger typeVariableIndex = null;\n+  115: \t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n+  116: \t\t\tType typeArgument = typeArguments[i];\n+  117: \t\t\tif ( typeArgument instanceof TypeVariable ) {\n+  118: \t\t\t\tif ( typeVariableIndex == null ) {\n+  119: \t\t\t\t\ttypeVariableIndex = i;\n+  120: \t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n+  121: \t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n+  122: \t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n+  123: \t\t\t\t\t\tthrow new UnsupportedOperationException(\n+  124: \t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n+  125: \t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n+  126: \t\t\t\t\t\t);\n+  127: \t\t\t\t\t}\n+  128: \t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n+  129: \t\t\t\t\t\tthrow new UnsupportedOperationException(\n+  130: \t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n+  131: \t\t\t\t\t\t);\n+  132: \t\t\t\t\t}\n+  133: \t\t\t\t}\n+  134: \t\t\t\telse {\n+  135: \t\t\t\t\tthrow new UnsupportedOperationException(\n+  136: \t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n+  137: \t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n+  138: \t\t\t\t\t);\n+  139: \t\t\t\t}\n+  140: \t\t\t}\n+  141: \t\t\telse if ( typeArgument instanceof WildcardType ) {\n+  142: \t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n+  143: \t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n+  144: \t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n+  145: \t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n+  146: \t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n+  147: \t\t\t\t\tthrow new UnsupportedOperationException(\n+  148: \t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n+  149: \t\t\t\t\t);\n",
        "uniqueId": "a4d2e5b39bb53fefd2c31be404a6682af6c3a5bb_42_149_94_107_42_73",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 13,
                "covered": 49
            },
            "BRANCH": {
                "missed": 1,
                "covered": 9
            },
            "LINE": {
                "missed": 1,
                "covered": 13
            },
            "COMPLEXITY": {
                "missed": 1,
                "covered": 5
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate createExtractingGenericArrayTypeMatcher(typePattern GenericArrayType, typeToExtract Type) : ExtractingTypePatternMatcher extracted from public createExtractingMatcher(typePattern Type, typeToExtract Type) : ExtractingTypePatternMatcher in class org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory",
        "diffLocations": [
            {
                "filePath": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
                "startLine": 42,
                "endLine": 149,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
                "startLine": 42,
                "endLine": 73,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
                "startLine": 75,
                "endLine": 92,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "/**\n\t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n\t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n\t * @return A type pattern matcher matching subtypes of {@code typePattern}\n\t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n\t * in the even of a match.\n\t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n\t * for the given types.\n\t */\n\tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n\t\tif ( typePattern instanceof TypeVariable ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof WildcardType ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof ParameterizedType ) {\n\t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n\t\t\tClass<?> rawTypePattern = (Class<?>) parameterizedTypePattern.getRawType();\n\t\t\tType[] typeArguments = parameterizedTypePattern.getActualTypeArguments();\n\n\t\t\tInteger typeVariableIndex = null;\n\t\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n\t\t\t\tType typeArgument = typeArguments[i];\n\t\t\t\tif ( typeArgument instanceof TypeVariable ) {\n\t\t\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\t\t\ttypeVariableIndex = i;\n\t\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n\t\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n\t\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n\t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n\t\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n\t\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse if ( typeArgument instanceof WildcardType ) {\n\t\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n\t\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n\t\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n\t\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n\t\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n\t\t}\n\t\telse if ( typePattern instanceof Class ) {\n\t\t\tif ( !( typeToExtract instanceof Class ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( (Class<?>) typePattern );\n\t\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n\t\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n\t\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n\t\t\t\t\ttypeToExtractModel\n\t\t\t);\n\t\t}\n\t\telse if ( typePattern instanceof GenericArrayType ) {\n\t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n\t\t\tif ( !( typeToExtract instanceof TypeVariable )\n\t\t\t\t\t|| !arrayTypePattern.getGenericComponentType().equals( typeToExtract ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n\t\t\tType[] upperBounds = resultTypeVariable.getBounds();\n\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new ArrayElementTypeMatcher();\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n\t\t}\n\t}",
        "filePathBefore": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
        "isPureRefactoring": true,
        "commitId": "a4d2e5b39bb53fefd2c31be404a6682af6c3a5bb",
        "packageNameBefore": "org.hibernate.search.mapper.pojo.model.typepattern.impl",
        "classNameBefore": "org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory",
        "methodNameBefore": "org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory#createExtractingMatcher",
        "classSignatureBefore": "public class TypePatternMatcherFactory ",
        "methodNameBeforeSet": [
            "org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory#createExtractingMatcher"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.mapper.pojo.model.typepattern.impl.TypePatternMatcherFactory"
        ],
        "classSignatureBeforeSet": [
            "public class TypePatternMatcherFactory "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.pojo.model.typepattern.impl;\n\nimport java.lang.reflect.GenericArrayType;\nimport java.lang.reflect.ParameterizedType;\nimport java.lang.reflect.Type;\nimport java.lang.reflect.TypeVariable;\nimport java.lang.reflect.WildcardType;\n\nimport org.hibernate.search.mapper.pojo.model.spi.PojoBootstrapIntrospector;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoGenericTypeModel;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeModel;\nimport org.hibernate.search.util.common.AssertionFailure;\n\npublic class TypePatternMatcherFactory {\n\n\tprivate final PojoBootstrapIntrospector introspector;\n\n\t/**\n\t * @param introspector An introspector to use for reflection,\n\t * mainly for {@link PojoBootstrapIntrospector#getGenericTypeModel(Class)}\n\t */\n\tpublic TypePatternMatcherFactory(PojoBootstrapIntrospector introspector) {\n\t\tthis.introspector = introspector;\n\t}\n\n\tpublic TypePatternMatcher createExactRawTypeMatcher(Class<?> exactTypeToMatch) {\n\t\tPojoRawTypeModel<?> exactTypeToMatchModel = introspector.getTypeModel( exactTypeToMatch );\n\t\treturn new ExactRawTypeMatcher( exactTypeToMatchModel );\n\t}\n\n\tpublic TypePatternMatcher createRawSuperTypeMatcher(Class<?> superTypeToMatch) {\n\t\tPojoRawTypeModel<?> superTypeToMatchModel = introspector.getTypeModel( superTypeToMatch );\n\t\treturn new RawSuperTypeMatcher( superTypeToMatchModel );\n\t}\n\n\t/**\n\t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n\t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n\t * @return A type pattern matcher matching subtypes of {@code typePattern}\n\t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n\t * in the even of a match.\n\t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n\t * for the given types.\n\t */\n\tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n\t\tif ( typePattern instanceof TypeVariable ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof WildcardType ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof ParameterizedType ) {\n\t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n\t\t\tClass<?> rawTypePattern = (Class<?>) parameterizedTypePattern.getRawType();\n\t\t\tType[] typeArguments = parameterizedTypePattern.getActualTypeArguments();\n\n\t\t\tInteger typeVariableIndex = null;\n\t\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n\t\t\t\tType typeArgument = typeArguments[i];\n\t\t\t\tif ( typeArgument instanceof TypeVariable ) {\n\t\t\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\t\t\ttypeVariableIndex = i;\n\t\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n\t\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n\t\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n\t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n\t\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t\t);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n\t\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse if ( typeArgument instanceof WildcardType ) {\n\t\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n\t\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n\t\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n\t\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n\t\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n\t\t}\n\t\telse if ( typePattern instanceof Class ) {\n\t\t\tif ( !( typeToExtract instanceof Class ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( (Class<?>) typePattern );\n\t\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n\t\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n\t\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n\t\t\t\t\ttypeToExtractModel\n\t\t\t);\n\t\t}\n\t\telse if ( typePattern instanceof GenericArrayType ) {\n\t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n\t\t\tif ( !( typeToExtract instanceof TypeVariable )\n\t\t\t\t\t|| !arrayTypePattern.getGenericComponentType().equals( typeToExtract ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n\t\t\tType[] upperBounds = resultTypeVariable.getBounds();\n\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn new ArrayElementTypeMatcher();\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "mapper/pojo-base/src/main/java/org/hibernate/search/mapper/pojo/model/typepattern/impl/TypePatternMatcherFactory.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.pojo.model.typepattern.impl;\n\nimport java.lang.reflect.GenericArrayType;\nimport java.lang.reflect.ParameterizedType;\nimport java.lang.reflect.Type;\nimport java.lang.reflect.TypeVariable;\nimport java.lang.reflect.WildcardType;\n\nimport org.hibernate.search.mapper.pojo.model.spi.PojoBootstrapIntrospector;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoGenericTypeModel;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeModel;\nimport org.hibernate.search.util.common.AssertionFailure;\n\npublic class TypePatternMatcherFactory {\n\n\tprivate final PojoBootstrapIntrospector introspector;\n\n\t/**\n\t * @param introspector An introspector to use for reflection,\n\t * mainly for {@link PojoBootstrapIntrospector#getGenericTypeModel(Class)}\n\t */\n\tpublic TypePatternMatcherFactory(PojoBootstrapIntrospector introspector) {\n\t\tthis.introspector = introspector;\n\t}\n\n\tpublic TypePatternMatcher createExactRawTypeMatcher(Class<?> exactTypeToMatch) {\n\t\tPojoRawTypeModel<?> exactTypeToMatchModel = introspector.getTypeModel( exactTypeToMatch );\n\t\treturn new ExactRawTypeMatcher( exactTypeToMatchModel );\n\t}\n\n\tpublic TypePatternMatcher createRawSuperTypeMatcher(Class<?> superTypeToMatch) {\n\t\tPojoRawTypeModel<?> superTypeToMatchModel = introspector.getTypeModel( superTypeToMatch );\n\t\treturn new RawSuperTypeMatcher( superTypeToMatchModel );\n\t}\n\n\t/**\n\t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n\t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n\t * @return A type pattern matcher matching subtypes of {@code typePattern}\n\t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n\t * in the even of a match.\n\t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n\t * for the given types.\n\t */\n\tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n\t\tif ( typePattern instanceof TypeVariable ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof WildcardType ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof ParameterizedType ) {\n\t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n\t\t\treturn createExtractingParameterizedTypeMatcher( parameterizedTypePattern, typeToExtract );\n\t\t}\n\t\telse if ( typePattern instanceof Class ) {\n\t\t\tClass<?> classTypePattern = (Class<?>) typePattern;\n\t\t\treturn createExtractingClassTypeMatcher( classTypePattern, typeToExtract );\n\t\t}\n\t\telse if ( typePattern instanceof GenericArrayType ) {\n\t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n\t\t\treturn createExtractingGenericArrayTypeMatcher( arrayTypePattern, typeToExtract );\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n\t\t}\n\t}\n\n\tprivate ExtractingTypePatternMatcher createExtractingGenericArrayTypeMatcher(GenericArrayType typePattern,\n\t\t\tType typeToExtract) {\n\t\tif ( !( typeToExtract instanceof TypeVariable )\n\t\t\t\t|| !typePattern.getGenericComponentType().equals( typeToExtract ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t);\n\t\t}\n\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n\t\tType[] upperBounds = resultTypeVariable.getBounds();\n\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n\t\t\t);\n\t\t}\n\t\treturn new ArrayElementTypeMatcher();\n\t}\n\n\tprivate ExtractingTypePatternMatcher createExtractingClassTypeMatcher(Class<?> typePattern,\n\t\t\tType typeToExtract) {\n\t\tif ( !( typeToExtract instanceof Class ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n\t\t\t);\n\t\t}\n\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( typePattern );\n\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n\t\t\t\ttypeToExtractModel\n\t\t);\n\t}\n\n\tprivate ExtractingTypePatternMatcher createExtractingParameterizedTypeMatcher(ParameterizedType typePattern,\n\t\t\tType typeToExtract) {\n\t\tClass<?> rawTypePattern = (Class<?>) typePattern.getRawType();\n\t\tType[] typeArguments = typePattern.getActualTypeArguments();\n\n\t\tInteger typeVariableIndex = null;\n\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n\t\t\tType typeArgument = typeArguments[i];\n\t\t\tif ( typeArgument instanceof TypeVariable ) {\n\t\t\t\tif ( typeVariableIndex == null ) {\n\t\t\t\t\ttypeVariableIndex = i;\n\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t\t);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if ( typeArgument instanceof WildcardType ) {\n\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n\t\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n\t\t\t\t\t);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n\t\t\t\t);\n\t\t\t}\n\t\t}\n\t\tif ( typeVariableIndex == null ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n\t\t\t);\n\t\t}\n\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private ExtractingTypePatternMatcher createExtractingGenericArrayTypeMatcher(GenericArrayType typePattern,\n\t\t\tType typeToExtract) {\n\t\tif ( !( typeToExtract instanceof TypeVariable )\n\t\t\t\t|| !typePattern.getGenericComponentType().equals( typeToExtract ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t);\n\t\t}\n\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n\t\tType[] upperBounds = resultTypeVariable.getBounds();\n\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n\t\t\t);\n\t\t}\n\t\treturn new ArrayElementTypeMatcher();\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "/**\n\t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n\t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n\t * @return A type pattern matcher matching subtypes of {@code typePattern}\n\t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n\t * in the even of a match.\n\t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n\t * for the given types.\n\t */\n\tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n\t\tif ( typePattern instanceof TypeVariable ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof WildcardType ) {\n\t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n\t\t}\n\t\telse if ( typePattern instanceof ParameterizedType ) {\n\t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n\t\t\treturn createExtractingParameterizedTypeMatcher( parameterizedTypePattern, typeToExtract );\n\t\t}\n\t\telse if ( typePattern instanceof Class ) {\n\t\t\tClass<?> classTypePattern = (Class<?>) typePattern;\n\t\t\treturn createExtractingClassTypeMatcher( classTypePattern, typeToExtract );\n\t\t}\n\t\telse if ( typePattern instanceof GenericArrayType ) {\n\t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n\t\t\treturn createExtractingGenericArrayTypeMatcher( arrayTypePattern, typeToExtract );\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n\t\t}\n\t}\nprivate ExtractingTypePatternMatcher createExtractingGenericArrayTypeMatcher(GenericArrayType typePattern,\n\t\t\tType typeToExtract) {\n\t\tif ( !( typeToExtract instanceof TypeVariable )\n\t\t\t\t|| !typePattern.getGenericComponentType().equals( typeToExtract ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n\t\t\t\t\t\t\t+ \" is not supported\"\n\t\t\t);\n\t\t}\n\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n\t\tType[] upperBounds = resultTypeVariable.getBounds();\n\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n\t\t\t);\n\t\t}\n\t\treturn new ArrayElementTypeMatcher();\n\t}",
        "diffSourceCode": "    42: \t/**\n    43: \t * @param typePattern The type used as a pattern to be matched. Not all types are accepted.\n    44: \t * @param typeToExtract The type to extract when matching the pattern. Not all types are accepted.\n    45: \t * @return A type pattern matcher matching subtypes of {@code typePattern}\n    46: \t * and returning the {@code typeToExtract} resolved against the type submitted to the matcher\n    47: \t * in the even of a match.\n    48: \t * @throws UnsupportedOperationException If this factory does not support creating a type pattern matcher\n    49: \t * for the given types.\n    50: \t */\n    51: \tpublic ExtractingTypePatternMatcher createExtractingMatcher(Type typePattern, Type typeToExtract) {\n    52: \t\tif ( typePattern instanceof TypeVariable ) {\n    53: \t\t\tthrow new UnsupportedOperationException( \"Matching a type variable is not supported\" );\n    54: \t\t}\n    55: \t\telse if ( typePattern instanceof WildcardType ) {\n    56: \t\t\tthrow new UnsupportedOperationException( \"Matching a wildcard type is not supported\" );\n    57: \t\t}\n    58: \t\telse if ( typePattern instanceof ParameterizedType ) {\n    59: \t\t\tParameterizedType parameterizedTypePattern = (ParameterizedType) typePattern;\n-   60: \t\t\tClass<?> rawTypePattern = (Class<?>) parameterizedTypePattern.getRawType();\n-   61: \t\t\tType[] typeArguments = parameterizedTypePattern.getActualTypeArguments();\n-   62: \n-   63: \t\t\tInteger typeVariableIndex = null;\n-   64: \t\t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n-   65: \t\t\t\tType typeArgument = typeArguments[i];\n-   66: \t\t\t\tif ( typeArgument instanceof TypeVariable ) {\n-   67: \t\t\t\t\tif ( typeVariableIndex == null ) {\n-   68: \t\t\t\t\t\ttypeVariableIndex = i;\n-   69: \t\t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n-   70: \t\t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n-   71: \t\t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n-   72: \t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n-   73: \t\t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n-   74: \t\t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n-   75: \t\t\t\t\t\t\t);\n-   76: \t\t\t\t\t\t}\n-   77: \t\t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n-   78: \t\t\t\t\t\t\tthrow new UnsupportedOperationException(\n-   79: \t\t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n-   80: \t\t\t\t\t\t\t);\n-   81: \t\t\t\t\t\t}\n-   82: \t\t\t\t\t}\n-   83: \t\t\t\t\telse {\n-   84: \t\t\t\t\t\tthrow new UnsupportedOperationException(\n-   85: \t\t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n-   86: \t\t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n-   87: \t\t\t\t\t\t);\n-   88: \t\t\t\t\t}\n-   89: \t\t\t\t}\n-   90: \t\t\t\telse if ( typeArgument instanceof WildcardType ) {\n-   91: \t\t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n-   92: \t\t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n-   93: \t\t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n-   94: \t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n-   95: \t\t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n-   96: \t\t\t\t\t\tthrow new UnsupportedOperationException(\n-   97: \t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n-   98: \t\t\t\t\t\t);\n-   99: \t\t\t\t\t}\n-  100: \t\t\t\t}\n-  101: \t\t\t\telse {\n-  102: \t\t\t\t\tthrow new UnsupportedOperationException(\n-  103: \t\t\t\t\t\t\t\"Only type variables and wildcard types are supported\"\n-  104: \t\t\t\t\t\t\t\t\t+ \" as arguments to a parameterized type to match\"\n-  105: \t\t\t\t\t);\n-  106: \t\t\t\t}\n-  107: \t\t\t}\n-  108: \t\t\tif ( typeVariableIndex == null ) {\n-  109: \t\t\t\tthrow new UnsupportedOperationException(\n-  110: \t\t\t\t\t\t\"Matching a parameterized type without a type variable in its arguments is not supported\"\n-  111: \t\t\t\t);\n-  112: \t\t\t}\n-  113: \t\t\treturn new ParameterizedTypeArgumentMatcher( rawTypePattern, typeVariableIndex );\n-  114: \t\t}\n-  115: \t\telse if ( typePattern instanceof Class ) {\n-  116: \t\t\tif ( !( typeToExtract instanceof Class ) ) {\n-  117: \t\t\t\tthrow new UnsupportedOperationException(\n-  118: \t\t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n-  119: \t\t\t\t);\n-  120: \t\t\t}\n-  121: \t\t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( (Class<?>) typePattern );\n-  122: \t\t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n-  123: \t\t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n-  124: \t\t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n-  125: \t\t\t\t\ttypeToExtractModel\n-  126: \t\t\t);\n-  127: \t\t}\n-  128: \t\telse if ( typePattern instanceof GenericArrayType ) {\n-  129: \t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n-  130: \t\t\tif ( !( typeToExtract instanceof TypeVariable )\n-  131: \t\t\t\t\t|| !arrayTypePattern.getGenericComponentType().equals( typeToExtract ) ) {\n-  132: \t\t\t\tthrow new UnsupportedOperationException(\n-  133: \t\t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n-  134: \t\t\t\t\t\t\t\t+ \" is not supported\"\n-  135: \t\t\t\t);\n-  136: \t\t\t}\n-  137: \t\t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n-  138: \t\t\tType[] upperBounds = resultTypeVariable.getBounds();\n-  139: \t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n-  140: \t\t\t\tthrow new UnsupportedOperationException(\n-  141: \t\t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n-  142: \t\t\t\t);\n-  143: \t\t\t}\n-  144: \t\t\treturn new ArrayElementTypeMatcher();\n-  145: \t\t}\n-  146: \t\telse {\n-  147: \t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n-  148: \t\t}\n-  149: \t}\n+   60: \t\t\treturn createExtractingParameterizedTypeMatcher( parameterizedTypePattern, typeToExtract );\n+   61: \t\t}\n+   62: \t\telse if ( typePattern instanceof Class ) {\n+   63: \t\t\tClass<?> classTypePattern = (Class<?>) typePattern;\n+   64: \t\t\treturn createExtractingClassTypeMatcher( classTypePattern, typeToExtract );\n+   65: \t\t}\n+   66: \t\telse if ( typePattern instanceof GenericArrayType ) {\n+   67: \t\t\tGenericArrayType arrayTypePattern = (GenericArrayType) typePattern;\n+   68: \t\t\treturn createExtractingGenericArrayTypeMatcher( arrayTypePattern, typeToExtract );\n+   69: \t\t}\n+   70: \t\telse {\n+   71: \t\t\tthrow new AssertionFailure( \"Unexpected java.lang.reflect.Type type: \" + typePattern.getClass() );\n+   72: \t\t}\n+   73: \t}\n+   74: \n+   75: \tprivate ExtractingTypePatternMatcher createExtractingGenericArrayTypeMatcher(GenericArrayType typePattern,\n+   76: \t\t\tType typeToExtract) {\n+   77: \t\tif ( !( typeToExtract instanceof TypeVariable )\n+   78: \t\t\t\t|| !typePattern.getGenericComponentType().equals( typeToExtract ) ) {\n+   79: \t\t\tthrow new UnsupportedOperationException(\n+   80: \t\t\t\t\t\"Extracting anything other than the array element type when matching array types\"\n+   81: \t\t\t\t\t\t\t+ \" is not supported\"\n+   82: \t\t\t);\n+   83: \t\t}\n+   84: \t\tTypeVariable<?> resultTypeVariable = (TypeVariable<?>) typeToExtract;\n+   85: \t\tType[] upperBounds = resultTypeVariable.getBounds();\n+   86: \t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n+   87: \t\t\tthrow new UnsupportedOperationException(\n+   88: \t\t\t\t\t\"Matching types with bounded type variables is not supported\"\n+   89: \t\t\t);\n+   90: \t\t}\n+   91: \t\treturn new ArrayElementTypeMatcher();\n+   92: \t}\n+   93: \n+   94: \tprivate ExtractingTypePatternMatcher createExtractingClassTypeMatcher(Class<?> typePattern,\n+   95: \t\t\tType typeToExtract) {\n+   96: \t\tif ( !( typeToExtract instanceof Class ) ) {\n+   97: \t\t\tthrow new UnsupportedOperationException(\n+   98: \t\t\t\t\t\"Extracting a non-raw result type when matching a raw type is not supported\"\n+   99: \t\t\t);\n+  100: \t\t}\n+  101: \t\tPojoRawTypeModel<?> typePatternModel = introspector.getTypeModel( typePattern );\n+  102: \t\tPojoGenericTypeModel<?> typeToExtractModel = introspector.getGenericTypeModel( (Class<?>) typeToExtract );\n+  103: \t\treturn new ConstantExtractingTypePatternMatcherAdapter(\n+  104: \t\t\t\tnew RawSuperTypeMatcher( typePatternModel ),\n+  105: \t\t\t\ttypeToExtractModel\n+  106: \t\t);\n+  107: \t}\n+  108: \n+  109: \tprivate ExtractingTypePatternMatcher createExtractingParameterizedTypeMatcher(ParameterizedType typePattern,\n+  110: \t\t\tType typeToExtract) {\n+  111: \t\tClass<?> rawTypePattern = (Class<?>) typePattern.getRawType();\n+  112: \t\tType[] typeArguments = typePattern.getActualTypeArguments();\n+  113: \n+  114: \t\tInteger typeVariableIndex = null;\n+  115: \t\tfor ( int i = 0; i < typeArguments.length; i++ ) {\n+  116: \t\t\tType typeArgument = typeArguments[i];\n+  117: \t\t\tif ( typeArgument instanceof TypeVariable ) {\n+  118: \t\t\t\tif ( typeVariableIndex == null ) {\n+  119: \t\t\t\t\ttypeVariableIndex = i;\n+  120: \t\t\t\t\tTypeVariable<?> typeVariable = (TypeVariable<?>) typeArgument;\n+  121: \t\t\t\t\tType[] upperBounds = typeVariable.getBounds();\n+  122: \t\t\t\t\tif ( !typeToExtract.equals( typeVariable ) ) {\n+  123: \t\t\t\t\t\tthrow new UnsupportedOperationException(\n+  124: \t\t\t\t\t\t\t\t\"Extracting anything other than the type variable when matching parameterized types\"\n+  125: \t\t\t\t\t\t\t\t\t\t+ \" is not supported\"\n+  126: \t\t\t\t\t\t);\n+  127: \t\t\t\t\t}\n+  128: \t\t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] ) ) {\n+  129: \t\t\t\t\t\tthrow new UnsupportedOperationException(\n+  130: \t\t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n+  131: \t\t\t\t\t\t);\n+  132: \t\t\t\t\t}\n+  133: \t\t\t\t}\n+  134: \t\t\t\telse {\n+  135: \t\t\t\t\tthrow new UnsupportedOperationException(\n+  136: \t\t\t\t\t\t\t\"Matching a parameterized type with multiple type variables\"\n+  137: \t\t\t\t\t\t\t\t\t+ \" in its arguments is not supported\"\n+  138: \t\t\t\t\t);\n+  139: \t\t\t\t}\n+  140: \t\t\t}\n+  141: \t\t\telse if ( typeArgument instanceof WildcardType ) {\n+  142: \t\t\t\tWildcardType wildcardTypeArgument = (WildcardType) typeArgument;\n+  143: \t\t\t\tType[] upperBounds = wildcardTypeArgument.getUpperBounds();\n+  144: \t\t\t\tType[] lowerBounds = wildcardTypeArgument.getLowerBounds();\n+  145: \t\t\t\tif ( upperBounds.length > 1 || !Object.class.equals( upperBounds[0] )\n+  146: \t\t\t\t\t\t|| lowerBounds.length > 0 ) {\n+  147: \t\t\t\t\tthrow new UnsupportedOperationException(\n+  148: \t\t\t\t\t\t\t\"Matching a parameterized type with bounded type arguments is not supported\"\n+  149: \t\t\t\t\t);\n",
        "uniqueId": "a4d2e5b39bb53fefd2c31be404a6682af6c3a5bb_42_149_75_92_42_73",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 13,
                "covered": 49
            },
            "BRANCH": {
                "missed": 1,
                "covered": 9
            },
            "LINE": {
                "missed": 1,
                "covered": 13
            },
            "COMPLEXITY": {
                "missed": 1,
                "covered": 5
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tpublic builder() : Builder extracted from private processBatch() : void in class org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor & moved to class org.hibernate.search.engine.reporting.FailureContext",
        "diffLocations": [
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutor.java",
                "startLine": 167,
                "endLine": 236,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutor.java",
                "startLine": 167,
                "endLine": 236,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutor.java",
                "startLine": 16,
                "endLine": 21,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "/**\n\t * Takes a batch of worksets from the queue and processes them.\n\t */\n\tprivate void processBatch() {\n\t\ttry {\n\t\t\tCompletableFuture<?> batchFuture;\n\t\t\tsynchronized (processor) {\n\t\t\t\tprocessor.beginBatch();\n\t\t\t\tworkBuffer.clear();\n\n\t\t\t\tworkQueue.drainTo( workBuffer, maxTasksPerBatch );\n\n\t\t\t\tfor ( W workset : workBuffer ) {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tworkset.submitTo( processor );\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Throwable e) {\n\t\t\t\t\t\tworkset.markAsFailed( e );\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Nothing more to do, end the batch and terminate\n\t\t\t\tbatchFuture = processor.endBatch();\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Wait for works to complete before trying to handle the next batch.\n\t\t\t * Note: timeout is expected to be handled by the processor\n\t\t\t * (the Elasticsearch client adds per-request timeouts, in particular),\n\t\t\t * so this \"join\" will not last forever\n\t\t\t */\n\t\t\tFutures.unwrappedExceptionJoin( batchFuture );\n\t\t}\n\t\tcatch (Throwable e) {\n\t\t\t// This will only happen if there is a bug in the processor\n\t\t\tFailureContextImpl.Builder contextBuilder = new FailureContextImpl.Builder();\n\t\t\tcontextBuilder.throwable( e );\n\t\t\tcontextBuilder.failingOperation( \"Work processing in executor '\" + name + \"'\" );\n\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t}\n\t\tfinally {\n\t\t\t// We're done executing this batch.\n\t\t\tif ( workQueue.isEmpty() ) {\n\t\t\t\t// We're done executing the whole queue: handle getCompletion().\n\t\t\t\tCompletableFuture<?> justFinishedQueueFuture = this.completionFuture;\n\t\t\t\tcompletionFuture = null;\n\t\t\t\tjustFinishedQueueFuture.complete( null );\n\t\t\t}\n\t\t\t// Allow this thread (or others) to schedule processing again.\n\t\t\tprocessingInProgress.set( false );\n\t\t\tif ( !workQueue.isEmpty() ) {\n\t\t\t\t/*\n\t\t\t\t * Either the work queue wasn't empty and the \"if\" block above wasn't executed,\n\t\t\t\t * or the \"if\" block above was executed but someone submitted new work between\n\t\t\t\t * the call to workQueue.isEmpty() and the call to processingInProgress.set( false ).\n\t\t\t\t * In either case, we need to re-schedule processing, because no one else will.\n\t\t\t\t */\n\t\t\t\ttry {\n\t\t\t\t\tensureProcessingScheduled();\n\t\t\t\t}\n\t\t\t\tcatch (Throwable e) {\n\t\t\t\t\t// This will only happen if there is a bug in this class, but we don't want to fail silently\n\t\t\t\t\tFailureContextImpl.Builder contextBuilder = new FailureContextImpl.Builder();\n\t\t\t\t\tcontextBuilder.throwable( e );\n\t\t\t\t\tcontextBuilder.failingOperation( \"Scheduling the next batch in executor '\" + name + \"'\" );\n\t\t\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}",
        "filePathBefore": "engine/src/main/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutor.java",
        "isPureRefactoring": true,
        "commitId": "daaa9ff20cae0f7751636f16bcaf0cf2512c16a4",
        "packageNameBefore": "org.hibernate.search.engine.backend.orchestration.spi",
        "classNameBefore": "org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor",
        "methodNameBefore": "org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#processBatch",
        "invokedMethod": "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneSingleWriteWorkSet#submitTo\n methodBody: public void submitTo(LuceneWriteWorkProcessor processor) {\nprocessor.beforeWorkSet(commitStrategy,refreshStrategy);\ntryT result=processor.submit(work);\nprocessor.afterSuccessfulWorkSet();\nfuture.complete(result);\ncatch(RuntimeException e)markAsFailed(e);\nIndexFailureContextImpl.Builder failureContextBuilder=new IndexFailureContextImpl.Builder();\nfailureContextBuilder.throwable(e);\nfailureContextBuilder.failingOperation(work.getInfo());\nIndexFailureContext failureContext=failureContextBuilder.build();\nprocessor.getFailureHandler().handle(failureContext);\n}\nmethodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#build\n methodBody: public CompletableFuture<Void> build() {\nfinal SequenceContext sequenceContext=currentlyBuildingSequenceContext;\nreturn Futures.whenCompleteExecute(currentlyBuildingSequenceTail,() -> sequenceContext.executionContext.executePendingRefreshes().whenComplete(Futures.copyHandler(sequenceContext.refreshFuture))).exceptionally(Futures.handler(t -> {\n  sequenceContext.notifySequenceFailed(t);\n  return null;\n}\n));\n}\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkSet#markAsFailed\n methodBody: void markAsFailed(Throwable t);\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#ensureProcessingScheduled\n methodBody: private void ensureProcessingScheduled() {\nif(processingInProgress.compareAndSet(false,true)){tryif(completionFuture == null){completionFuture=new CompletableFuture<>();\n}executorService.submit(this::processBatch);\ncatch(Throwable e)tryCompletableFuture<?> future=completionFuture;\ncompletionFuture=null;\nprocessingInProgress.set(false);\nfuture.completeExceptionally(e);\ncatch(Throwable e2)e.addSuppressed(e2);\nthrow e;\n}}\nmethodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#throwable\n methodBody: public void throwable(Throwable th) {\nthis.throwable=th;\n}\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkSet#submitTo\n methodBody: void submitTo(P processor);\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneEnsureIndexExistsWriteWorkSet#markAsFailed\n methodBody: public void markAsFailed(Throwable t) {\nfuture.completeExceptionally(t);\n}\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkProcessor#beginBatch\n methodBody: void beginBatch();\nmethodSignature: org.hibernate.search.backend.lucene.work.execution.impl.LuceneIndexingPlanWriteWorkSet#submitTo\n methodBody: public void submitTo(LuceneWriteWorkProcessor processor) {\nIndexIndexingPlanExecutionReport.Builder reportBuilder=IndexIndexingPlanExecutionReport.builder();\nprocessor.beforeWorkSet(commitStrategy,refreshStrategy);\nThrowable throwable=null;\nObject failingOperation=null;\nfor(LuceneWriteWork<?> work: works){tryprocessor.submit(work);\ncatch(RuntimeException e)reportBuilder.throwable(e);\nthrowable=e;\nfailingOperation=work.getInfo();\nbreak;\n}if(throwable == null){tryprocessor.afterSuccessfulWorkSet();\ncatch(RuntimeException e)reportBuilder.throwable(e);\nthrowable=e;\nfailingOperation=\"Commit after a set of index works\";\n}if(throwable == null){indexingPlanFuture.complete(reportBuilder.build());\n}{IndexFailureContextImpl.Builder failureContextBuilder=new IndexFailureContextImpl.Builder();\nfailureContextBuilder.throwable(throwable);\nfailureContextBuilder.failingOperation(failingOperation);\nfor(LuceneSingleDocumentWriteWork<?> work: works){reportBuilder.failingDocument(new LuceneDocumentReference(indexName,work.getDocumentId()));\nfailureContextBuilder.uncommittedOperation(work.getInfo());\n}indexingPlanFuture.complete(reportBuilder.build());\nprocessor.getFailureHandler().handle(failureContextBuilder.build());\n}}\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneSingleWriteWorkSet#markAsFailed\n methodBody: public void markAsFailed(Throwable t) {\nfuture.completeExceptionally(t);\n}\nmethodSignature: org.hibernate.search.backend.lucene.work.execution.impl.LuceneIndexingPlanWriteWorkSet#markAsFailed\n methodBody: public void markAsFailed(Throwable t) {\nindexingPlanFuture.completeExceptionally(t);\n}\nmethodSignature: org.hibernate.search.engine.reporting.spi.IndexFailureContextImpl.Builder#build\n methodBody: public IndexFailureContext build() {\nreturn new IndexFailureContextImpl(this);\n}\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#beginBatch\n methodBody: public void beginBatch() {\n}\nmethodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#build\n methodBody: public FailureContext build() {\nreturn new FailureContextImpl(this);\n}\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkProcessor#endBatch\n methodBody: CompletableFuture<?> endBatch();\nmethodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#failingOperation\n methodBody: public void failingOperation(Object failingOperation) {\nthis.failingOperation=failingOperation;\n}\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#endBatch\n methodBody: public CompletableFuture<?> endBatch() {\nif(!previousWorkSetsUncommittedWorks.isEmpty()){trycommit();\ncatch(RuntimeException e)cleanUpAfterFailure(e,\"Commit after a batch of index works\");\nfinallypreviousWorkSetsUncommittedWorks.clear();\n}return CompletableFuture.completedFuture(null);\n}\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneEnsureIndexExistsWriteWorkSet#submitTo\n methodBody: public void submitTo(LuceneWriteWorkProcessor processor) {\ntryprocessor.ensureIndexExists();\nfuture.complete(null);\ncatch(RuntimeException e)markAsFailed(e);\nIndexFailureContextImpl.Builder failureContextBuilder=new IndexFailureContextImpl.Builder();\nfailureContextBuilder.throwable(e);\nfailureContextBuilder.failingOperation(\"Index initialization\");\nprocessor.getFailureHandler().handle(failureContextBuilder.build());\n}",
        "classSignatureBefore": "public final class BatchingExecutor<W extends BatchingExecutor.WorkSet<? super P>, P extends BatchingExecutor.WorkProcessor> ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#processBatch"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor"
        ],
        "classSignatureBeforeSet": [
            "public final class BatchingExecutor<W extends BatchingExecutor.WorkSet<? super P>, P extends BatchingExecutor.WorkProcessor> "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Tolerable changes in the body\n",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.backend.orchestration.spi;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.atomic.AtomicBoolean;\n\nimport org.hibernate.search.engine.reporting.FailureHandler;\nimport org.hibernate.search.engine.reporting.spi.FailureContextImpl;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.impl.Closer;\nimport org.hibernate.search.util.common.impl.Executors;\nimport org.hibernate.search.util.common.impl.Futures;\n\n/**\n * An executor of works that accepts works from multiple threads, puts them in a queue,\n * and processes them in batches in a single background thread.\n * <p>\n * Useful when works can be merged together for optimization purposes (bulking in Elasticsearch),\n * or when they should never be executed in parallel (writes to a Lucene index).\n */\npublic final class BatchingExecutor<W extends BatchingExecutor.WorkSet<? super P>, P extends BatchingExecutor.WorkProcessor> {\n\n\tprivate final String name;\n\n\tprivate final P processor;\n\tprivate final FailureHandler failureHandler;\n\tprivate final int maxTasksPerBatch;\n\n\tprivate final BlockingQueue<W> workQueue;\n\tprivate final List<W> workBuffer;\n\tprivate final AtomicBoolean processingInProgress;\n\n\tprivate ExecutorService executorService;\n\tprivate volatile CompletableFuture<?> completionFuture;\n\n\t/**\n\t * @param name The name of the executor thread (and of this executor when reporting errors)\n\t * @param processor A task processor. May not be thread-safe.\n\t * @param maxTasksPerBatch The maximum number of tasks to process in a single batch.\n\t * Higher values mean more opportunity for the processor to optimize execution, but higher heap consumption.\n\t * @param fair if {@code true} tasks are always submitted to the\n\t * processor in FIFO order, if {@code false} tasks submitted\n\t * when the internal queue is full may be submitted out of order.\n\t * @param failureHandler A failure handler to report failures of the background thread.\n\t */\n\tpublic BatchingExecutor(String name, P processor, int maxTasksPerBatch, boolean fair,\n\t\t\tFailureHandler failureHandler) {\n\t\tthis.name = name;\n\t\tthis.processor = processor;\n\t\tthis.failureHandler = failureHandler;\n\t\tthis.maxTasksPerBatch = maxTasksPerBatch;\n\t\tworkQueue = new ArrayBlockingQueue<>( maxTasksPerBatch, fair );\n\t\tworkBuffer = new ArrayList<>( maxTasksPerBatch );\n\t\tprocessingInProgress = new AtomicBoolean( false );\n\t}\n\n\t/**\n\t * Start the executor, allowing works to be submitted\n\t * through {@link #submit(WorkSet)}.\n\t */\n\tpublic synchronized void start() {\n\t\texecutorService = Executors.newFixedThreadPool( 1, name );\n\t}\n\n\t/**\n\t * Stop the executor, no longer allowing works to be submitted\n\t * through {@link #submit(WorkSet)}.\n\t * <p>\n\t * This will attempt to forcibly terminate currently executing works,\n\t * and will remove pending works from the queue.\n\t */\n\tpublic synchronized void stop() {\n\t\ttry ( Closer<RuntimeException> closer = new Closer<>() ) {\n\t\t\tcloser.push( ExecutorService::shutdownNow, executorService );\n\t\t\texecutorService = null;\n\t\t\tworkQueue.clear();\n\t\t\t// It's possible that processing was successfully scheduled in the executor service but had no chance to run,\n\t\t\t// so we need to release waiting threads:\n\t\t\tif ( completionFuture != null ) {\n\t\t\t\tcompletionFuture.cancel( false );\n\t\t\t\tcompletionFuture = null;\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Submit a set of works for execution.\n\t * <p>\n\t * Must not be called when the executor is stopped.\n\t * @param workset A set of works to execute.\n\t * @throws InterruptedException If the current thread is interrupted while enqueuing the workset.\n\t */\n\tpublic void submit(W workset) throws InterruptedException {\n\t\tif ( executorService == null ) {\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"Attempt to submit a workset to executor '\" + name + \"', which is stopped\"\n\t\t\t\t\t+ \" There is probably a bug in Hibernate Search, please report it.\"\n\t\t\t);\n\t\t}\n\t\tworkQueue.put( workset );\n\t\tensureProcessingScheduled();\n\t}\n\n\t/**\n\t * @return A future that completes when all works submitted to the executor so far are completely executed.\n\t * Works submitted to the executor after entering this method may delay the wait.\n\t */\n\tpublic CompletableFuture<?> getCompletion() {\n\t\tCompletableFuture<?> future = completionFuture;\n\t\tif ( future == null ) {\n\t\t\t// No processing in progress or scheduled.\n\t\t\treturn CompletableFuture.completedFuture( null );\n\t\t}\n\t\telse {\n\t\t\t// Processing in progress or scheduled; the future will be completed when the queue becomes empty.\n\t\t\treturn future;\n\t\t}\n\t}\n\n\tprivate void ensureProcessingScheduled() {\n\t\tif ( processingInProgress.compareAndSet( false, true ) ) {\n\t\t\t/*\n\t\t\t * Our thread successfully flipped the boolean:\n\t\t\t * processing wasn't in progress, and we're now responsible for scheduling it.\n\t\t\t */\n\t\t\ttry {\n\t\t\t\tif ( completionFuture == null ) {\n\t\t\t\t\t/*\n\t\t\t\t\t * The executor was previously idle:\n\t\t\t\t\t * we need to create a new future for the completion of the queue.\n\t\t\t\t\t * This is not executed when re-scheduling processing between two batches.\n\t\t\t\t\t */\n\t\t\t\t\tcompletionFuture = new CompletableFuture<>();\n\t\t\t\t}\n\t\t\t\texecutorService.submit( this::processBatch );\n\t\t\t}\n\t\t\tcatch (Throwable e) {\n\t\t\t\t/*\n\t\t\t\t * Make sure a failure to submit the processing task\n\t\t\t\t * to the executor service\n\t\t\t\t * doesn't leave other threads waiting indefinitely.\n\t\t\t\t */\n\t\t\t\ttry {\n\t\t\t\t\tCompletableFuture<?> future = completionFuture;\n\t\t\t\t\tcompletionFuture = null;\n\t\t\t\t\tprocessingInProgress.set( false );\n\t\t\t\t\tfuture.completeExceptionally( e );\n\t\t\t\t}\n\t\t\t\tcatch (Throwable e2) {\n\t\t\t\t\te.addSuppressed( e2 );\n\t\t\t\t}\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Takes a batch of worksets from the queue and processes them.\n\t */\n\tprivate void processBatch() {\n\t\ttry {\n\t\t\tCompletableFuture<?> batchFuture;\n\t\t\tsynchronized (processor) {\n\t\t\t\tprocessor.beginBatch();\n\t\t\t\tworkBuffer.clear();\n\n\t\t\t\tworkQueue.drainTo( workBuffer, maxTasksPerBatch );\n\n\t\t\t\tfor ( W workset : workBuffer ) {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tworkset.submitTo( processor );\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Throwable e) {\n\t\t\t\t\t\tworkset.markAsFailed( e );\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Nothing more to do, end the batch and terminate\n\t\t\t\tbatchFuture = processor.endBatch();\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Wait for works to complete before trying to handle the next batch.\n\t\t\t * Note: timeout is expected to be handled by the processor\n\t\t\t * (the Elasticsearch client adds per-request timeouts, in particular),\n\t\t\t * so this \"join\" will not last forever\n\t\t\t */\n\t\t\tFutures.unwrappedExceptionJoin( batchFuture );\n\t\t}\n\t\tcatch (Throwable e) {\n\t\t\t// This will only happen if there is a bug in the processor\n\t\t\tFailureContextImpl.Builder contextBuilder = new FailureContextImpl.Builder();\n\t\t\tcontextBuilder.throwable( e );\n\t\t\tcontextBuilder.failingOperation( \"Work processing in executor '\" + name + \"'\" );\n\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t}\n\t\tfinally {\n\t\t\t// We're done executing this batch.\n\t\t\tif ( workQueue.isEmpty() ) {\n\t\t\t\t// We're done executing the whole queue: handle getCompletion().\n\t\t\t\tCompletableFuture<?> justFinishedQueueFuture = this.completionFuture;\n\t\t\t\tcompletionFuture = null;\n\t\t\t\tjustFinishedQueueFuture.complete( null );\n\t\t\t}\n\t\t\t// Allow this thread (or others) to schedule processing again.\n\t\t\tprocessingInProgress.set( false );\n\t\t\tif ( !workQueue.isEmpty() ) {\n\t\t\t\t/*\n\t\t\t\t * Either the work queue wasn't empty and the \"if\" block above wasn't executed,\n\t\t\t\t * or the \"if\" block above was executed but someone submitted new work between\n\t\t\t\t * the call to workQueue.isEmpty() and the call to processingInProgress.set( false ).\n\t\t\t\t * In either case, we need to re-schedule processing, because no one else will.\n\t\t\t\t */\n\t\t\t\ttry {\n\t\t\t\t\tensureProcessingScheduled();\n\t\t\t\t}\n\t\t\t\tcatch (Throwable e) {\n\t\t\t\t\t// This will only happen if there is a bug in this class, but we don't want to fail silently\n\t\t\t\t\tFailureContextImpl.Builder contextBuilder = new FailureContextImpl.Builder();\n\t\t\t\t\tcontextBuilder.throwable( e );\n\t\t\t\t\tcontextBuilder.failingOperation( \"Scheduling the next batch in executor '\" + name + \"'\" );\n\t\t\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic interface WorkProcessor {\n\n\t\tvoid beginBatch();\n\n\t\t/**\n\t\t * Ensure all works submitted since the last call to {@link #beginBatch()} will actually be executed,\n\t\t * along with any finishing task (commit, ...).\n\t\t *\n\t\t * @return A future completing when all works submitted since the last call to {@link #beginBatch()}\n\t\t * have completed.\n\t\t */\n\t\tCompletableFuture<?> endBatch();\n\n\t}\n\n\tpublic interface WorkSet<P extends WorkProcessor> {\n\n\t\tvoid submitTo(P processor);\n\n\t\tvoid markAsFailed(Throwable t);\n\n\t}\n\n}\n",
        "filePathAfter": "engine/src/main/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutor.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.backend.orchestration.spi;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.atomic.AtomicBoolean;\n\nimport org.hibernate.search.engine.reporting.FailureContext;\nimport org.hibernate.search.engine.reporting.FailureHandler;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.impl.Closer;\nimport org.hibernate.search.util.common.impl.Executors;\nimport org.hibernate.search.util.common.impl.Futures;\n\n/**\n * An executor of works that accepts works from multiple threads, puts them in a queue,\n * and processes them in batches in a single background thread.\n * <p>\n * Useful when works can be merged together for optimization purposes (bulking in Elasticsearch),\n * or when they should never be executed in parallel (writes to a Lucene index).\n */\npublic final class BatchingExecutor<W extends BatchingExecutor.WorkSet<? super P>, P extends BatchingExecutor.WorkProcessor> {\n\n\tprivate final String name;\n\n\tprivate final P processor;\n\tprivate final FailureHandler failureHandler;\n\tprivate final int maxTasksPerBatch;\n\n\tprivate final BlockingQueue<W> workQueue;\n\tprivate final List<W> workBuffer;\n\tprivate final AtomicBoolean processingInProgress;\n\n\tprivate ExecutorService executorService;\n\tprivate volatile CompletableFuture<?> completionFuture;\n\n\t/**\n\t * @param name The name of the executor thread (and of this executor when reporting errors)\n\t * @param processor A task processor. May not be thread-safe.\n\t * @param maxTasksPerBatch The maximum number of tasks to process in a single batch.\n\t * Higher values mean more opportunity for the processor to optimize execution, but higher heap consumption.\n\t * @param fair if {@code true} tasks are always submitted to the\n\t * processor in FIFO order, if {@code false} tasks submitted\n\t * when the internal queue is full may be submitted out of order.\n\t * @param failureHandler A failure handler to report failures of the background thread.\n\t */\n\tpublic BatchingExecutor(String name, P processor, int maxTasksPerBatch, boolean fair,\n\t\t\tFailureHandler failureHandler) {\n\t\tthis.name = name;\n\t\tthis.processor = processor;\n\t\tthis.failureHandler = failureHandler;\n\t\tthis.maxTasksPerBatch = maxTasksPerBatch;\n\t\tworkQueue = new ArrayBlockingQueue<>( maxTasksPerBatch, fair );\n\t\tworkBuffer = new ArrayList<>( maxTasksPerBatch );\n\t\tprocessingInProgress = new AtomicBoolean( false );\n\t}\n\n\t/**\n\t * Start the executor, allowing works to be submitted\n\t * through {@link #submit(WorkSet)}.\n\t */\n\tpublic synchronized void start() {\n\t\texecutorService = Executors.newFixedThreadPool( 1, name );\n\t}\n\n\t/**\n\t * Stop the executor, no longer allowing works to be submitted\n\t * through {@link #submit(WorkSet)}.\n\t * <p>\n\t * This will attempt to forcibly terminate currently executing works,\n\t * and will remove pending works from the queue.\n\t */\n\tpublic synchronized void stop() {\n\t\ttry ( Closer<RuntimeException> closer = new Closer<>() ) {\n\t\t\tcloser.push( ExecutorService::shutdownNow, executorService );\n\t\t\texecutorService = null;\n\t\t\tworkQueue.clear();\n\t\t\t// It's possible that processing was successfully scheduled in the executor service but had no chance to run,\n\t\t\t// so we need to release waiting threads:\n\t\t\tif ( completionFuture != null ) {\n\t\t\t\tcompletionFuture.cancel( false );\n\t\t\t\tcompletionFuture = null;\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Submit a set of works for execution.\n\t * <p>\n\t * Must not be called when the executor is stopped.\n\t * @param workset A set of works to execute.\n\t * @throws InterruptedException If the current thread is interrupted while enqueuing the workset.\n\t */\n\tpublic void submit(W workset) throws InterruptedException {\n\t\tif ( executorService == null ) {\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"Attempt to submit a workset to executor '\" + name + \"', which is stopped\"\n\t\t\t\t\t+ \" There is probably a bug in Hibernate Search, please report it.\"\n\t\t\t);\n\t\t}\n\t\tworkQueue.put( workset );\n\t\tensureProcessingScheduled();\n\t}\n\n\t/**\n\t * @return A future that completes when all works submitted to the executor so far are completely executed.\n\t * Works submitted to the executor after entering this method may delay the wait.\n\t */\n\tpublic CompletableFuture<?> getCompletion() {\n\t\tCompletableFuture<?> future = completionFuture;\n\t\tif ( future == null ) {\n\t\t\t// No processing in progress or scheduled.\n\t\t\treturn CompletableFuture.completedFuture( null );\n\t\t}\n\t\telse {\n\t\t\t// Processing in progress or scheduled; the future will be completed when the queue becomes empty.\n\t\t\treturn future;\n\t\t}\n\t}\n\n\tprivate void ensureProcessingScheduled() {\n\t\tif ( processingInProgress.compareAndSet( false, true ) ) {\n\t\t\t/*\n\t\t\t * Our thread successfully flipped the boolean:\n\t\t\t * processing wasn't in progress, and we're now responsible for scheduling it.\n\t\t\t */\n\t\t\ttry {\n\t\t\t\tif ( completionFuture == null ) {\n\t\t\t\t\t/*\n\t\t\t\t\t * The executor was previously idle:\n\t\t\t\t\t * we need to create a new future for the completion of the queue.\n\t\t\t\t\t * This is not executed when re-scheduling processing between two batches.\n\t\t\t\t\t */\n\t\t\t\t\tcompletionFuture = new CompletableFuture<>();\n\t\t\t\t}\n\t\t\t\texecutorService.submit( this::processBatch );\n\t\t\t}\n\t\t\tcatch (Throwable e) {\n\t\t\t\t/*\n\t\t\t\t * Make sure a failure to submit the processing task\n\t\t\t\t * to the executor service\n\t\t\t\t * doesn't leave other threads waiting indefinitely.\n\t\t\t\t */\n\t\t\t\ttry {\n\t\t\t\t\tCompletableFuture<?> future = completionFuture;\n\t\t\t\t\tcompletionFuture = null;\n\t\t\t\t\tprocessingInProgress.set( false );\n\t\t\t\t\tfuture.completeExceptionally( e );\n\t\t\t\t}\n\t\t\t\tcatch (Throwable e2) {\n\t\t\t\t\te.addSuppressed( e2 );\n\t\t\t\t}\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Takes a batch of worksets from the queue and processes them.\n\t */\n\tprivate void processBatch() {\n\t\ttry {\n\t\t\tCompletableFuture<?> batchFuture;\n\t\t\tsynchronized (processor) {\n\t\t\t\tprocessor.beginBatch();\n\t\t\t\tworkBuffer.clear();\n\n\t\t\t\tworkQueue.drainTo( workBuffer, maxTasksPerBatch );\n\n\t\t\t\tfor ( W workset : workBuffer ) {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tworkset.submitTo( processor );\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Throwable e) {\n\t\t\t\t\t\tworkset.markAsFailed( e );\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Nothing more to do, end the batch and terminate\n\t\t\t\tbatchFuture = processor.endBatch();\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Wait for works to complete before trying to handle the next batch.\n\t\t\t * Note: timeout is expected to be handled by the processor\n\t\t\t * (the Elasticsearch client adds per-request timeouts, in particular),\n\t\t\t * so this \"join\" will not last forever\n\t\t\t */\n\t\t\tFutures.unwrappedExceptionJoin( batchFuture );\n\t\t}\n\t\tcatch (Throwable e) {\n\t\t\t// This will only happen if there is a bug in the processor\n\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n\t\t\tcontextBuilder.throwable( e );\n\t\t\tcontextBuilder.failingOperation( \"Work processing in executor '\" + name + \"'\" );\n\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t}\n\t\tfinally {\n\t\t\t// We're done executing this batch.\n\t\t\tif ( workQueue.isEmpty() ) {\n\t\t\t\t// We're done executing the whole queue: handle getCompletion().\n\t\t\t\tCompletableFuture<?> justFinishedQueueFuture = this.completionFuture;\n\t\t\t\tcompletionFuture = null;\n\t\t\t\tjustFinishedQueueFuture.complete( null );\n\t\t\t}\n\t\t\t// Allow this thread (or others) to schedule processing again.\n\t\t\tprocessingInProgress.set( false );\n\t\t\tif ( !workQueue.isEmpty() ) {\n\t\t\t\t/*\n\t\t\t\t * Either the work queue wasn't empty and the \"if\" block above wasn't executed,\n\t\t\t\t * or the \"if\" block above was executed but someone submitted new work between\n\t\t\t\t * the call to workQueue.isEmpty() and the call to processingInProgress.set( false ).\n\t\t\t\t * In either case, we need to re-schedule processing, because no one else will.\n\t\t\t\t */\n\t\t\t\ttry {\n\t\t\t\t\tensureProcessingScheduled();\n\t\t\t\t}\n\t\t\t\tcatch (Throwable e) {\n\t\t\t\t\t// This will only happen if there is a bug in this class, but we don't want to fail silently\n\t\t\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n\t\t\t\t\tcontextBuilder.throwable( e );\n\t\t\t\t\tcontextBuilder.failingOperation( \"Scheduling the next batch in executor '\" + name + \"'\" );\n\t\t\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic interface WorkProcessor {\n\n\t\tvoid beginBatch();\n\n\t\t/**\n\t\t * Ensure all works submitted since the last call to {@link #beginBatch()} will actually be executed,\n\t\t * along with any finishing task (commit, ...).\n\t\t *\n\t\t * @return A future completing when all works submitted since the last call to {@link #beginBatch()}\n\t\t * have completed.\n\t\t */\n\t\tCompletableFuture<?> endBatch();\n\n\t}\n\n\tpublic interface WorkSet<P extends WorkProcessor> {\n\n\t\tvoid submitTo(P processor);\n\n\t\tvoid markAsFailed(Throwable t);\n\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "import org.hibernate.search.engine.reporting.FailureContext;\nimport org.hibernate.search.engine.reporting.FailureHandler;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.impl.Closer;\nimport org.hibernate.search.util.common.impl.Executors;"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneSingleWriteWorkSet#submitTo\n methodBody: public void submitTo(LuceneWriteWorkProcessor processor) {\nprocessor.beforeWorkSet(commitStrategy,refreshStrategy);\ntryT result=processor.submit(work);\nprocessor.afterSuccessfulWorkSet();\nfuture.complete(result);\ncatch(RuntimeException e)markAsFailed(e);\nIndexFailureContextImpl.Builder failureContextBuilder=new IndexFailureContextImpl.Builder();\nfailureContextBuilder.throwable(e);\nfailureContextBuilder.failingOperation(work.getInfo());\nIndexFailureContext failureContext=failureContextBuilder.build();\nprocessor.getFailureHandler().handle(failureContext);\n}",
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#build\n methodBody: public CompletableFuture<Void> build() {\nfinal SequenceContext sequenceContext=currentlyBuildingSequenceContext;\nreturn Futures.whenCompleteExecute(currentlyBuildingSequenceTail,() -> sequenceContext.executionContext.executePendingRefreshes().whenComplete(Futures.copyHandler(sequenceContext.refreshFuture))).exceptionally(Futures.handler(t -> {\n  sequenceContext.notifySequenceFailed(t);\n  return null;\n}\n));\n}",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkSet#markAsFailed\n methodBody: void markAsFailed(Throwable t);",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#ensureProcessingScheduled\n methodBody: private void ensureProcessingScheduled() {\nif(processingInProgress.compareAndSet(false,true)){tryif(completionFuture == null){completionFuture=new CompletableFuture<>();\n}executorService.submit(this::processBatch);\ncatch(Throwable e)tryCompletableFuture<?> future=completionFuture;\ncompletionFuture=null;\nprocessingInProgress.set(false);\nfuture.completeExceptionally(e);\ncatch(Throwable e2)e.addSuppressed(e2);\nthrow e;\n}}",
            "methodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#throwable\n methodBody: public void throwable(Throwable th) {\nthis.throwable=th;\n}",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkSet#submitTo\n methodBody: void submitTo(P processor);",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneEnsureIndexExistsWriteWorkSet#markAsFailed\n methodBody: public void markAsFailed(Throwable t) {\nfuture.completeExceptionally(t);\n}",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkProcessor#beginBatch\n methodBody: void beginBatch();",
            "methodSignature: org.hibernate.search.backend.lucene.work.execution.impl.LuceneIndexingPlanWriteWorkSet#submitTo\n methodBody: public void submitTo(LuceneWriteWorkProcessor processor) {\nIndexIndexingPlanExecutionReport.Builder reportBuilder=IndexIndexingPlanExecutionReport.builder();\nprocessor.beforeWorkSet(commitStrategy,refreshStrategy);\nThrowable throwable=null;\nObject failingOperation=null;\nfor(LuceneWriteWork<?> work: works){tryprocessor.submit(work);\ncatch(RuntimeException e)reportBuilder.throwable(e);\nthrowable=e;\nfailingOperation=work.getInfo();\nbreak;\n}if(throwable == null){tryprocessor.afterSuccessfulWorkSet();\ncatch(RuntimeException e)reportBuilder.throwable(e);\nthrowable=e;\nfailingOperation=\"Commit after a set of index works\";\n}if(throwable == null){indexingPlanFuture.complete(reportBuilder.build());\n}{IndexFailureContextImpl.Builder failureContextBuilder=new IndexFailureContextImpl.Builder();\nfailureContextBuilder.throwable(throwable);\nfailureContextBuilder.failingOperation(failingOperation);\nfor(LuceneSingleDocumentWriteWork<?> work: works){reportBuilder.failingDocument(new LuceneDocumentReference(indexName,work.getDocumentId()));\nfailureContextBuilder.uncommittedOperation(work.getInfo());\n}indexingPlanFuture.complete(reportBuilder.build());\nprocessor.getFailureHandler().handle(failureContextBuilder.build());\n}}",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneSingleWriteWorkSet#markAsFailed\n methodBody: public void markAsFailed(Throwable t) {\nfuture.completeExceptionally(t);\n}",
            "methodSignature: org.hibernate.search.backend.lucene.work.execution.impl.LuceneIndexingPlanWriteWorkSet#markAsFailed\n methodBody: public void markAsFailed(Throwable t) {\nindexingPlanFuture.completeExceptionally(t);\n}",
            "methodSignature: org.hibernate.search.engine.reporting.spi.IndexFailureContextImpl.Builder#build\n methodBody: public IndexFailureContext build() {\nreturn new IndexFailureContextImpl(this);\n}",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#beginBatch\n methodBody: public void beginBatch() {\n}",
            "methodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#build\n methodBody: public FailureContext build() {\nreturn new FailureContextImpl(this);\n}",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkProcessor#endBatch\n methodBody: CompletableFuture<?> endBatch();",
            "methodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#failingOperation\n methodBody: public void failingOperation(Object failingOperation) {\nthis.failingOperation=failingOperation;\n}",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#endBatch\n methodBody: public CompletableFuture<?> endBatch() {\nif(!previousWorkSetsUncommittedWorks.isEmpty()){trycommit();\ncatch(RuntimeException e)cleanUpAfterFailure(e,\"Commit after a batch of index works\");\nfinallypreviousWorkSetsUncommittedWorks.clear();\n}return CompletableFuture.completedFuture(null);\n}",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneEnsureIndexExistsWriteWorkSet#submitTo\n methodBody: public void submitTo(LuceneWriteWorkProcessor processor) {\ntryprocessor.ensureIndexExists();\nfuture.complete(null);\ncatch(RuntimeException e)markAsFailed(e);\nIndexFailureContextImpl.Builder failureContextBuilder=new IndexFailureContextImpl.Builder();\nfailureContextBuilder.throwable(e);\nfailureContextBuilder.failingOperation(\"Index initialization\");\nprocessor.getFailureHandler().handle(failureContextBuilder.build());\n}"
        ],
        "sourceCodeAfterRefactoring": "/**\n\t * Takes a batch of worksets from the queue and processes them.\n\t */\n\tprivate void processBatch() {\n\t\ttry {\n\t\t\tCompletableFuture<?> batchFuture;\n\t\t\tsynchronized (processor) {\n\t\t\t\tprocessor.beginBatch();\n\t\t\t\tworkBuffer.clear();\n\n\t\t\t\tworkQueue.drainTo( workBuffer, maxTasksPerBatch );\n\n\t\t\t\tfor ( W workset : workBuffer ) {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tworkset.submitTo( processor );\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Throwable e) {\n\t\t\t\t\t\tworkset.markAsFailed( e );\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Nothing more to do, end the batch and terminate\n\t\t\t\tbatchFuture = processor.endBatch();\n\t\t\t}\n\n\t\t\t/*\n\t\t\t * Wait for works to complete before trying to handle the next batch.\n\t\t\t * Note: timeout is expected to be handled by the processor\n\t\t\t * (the Elasticsearch client adds per-request timeouts, in particular),\n\t\t\t * so this \"join\" will not last forever\n\t\t\t */\n\t\t\tFutures.unwrappedExceptionJoin( batchFuture );\n\t\t}\n\t\tcatch (Throwable e) {\n\t\t\t// This will only happen if there is a bug in the processor\n\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n\t\t\tcontextBuilder.throwable( e );\n\t\t\tcontextBuilder.failingOperation( \"Work processing in executor '\" + name + \"'\" );\n\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t}\n\t\tfinally {\n\t\t\t// We're done executing this batch.\n\t\t\tif ( workQueue.isEmpty() ) {\n\t\t\t\t// We're done executing the whole queue: handle getCompletion().\n\t\t\t\tCompletableFuture<?> justFinishedQueueFuture = this.completionFuture;\n\t\t\t\tcompletionFuture = null;\n\t\t\t\tjustFinishedQueueFuture.complete( null );\n\t\t\t}\n\t\t\t// Allow this thread (or others) to schedule processing again.\n\t\t\tprocessingInProgress.set( false );\n\t\t\tif ( !workQueue.isEmpty() ) {\n\t\t\t\t/*\n\t\t\t\t * Either the work queue wasn't empty and the \"if\" block above wasn't executed,\n\t\t\t\t * or the \"if\" block above was executed but someone submitted new work between\n\t\t\t\t * the call to workQueue.isEmpty() and the call to processingInProgress.set( false ).\n\t\t\t\t * In either case, we need to re-schedule processing, because no one else will.\n\t\t\t\t */\n\t\t\t\ttry {\n\t\t\t\t\tensureProcessingScheduled();\n\t\t\t\t}\n\t\t\t\tcatch (Throwable e) {\n\t\t\t\t\t// This will only happen if there is a bug in this class, but we don't want to fail silently\n\t\t\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n\t\t\t\t\tcontextBuilder.throwable( e );\n\t\t\t\t\tcontextBuilder.failingOperation( \"Scheduling the next batch in executor '\" + name + \"'\" );\n\t\t\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\nimport org.hibernate.search.engine.reporting.FailureContext;\nimport org.hibernate.search.engine.reporting.FailureHandler;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.impl.Closer;\nimport org.hibernate.search.util.common.impl.Executors;",
        "diffSourceCode": "    16: \n-   17: import org.hibernate.search.engine.reporting.FailureHandler;\n-   18: import org.hibernate.search.engine.reporting.spi.FailureContextImpl;\n+   17: import org.hibernate.search.engine.reporting.FailureContext;\n+   18: import org.hibernate.search.engine.reporting.FailureHandler;\n    19: import org.hibernate.search.util.common.AssertionFailure;\n    20: import org.hibernate.search.util.common.impl.Closer;\n    21: import org.hibernate.search.util.common.impl.Executors;\n   167: \t/**\n   168: \t * Takes a batch of worksets from the queue and processes them.\n   169: \t */\n   170: \tprivate void processBatch() {\n   171: \t\ttry {\n   172: \t\t\tCompletableFuture<?> batchFuture;\n   173: \t\t\tsynchronized (processor) {\n   174: \t\t\t\tprocessor.beginBatch();\n   175: \t\t\t\tworkBuffer.clear();\n   176: \n   177: \t\t\t\tworkQueue.drainTo( workBuffer, maxTasksPerBatch );\n   178: \n   179: \t\t\t\tfor ( W workset : workBuffer ) {\n   180: \t\t\t\t\ttry {\n   181: \t\t\t\t\t\tworkset.submitTo( processor );\n   182: \t\t\t\t\t}\n   183: \t\t\t\t\tcatch (Throwable e) {\n   184: \t\t\t\t\t\tworkset.markAsFailed( e );\n   185: \t\t\t\t\t}\n   186: \t\t\t\t}\n   187: \n   188: \t\t\t\t// Nothing more to do, end the batch and terminate\n   189: \t\t\t\tbatchFuture = processor.endBatch();\n   190: \t\t\t}\n   191: \n   192: \t\t\t/*\n   193: \t\t\t * Wait for works to complete before trying to handle the next batch.\n   194: \t\t\t * Note: timeout is expected to be handled by the processor\n   195: \t\t\t * (the Elasticsearch client adds per-request timeouts, in particular),\n   196: \t\t\t * so this \"join\" will not last forever\n   197: \t\t\t */\n   198: \t\t\tFutures.unwrappedExceptionJoin( batchFuture );\n   199: \t\t}\n   200: \t\tcatch (Throwable e) {\n   201: \t\t\t// This will only happen if there is a bug in the processor\n-  202: \t\t\tFailureContextImpl.Builder contextBuilder = new FailureContextImpl.Builder();\n+  202: \t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n   203: \t\t\tcontextBuilder.throwable( e );\n   204: \t\t\tcontextBuilder.failingOperation( \"Work processing in executor '\" + name + \"'\" );\n   205: \t\t\tfailureHandler.handle( contextBuilder.build() );\n   206: \t\t}\n   207: \t\tfinally {\n   208: \t\t\t// We're done executing this batch.\n   209: \t\t\tif ( workQueue.isEmpty() ) {\n   210: \t\t\t\t// We're done executing the whole queue: handle getCompletion().\n   211: \t\t\t\tCompletableFuture<?> justFinishedQueueFuture = this.completionFuture;\n   212: \t\t\t\tcompletionFuture = null;\n   213: \t\t\t\tjustFinishedQueueFuture.complete( null );\n   214: \t\t\t}\n   215: \t\t\t// Allow this thread (or others) to schedule processing again.\n   216: \t\t\tprocessingInProgress.set( false );\n   217: \t\t\tif ( !workQueue.isEmpty() ) {\n   218: \t\t\t\t/*\n   219: \t\t\t\t * Either the work queue wasn't empty and the \"if\" block above wasn't executed,\n   220: \t\t\t\t * or the \"if\" block above was executed but someone submitted new work between\n   221: \t\t\t\t * the call to workQueue.isEmpty() and the call to processingInProgress.set( false ).\n   222: \t\t\t\t * In either case, we need to re-schedule processing, because no one else will.\n   223: \t\t\t\t */\n   224: \t\t\t\ttry {\n   225: \t\t\t\t\tensureProcessingScheduled();\n   226: \t\t\t\t}\n   227: \t\t\t\tcatch (Throwable e) {\n   228: \t\t\t\t\t// This will only happen if there is a bug in this class, but we don't want to fail silently\n-  229: \t\t\t\t\tFailureContextImpl.Builder contextBuilder = new FailureContextImpl.Builder();\n+  229: \t\t\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n   230: \t\t\t\t\tcontextBuilder.throwable( e );\n   231: \t\t\t\t\tcontextBuilder.failingOperation( \"Scheduling the next batch in executor '\" + name + \"'\" );\n   232: \t\t\t\t\tfailureHandler.handle( contextBuilder.build() );\n   233: \t\t\t\t}\n   234: \t\t\t}\n   235: \t\t}\n   236: \t}\n",
        "uniqueId": "daaa9ff20cae0f7751636f16bcaf0cf2512c16a4_167_236_16_21_167_236",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 24,
                "covered": 100
            },
            "BRANCH": {
                "missed": 0,
                "covered": 6
            },
            "LINE": {
                "missed": 5,
                "covered": 27
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 4
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tpublic apply(context AutomaticIndexingSynchronizationConfigurationContext) : void extracted from public override_committedToCustom() : void in class org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT & moved to class org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT.CustomAutomaticIndexingSynchronizationStrategy",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/automaticindexing/AutomaticIndexingSynchronizationStrategyIT.java",
                "startLine": 86,
                "endLine": 133,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/automaticindexing/AutomaticIndexingSynchronizationStrategyIT.java",
                "startLine": 158,
                "endLine": 177,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/automaticindexing/AutomaticIndexingSynchronizationStrategyIT.java",
                "startLine": 316,
                "endLine": 337,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void override_committedToCustom() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.COMMITTED );\n\t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n\n\t\tAtomicReference<CompletableFuture<?>> futurePushedToBackgroundServiceReference = new AtomicReference<>( null );\n\n\t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\tcontext -> {\n\t\t\t\t\tcontext.documentCommitStrategy( DocumentCommitStrategy.FORCE );\n\t\t\t\t\tcontext.documentRefreshStrategy( DocumentRefreshStrategy.FORCE );\n\t\t\t\t\tcontext.indexingFutureHandler( future -> {\n\t\t\t\t\t\t// try to wait for the future to complete for a small duration...\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tfuture.get( SMALL_DURATION_VALUE, SMALL_DURATION_UNIT );\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcatch (TimeoutException e) {\n\t\t\t\t\t\t\t/*\n\t\t\t\t\t\t\t * If it takes too long, push the the completable future to some background service\n\t\t\t\t\t\t\t * to wait on it and report errors asynchronously if necessary.\n\t\t\t\t\t\t\t * Here we just simulate this by setting an AtomicReference.\n\t\t\t\t\t\t\t */\n\t\t\t\t\t\t\tfuturePushedToBackgroundServiceReference.set( future );\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcatch (InterruptedException | ExecutionException e) {\n\t\t\t\t\t\t\tAssertions.fail( \"Unexpected exception: \" + e, e );\n\t\t\t\t\t\t}\n\t\t\t\t\t} );\n\t\t\t\t},\n\t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE,\n\t\t\t\tworkFuture,\n\t\t\t\t/*\n\t\t\t\t * With this synchronization strategy, the transaction will unblock the thread\n\t\t\t\t * before the work future is complete.\n\t\t\t\t */\n\t\t\t\t() -> assertThat( workFuture ).isPending()\n\t\t);\n\n\t\t/*\n\t\t * We didn't complete the work, but the transaction should unblock the thread anyway after some time.\n\t\t * Note that this will throw an ExecutionException it the transaction failed\n\t\t * or an assertion failed in the other thread.\n\t\t */\n\t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\t\t// The strategy should have timed out and it should have set the future on this reference\n\t\tAssertions.assertThat( futurePushedToBackgroundServiceReference ).isNotNull();\n\t}",
        "filePathBefore": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/automaticindexing/AutomaticIndexingSynchronizationStrategyIT.java",
        "isPureRefactoring": true,
        "commitId": "f6398f46e661b12bb5cd7d429c52bd84f5830f79",
        "packageNameBefore": "org.hibernate.search.integrationtest.mapper.orm.automaticindexing",
        "classNameBefore": "org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT#override_committedToCustom",
        "invokedMethod": "methodSignature: org.hibernate.search.util.impl.test.FutureAssert#assertThat\n methodBody: public static <T> FutureAssert<T> assertThat(Future<T> future) {\nreturn new FutureAssert<>(future);\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT#setup\n methodBody: private SessionFactory setup(AutomaticIndexingSynchronizationStrategyName strategyName) {\nOrmSetupHelper.SetupContext setupContext=ormSetupHelper.start();\nif(strategyName != null){setupContext.withProperty(HibernateOrmMapperSettings.AUTOMATIC_INDEXING_SYNCHRONIZATION_STRATEGY,strategyName);\n}backendMock.expectSchema(IndexedEntity.INDEX,b -> b.field(\"indexedField\",String.class));\nSessionFactory sessionFactory=setupContext.setup(IndexedEntity.class);\nbackendMock.verifyExpectationsMet();\nreturn sessionFactory;\n}\nmethodSignature: org.hibernate.search.util.impl.test.FutureAssert#isPending\n methodBody: public FutureAssert<T> isPending() {\ntryObject result=getNow();\nfailWithMessage(\"future <%s> should be pending, but instead it succeeded with result <%s>\",actual,result);\ncatch(TimeoutException e)catch(CancellationException e)failWithCauseAndMessage(e,\"future <%s> should be pending, but instead it's been cancelled\",actual,e);\ncatch(ExecutionException e)failWithCauseAndMessage(e,\"future <%s> should be pending, but instead it failed with exception: %s\",actual,e);\nreturn this;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT#runTransactionInDifferentThread\n methodBody: private CompletableFuture<?> runTransactionInDifferentThread(SessionFactory sessionFactory,\n\t\t\tAutomaticIndexingSynchronizationStrategy customStrategy,\n\t\t\tDocumentCommitStrategy expectedCommitStrategy,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy,\n\t\t\tCompletableFuture<?> workFuture,\n\t\t\tRunnable afterTransactionAssertion)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\nCompletableFuture<?> justBeforeTransactionCommitFuture=new CompletableFuture<>();\nCompletableFuture<?> transactionFuture=CompletableFuture.runAsync(() -> {\n  OrmUtils.withinTransaction(sessionFactory,session -> {\n    if (customStrategy != null) {\n      Search.session(session).setAutomaticIndexingSynchronizationStrategy(customStrategy);\n    }\n    IndexedEntity entity1=new IndexedEntity();\n    entity1.setId(1);\n    entity1.setIndexedField(\"initialValue\");\n    session.persist(entity1);\n    backendMock.expectWorks(IndexedEntity.INDEX,expectedCommitStrategy,expectedRefreshStrategy).add(\"1\",b -> b.field(\"indexedField\",entity1.getIndexedField())).processedThenExecuted(workFuture);\n    justBeforeTransactionCommitFuture.complete(null);\n  }\n);\n  backendMock.verifyExpectationsMet();\n  afterTransactionAssertion.run();\n}\n);\njustBeforeTransactionCommitFuture.get(ALMOST_FOREVER_VALUE,ALMOST_FOREVER_UNIT);\nreturn transactionFuture;\n}",
        "classSignatureBefore": "public class AutomaticIndexingSynchronizationStrategyIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT#override_committedToCustom"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT"
        ],
        "classSignatureBeforeSet": [
            "public class AutomaticIndexingSynchronizationStrategyIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Overlapped refactoring - can be identical by undoing the overlapped refactoring\n- Rename Variable-",
                "description": "Rename Variable on top of the extract method - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.automaticindexing;\n\nimport static org.hibernate.search.util.impl.test.FutureAssert.assertThat;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicReference;\nimport javax.persistence.Basic;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyName;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.session.AutomaticIndexingSynchronizationStrategy;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.assertj.core.api.Assertions;\n\npublic class AutomaticIndexingSynchronizationStrategyIT {\n\n\t// Let's say 3 seconds are long enough to consider that, if nothing changed after this time, nothing ever will.\n\tprivate static final long ALMOST_FOREVER_VALUE = 3L;\n\tprivate static final TimeUnit ALMOST_FOREVER_UNIT = TimeUnit.SECONDS;\n\n\tprivate static final long SMALL_DURATION_VALUE = 100L;\n\tprivate static final TimeUnit SMALL_DURATION_UNIT = TimeUnit.MILLISECONDS;\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock );\n\n\t@Test\n\tpublic void queued() throws InterruptedException, ExecutionException, TimeoutException {\n\t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.QUEUED );\n\t\ttestAsynchronous( sessionFactory, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE );\n\t}\n\n\t@Test\n\tpublic void committed_default() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( null );\n\t\ttestSynchronous( sessionFactory, DocumentCommitStrategy.FORCE, DocumentRefreshStrategy.NONE );\n\t}\n\n\t@Test\n\tpublic void committed_explicit() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.COMMITTED );\n\t\ttestSynchronous( sessionFactory, DocumentCommitStrategy.FORCE, DocumentRefreshStrategy.NONE );\n\t}\n\n\t@Test\n\tpublic void searchable() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.SEARCHABLE );\n\t\ttestSynchronous( sessionFactory, DocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE );\n\t}\n\n\t@Test\n\tpublic void override_committedToSearchable() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.COMMITTED );\n\t\ttestSynchronous(\n\t\t\t\tsessionFactory, AutomaticIndexingSynchronizationStrategy.searchable(),\n\t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE\n\t\t);\n\t}\n\n\t@Test\n\tpublic void override_committedToCustom() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.COMMITTED );\n\t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n\n\t\tAtomicReference<CompletableFuture<?>> futurePushedToBackgroundServiceReference = new AtomicReference<>( null );\n\n\t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\tcontext -> {\n\t\t\t\t\tcontext.documentCommitStrategy( DocumentCommitStrategy.FORCE );\n\t\t\t\t\tcontext.documentRefreshStrategy( DocumentRefreshStrategy.FORCE );\n\t\t\t\t\tcontext.indexingFutureHandler( future -> {\n\t\t\t\t\t\t// try to wait for the future to complete for a small duration...\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tfuture.get( SMALL_DURATION_VALUE, SMALL_DURATION_UNIT );\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcatch (TimeoutException e) {\n\t\t\t\t\t\t\t/*\n\t\t\t\t\t\t\t * If it takes too long, push the the completable future to some background service\n\t\t\t\t\t\t\t * to wait on it and report errors asynchronously if necessary.\n\t\t\t\t\t\t\t * Here we just simulate this by setting an AtomicReference.\n\t\t\t\t\t\t\t */\n\t\t\t\t\t\t\tfuturePushedToBackgroundServiceReference.set( future );\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcatch (InterruptedException | ExecutionException e) {\n\t\t\t\t\t\t\tAssertions.fail( \"Unexpected exception: \" + e, e );\n\t\t\t\t\t\t}\n\t\t\t\t\t} );\n\t\t\t\t},\n\t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE,\n\t\t\t\tworkFuture,\n\t\t\t\t/*\n\t\t\t\t * With this synchronization strategy, the transaction will unblock the thread\n\t\t\t\t * before the work future is complete.\n\t\t\t\t */\n\t\t\t\t() -> assertThat( workFuture ).isPending()\n\t\t);\n\n\t\t/*\n\t\t * We didn't complete the work, but the transaction should unblock the thread anyway after some time.\n\t\t * Note that this will throw an ExecutionException it the transaction failed\n\t\t * or an assertion failed in the other thread.\n\t\t */\n\t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\t\t// The strategy should have timed out and it should have set the future on this reference\n\t\tAssertions.assertThat( futurePushedToBackgroundServiceReference ).isNotNull();\n\t}\n\n\tprivate void testSynchronous(SessionFactory sessionFactory,\n\t\t\tDocumentCommitStrategy expectedCommitStrategy,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\ttestSynchronous( sessionFactory, null, expectedCommitStrategy, expectedRefreshStrategy );\n\t}\n\n\tprivate void testSynchronous(SessionFactory sessionFactory,\n\t\t\tAutomaticIndexingSynchronizationStrategy customStrategy,\n\t\t\tDocumentCommitStrategy expectedCommitStrategy,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\tcustomStrategy,\n\t\t\t\texpectedCommitStrategy, expectedRefreshStrategy,\n\t\t\t\tworkFuture,\n\t\t\t\t/*\n\t\t\t\t * With this synchronization strategy, the transaction may NOT unblock the thread\n\t\t\t\t * until the work future is complete.\n\t\t\t\t */\n\t\t\t\t() -> assertThat( workFuture ).isSuccessful()\n\t\t);\n\n\t\t// We expect the transaction to block forever, because the work future isn't complete\n\t\tALMOST_FOREVER_UNIT.sleep( ALMOST_FOREVER_VALUE );\n\t\tassertThat( transactionFuture ).isPending();\n\n\t\t// Completing the work should allow the transaction to unblock the thread\n\t\tworkFuture.complete( null );\n\t\t/*\n\t\t * Note that this will throw an ExecutionException it the transaction failed\n\t\t * or an assertion failed in the other thread.\n\t\t */\n\t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\t\tassertThat( transactionFuture ).isSuccessful();\n\t}\n\n\tprivate void testAsynchronous(SessionFactory sessionFactory,\n\t\t\tDocumentCommitStrategy expectedCommitStrategy,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\tnull,\n\t\t\t\texpectedCommitStrategy, expectedRefreshStrategy,\n\t\t\t\tworkFuture,\n\t\t\t\t/*\n\t\t\t\t * With this synchronization strategy, the transaction will unblock the thread\n\t\t\t\t * before the work future is complete.\n\t\t\t\t */\n\t\t\t\t() -> assertThat( workFuture ).isPending()\n\t\t);\n\n\t\t/*\n\t\t * We didn't complete the work, but the transaction should unblock the thread anyway.\n\t\t * Note that this will throw an ExecutionException it the transaction failed\n\t\t * or an assertion failed in the other thread.\n\t\t */\n\t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\t}\n\n\t/*\n\t * Run a transaction in a different thread so that its progress can be inspected from the current thread.\n\t */\n\tprivate CompletableFuture<?> runTransactionInDifferentThread(SessionFactory sessionFactory,\n\t\t\tAutomaticIndexingSynchronizationStrategy customStrategy,\n\t\t\tDocumentCommitStrategy expectedCommitStrategy,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy,\n\t\t\tCompletableFuture<?> workFuture,\n\t\t\tRunnable afterTransactionAssertion)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> justBeforeTransactionCommitFuture = new CompletableFuture<>();\n\t\tCompletableFuture<?> transactionFuture = CompletableFuture.runAsync( () -> {\n\t\t\tOrmUtils.withinTransaction( sessionFactory, session -> {\n\t\t\t\tif ( customStrategy != null ) {\n\t\t\t\t\tSearch.session( session ).setAutomaticIndexingSynchronizationStrategy( customStrategy );\n\t\t\t\t}\n\t\t\t\tIndexedEntity entity1 = new IndexedEntity();\n\t\t\t\tentity1.setId( 1 );\n\t\t\t\tentity1.setIndexedField( \"initialValue\" );\n\n\t\t\t\tsession.persist( entity1 );\n\n\t\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX, expectedCommitStrategy, expectedRefreshStrategy )\n\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t.field( \"indexedField\", entity1.getIndexedField() )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.processedThenExecuted( workFuture );\n\t\t\t\tjustBeforeTransactionCommitFuture.complete( null );\n\t\t\t} );\n\t\t\tbackendMock.verifyExpectationsMet();\n\n\t\t\tafterTransactionAssertion.run();\n\t\t} );\n\n\t\t// Ensure the transaction at least reached the point just before commit\n\t\tjustBeforeTransactionCommitFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\n\t\treturn transactionFuture;\n\t}\n\n\tprivate SessionFactory setup(AutomaticIndexingSynchronizationStrategyName strategyName) {\n\t\tOrmSetupHelper.SetupContext setupContext = ormSetupHelper.start();\n\t\tif ( strategyName != null ) {\n\t\t\tsetupContext.withProperty(\n\t\t\t\t\tHibernateOrmMapperSettings.AUTOMATIC_INDEXING_SYNCHRONIZATION_STRATEGY,\n\t\t\t\t\tstrategyName\n\t\t\t);\n\t\t}\n\n\t\tbackendMock.expectSchema( IndexedEntity.INDEX, b -> b\n\t\t\t\t.field( \"indexedField\", String.class )\n\t\t);\n\t\tSessionFactory sessionFactory = setupContext.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\treturn sessionFactory;\n\t}\n\n\t@Entity(name = \"indexed\")\n\t@Indexed(index = IndexedEntity.INDEX)\n\tpublic static class IndexedEntity {\n\n\t\tstatic final String INDEX = \"IndexedEntity\";\n\n\t\t@Id\n\t\tprivate Integer id;\n\n\t\t@Basic\n\t\t@GenericField\n\t\tprivate String indexedField;\n\n\t\tpublic Integer getId() {\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\tpublic String getIndexedField() {\n\t\t\treturn indexedField;\n\t\t}\n\n\t\tpublic void setIndexedField(String indexedField) {\n\t\t\tthis.indexedField = indexedField;\n\t\t}\n\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/automaticindexing/AutomaticIndexingSynchronizationStrategyIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.automaticindexing;\n\nimport static org.hibernate.search.util.impl.test.FutureAssert.assertThat;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.concurrent.atomic.AtomicReference;\nimport javax.persistence.Basic;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyName;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.session.AutomaticIndexingSynchronizationConfigurationContext;\nimport org.hibernate.search.mapper.orm.session.AutomaticIndexingSynchronizationStrategy;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.assertj.core.api.Assertions;\nimport org.awaitility.Awaitility;\n\npublic class AutomaticIndexingSynchronizationStrategyIT {\n\n\t// Let's say 3 seconds are long enough to consider that, if nothing changed after this time, nothing ever will.\n\tprivate static final long ALMOST_FOREVER_VALUE = 3L;\n\tprivate static final TimeUnit ALMOST_FOREVER_UNIT = TimeUnit.SECONDS;\n\n\tprivate static final long SMALL_DURATION_VALUE = 100L;\n\tprivate static final TimeUnit SMALL_DURATION_UNIT = TimeUnit.MILLISECONDS;\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock );\n\n\t@Test\n\tpublic void queued() throws InterruptedException, ExecutionException, TimeoutException {\n\t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.QUEUED );\n\t\tCompletableFuture<?> indexingWorkFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionThreadFuture = runTransactionInDifferentThreadExpectingNoBlock(\n\t\t\t\tsessionFactory, null,\n\t\t\t\tDocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE, indexingWorkFuture\n\t\t);\n\n\t\t// The transaction thread should proceed successfully,\n\t\t// regardless of the indexing work.\n\t\tassertThat( transactionThreadFuture ).isSuccessful();\n\t}\n\n\t@Test\n\tpublic void committed_default() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( null );\n\t\tCompletableFuture<?> indexingWorkFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionThreadFuture = runTransactionInDifferentThreadExpectingBlock(\n\t\t\t\tsessionFactory, null,\n\t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.NONE, indexingWorkFuture\n\t\t);\n\n\t\t// The transaction thread should be blocked because the indexing work is not complete\n\t\tassertThat( transactionThreadFuture ).isPending();\n\n\t\t// Completing the work should allow the synchronization strategy to unblock the transaction thread\n\t\tindexingWorkFuture.complete( null );\n\t\tAwaitility.await().atMost( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT )\n\t\t\t\t.until( transactionThreadFuture::isDone );\n\t\t// The transaction thread should proceed successfully,\n\t\t// because the indexing work was successful.\n\t\tassertThat( transactionThreadFuture ).isSuccessful();\n\t}\n\n\t@Test\n\tpublic void committed_explicit() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.COMMITTED );\n\t\tCompletableFuture<?> indexingWorkFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionThreadFuture = runTransactionInDifferentThreadExpectingBlock(\n\t\t\t\tsessionFactory, null,\n\t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.NONE, indexingWorkFuture\n\t\t);\n\n\t\t// The transaction thread should be blocked because the indexing work is not complete\n\t\tassertThat( transactionThreadFuture ).isPending();\n\n\t\t// Completing the work should allow the synchronization strategy to unblock the transaction thread\n\t\tindexingWorkFuture.complete( null );\n\t\tAwaitility.await().atMost( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT )\n\t\t\t\t.until( transactionThreadFuture::isDone );\n\t\t// The transaction thread should proceed successfully,\n\t\t// because the indexing work was successful.\n\t\tassertThat( transactionThreadFuture ).isSuccessful();\n\t}\n\n\t@Test\n\tpublic void searchable() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.SEARCHABLE );\n\t\tCompletableFuture<?> indexingWorkFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionThreadFuture = runTransactionInDifferentThreadExpectingBlock(\n\t\t\t\tsessionFactory, null,\n\t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE, indexingWorkFuture\n\t\t);\n\n\t\t// The transaction thread should be blocked because the indexing work is not complete\n\t\tassertThat( transactionThreadFuture ).isPending();\n\n\t\t// Completing the work should allow the synchronization strategy to unblock the transaction thread\n\t\tindexingWorkFuture.complete( null );\n\t\tAwaitility.await().atMost( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT )\n\t\t\t\t.until( transactionThreadFuture::isDone );\n\t\t// The transaction thread should proceed successfully,\n\t\t// because the indexing work was successful.\n\t\tassertThat( transactionThreadFuture ).isSuccessful();\n\t}\n\n\t@Test\n\tpublic void override_committedToSearchable() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.COMMITTED );\n\t\tCompletableFuture<?> indexingWorkFuture = new CompletableFuture<>();\n\n\t\tCompletableFuture<?> transactionThreadFuture = runTransactionInDifferentThreadExpectingBlock(\n\t\t\t\tsessionFactory, AutomaticIndexingSynchronizationStrategy.searchable(),\n\t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE, indexingWorkFuture\n\t\t);\n\n\t\t// The transaction thread should be blocked because the indexing work is not complete\n\t\tassertThat( transactionThreadFuture ).isPending();\n\n\t\t// Completing the work should allow the synchronization strategy to unblock the transaction thread\n\t\tindexingWorkFuture.complete( null );\n\t\tAwaitility.await().atMost( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT )\n\t\t\t\t.until( transactionThreadFuture::isDone );\n\t\t// The transaction thread should proceed successfully,\n\t\t// because the indexing work was successful.\n\t\tassertThat( transactionThreadFuture ).isSuccessful();\n\t}\n\n\t@Test\n\tpublic void override_committedToCustom() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.COMMITTED );\n\t\tCompletableFuture<?> indexingWorkFuture = new CompletableFuture<>();\n\n\t\tAtomicReference<CompletableFuture<?>> futureThatTookTooLong = new AtomicReference<>( null );\n\n\t\tCompletableFuture<?> transactionThreadFuture = runTransactionInDifferentThreadExpectingNoBlock(\n\t\t\t\tsessionFactory, new CustomAutomaticIndexingSynchronizationStrategy( futureThatTookTooLong ),\n\t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE, indexingWorkFuture\n\t\t);\n\n\t\t// The transaction thread should unblock and proceed successfully,\n\t\t// because the indexing work took too long to execute\n\t\t// (this is how the custom automatic indexing strategy is implemented)\n\t\tassertThat( transactionThreadFuture ).isSuccessful();\n\n\t\t// Upon timing out, the strategy should have set this reference\n\t\tAssertions.assertThat( futureThatTookTooLong ).doesNotHaveValue( null );\n\t}\n\n\tprivate CompletableFuture<?> runTransactionInDifferentThreadExpectingBlock(SessionFactory sessionFactory,\n\t\t\tAutomaticIndexingSynchronizationStrategy customStrategy,\n\t\t\tDocumentCommitStrategy expectedCommitStrategy,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy,\n\t\t\tCompletableFuture<?> indexingWorkFuture)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> transactionThreadFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\tcustomStrategy,\n\t\t\t\texpectedCommitStrategy, expectedRefreshStrategy,\n\t\t\t\tindexingWorkFuture\n\t\t);\n\n\t\t// Wait for some time...\n\t\tALMOST_FOREVER_UNIT.sleep( ALMOST_FOREVER_VALUE );\n\n\t\t// We expect the transaction to block forever, because the work future isn't complete\n\t\tassertThat( transactionThreadFuture ).isPending();\n\n\t\treturn transactionThreadFuture;\n\t}\n\n\tprivate CompletableFuture<?> runTransactionInDifferentThreadExpectingNoBlock(SessionFactory sessionFactory,\n\t\t\tAutomaticIndexingSynchronizationStrategy customStrategy,\n\t\t\tDocumentCommitStrategy expectedCommitStrategy,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy,\n\t\t\tCompletableFuture<?> indexingWorkFuture)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> transactionThreadFuture = runTransactionInDifferentThread(\n\t\t\t\tsessionFactory,\n\t\t\t\tcustomStrategy,\n\t\t\t\texpectedCommitStrategy, expectedRefreshStrategy,\n\t\t\t\tindexingWorkFuture\n\t\t);\n\n\t\t// We expect the transaction to complete even if the indexing work isn't completed\n\t\tAwaitility.await().atMost( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT )\n\t\t\t\t.until( transactionThreadFuture::isDone );\n\n\t\treturn transactionThreadFuture;\n\t}\n\n\t/*\n\t * Run a transaction in a different thread so that its progress can be inspected from the current thread.\n\t */\n\tprivate CompletableFuture<?> runTransactionInDifferentThread(SessionFactory sessionFactory,\n\t\t\tAutomaticIndexingSynchronizationStrategy customStrategy,\n\t\t\tDocumentCommitStrategy expectedCommitStrategy,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy,\n\t\t\tCompletableFuture<?> indexingWorkFuture)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\n\t\tCompletableFuture<?> justBeforeTransactionCommitFuture = new CompletableFuture<>();\n\t\tCompletableFuture<?> transactionThreadFuture = CompletableFuture.runAsync( () -> {\n\t\t\tOrmUtils.withinTransaction( sessionFactory, session -> {\n\t\t\t\tif ( customStrategy != null ) {\n\t\t\t\t\tSearch.session( session ).setAutomaticIndexingSynchronizationStrategy( customStrategy );\n\t\t\t\t}\n\t\t\t\tIndexedEntity entity1 = new IndexedEntity();\n\t\t\t\tentity1.setId( 1 );\n\t\t\t\tentity1.setIndexedField( \"initialValue\" );\n\n\t\t\t\tsession.persist( entity1 );\n\n\t\t\t\tbackendMock.expectWorks( IndexedEntity.INDEX, expectedCommitStrategy, expectedRefreshStrategy )\n\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t.field( \"indexedField\", entity1.getIndexedField() )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.processedThenExecuted( indexingWorkFuture );\n\t\t\t\tjustBeforeTransactionCommitFuture.complete( null );\n\t\t\t} );\n\t\t\tbackendMock.verifyExpectationsMet();\n\t\t} );\n\n\t\t// Ensure the transaction at least reached the point just before commit\n\t\tjustBeforeTransactionCommitFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n\n\t\treturn transactionThreadFuture;\n\t}\n\n\tprivate SessionFactory setup(AutomaticIndexingSynchronizationStrategyName strategyName) {\n\t\tOrmSetupHelper.SetupContext setupContext = ormSetupHelper.start();\n\t\tif ( strategyName != null ) {\n\t\t\tsetupContext.withProperty(\n\t\t\t\t\tHibernateOrmMapperSettings.AUTOMATIC_INDEXING_SYNCHRONIZATION_STRATEGY,\n\t\t\t\t\tstrategyName\n\t\t\t);\n\t\t}\n\n\t\tbackendMock.expectSchema( IndexedEntity.INDEX, b -> b\n\t\t\t\t.field( \"indexedField\", String.class )\n\t\t);\n\t\tSessionFactory sessionFactory = setupContext.setup( IndexedEntity.class );\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\treturn sessionFactory;\n\t}\n\n\t@Entity(name = \"indexed\")\n\t@Indexed(index = IndexedEntity.INDEX)\n\tpublic static class IndexedEntity {\n\n\t\tstatic final String INDEX = \"IndexedEntity\";\n\n\t\t@Id\n\t\tprivate Integer id;\n\n\t\t@Basic\n\t\t@GenericField\n\t\tprivate String indexedField;\n\n\t\tpublic Integer getId() {\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\tpublic String getIndexedField() {\n\t\t\treturn indexedField;\n\t\t}\n\n\t\tpublic void setIndexedField(String indexedField) {\n\t\t\tthis.indexedField = indexedField;\n\t\t}\n\n\t}\n\n\tprivate class CustomAutomaticIndexingSynchronizationStrategy implements AutomaticIndexingSynchronizationStrategy {\n\n\t\tprivate final AtomicReference<CompletableFuture<?>> futureThatTookTooLong;\n\n\t\tprivate CustomAutomaticIndexingSynchronizationStrategy(\n\t\t\t\tAtomicReference<CompletableFuture<?>> futureThatTookTooLong) {\n\t\t\tthis.futureThatTookTooLong = futureThatTookTooLong;\n\t\t}\n\n\t\t@Override\n\t\tpublic void apply(AutomaticIndexingSynchronizationConfigurationContext context) {\n\t\t\tcontext.documentCommitStrategy( DocumentCommitStrategy.FORCE );\n\t\t\tcontext.documentRefreshStrategy( DocumentRefreshStrategy.FORCE );\n\t\t\tcontext.indexingFutureHandler( future -> {\n\t\t\t\t// try to wait for the future to complete for a small duration...\n\t\t\t\ttry {\n\t\t\t\t\tfuture.get( SMALL_DURATION_VALUE, SMALL_DURATION_UNIT );\n\t\t\t\t}\n\t\t\t\tcatch (TimeoutException e) {\n\t\t\t\t\t/*\n\t\t\t\t\t * If it takes too long, push the the completable future to some background service\n\t\t\t\t\t * to wait on it and report errors asynchronously if necessary.\n\t\t\t\t\t * Here we just simulate this by setting an AtomicReference.\n\t\t\t\t\t */\n\t\t\t\t\tfutureThatTookTooLong.set( future );\n\t\t\t\t}\n\t\t\t\tcatch (InterruptedException | ExecutionException e) {\n\t\t\t\t\tAssertions.fail( \"Unexpected exception: \" + e, e );\n\t\t\t\t}\n\t\t\t} );\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "@Override\n\t\tpublic void apply(AutomaticIndexingSynchronizationConfigurationContext context) {\n\t\t\tcontext.documentCommitStrategy( DocumentCommitStrategy.FORCE );\n\t\t\tcontext.documentRefreshStrategy( DocumentRefreshStrategy.FORCE );\n\t\t\tcontext.indexingFutureHandler( future -> {\n\t\t\t\t// try to wait for the future to complete for a small duration...\n\t\t\t\ttry {\n\t\t\t\t\tfuture.get( SMALL_DURATION_VALUE, SMALL_DURATION_UNIT );\n\t\t\t\t}\n\t\t\t\tcatch (TimeoutException e) {\n\t\t\t\t\t/*\n\t\t\t\t\t * If it takes too long, push the the completable future to some background service\n\t\t\t\t\t * to wait on it and report errors asynchronously if necessary.\n\t\t\t\t\t * Here we just simulate this by setting an AtomicReference.\n\t\t\t\t\t */\n\t\t\t\t\tfutureThatTookTooLong.set( future );\n\t\t\t\t}\n\t\t\t\tcatch (InterruptedException | ExecutionException e) {\n\t\t\t\t\tAssertions.fail( \"Unexpected exception: \" + e, e );\n\t\t\t\t}\n\t\t\t} );\n\t\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.util.impl.test.FutureAssert#assertThat\n methodBody: public static <T> FutureAssert<T> assertThat(Future<T> future) {\nreturn new FutureAssert<>(future);\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT#setup\n methodBody: private SessionFactory setup(AutomaticIndexingSynchronizationStrategyName strategyName) {\nOrmSetupHelper.SetupContext setupContext=ormSetupHelper.start();\nif(strategyName != null){setupContext.withProperty(HibernateOrmMapperSettings.AUTOMATIC_INDEXING_SYNCHRONIZATION_STRATEGY,strategyName);\n}backendMock.expectSchema(IndexedEntity.INDEX,b -> b.field(\"indexedField\",String.class));\nSessionFactory sessionFactory=setupContext.setup(IndexedEntity.class);\nbackendMock.verifyExpectationsMet();\nreturn sessionFactory;\n}",
            "methodSignature: org.hibernate.search.util.impl.test.FutureAssert#isPending\n methodBody: public FutureAssert<T> isPending() {\ntryObject result=getNow();\nfailWithMessage(\"future <%s> should be pending, but instead it succeeded with result <%s>\",actual,result);\ncatch(TimeoutException e)catch(CancellationException e)failWithCauseAndMessage(e,\"future <%s> should be pending, but instead it's been cancelled\",actual,e);\ncatch(ExecutionException e)failWithCauseAndMessage(e,\"future <%s> should be pending, but instead it failed with exception: %s\",actual,e);\nreturn this;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.automaticindexing.AutomaticIndexingSynchronizationStrategyIT#runTransactionInDifferentThread\n methodBody: private CompletableFuture<?> runTransactionInDifferentThread(SessionFactory sessionFactory,\n\t\t\tAutomaticIndexingSynchronizationStrategy customStrategy,\n\t\t\tDocumentCommitStrategy expectedCommitStrategy,\n\t\t\tDocumentRefreshStrategy expectedRefreshStrategy,\n\t\t\tCompletableFuture<?> workFuture,\n\t\t\tRunnable afterTransactionAssertion)\n\t\t\tthrows InterruptedException, ExecutionException, TimeoutException {\nCompletableFuture<?> justBeforeTransactionCommitFuture=new CompletableFuture<>();\nCompletableFuture<?> transactionFuture=CompletableFuture.runAsync(() -> {\n  OrmUtils.withinTransaction(sessionFactory,session -> {\n    if (customStrategy != null) {\n      Search.session(session).setAutomaticIndexingSynchronizationStrategy(customStrategy);\n    }\n    IndexedEntity entity1=new IndexedEntity();\n    entity1.setId(1);\n    entity1.setIndexedField(\"initialValue\");\n    session.persist(entity1);\n    backendMock.expectWorks(IndexedEntity.INDEX,expectedCommitStrategy,expectedRefreshStrategy).add(\"1\",b -> b.field(\"indexedField\",entity1.getIndexedField())).processedThenExecuted(workFuture);\n    justBeforeTransactionCommitFuture.complete(null);\n  }\n);\n  backendMock.verifyExpectationsMet();\n  afterTransactionAssertion.run();\n}\n);\njustBeforeTransactionCommitFuture.get(ALMOST_FOREVER_VALUE,ALMOST_FOREVER_UNIT);\nreturn transactionFuture;\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void override_committedToCustom() throws InterruptedException, TimeoutException, ExecutionException {\n\t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.COMMITTED );\n\t\tCompletableFuture<?> indexingWorkFuture = new CompletableFuture<>();\n\n\t\tAtomicReference<CompletableFuture<?>> futureThatTookTooLong = new AtomicReference<>( null );\n\n\t\tCompletableFuture<?> transactionThreadFuture = runTransactionInDifferentThreadExpectingNoBlock(\n\t\t\t\tsessionFactory, new CustomAutomaticIndexingSynchronizationStrategy( futureThatTookTooLong ),\n\t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE, indexingWorkFuture\n\t\t);\n\n\t\t// The transaction thread should unblock and proceed successfully,\n\t\t// because the indexing work took too long to execute\n\t\t// (this is how the custom automatic indexing strategy is implemented)\n\t\tassertThat( transactionThreadFuture ).isSuccessful();\n\n\t\t// Upon timing out, the strategy should have set this reference\n\t\tAssertions.assertThat( futureThatTookTooLong ).doesNotHaveValue( null );\n\t}\n@Override\n\t\tpublic void apply(AutomaticIndexingSynchronizationConfigurationContext context) {\n\t\t\tcontext.documentCommitStrategy( DocumentCommitStrategy.FORCE );\n\t\t\tcontext.documentRefreshStrategy( DocumentRefreshStrategy.FORCE );\n\t\t\tcontext.indexingFutureHandler( future -> {\n\t\t\t\t// try to wait for the future to complete for a small duration...\n\t\t\t\ttry {\n\t\t\t\t\tfuture.get( SMALL_DURATION_VALUE, SMALL_DURATION_UNIT );\n\t\t\t\t}\n\t\t\t\tcatch (TimeoutException e) {\n\t\t\t\t\t/*\n\t\t\t\t\t * If it takes too long, push the the completable future to some background service\n\t\t\t\t\t * to wait on it and report errors asynchronously if necessary.\n\t\t\t\t\t * Here we just simulate this by setting an AtomicReference.\n\t\t\t\t\t */\n\t\t\t\t\tfutureThatTookTooLong.set( future );\n\t\t\t\t}\n\t\t\t\tcatch (InterruptedException | ExecutionException e) {\n\t\t\t\t\tAssertions.fail( \"Unexpected exception: \" + e, e );\n\t\t\t\t}\n\t\t\t} );\n\t\t}",
        "diffSourceCode": "-   86: \t@Test\n-   87: \tpublic void override_committedToCustom() throws InterruptedException, TimeoutException, ExecutionException {\n-   88: \t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.COMMITTED );\n-   89: \t\tCompletableFuture<?> workFuture = new CompletableFuture<>();\n-   90: \n-   91: \t\tAtomicReference<CompletableFuture<?>> futurePushedToBackgroundServiceReference = new AtomicReference<>( null );\n-   92: \n-   93: \t\tCompletableFuture<?> transactionFuture = runTransactionInDifferentThread(\n-   94: \t\t\t\tsessionFactory,\n-   95: \t\t\t\tcontext -> {\n-   96: \t\t\t\t\tcontext.documentCommitStrategy( DocumentCommitStrategy.FORCE );\n-   97: \t\t\t\t\tcontext.documentRefreshStrategy( DocumentRefreshStrategy.FORCE );\n-   98: \t\t\t\t\tcontext.indexingFutureHandler( future -> {\n-   99: \t\t\t\t\t\t// try to wait for the future to complete for a small duration...\n-  100: \t\t\t\t\t\ttry {\n-  101: \t\t\t\t\t\t\tfuture.get( SMALL_DURATION_VALUE, SMALL_DURATION_UNIT );\n-  102: \t\t\t\t\t\t}\n-  103: \t\t\t\t\t\tcatch (TimeoutException e) {\n-  104: \t\t\t\t\t\t\t/*\n-  105: \t\t\t\t\t\t\t * If it takes too long, push the the completable future to some background service\n-  106: \t\t\t\t\t\t\t * to wait on it and report errors asynchronously if necessary.\n-  107: \t\t\t\t\t\t\t * Here we just simulate this by setting an AtomicReference.\n-  108: \t\t\t\t\t\t\t */\n-  109: \t\t\t\t\t\t\tfuturePushedToBackgroundServiceReference.set( future );\n-  110: \t\t\t\t\t\t}\n-  111: \t\t\t\t\t\tcatch (InterruptedException | ExecutionException e) {\n-  112: \t\t\t\t\t\t\tAssertions.fail( \"Unexpected exception: \" + e, e );\n-  113: \t\t\t\t\t\t}\n-  114: \t\t\t\t\t} );\n-  115: \t\t\t\t},\n-  116: \t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE,\n-  117: \t\t\t\tworkFuture,\n-  118: \t\t\t\t/*\n-  119: \t\t\t\t * With this synchronization strategy, the transaction will unblock the thread\n-  120: \t\t\t\t * before the work future is complete.\n-  121: \t\t\t\t */\n-  122: \t\t\t\t() -> assertThat( workFuture ).isPending()\n-  123: \t\t);\n-  124: \n-  125: \t\t/*\n-  126: \t\t * We didn't complete the work, but the transaction should unblock the thread anyway after some time.\n-  127: \t\t * Note that this will throw an ExecutionException it the transaction failed\n-  128: \t\t * or an assertion failed in the other thread.\n-  129: \t\t */\n-  130: \t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n-  131: \t\t// The strategy should have timed out and it should have set the future on this reference\n-  132: \t\tAssertions.assertThat( futurePushedToBackgroundServiceReference ).isNotNull();\n-  133: \t}\n-  158: \t\t\t\t() -> assertThat( workFuture ).isSuccessful()\n-  159: \t\t);\n-  160: \n-  161: \t\t// We expect the transaction to block forever, because the work future isn't complete\n-  162: \t\tALMOST_FOREVER_UNIT.sleep( ALMOST_FOREVER_VALUE );\n-  163: \t\tassertThat( transactionFuture ).isPending();\n+   86: \t\t\t\t.until( transactionThreadFuture::isDone );\n+   87: \t\t// The transaction thread should proceed successfully,\n+   88: \t\t// because the indexing work was successful.\n+   89: \t\tassertThat( transactionThreadFuture ).isSuccessful();\n+   90: \t}\n+   91: \n+   92: \t@Test\n+   93: \tpublic void committed_explicit() throws InterruptedException, TimeoutException, ExecutionException {\n+   94: \t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.COMMITTED );\n+   95: \t\tCompletableFuture<?> indexingWorkFuture = new CompletableFuture<>();\n+   96: \n+   97: \t\tCompletableFuture<?> transactionThreadFuture = runTransactionInDifferentThreadExpectingBlock(\n+   98: \t\t\t\tsessionFactory, null,\n+   99: \t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.NONE, indexingWorkFuture\n+  100: \t\t);\n+  101: \n+  102: \t\t// The transaction thread should be blocked because the indexing work is not complete\n+  103: \t\tassertThat( transactionThreadFuture ).isPending();\n+  104: \n+  105: \t\t// Completing the work should allow the synchronization strategy to unblock the transaction thread\n+  106: \t\tindexingWorkFuture.complete( null );\n+  107: \t\tAwaitility.await().atMost( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT )\n+  108: \t\t\t\t.until( transactionThreadFuture::isDone );\n+  109: \t\t// The transaction thread should proceed successfully,\n+  110: \t\t// because the indexing work was successful.\n+  111: \t\tassertThat( transactionThreadFuture ).isSuccessful();\n+  112: \t}\n+  113: \n+  114: \t@Test\n+  115: \tpublic void searchable() throws InterruptedException, TimeoutException, ExecutionException {\n+  116: \t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.SEARCHABLE );\n+  117: \t\tCompletableFuture<?> indexingWorkFuture = new CompletableFuture<>();\n+  118: \n+  119: \t\tCompletableFuture<?> transactionThreadFuture = runTransactionInDifferentThreadExpectingBlock(\n+  120: \t\t\t\tsessionFactory, null,\n+  121: \t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE, indexingWorkFuture\n+  122: \t\t);\n+  123: \n+  124: \t\t// The transaction thread should be blocked because the indexing work is not complete\n+  125: \t\tassertThat( transactionThreadFuture ).isPending();\n+  126: \n+  127: \t\t// Completing the work should allow the synchronization strategy to unblock the transaction thread\n+  128: \t\tindexingWorkFuture.complete( null );\n+  129: \t\tAwaitility.await().atMost( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT )\n+  130: \t\t\t\t.until( transactionThreadFuture::isDone );\n+  131: \t\t// The transaction thread should proceed successfully,\n+  132: \t\t// because the indexing work was successful.\n+  133: \t\tassertThat( transactionThreadFuture ).isSuccessful();\n+  158: \t@Test\n+  159: \tpublic void override_committedToCustom() throws InterruptedException, TimeoutException, ExecutionException {\n+  160: \t\tSessionFactory sessionFactory = setup( AutomaticIndexingSynchronizationStrategyName.COMMITTED );\n+  161: \t\tCompletableFuture<?> indexingWorkFuture = new CompletableFuture<>();\n+  162: \n+  163: \t\tAtomicReference<CompletableFuture<?>> futureThatTookTooLong = new AtomicReference<>( null );\n   164: \n-  165: \t\t// Completing the work should allow the transaction to unblock the thread\n-  166: \t\tworkFuture.complete( null );\n-  167: \t\t/*\n-  168: \t\t * Note that this will throw an ExecutionException it the transaction failed\n-  169: \t\t * or an assertion failed in the other thread.\n-  170: \t\t */\n-  171: \t\ttransactionFuture.get( ALMOST_FOREVER_VALUE, ALMOST_FOREVER_UNIT );\n-  172: \t\tassertThat( transactionFuture ).isSuccessful();\n-  173: \t}\n+  165: \t\tCompletableFuture<?> transactionThreadFuture = runTransactionInDifferentThreadExpectingNoBlock(\n+  166: \t\t\t\tsessionFactory, new CustomAutomaticIndexingSynchronizationStrategy( futureThatTookTooLong ),\n+  167: \t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE, indexingWorkFuture\n+  168: \t\t);\n+  169: \n+  170: \t\t// The transaction thread should unblock and proceed successfully,\n+  171: \t\t// because the indexing work took too long to execute\n+  172: \t\t// (this is how the custom automatic indexing strategy is implemented)\n+  173: \t\tassertThat( transactionThreadFuture ).isSuccessful();\n   174: \n-  175: \tprivate void testAsynchronous(SessionFactory sessionFactory,\n-  176: \t\t\tDocumentCommitStrategy expectedCommitStrategy,\n-  177: \t\t\tDocumentRefreshStrategy expectedRefreshStrategy)\n+  175: \t\t// Upon timing out, the strategy should have set this reference\n+  176: \t\tAssertions.assertThat( futureThatTookTooLong ).doesNotHaveValue( null );\n+  177: \t}\n+  316: \t\t@Override\n+  317: \t\tpublic void apply(AutomaticIndexingSynchronizationConfigurationContext context) {\n+  318: \t\t\tcontext.documentCommitStrategy( DocumentCommitStrategy.FORCE );\n+  319: \t\t\tcontext.documentRefreshStrategy( DocumentRefreshStrategy.FORCE );\n+  320: \t\t\tcontext.indexingFutureHandler( future -> {\n+  321: \t\t\t\t// try to wait for the future to complete for a small duration...\n+  322: \t\t\t\ttry {\n+  323: \t\t\t\t\tfuture.get( SMALL_DURATION_VALUE, SMALL_DURATION_UNIT );\n+  324: \t\t\t\t}\n+  325: \t\t\t\tcatch (TimeoutException e) {\n+  326: \t\t\t\t\t/*\n+  327: \t\t\t\t\t * If it takes too long, push the the completable future to some background service\n+  328: \t\t\t\t\t * to wait on it and report errors asynchronously if necessary.\n+  329: \t\t\t\t\t * Here we just simulate this by setting an AtomicReference.\n+  330: \t\t\t\t\t */\n+  331: \t\t\t\t\tfutureThatTookTooLong.set( future );\n+  332: \t\t\t\t}\n+  333: \t\t\t\tcatch (InterruptedException | ExecutionException e) {\n+  334: \t\t\t\t\tAssertions.fail( \"Unexpected exception: \" + e, e );\n+  335: \t\t\t\t}\n+  336: \t\t\t} );\n+  337: \t\t}\n",
        "uniqueId": "f6398f46e661b12bb5cd7d429c52bd84f5830f79_86_133_316_337_158_177",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tpublic getIncludePaths() : Set<String> extracted from public getUselessIncludePaths() : Set<String> in class org.hibernate.search.engine.mapper.mapping.building.impl.ConfiguredIndexSchemaNestingContext & moved to class org.hibernate.search.engine.mapper.mapping.building.spi.IndexedEmbeddedDefinition",
        "diffLocations": [
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/ConfiguredIndexSchemaNestingContext.java",
                "startLine": 114,
                "endLine": 126,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/IndexedEmbeddedPathTracker.java",
                "startLine": 37,
                "endLine": 47,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/IndexedEmbeddedPathTracker.java",
                "startLine": 85,
                "endLine": 87,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public Set<String> getUselessIncludePaths() {\n\t\tSet<String> includePaths = filter.getConfiguredIncludedPaths();\n\t\tMap<String, Boolean> encounteredFieldPaths = filter.getEncounteredFieldPaths();\n\t\tSet<String> uselessIncludePaths = new LinkedHashSet<>();\n\t\tfor ( String path : includePaths ) {\n\t\t\tBoolean included = encounteredFieldPaths.get( path );\n\t\t\tif ( included == null /* not encountered */ || !included ) {\n\t\t\t\t// An \"includePaths\" filter that does not result in inclusion is useless\n\t\t\t\tuselessIncludePaths.add( path );\n\t\t\t}\n\t\t}\n\t\treturn uselessIncludePaths;\n\t}",
        "filePathBefore": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/ConfiguredIndexSchemaNestingContext.java",
        "isPureRefactoring": true,
        "commitId": "8233e6e5e455472b841a7a2b0aee22089f1e2b43",
        "packageNameBefore": "org.hibernate.search.engine.mapper.mapping.building.impl",
        "classNameBefore": "org.hibernate.search.engine.mapper.mapping.building.impl.ConfiguredIndexSchemaNestingContext",
        "methodNameBefore": "org.hibernate.search.engine.mapper.mapping.building.impl.ConfiguredIndexSchemaNestingContext#getUselessIncludePaths",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.ConfiguredIndexSchemaNestingContext#getEncounteredFieldPaths\n methodBody: public Set<String> getEncounteredFieldPaths() {\nreturn filter.getEncounteredFieldPaths().keySet();\n}\nmethodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.IndexedEmbeddedBindingContextImpl#getEncounteredFieldPaths\n methodBody: public Set<String> getEncounteredFieldPaths() {\nreturn nestingContext.getEncounteredFieldPaths();\n}\nmethodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#getEncounteredFieldPaths\n methodBody: public Map<String, Boolean> getEncounteredFieldPaths() {\nreturn encounteredFieldPaths;\n}\nmethodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.PathFilter#getConfiguredIncludedPaths\n methodBody: Set<String> getConfiguredIncludedPaths() {\nreturn configuredIncludedPaths;\n}\nmethodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#getConfiguredIncludedPaths\n methodBody: public Set<String> getConfiguredIncludedPaths() {\nreturn pathFilter.getConfiguredIncludedPaths();\n}",
        "classSignatureBefore": "class ConfiguredIndexSchemaNestingContext implements IndexSchemaNestingContext ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.mapper.mapping.building.impl.ConfiguredIndexSchemaNestingContext#getUselessIncludePaths"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.mapper.mapping.building.impl.ConfiguredIndexSchemaNestingContext"
        ],
        "classSignatureBeforeSet": [
            "class ConfiguredIndexSchemaNestingContext implements IndexSchemaNestingContext "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.mapper.mapping.building.impl;\n\nimport java.util.LinkedHashSet;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.Set;\nimport java.util.function.BiFunction;\nimport java.util.function.Function;\n\nimport org.hibernate.search.engine.backend.document.model.dsl.impl.IndexSchemaNestingContext;\nimport org.hibernate.search.engine.mapper.model.spi.MappableTypeModel;\n\n\nclass ConfiguredIndexSchemaNestingContext implements IndexSchemaNestingContext {\n\n\tprivate static final ConfiguredIndexSchemaNestingContext ROOT =\n\t\t\tnew ConfiguredIndexSchemaNestingContext( IndexSchemaFilter.root(), \"\", \"\" );\n\n\tpublic static ConfiguredIndexSchemaNestingContext root() {\n\t\treturn ROOT;\n\t}\n\n\tprivate final IndexSchemaFilter filter;\n\tprivate final String prefixFromFilter;\n\tprivate final String unconsumedPrefix;\n\n\tprivate ConfiguredIndexSchemaNestingContext(IndexSchemaFilter filter, String prefixFromFilter,\n\t\t\tString unconsumedPrefix) {\n\t\tthis.filter = filter;\n\t\tthis.prefixFromFilter = prefixFromFilter;\n\t\tthis.unconsumedPrefix = unconsumedPrefix;\n\t}\n\n\t@Override\n\tpublic String toString() {\n\t\treturn new StringBuilder( getClass().getSimpleName() )\n\t\t\t\t.append( \"[\" )\n\t\t\t\t.append( \"filter=\" ).append( filter )\n\t\t\t\t.append( \",prefixFromFilter=\" ).append( prefixFromFilter )\n\t\t\t\t.append( \",unconsumedPrefix=\" ).append( unconsumedPrefix )\n\t\t\t\t.append( \"]\" )\n\t\t\t\t.toString();\n\t}\n\n\t@Override\n\tpublic <T> T nest(String relativeFieldName, Function<String, T> nestedElementFactoryIfIncluded,\n\t\t\tFunction<String, T> nestedElementFactoryIfExcluded) {\n\t\tString nameRelativeToFilter = prefixFromFilter + relativeFieldName;\n\t\tString prefixedRelativeName = unconsumedPrefix + relativeFieldName;\n\t\tif ( filter.isPathIncluded( nameRelativeToFilter ) ) {\n\t\t\treturn nestedElementFactoryIfIncluded.apply( prefixedRelativeName );\n\t\t}\n\t\telse {\n\t\t\treturn nestedElementFactoryIfExcluded.apply( prefixedRelativeName );\n\t\t}\n\t}\n\n\t@Override\n\tpublic <T> T nest(String relativeFieldName,\n\t\t\tBiFunction<String, IndexSchemaNestingContext, T> nestedElementFactoryIfIncluded,\n\t\t\tBiFunction<String, IndexSchemaNestingContext, T> nestedElementFactoryIfExcluded) {\n\t\tString nameRelativeToFilter = prefixFromFilter + relativeFieldName;\n\t\tString prefixedRelativeName = unconsumedPrefix + relativeFieldName;\n\t\tif ( filter.isPathIncluded( nameRelativeToFilter ) ) {\n\t\t\tConfiguredIndexSchemaNestingContext nestedFilter =\n\t\t\t\t\tnew ConfiguredIndexSchemaNestingContext( filter, nameRelativeToFilter + \".\", \"\" );\n\t\t\treturn nestedElementFactoryIfIncluded.apply( prefixedRelativeName, nestedFilter );\n\t\t}\n\t\telse {\n\t\t\treturn nestedElementFactoryIfExcluded.apply( prefixedRelativeName, IndexSchemaNestingContext.excludeAll() );\n\t\t}\n\t}\n\n\tpublic <T> Optional<T> addIndexedEmbeddedIfIncluded(\n\t\t\tMappableTypeModel parentTypeModel, String relativePrefix,\n\t\t\tInteger nestedMaxDepth, Set<String> nestedPathFilters,\n\t\t\tNestedContextBuilder<T> contextBuilder) {\n\t\tIndexSchemaFilter composedFilter = filter.compose(\n\t\t\t\tparentTypeModel, relativePrefix, nestedMaxDepth, nestedPathFilters\n\t\t);\n\t\tif ( !composedFilter.isEveryPathExcluded() ) {\n\t\t\tString prefixToParse = unconsumedPrefix + relativePrefix;\n\t\t\tint afterPreviousDotIndex = 0;\n\t\t\tint nextDotIndex = prefixToParse.indexOf( '.', afterPreviousDotIndex );\n\t\t\twhile ( nextDotIndex >= 0 ) {\n\t\t\t\tString objectName = prefixToParse.substring( afterPreviousDotIndex, nextDotIndex );\n\t\t\t\tcontextBuilder.appendObject( objectName );\n\n\t\t\t\t// Make sure to mark the paths as encountered in the filter\n\t\t\t\tString objectNameRelativeToFilter = prefixToParse.substring( 0, nextDotIndex );\n\t\t\t\t// We only use isPathIncluded for its side effect: it marks the path as encountered\n\t\t\t\tfilter.isPathIncluded( objectNameRelativeToFilter );\n\n\t\t\t\tafterPreviousDotIndex = nextDotIndex + 1;\n\t\t\t\tnextDotIndex = prefixToParse.indexOf( '.', afterPreviousDotIndex );\n\t\t\t}\n\t\t\tString unconsumedPrefix = prefixToParse.substring( afterPreviousDotIndex );\n\n\t\t\tConfiguredIndexSchemaNestingContext nestedContext =\n\t\t\t\t\tnew ConfiguredIndexSchemaNestingContext( composedFilter, \"\", unconsumedPrefix );\n\t\t\treturn Optional.of( contextBuilder.build( nestedContext ) );\n\t\t}\n\t\telse {\n\t\t\treturn Optional.empty();\n\t\t}\n\t}\n\n\tpublic Set<String> getUselessIncludePaths() {\n\t\tSet<String> includePaths = filter.getConfiguredIncludedPaths();\n\t\tMap<String, Boolean> encounteredFieldPaths = filter.getEncounteredFieldPaths();\n\t\tSet<String> uselessIncludePaths = new LinkedHashSet<>();\n\t\tfor ( String path : includePaths ) {\n\t\t\tBoolean included = encounteredFieldPaths.get( path );\n\t\t\tif ( included == null /* not encountered */ || !included ) {\n\t\t\t\t// An \"includePaths\" filter that does not result in inclusion is useless\n\t\t\t\tuselessIncludePaths.add( path );\n\t\t\t}\n\t\t}\n\t\treturn uselessIncludePaths;\n\t}\n\n\tpublic Set<String> getEncounteredFieldPaths() {\n\t\treturn filter.getEncounteredFieldPaths().keySet();\n\t}\n\n\tpublic interface NestedContextBuilder<T> {\n\n\t\tvoid appendObject(String objectName);\n\n\t\tT build(ConfiguredIndexSchemaNestingContext nestingContext);\n\n\t}\n}",
        "filePathAfter": "engine/src/main/java/org/hibernate/search/engine/mapper/mapping/building/impl/IndexedEmbeddedPathTracker.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.mapper.mapping.building.impl;\n\nimport java.util.LinkedHashMap;\nimport java.util.LinkedHashSet;\nimport java.util.Map;\nimport java.util.Set;\n\nimport org.hibernate.search.engine.mapper.mapping.building.spi.IndexedEmbeddedDefinition;\n\n/**\n * A tracker for paths actually affected by an indexed embedded definition.\n * <p>\n * Used to detect invalid configuration in an indexed embedded definition,\n * for example useless includePaths.\n */\nclass IndexedEmbeddedPathTracker {\n\n\tprivate final IndexedEmbeddedDefinition definition;\n\n\t/**\n\t * The {@code paths} that were encountered, i.e. passed to {@link IndexSchemaFilter#isPathIncluded(String)}\n\t * or to the same method of a child filter\n\t */\n\t// Use a LinkedHashSet, since the set will be exposed through a getter and may be iterated on\n\tprivate final Map<String, Boolean> encounteredFieldPaths = new LinkedHashMap<>();\n\n\tIndexedEmbeddedPathTracker(IndexedEmbeddedDefinition definition) {\n\t\tthis.definition = definition;\n\t}\n\n\tSet<String> getUselessIncludePaths() {\n\t\tSet<String> uselessIncludePaths = new LinkedHashSet<>();\n\t\tfor ( String path : definition.getIncludePaths() ) {\n\t\t\tBoolean included = encounteredFieldPaths.get( path );\n\t\t\tif ( included == null /* not encountered */ || !included ) {\n\t\t\t\t// An \"includePaths\" filter that does not result in inclusion is useless\n\t\t\t\tuselessIncludePaths.add( path );\n\t\t\t}\n\t\t}\n\t\treturn uselessIncludePaths;\n\t}\n\n\tSet<String> getEncounteredFieldPaths() {\n\t\treturn encounteredFieldPaths.keySet();\n\t}\n\n\tvoid markAsEncountered(String relativePath, boolean included) {\n\t\tencounteredFieldPaths.merge(\n\t\t\t\trelativePath, included,\n\t\t\t\t(included1, included2) -> included1 || included2\n\t\t);\n\t}\n}\n",
        "diffSourceCodeSet": [
            ""
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.ConfiguredIndexSchemaNestingContext#getEncounteredFieldPaths\n methodBody: public Set<String> getEncounteredFieldPaths() {\nreturn filter.getEncounteredFieldPaths().keySet();\n}",
            "methodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.IndexedEmbeddedBindingContextImpl#getEncounteredFieldPaths\n methodBody: public Set<String> getEncounteredFieldPaths() {\nreturn nestingContext.getEncounteredFieldPaths();\n}",
            "methodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#getEncounteredFieldPaths\n methodBody: public Map<String, Boolean> getEncounteredFieldPaths() {\nreturn encounteredFieldPaths;\n}",
            "methodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.PathFilter#getConfiguredIncludedPaths\n methodBody: Set<String> getConfiguredIncludedPaths() {\nreturn configuredIncludedPaths;\n}",
            "methodSignature: org.hibernate.search.engine.mapper.mapping.building.impl.IndexSchemaFilter#getConfiguredIncludedPaths\n methodBody: public Set<String> getConfiguredIncludedPaths() {\nreturn pathFilter.getConfiguredIncludedPaths();\n}"
        ],
        "sourceCodeAfterRefactoring": "Set<String> getUselessIncludePaths() {\n\t\tSet<String> uselessIncludePaths = new LinkedHashSet<>();\n\t\tfor ( String path : definition.getIncludePaths() ) {\n\t\t\tBoolean included = encounteredFieldPaths.get( path );\n\t\t\tif ( included == null /* not encountered */ || !included ) {\n\t\t\t\t// An \"includePaths\" filter that does not result in inclusion is useless\n\t\t\t\tuselessIncludePaths.add( path );\n\t\t\t}\n\t\t}\n\t\treturn uselessIncludePaths;\n\t}\n",
        "diffSourceCode": "-   37: \t\tthis.unconsumedPrefix = unconsumedPrefix;\n-   38: \t}\n-   39: \n-   40: \t@Override\n-   41: \tpublic String toString() {\n-   42: \t\treturn new StringBuilder( getClass().getSimpleName() )\n-   43: \t\t\t\t.append( \"[\" )\n-   44: \t\t\t\t.append( \"filter=\" ).append( filter )\n-   45: \t\t\t\t.append( \",prefixFromFilter=\" ).append( prefixFromFilter )\n-   46: \t\t\t\t.append( \",unconsumedPrefix=\" ).append( unconsumedPrefix )\n-   47: \t\t\t\t.append( \"]\" )\n-   85: \t\t\t\tparentTypeModel, relativePrefix, nestedMaxDepth, nestedPathFilters\n-   86: \t\t);\n-   87: \t\tif ( !composedFilter.isEveryPathExcluded() ) {\n-  114: \tpublic Set<String> getUselessIncludePaths() {\n-  115: \t\tSet<String> includePaths = filter.getConfiguredIncludedPaths();\n-  116: \t\tMap<String, Boolean> encounteredFieldPaths = filter.getEncounteredFieldPaths();\n-  117: \t\tSet<String> uselessIncludePaths = new LinkedHashSet<>();\n-  118: \t\tfor ( String path : includePaths ) {\n-  119: \t\t\tBoolean included = encounteredFieldPaths.get( path );\n-  120: \t\t\tif ( included == null /* not encountered */ || !included ) {\n-  121: \t\t\t\t// An \"includePaths\" filter that does not result in inclusion is useless\n-  122: \t\t\t\tuselessIncludePaths.add( path );\n-  123: \t\t\t}\n-  124: \t\t}\n-  125: \t\treturn uselessIncludePaths;\n-  126: \t}\n+   37: \tSet<String> getUselessIncludePaths() {\n+   38: \t\tSet<String> uselessIncludePaths = new LinkedHashSet<>();\n+   39: \t\tfor ( String path : definition.getIncludePaths() ) {\n+   40: \t\t\tBoolean included = encounteredFieldPaths.get( path );\n+   41: \t\t\tif ( included == null /* not encountered */ || !included ) {\n+   42: \t\t\t\t// An \"includePaths\" filter that does not result in inclusion is useless\n+   43: \t\t\t\tuselessIncludePaths.add( path );\n+   44: \t\t\t}\n+   45: \t\t}\n+   46: \t\treturn uselessIncludePaths;\n+   47: \t}\n",
        "uniqueId": "8233e6e5e455472b841a7a2b0aee22089f1e2b43_114_126_85_87_37_47",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 39
            },
            "BRANCH": {
                "missed": 0,
                "covered": 6
            },
            "LINE": {
                "missed": 0,
                "covered": 9
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 4
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tpublic add(key Key, count int) : void extracted from public increment(key Key) : void in class org.hibernate.search.util.impl.test.rule.StaticCounters",
        "diffLocations": [
            {
                "filePath": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/rule/StaticCounters.java",
                "startLine": 73,
                "endLine": 75,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/rule/StaticCounters.java",
                "startLine": 73,
                "endLine": 75,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/rule/StaticCounters.java",
                "startLine": 77,
                "endLine": 79,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public void increment(Key key) {\n\t\tcounters.merge( key, 1, (left, right) -> left + right );\n\t}",
        "filePathBefore": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/rule/StaticCounters.java",
        "isPureRefactoring": true,
        "commitId": "0b04d71653d50db1983e907e97b232199649b872",
        "packageNameBefore": "org.hibernate.search.util.impl.test.rule",
        "classNameBefore": "org.hibernate.search.util.impl.test.rule.StaticCounters",
        "methodNameBefore": "org.hibernate.search.util.impl.test.rule.StaticCounters#increment",
        "classSignatureBefore": "public final class StaticCounters implements TestRule ",
        "methodNameBeforeSet": [
            "org.hibernate.search.util.impl.test.rule.StaticCounters#increment"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.util.impl.test.rule.StaticCounters"
        ],
        "classSignatureBeforeSet": [
            "public final class StaticCounters implements TestRule "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics",
                "description": "All the mappings are matched! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.test.rule;\n\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\n\nimport org.junit.rules.TestRule;\nimport org.junit.runner.Description;\nimport org.junit.runners.model.Statement;\n\n/**\n * Rule useful when mocks are not an option, for instance because objects are instantiated using reflection.\n * This rule ensures static counters are set to zero before the test,\n * allows to increment them from static methods, and allows to check the counters from the rule itself.\n */\npublic final class StaticCounters implements TestRule {\n\n\tprivate static final StaticCounters DUMMY_INSTANCE = new StaticCounters();\n\tprivate static StaticCounters activeInstance = null;\n\n\tpublic static final class Key {\n\t\tprivate Key() {\n\t\t}\n\t}\n\n\tpublic static Key createKey() {\n\t\treturn new Key();\n\t}\n\n\t/**\n\t * @return A {@link StaticCounters} instance for use by stubs and mocks.\n\t * May be a dummy instance if no test is currently using static counters.\n\t */\n\tpublic static StaticCounters get() {\n\t\tif ( activeInstance != null ) {\n\t\t\treturn activeInstance;\n\t\t}\n\t\telse {\n\t\t\treturn DUMMY_INSTANCE;\n\t\t}\n\t}\n\n\tprivate final Map<Key, Integer> counters = new ConcurrentHashMap<>();\n\n\t@Override\n\tpublic Statement apply(Statement base, Description description) {\n\t\treturn new Statement() {\n\t\t\t@Override\n\t\t\tpublic void evaluate() throws Throwable {\n\t\t\t\tcounters.clear();\n\t\t\t\tif ( activeInstance != null ) {\n\t\t\t\t\tthrow new IllegalStateException( \"Using StaticCounters twice in a single test is forbidden.\"\n\t\t\t\t\t\t\t+ \" Make sure you added one (and only one)\"\n\t\t\t\t\t\t\t+ \" '@Rule public StaticCounter counters = new StaticCounters()' to your test.\" );\n\t\t\t\t}\n\t\t\t\tactiveInstance = StaticCounters.this;\n\t\t\t\ttry {\n\t\t\t\t\tbase.evaluate();\n\t\t\t\t}\n\t\t\t\tfinally {\n\t\t\t\t\tactiveInstance = null;\n\t\t\t\t\tcounters.clear();\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t}\n\n\tpublic void increment(Key key) {\n\t\tcounters.merge( key, 1, (left, right) -> left + right );\n\t}\n\n\tpublic int get(Key key) {\n\t\tif ( activeInstance == null ) {\n\t\t\tthrow new IllegalStateException( \"Checking StaticCounters outside of a test is forbidden.\"\n\t\t\t\t\t+ \" Make sure you added one (and only one)\"\n\t\t\t\t\t+ \" '@Rule public StaticCounter counters = new StaticCounters()' to your test.\" );\n\t\t}\n\t\treturn counters.computeIfAbsent( key, ignored -> 0 );\n\t}\n}\n",
        "filePathAfter": "util/internal/test/src/main/java/org/hibernate/search/util/impl/test/rule/StaticCounters.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.test.rule;\n\nimport java.util.Map;\nimport java.util.concurrent.ConcurrentHashMap;\n\nimport org.junit.rules.TestRule;\nimport org.junit.runner.Description;\nimport org.junit.runners.model.Statement;\n\n/**\n * Rule useful when mocks are not an option, for instance because objects are instantiated using reflection.\n * This rule ensures static counters are set to zero before the test,\n * allows to increment them from static methods, and allows to check the counters from the rule itself.\n */\npublic final class StaticCounters implements TestRule {\n\n\tprivate static final StaticCounters DUMMY_INSTANCE = new StaticCounters();\n\tprivate static StaticCounters activeInstance = null;\n\n\tpublic static final class Key {\n\t\tprivate Key() {\n\t\t}\n\t}\n\n\tpublic static Key createKey() {\n\t\treturn new Key();\n\t}\n\n\t/**\n\t * @return A {@link StaticCounters} instance for use by stubs and mocks.\n\t * May be a dummy instance if no test is currently using static counters.\n\t */\n\tpublic static StaticCounters get() {\n\t\tif ( activeInstance != null ) {\n\t\t\treturn activeInstance;\n\t\t}\n\t\telse {\n\t\t\treturn DUMMY_INSTANCE;\n\t\t}\n\t}\n\n\tprivate final Map<Key, Integer> counters = new ConcurrentHashMap<>();\n\n\t@Override\n\tpublic Statement apply(Statement base, Description description) {\n\t\treturn new Statement() {\n\t\t\t@Override\n\t\t\tpublic void evaluate() throws Throwable {\n\t\t\t\tcounters.clear();\n\t\t\t\tif ( activeInstance != null ) {\n\t\t\t\t\tthrow new IllegalStateException( \"Using StaticCounters twice in a single test is forbidden.\"\n\t\t\t\t\t\t\t+ \" Make sure you added one (and only one)\"\n\t\t\t\t\t\t\t+ \" '@Rule public StaticCounter counters = new StaticCounters()' to your test.\" );\n\t\t\t\t}\n\t\t\t\tactiveInstance = StaticCounters.this;\n\t\t\t\ttry {\n\t\t\t\t\tbase.evaluate();\n\t\t\t\t}\n\t\t\t\tfinally {\n\t\t\t\t\tactiveInstance = null;\n\t\t\t\t\tcounters.clear();\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\t}\n\n\tpublic void increment(Key key) {\n\t\tadd( key, 1 );\n\t}\n\n\tpublic void add(Key key, int count) {\n\t\tcounters.merge( key, count, (left, right) -> left + right );\n\t}\n\n\tpublic int get(Key key) {\n\t\tif ( activeInstance == null ) {\n\t\t\tthrow new IllegalStateException( \"Checking StaticCounters outside of a test is forbidden.\"\n\t\t\t\t\t+ \" Make sure you added one (and only one)\"\n\t\t\t\t\t+ \" '@Rule public StaticCounter counters = new StaticCounters()' to your test.\" );\n\t\t}\n\t\treturn counters.computeIfAbsent( key, ignored -> 0 );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "public void add(Key key, int count) {\n\t\tcounters.merge( key, count, (left, right) -> left + right );\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "public void increment(Key key) {\n\t\tadd( key, 1 );\n\t}\npublic void add(Key key, int count) {\n\t\tcounters.merge( key, count, (left, right) -> left + right );\n\t}",
        "diffSourceCode": "    73: \tpublic void increment(Key key) {\n-   74: \t\tcounters.merge( key, 1, (left, right) -> left + right );\n+   74: \t\tadd( key, 1 );\n    75: \t}\n-   77: \tpublic int get(Key key) {\n-   78: \t\tif ( activeInstance == null ) {\n-   79: \t\t\tthrow new IllegalStateException( \"Checking StaticCounters outside of a test is forbidden.\"\n+   77: \tpublic void add(Key key, int count) {\n+   78: \t\tcounters.merge( key, count, (left, right) -> left + right );\n+   79: \t}\n",
        "uniqueId": "0b04d71653d50db1983e907e97b232199649b872_73_75_77_79_73_75",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tpublic getIndexManagerBuildingState(backendName Optional<String>, indexName String, multiTenancyEnabled boolean) : IndexManagerBuildingState<?> extracted from public success() : void in class org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolderTest & moved to class org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolder",
        "diffLocations": [
            {
                "filePath": "engine/src/test/java/org/hibernate/search/engine/common/impl/IndexManagerBuildingStateHolderTest.java",
                "startLine": 53,
                "endLine": 128,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/test/java/org/hibernate/search/engine/common/impl/IndexManagerBuildingStateHolderTest.java",
                "startLine": 53,
                "endLine": 123,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/test/java/org/hibernate/search/engine/common/impl/IndexManagerBuildingStateHolderTest.java",
                "startLine": 91,
                "endLine": 96,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void success() {\n\t\tBackendFactory backendFactoryMock = createMock( BackendFactory.class );\n\t\tBackendImplementor<DocumentElement> backendMock = createMock( BackendImplementor.class );\n\t\tIndexManagerBuilder<DocumentElement> indexManagerBuilderMock = createMock( IndexManagerBuilder.class );\n\t\tIndexSchemaRootNodeBuilder indexSchemaRootNodeBuilderMock = createMock( IndexSchemaRootNodeBuilder.class );\n\n\t\tCapture<ConfigurationPropertySource> backendPropertySourceCapture = Capture.newInstance();\n\t\tCapture<ConfigurationPropertySource> indexPropertySourceCapture = Capture.newInstance();\n\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.type\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"someBackendType\" ) );\n\t\tEasyMock.expect( beanResolverMock.resolve( BackendFactory.class, \"someBackendType\" ) )\n\t\t\t\t.andReturn( BeanHolder.of( backendFactoryMock ) );\n\t\tEasyMock.expect( backendFactoryMock.create(\n\t\t\t\tEasyMock.eq( \"myBackend\" ),\n\t\t\t\tEasyMock.anyObject(),\n\t\t\t\tEasyMock.capture( backendPropertySourceCapture )\n\t\t) )\n\t\t\t\t.andReturn( (BackendImplementor) backendMock );\n\t\treplayAll();\n\t\tholder.createBackends( CollectionHelper.asSet( \"myBackend\" ) );\n\t\tverifyAll();\n\n\t\tresetAll();\n\t\treplayAll();\n\t\tIndexManagerBuildingStateHolder.BackendInitialBuildState<?> backend = holder.getBackend( \"myBackend\" );\n\t\tverifyAll();\n\n\t\tresetAll();\n\t\tEasyMock.expect( backendMock.createIndexManagerBuilder(\n\t\t\t\tEasyMock.eq( \"myIndex\" ),\n\t\t\t\tEasyMock.eq( false ),\n\t\t\t\tEasyMock.anyObject(),\n\t\t\t\tEasyMock.capture( indexPropertySourceCapture )\n\t\t) )\n\t\t\t\t.andReturn( (IndexManagerBuilder) indexManagerBuilderMock );\n\t\tEasyMock.expect( indexManagerBuilderMock.getSchemaRootNodeBuilder() )\n\t\t\t\t.andStubReturn( indexSchemaRootNodeBuilderMock );\n\t\treplayAll();\n\t\tbackend.getIndexManagerBuildingState( \"myIndex\", false );\n\t\tverifyAll();\n\n\t\t// Check that configuration property sources behave as expected\n\t\tOptional result;\n\n\t\t// Backend configuration\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.foo\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n\t\treplayAll();\n\t\tresult = backendPropertySourceCapture.getValue().get( \"foo\" );\n\t\tverifyAll();\n\t\tassertThat( result ).contains( \"bar\" );\n\n\t\t// Index configuration\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.indexes.myIndex.foo\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n\t\treplayAll();\n\t\tresult = indexPropertySourceCapture.getValue().get( \"foo\" );\n\t\tverifyAll();\n\t\tassertThat( result ).contains( \"bar\" );\n\n\t\t// Index configuration defaults\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.indexes.myIndex.foo\" ) )\n\t\t\t\t.andReturn( Optional.empty() );\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.index_defaults.foo\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n\t\treplayAll();\n\t\tresult = indexPropertySourceCapture.getValue().get( \"foo\" );\n\t\tverifyAll();\n\t\tassertThat( result ).contains( \"bar\" );\n\t}",
        "filePathBefore": "engine/src/test/java/org/hibernate/search/engine/common/impl/IndexManagerBuildingStateHolderTest.java",
        "isPureRefactoring": true,
        "commitId": "be929dfca72583e5cef800af2f97727e75184a76",
        "packageNameBefore": "org.hibernate.search.engine.common.impl",
        "classNameBefore": "org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolderTest",
        "methodNameBefore": "org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolderTest#success",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.common.impl.SearchIntegrationBuilderImpl.MappingBuildingState.TypeMetadataContributorProviderImpl#get\n methodBody: public Set<C> get(MappableTypeModel typeModel) {\nreturn typeModel.getDescendingSuperTypes().map(MappingBuildingState.this::getContributionIncludingAutomaticallyDiscovered).filter(Objects::nonNull).flatMap(TypeMappingContribution::getContributors).collect(Collectors.toCollection(LinkedHashSet::new));\n}\nmethodSignature: org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolder#createBackends\n methodBody: void createBackends(Set<String> backendNames) {\nif(backendNames.contains(\"\") || backendNames.contains(null)){backendNames.remove(\"\");\nbackendNames.remove(null);\nbackendNames.add(getDefaultBackendName());\n}for(String backendName: backendNames){BackendInitialBuildState<?> backendBuildState;\ntrybackendBuildState=createBackend(backendName);\ncatch(RuntimeException e)rootBuildContext.getFailureCollector().withContext(EventContexts.fromBackendName(backendName)).add(e);\ncontinue;\nbackendBuildStateByName.put(backendName,backendBuildState);\n}}\nmethodSignature: org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolder.BackendInitialBuildState#getIndexManagerBuildingState\n methodBody: IndexManagerInitialBuildState<?> getIndexManagerBuildingState(\n\t\t\t\tString indexName, boolean multiTenancyEnabled) {\nIndexManagerInitialBuildState<?> state=indexManagerBuildStateByName.get(indexName);\nif(state == null){ConfigurationPropertySource indexPropertySource=EngineConfigurationUtils.getIndex(backendPropertySource,defaultIndexPropertySource,indexName);\nIndexManagerBuilder<D> builder=backend.createIndexManagerBuilder(indexName,multiTenancyEnabled,backendBuildContext,indexPropertySource);\nIndexSchemaRootNodeBuilder schemaRootNodeBuilder=builder.getSchemaRootNodeBuilder();\nIndexedEntityBindingContext bindingContext=new IndexedEntityBindingContextImpl(schemaRootNodeBuilder);\nstate=new IndexManagerInitialBuildState<>(backendName,indexName,builder,bindingContext);\nindexManagerBuildStateByName.put(indexName,state);\n}return state;\n}\nmethodSignature: org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolder#getBackend\n methodBody: BackendInitialBuildState<?> getBackend(String backendName) {\nif(StringHelper.isEmpty(backendName)){backendName=getDefaultBackendName();\n}BackendInitialBuildState<?> backendBuildState=backendBuildStateByName.get(backendName);\nif(backendBuildState == null){throw new AssertionFailure(\"Mapper asking for a reference to backend '\" + backendName + \"', which was not declared in advance.\" + \" There is a bug in Hibernate Search, please report it.\");\n}return backendBuildState;\n}",
        "classSignatureBefore": "public class IndexManagerBuildingStateHolderTest extends EasyMockSupport ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolderTest#success"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolderTest"
        ],
        "classSignatureBeforeSet": [
            "public class IndexManagerBuildingStateHolderTest extends EasyMockSupport "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics\nOverlapped refactoring - can be identical by undoing the overlapped refactoring\n- Encapsulate Opposite-",
                "description": "Getter method got replaced with direct access or vice verca - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.common.impl;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport java.util.Optional;\n\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.model.dsl.spi.IndexSchemaRootNodeBuilder;\nimport org.hibernate.search.engine.backend.index.spi.IndexManagerBuilder;\nimport org.hibernate.search.engine.backend.spi.BackendFactory;\nimport org.hibernate.search.engine.backend.spi.BackendImplementor;\nimport org.hibernate.search.engine.cfg.spi.ConfigurationPropertySource;\nimport org.hibernate.search.engine.environment.bean.BeanHolder;\nimport org.hibernate.search.engine.environment.bean.BeanResolver;\nimport org.hibernate.search.engine.reporting.spi.ContextualFailureCollector;\nimport org.hibernate.search.engine.reporting.spi.EventContexts;\nimport org.hibernate.search.engine.reporting.spi.FailureCollector;\nimport org.hibernate.search.engine.testsupport.util.AbstractBeanResolverPartialMock;\nimport org.hibernate.search.engine.testsupport.util.AbstractConfigurationPropertySourcePartialMock;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.common.impl.CollectionHelper;\n\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\n\nimport org.easymock.Capture;\nimport org.easymock.EasyMock;\nimport org.easymock.EasyMockSupport;\n\n// We have to use raw types to mock methods returning generic types with wildcards\n@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\npublic class IndexManagerBuildingStateHolderTest extends EasyMockSupport {\n\n\t@Rule\n\tpublic final ExpectedException thrown = ExpectedException.none();\n\n\tprivate RootBuildContext rootBuildContextMock = createMock( RootBuildContext.class );\n\tprivate ConfigurationPropertySource configurationSourceMock =\n\t\t\tpartialMockBuilder( AbstractConfigurationPropertySourcePartialMock.class ).mock();\n\tprivate BeanResolver beanResolverMock =\n\t\t\tpartialMockBuilder( AbstractBeanResolverPartialMock.class ).mock();\n\n\tprivate IndexManagerBuildingStateHolder holder =\n\t\t\tnew IndexManagerBuildingStateHolder( beanResolverMock, configurationSourceMock, rootBuildContextMock );\n\n\t@Test\n\tpublic void success() {\n\t\tBackendFactory backendFactoryMock = createMock( BackendFactory.class );\n\t\tBackendImplementor<DocumentElement> backendMock = createMock( BackendImplementor.class );\n\t\tIndexManagerBuilder<DocumentElement> indexManagerBuilderMock = createMock( IndexManagerBuilder.class );\n\t\tIndexSchemaRootNodeBuilder indexSchemaRootNodeBuilderMock = createMock( IndexSchemaRootNodeBuilder.class );\n\n\t\tCapture<ConfigurationPropertySource> backendPropertySourceCapture = Capture.newInstance();\n\t\tCapture<ConfigurationPropertySource> indexPropertySourceCapture = Capture.newInstance();\n\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.type\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"someBackendType\" ) );\n\t\tEasyMock.expect( beanResolverMock.resolve( BackendFactory.class, \"someBackendType\" ) )\n\t\t\t\t.andReturn( BeanHolder.of( backendFactoryMock ) );\n\t\tEasyMock.expect( backendFactoryMock.create(\n\t\t\t\tEasyMock.eq( \"myBackend\" ),\n\t\t\t\tEasyMock.anyObject(),\n\t\t\t\tEasyMock.capture( backendPropertySourceCapture )\n\t\t) )\n\t\t\t\t.andReturn( (BackendImplementor) backendMock );\n\t\treplayAll();\n\t\tholder.createBackends( CollectionHelper.asSet( \"myBackend\" ) );\n\t\tverifyAll();\n\n\t\tresetAll();\n\t\treplayAll();\n\t\tIndexManagerBuildingStateHolder.BackendInitialBuildState<?> backend = holder.getBackend( \"myBackend\" );\n\t\tverifyAll();\n\n\t\tresetAll();\n\t\tEasyMock.expect( backendMock.createIndexManagerBuilder(\n\t\t\t\tEasyMock.eq( \"myIndex\" ),\n\t\t\t\tEasyMock.eq( false ),\n\t\t\t\tEasyMock.anyObject(),\n\t\t\t\tEasyMock.capture( indexPropertySourceCapture )\n\t\t) )\n\t\t\t\t.andReturn( (IndexManagerBuilder) indexManagerBuilderMock );\n\t\tEasyMock.expect( indexManagerBuilderMock.getSchemaRootNodeBuilder() )\n\t\t\t\t.andStubReturn( indexSchemaRootNodeBuilderMock );\n\t\treplayAll();\n\t\tbackend.getIndexManagerBuildingState( \"myIndex\", false );\n\t\tverifyAll();\n\n\t\t// Check that configuration property sources behave as expected\n\t\tOptional result;\n\n\t\t// Backend configuration\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.foo\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n\t\treplayAll();\n\t\tresult = backendPropertySourceCapture.getValue().get( \"foo\" );\n\t\tverifyAll();\n\t\tassertThat( result ).contains( \"bar\" );\n\n\t\t// Index configuration\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.indexes.myIndex.foo\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n\t\treplayAll();\n\t\tresult = indexPropertySourceCapture.getValue().get( \"foo\" );\n\t\tverifyAll();\n\t\tassertThat( result ).contains( \"bar\" );\n\n\t\t// Index configuration defaults\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.indexes.myIndex.foo\" ) )\n\t\t\t\t.andReturn( Optional.empty() );\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.index_defaults.foo\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n\t\treplayAll();\n\t\tresult = indexPropertySourceCapture.getValue().get( \"foo\" );\n\t\tverifyAll();\n\t\tassertThat( result ).contains( \"bar\" );\n\t}\n\n\t@Test\n\tpublic void error_missingBackend_nullName() {\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.type\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"someBackendType\" ) );\n\t\tString keyPrefix = \"somePrefix.\";\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"default_backend\" ) )\n\t\t\t\t.andReturn( Optional.empty() );\n\t\tEasyMock.expect( configurationSourceMock.resolve( \"default_backend\" ) )\n\t\t\t\t.andReturn( Optional.of( keyPrefix + \"default_backend\" ) );\n\t\treplayAll();\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"The name of the default backend is not set\" );\n\t\tthrown.expectMessage( \"Set it through the configuration property 'somePrefix.default_backend'\" );\n\t\tthrown.expectMessage( \"or set the backend name explicitly for each indexed type in your mapping\" );\n\t\tholder.createBackends( CollectionHelper.asSet( (String) null ) );\n\t\tverifyAll();\n\t}\n\n\t@Test\n\tpublic void error_missingIndexBackend_emptyName() {\n\t\tString keyPrefix = \"somePrefix.\";\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"indexes.indexName.backend\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"\" ) );\n\t\tEasyMock.expect( configurationSourceMock.resolve( \"indexes.indexName.backend\" ) )\n\t\t\t\t.andReturn( Optional.of( keyPrefix + \"indexes.indexName.backend\" ) );\n\t\tEasyMock.expect( configurationSourceMock.get( \"default_backend\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"\" ) );\n\t\tEasyMock.expect( configurationSourceMock.resolve( \"default_backend\" ) )\n\t\t\t\t.andReturn( Optional.of( keyPrefix + \"default_backend\" ) );\n\t\treplayAll();\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"The name of the default backend is not set\" );\n\t\tthrown.expectMessage( \"Set it through the configuration property 'somePrefix.default_backend'\" );\n\t\tthrown.expectMessage( \"or set the backend name explicitly for each indexed type in your mapping\" );\n\t\tholder.createBackends( CollectionHelper.asSet( \"\" ) );\n\t\tverifyAll();\n\t}\n\n\t@Test\n\tpublic void error_missingBackendType_nullType() {\n\t\tString keyPrefix = \"somePrefix.\";\n\n\t\tFailureCollector rootFailureCollectorMock = createMock( FailureCollector.class );\n\t\tContextualFailureCollector backendFailureCollectorMock = createMock( ContextualFailureCollector.class );\n\n\t\tCapture<Throwable> throwableCapture = Capture.newInstance();\n\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.backendName.type\" ) )\n\t\t\t\t.andReturn( Optional.empty() );\n\t\tEasyMock.expect( configurationSourceMock.resolve( \"backends.backendName.type\" ) )\n\t\t\t\t.andReturn( Optional.of( keyPrefix + \"backends.backendName.type\" ) );\n\t\tEasyMock.expect( rootBuildContextMock.getFailureCollector() )\n\t\t\t\t.andReturn( rootFailureCollectorMock );\n\t\tEasyMock.expect( rootFailureCollectorMock.withContext( EventContexts.fromBackendName( \"backendName\" ) ) )\n\t\t\t\t.andReturn( backendFailureCollectorMock );\n\t\tbackendFailureCollectorMock.add( EasyMock.capture( throwableCapture ) );\n\t\treplayAll();\n\t\tholder.createBackends( CollectionHelper.asSet( \"backendName\" ) );\n\t\tverifyAll();\n\n\t\tassertThat( throwableCapture.getValue() )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Missing backend type for backend 'backendName'\" )\n\t\t\t\t.hasMessageContaining( \"Set the property 'somePrefix.backends.backendName.type' to a supported value\" );\n\t}\n\n\t@Test\n\tpublic void error_missingBackendType_emptyType() {\n\t\tString keyPrefix = \"somePrefix.\";\n\n\t\tFailureCollector rootFailureCollectorMock = createMock( FailureCollector.class );\n\t\tContextualFailureCollector backendFailureCollectorMock = createMock( ContextualFailureCollector.class );\n\n\t\tCapture<Throwable> throwableCapture = Capture.newInstance();\n\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.backendName.type\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"\" ) );\n\t\tEasyMock.expect( configurationSourceMock.resolve( \"backends.backendName.type\" ) )\n\t\t\t\t.andReturn( Optional.of( keyPrefix + \"backends.backendName.type\" ) );\n\t\tEasyMock.expect( rootBuildContextMock.getFailureCollector() )\n\t\t\t\t.andReturn( rootFailureCollectorMock );\n\t\tEasyMock.expect( rootFailureCollectorMock.withContext( EventContexts.fromBackendName( \"backendName\" ) ) )\n\t\t\t\t.andReturn( backendFailureCollectorMock );\n\t\tbackendFailureCollectorMock.add( EasyMock.capture( throwableCapture ) );\n\t\treplayAll();\n\t\tholder.createBackends( CollectionHelper.asSet( \"backendName\" ) );\n\t\tverifyAll();\n\n\t\tassertThat( throwableCapture.getValue() )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Missing backend type for backend 'backendName'\" )\n\t\t\t\t.hasMessageContaining( \"Set the property 'somePrefix.backends.backendName.type' to a supported value\" );\n\t}\n\n}\n",
        "filePathAfter": "engine/src/test/java/org/hibernate/search/engine/common/impl/IndexManagerBuildingStateHolderTest.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.common.impl;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport java.util.Optional;\n\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.model.dsl.spi.IndexSchemaRootNodeBuilder;\nimport org.hibernate.search.engine.backend.index.spi.IndexManagerBuilder;\nimport org.hibernate.search.engine.backend.spi.BackendFactory;\nimport org.hibernate.search.engine.backend.spi.BackendImplementor;\nimport org.hibernate.search.engine.cfg.spi.ConfigurationPropertySource;\nimport org.hibernate.search.engine.environment.bean.BeanHolder;\nimport org.hibernate.search.engine.environment.bean.BeanResolver;\nimport org.hibernate.search.engine.reporting.spi.ContextualFailureCollector;\nimport org.hibernate.search.engine.reporting.spi.EventContexts;\nimport org.hibernate.search.engine.reporting.spi.FailureCollector;\nimport org.hibernate.search.engine.testsupport.util.AbstractBeanResolverPartialMock;\nimport org.hibernate.search.engine.testsupport.util.AbstractConfigurationPropertySourcePartialMock;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.common.impl.CollectionHelper;\n\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\n\nimport org.easymock.Capture;\nimport org.easymock.EasyMock;\nimport org.easymock.EasyMockSupport;\n\n// We have to use raw types to mock methods returning generic types with wildcards\n@SuppressWarnings({ \"unchecked\", \"rawtypes\" })\npublic class IndexManagerBuildingStateHolderTest extends EasyMockSupport {\n\n\t@Rule\n\tpublic final ExpectedException thrown = ExpectedException.none();\n\n\tprivate RootBuildContext rootBuildContextMock = createMock( RootBuildContext.class );\n\tprivate ConfigurationPropertySource configurationSourceMock =\n\t\t\tpartialMockBuilder( AbstractConfigurationPropertySourcePartialMock.class ).mock();\n\tprivate BeanResolver beanResolverMock =\n\t\t\tpartialMockBuilder( AbstractBeanResolverPartialMock.class ).mock();\n\n\tprivate IndexManagerBuildingStateHolder holder =\n\t\t\tnew IndexManagerBuildingStateHolder( beanResolverMock, configurationSourceMock, rootBuildContextMock );\n\n\t@Test\n\tpublic void success() {\n\t\tBackendFactory backendFactoryMock = createMock( BackendFactory.class );\n\t\tBackendImplementor<DocumentElement> backendMock = createMock( BackendImplementor.class );\n\t\tIndexManagerBuilder<DocumentElement> indexManagerBuilderMock = createMock( IndexManagerBuilder.class );\n\t\tIndexSchemaRootNodeBuilder indexSchemaRootNodeBuilderMock = createMock( IndexSchemaRootNodeBuilder.class );\n\n\t\tCapture<ConfigurationPropertySource> backendPropertySourceCapture = Capture.newInstance();\n\t\tCapture<ConfigurationPropertySource> indexPropertySourceCapture = Capture.newInstance();\n\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.type\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"someBackendType\" ) );\n\t\tEasyMock.expect( beanResolverMock.resolve( BackendFactory.class, \"someBackendType\" ) )\n\t\t\t\t.andReturn( BeanHolder.of( backendFactoryMock ) );\n\t\tEasyMock.expect( backendFactoryMock.create(\n\t\t\t\tEasyMock.eq( \"myBackend\" ),\n\t\t\t\tEasyMock.anyObject(),\n\t\t\t\tEasyMock.capture( backendPropertySourceCapture )\n\t\t) )\n\t\t\t\t.andReturn( (BackendImplementor) backendMock );\n\t\treplayAll();\n\t\tholder.createBackends( CollectionHelper.asSet( Optional.of( \"myBackend\" ) ) );\n\t\tverifyAll();\n\n\t\tresetAll();\n\t\tEasyMock.expect( backendMock.createIndexManagerBuilder(\n\t\t\t\tEasyMock.eq( \"myIndex\" ),\n\t\t\t\tEasyMock.eq( false ),\n\t\t\t\tEasyMock.anyObject(),\n\t\t\t\tEasyMock.capture( indexPropertySourceCapture )\n\t\t) )\n\t\t\t\t.andReturn( (IndexManagerBuilder) indexManagerBuilderMock );\n\t\tEasyMock.expect( indexManagerBuilderMock.getSchemaRootNodeBuilder() )\n\t\t\t\t.andStubReturn( indexSchemaRootNodeBuilderMock );\n\t\treplayAll();\n\t\tholder.getIndexManagerBuildingState( Optional.of( \"myBackend\" ), \"myIndex\", false );\n\t\tverifyAll();\n\n\t\t// Check that configuration property sources behave as expected\n\t\tOptional result;\n\n\t\t// Backend configuration\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.foo\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n\t\treplayAll();\n\t\tresult = backendPropertySourceCapture.getValue().get( \"foo\" );\n\t\tverifyAll();\n\t\tassertThat( result ).contains( \"bar\" );\n\n\t\t// Index configuration\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.indexes.myIndex.foo\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n\t\treplayAll();\n\t\tresult = indexPropertySourceCapture.getValue().get( \"foo\" );\n\t\tverifyAll();\n\t\tassertThat( result ).contains( \"bar\" );\n\n\t\t// Index configuration defaults\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.indexes.myIndex.foo\" ) )\n\t\t\t\t.andReturn( Optional.empty() );\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.index_defaults.foo\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n\t\treplayAll();\n\t\tresult = indexPropertySourceCapture.getValue().get( \"foo\" );\n\t\tverifyAll();\n\t\tassertThat( result ).contains( \"bar\" );\n\t}\n\n\t@Test\n\tpublic void error_missingBackend_emptyName() {\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.type\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"someBackendType\" ) );\n\t\tString keyPrefix = \"somePrefix.\";\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"default_backend\" ) )\n\t\t\t\t.andReturn( Optional.empty() );\n\t\tEasyMock.expect( configurationSourceMock.resolve( \"default_backend\" ) )\n\t\t\t\t.andReturn( Optional.of( keyPrefix + \"default_backend\" ) );\n\t\treplayAll();\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"The name of the default backend is not set\" );\n\t\tthrown.expectMessage( \"Set it through the configuration property 'somePrefix.default_backend'\" );\n\t\tthrown.expectMessage( \"or set the backend name explicitly for each indexed type in your mapping\" );\n\t\tholder.createBackends( CollectionHelper.asSet( Optional.empty() ) );\n\t\tverifyAll();\n\t}\n\n\t@Test\n\tpublic void error_missingBackendType_nullType() {\n\t\tString keyPrefix = \"somePrefix.\";\n\n\t\tFailureCollector rootFailureCollectorMock = createMock( FailureCollector.class );\n\t\tContextualFailureCollector backendFailureCollectorMock = createMock( ContextualFailureCollector.class );\n\n\t\tCapture<Throwable> throwableCapture = Capture.newInstance();\n\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.backendName.type\" ) )\n\t\t\t\t.andReturn( Optional.empty() );\n\t\tEasyMock.expect( configurationSourceMock.resolve( \"backends.backendName.type\" ) )\n\t\t\t\t.andReturn( Optional.of( keyPrefix + \"backends.backendName.type\" ) );\n\t\tEasyMock.expect( rootBuildContextMock.getFailureCollector() )\n\t\t\t\t.andReturn( rootFailureCollectorMock );\n\t\tEasyMock.expect( rootFailureCollectorMock.withContext( EventContexts.fromBackendName( \"backendName\" ) ) )\n\t\t\t\t.andReturn( backendFailureCollectorMock );\n\t\tbackendFailureCollectorMock.add( EasyMock.capture( throwableCapture ) );\n\t\treplayAll();\n\t\tholder.createBackends( CollectionHelper.asSet( Optional.of( \"backendName\" ) ) );\n\t\tverifyAll();\n\n\t\tassertThat( throwableCapture.getValue() )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Missing backend type for backend 'backendName'\" )\n\t\t\t\t.hasMessageContaining( \"Set the property 'somePrefix.backends.backendName.type' to a supported value\" );\n\t}\n\n\t@Test\n\tpublic void error_missingBackendType_emptyType() {\n\t\tString keyPrefix = \"somePrefix.\";\n\n\t\tFailureCollector rootFailureCollectorMock = createMock( FailureCollector.class );\n\t\tContextualFailureCollector backendFailureCollectorMock = createMock( ContextualFailureCollector.class );\n\n\t\tCapture<Throwable> throwableCapture = Capture.newInstance();\n\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.backendName.type\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"\" ) );\n\t\tEasyMock.expect( configurationSourceMock.resolve( \"backends.backendName.type\" ) )\n\t\t\t\t.andReturn( Optional.of( keyPrefix + \"backends.backendName.type\" ) );\n\t\tEasyMock.expect( rootBuildContextMock.getFailureCollector() )\n\t\t\t\t.andReturn( rootFailureCollectorMock );\n\t\tEasyMock.expect( rootFailureCollectorMock.withContext( EventContexts.fromBackendName( \"backendName\" ) ) )\n\t\t\t\t.andReturn( backendFailureCollectorMock );\n\t\tbackendFailureCollectorMock.add( EasyMock.capture( throwableCapture ) );\n\t\treplayAll();\n\t\tholder.createBackends( CollectionHelper.asSet( Optional.of( \"backendName\" ) ) );\n\t\tverifyAll();\n\n\t\tassertThat( throwableCapture.getValue() )\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Missing backend type for backend 'backendName'\" )\n\t\t\t\t.hasMessageContaining( \"Set the property 'somePrefix.backends.backendName.type' to a supported value\" );\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "// Check that configuration property sources behave as expected\n\t\tOptional result;\n\n\t\t// Backend configuration\n\t\tresetAll();"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.common.impl.SearchIntegrationBuilderImpl.MappingBuildingState.TypeMetadataContributorProviderImpl#get\n methodBody: public Set<C> get(MappableTypeModel typeModel) {\nreturn typeModel.getDescendingSuperTypes().map(MappingBuildingState.this::getContributionIncludingAutomaticallyDiscovered).filter(Objects::nonNull).flatMap(TypeMappingContribution::getContributors).collect(Collectors.toCollection(LinkedHashSet::new));\n}",
            "methodSignature: org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolder#createBackends\n methodBody: void createBackends(Set<String> backendNames) {\nif(backendNames.contains(\"\") || backendNames.contains(null)){backendNames.remove(\"\");\nbackendNames.remove(null);\nbackendNames.add(getDefaultBackendName());\n}for(String backendName: backendNames){BackendInitialBuildState<?> backendBuildState;\ntrybackendBuildState=createBackend(backendName);\ncatch(RuntimeException e)rootBuildContext.getFailureCollector().withContext(EventContexts.fromBackendName(backendName)).add(e);\ncontinue;\nbackendBuildStateByName.put(backendName,backendBuildState);\n}}",
            "methodSignature: org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolder.BackendInitialBuildState#getIndexManagerBuildingState\n methodBody: IndexManagerInitialBuildState<?> getIndexManagerBuildingState(\n\t\t\t\tString indexName, boolean multiTenancyEnabled) {\nIndexManagerInitialBuildState<?> state=indexManagerBuildStateByName.get(indexName);\nif(state == null){ConfigurationPropertySource indexPropertySource=EngineConfigurationUtils.getIndex(backendPropertySource,defaultIndexPropertySource,indexName);\nIndexManagerBuilder<D> builder=backend.createIndexManagerBuilder(indexName,multiTenancyEnabled,backendBuildContext,indexPropertySource);\nIndexSchemaRootNodeBuilder schemaRootNodeBuilder=builder.getSchemaRootNodeBuilder();\nIndexedEntityBindingContext bindingContext=new IndexedEntityBindingContextImpl(schemaRootNodeBuilder);\nstate=new IndexManagerInitialBuildState<>(backendName,indexName,builder,bindingContext);\nindexManagerBuildStateByName.put(indexName,state);\n}return state;\n}",
            "methodSignature: org.hibernate.search.engine.common.impl.IndexManagerBuildingStateHolder#getBackend\n methodBody: BackendInitialBuildState<?> getBackend(String backendName) {\nif(StringHelper.isEmpty(backendName)){backendName=getDefaultBackendName();\n}BackendInitialBuildState<?> backendBuildState=backendBuildStateByName.get(backendName);\nif(backendBuildState == null){throw new AssertionFailure(\"Mapper asking for a reference to backend '\" + backendName + \"', which was not declared in advance.\" + \" There is a bug in Hibernate Search, please report it.\");\n}return backendBuildState;\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void success() {\n\t\tBackendFactory backendFactoryMock = createMock( BackendFactory.class );\n\t\tBackendImplementor<DocumentElement> backendMock = createMock( BackendImplementor.class );\n\t\tIndexManagerBuilder<DocumentElement> indexManagerBuilderMock = createMock( IndexManagerBuilder.class );\n\t\tIndexSchemaRootNodeBuilder indexSchemaRootNodeBuilderMock = createMock( IndexSchemaRootNodeBuilder.class );\n\n\t\tCapture<ConfigurationPropertySource> backendPropertySourceCapture = Capture.newInstance();\n\t\tCapture<ConfigurationPropertySource> indexPropertySourceCapture = Capture.newInstance();\n\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.type\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"someBackendType\" ) );\n\t\tEasyMock.expect( beanResolverMock.resolve( BackendFactory.class, \"someBackendType\" ) )\n\t\t\t\t.andReturn( BeanHolder.of( backendFactoryMock ) );\n\t\tEasyMock.expect( backendFactoryMock.create(\n\t\t\t\tEasyMock.eq( \"myBackend\" ),\n\t\t\t\tEasyMock.anyObject(),\n\t\t\t\tEasyMock.capture( backendPropertySourceCapture )\n\t\t) )\n\t\t\t\t.andReturn( (BackendImplementor) backendMock );\n\t\treplayAll();\n\t\tholder.createBackends( CollectionHelper.asSet( Optional.of( \"myBackend\" ) ) );\n\t\tverifyAll();\n\n\t\tresetAll();\n\t\tEasyMock.expect( backendMock.createIndexManagerBuilder(\n\t\t\t\tEasyMock.eq( \"myIndex\" ),\n\t\t\t\tEasyMock.eq( false ),\n\t\t\t\tEasyMock.anyObject(),\n\t\t\t\tEasyMock.capture( indexPropertySourceCapture )\n\t\t) )\n\t\t\t\t.andReturn( (IndexManagerBuilder) indexManagerBuilderMock );\n\t\tEasyMock.expect( indexManagerBuilderMock.getSchemaRootNodeBuilder() )\n\t\t\t\t.andStubReturn( indexSchemaRootNodeBuilderMock );\n\t\treplayAll();\n\t\tholder.getIndexManagerBuildingState( Optional.of( \"myBackend\" ), \"myIndex\", false );\n\t\tverifyAll();\n\n\t\t// Check that configuration property sources behave as expected\n\t\tOptional result;\n\n\t\t// Backend configuration\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.foo\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n\t\treplayAll();\n\t\tresult = backendPropertySourceCapture.getValue().get( \"foo\" );\n\t\tverifyAll();\n\t\tassertThat( result ).contains( \"bar\" );\n\n\t\t// Index configuration\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.indexes.myIndex.foo\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n\t\treplayAll();\n\t\tresult = indexPropertySourceCapture.getValue().get( \"foo\" );\n\t\tverifyAll();\n\t\tassertThat( result ).contains( \"bar\" );\n\n\t\t// Index configuration defaults\n\t\tresetAll();\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.indexes.myIndex.foo\" ) )\n\t\t\t\t.andReturn( Optional.empty() );\n\t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.index_defaults.foo\" ) )\n\t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n\t\treplayAll();\n\t\tresult = indexPropertySourceCapture.getValue().get( \"foo\" );\n\t\tverifyAll();\n\t\tassertThat( result ).contains( \"bar\" );\n\t}\n// Check that configuration property sources behave as expected\n\t\tOptional result;\n\n\t\t// Backend configuration\n\t\tresetAll();",
        "diffSourceCode": "    53: \t@Test\n    54: \tpublic void success() {\n    55: \t\tBackendFactory backendFactoryMock = createMock( BackendFactory.class );\n    56: \t\tBackendImplementor<DocumentElement> backendMock = createMock( BackendImplementor.class );\n    57: \t\tIndexManagerBuilder<DocumentElement> indexManagerBuilderMock = createMock( IndexManagerBuilder.class );\n    58: \t\tIndexSchemaRootNodeBuilder indexSchemaRootNodeBuilderMock = createMock( IndexSchemaRootNodeBuilder.class );\n    59: \n    60: \t\tCapture<ConfigurationPropertySource> backendPropertySourceCapture = Capture.newInstance();\n    61: \t\tCapture<ConfigurationPropertySource> indexPropertySourceCapture = Capture.newInstance();\n    62: \n    63: \t\tresetAll();\n    64: \t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.type\" ) )\n    65: \t\t\t\t.andReturn( (Optional) Optional.of( \"someBackendType\" ) );\n    66: \t\tEasyMock.expect( beanResolverMock.resolve( BackendFactory.class, \"someBackendType\" ) )\n    67: \t\t\t\t.andReturn( BeanHolder.of( backendFactoryMock ) );\n    68: \t\tEasyMock.expect( backendFactoryMock.create(\n    69: \t\t\t\tEasyMock.eq( \"myBackend\" ),\n    70: \t\t\t\tEasyMock.anyObject(),\n    71: \t\t\t\tEasyMock.capture( backendPropertySourceCapture )\n    72: \t\t) )\n    73: \t\t\t\t.andReturn( (BackendImplementor) backendMock );\n    74: \t\treplayAll();\n-   75: \t\tholder.createBackends( CollectionHelper.asSet( \"myBackend\" ) );\n+   75: \t\tholder.createBackends( CollectionHelper.asSet( Optional.of( \"myBackend\" ) ) );\n    76: \t\tverifyAll();\n    77: \n    78: \t\tresetAll();\n-   79: \t\treplayAll();\n-   80: \t\tIndexManagerBuildingStateHolder.BackendInitialBuildState<?> backend = holder.getBackend( \"myBackend\" );\n-   81: \t\tverifyAll();\n-   82: \n-   83: \t\tresetAll();\n-   84: \t\tEasyMock.expect( backendMock.createIndexManagerBuilder(\n-   85: \t\t\t\tEasyMock.eq( \"myIndex\" ),\n-   86: \t\t\t\tEasyMock.eq( false ),\n-   87: \t\t\t\tEasyMock.anyObject(),\n-   88: \t\t\t\tEasyMock.capture( indexPropertySourceCapture )\n-   89: \t\t) )\n-   90: \t\t\t\t.andReturn( (IndexManagerBuilder) indexManagerBuilderMock );\n-   91: \t\tEasyMock.expect( indexManagerBuilderMock.getSchemaRootNodeBuilder() )\n-   92: \t\t\t\t.andStubReturn( indexSchemaRootNodeBuilderMock );\n-   93: \t\treplayAll();\n-   94: \t\tbackend.getIndexManagerBuildingState( \"myIndex\", false );\n-   95: \t\tverifyAll();\n-   96: \n-   97: \t\t// Check that configuration property sources behave as expected\n-   98: \t\tOptional result;\n-   99: \n-  100: \t\t// Backend configuration\n-  101: \t\tresetAll();\n-  102: \t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.foo\" ) )\n-  103: \t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n-  104: \t\treplayAll();\n-  105: \t\tresult = backendPropertySourceCapture.getValue().get( \"foo\" );\n-  106: \t\tverifyAll();\n-  107: \t\tassertThat( result ).contains( \"bar\" );\n-  108: \n-  109: \t\t// Index configuration\n-  110: \t\tresetAll();\n-  111: \t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.indexes.myIndex.foo\" ) )\n-  112: \t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n-  113: \t\treplayAll();\n-  114: \t\tresult = indexPropertySourceCapture.getValue().get( \"foo\" );\n-  115: \t\tverifyAll();\n-  116: \t\tassertThat( result ).contains( \"bar\" );\n-  117: \n-  118: \t\t// Index configuration defaults\n-  119: \t\tresetAll();\n-  120: \t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.indexes.myIndex.foo\" ) )\n-  121: \t\t\t\t.andReturn( Optional.empty() );\n-  122: \t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.index_defaults.foo\" ) )\n-  123: \t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n-  124: \t\treplayAll();\n-  125: \t\tresult = indexPropertySourceCapture.getValue().get( \"foo\" );\n-  126: \t\tverifyAll();\n-  127: \t\tassertThat( result ).contains( \"bar\" );\n-  128: \t}\n+   79: \t\tEasyMock.expect( backendMock.createIndexManagerBuilder(\n+   80: \t\t\t\tEasyMock.eq( \"myIndex\" ),\n+   81: \t\t\t\tEasyMock.eq( false ),\n+   82: \t\t\t\tEasyMock.anyObject(),\n+   83: \t\t\t\tEasyMock.capture( indexPropertySourceCapture )\n+   84: \t\t) )\n+   85: \t\t\t\t.andReturn( (IndexManagerBuilder) indexManagerBuilderMock );\n+   86: \t\tEasyMock.expect( indexManagerBuilderMock.getSchemaRootNodeBuilder() )\n+   87: \t\t\t\t.andStubReturn( indexSchemaRootNodeBuilderMock );\n+   88: \t\treplayAll();\n+   89: \t\tholder.getIndexManagerBuildingState( Optional.of( \"myBackend\" ), \"myIndex\", false );\n+   90: \t\tverifyAll();\n+   91: \n+   92: \t\t// Check that configuration property sources behave as expected\n+   93: \t\tOptional result;\n+   94: \n+   95: \t\t// Backend configuration\n+   96: \t\tresetAll();\n+   97: \t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.foo\" ) )\n+   98: \t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n+   99: \t\treplayAll();\n+  100: \t\tresult = backendPropertySourceCapture.getValue().get( \"foo\" );\n+  101: \t\tverifyAll();\n+  102: \t\tassertThat( result ).contains( \"bar\" );\n+  103: \n+  104: \t\t// Index configuration\n+  105: \t\tresetAll();\n+  106: \t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.indexes.myIndex.foo\" ) )\n+  107: \t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n+  108: \t\treplayAll();\n+  109: \t\tresult = indexPropertySourceCapture.getValue().get( \"foo\" );\n+  110: \t\tverifyAll();\n+  111: \t\tassertThat( result ).contains( \"bar\" );\n+  112: \n+  113: \t\t// Index configuration defaults\n+  114: \t\tresetAll();\n+  115: \t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.indexes.myIndex.foo\" ) )\n+  116: \t\t\t\t.andReturn( Optional.empty() );\n+  117: \t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.index_defaults.foo\" ) )\n+  118: \t\t\t\t.andReturn( (Optional) Optional.of( \"bar\" ) );\n+  119: \t\treplayAll();\n+  120: \t\tresult = indexPropertySourceCapture.getValue().get( \"foo\" );\n+  121: \t\tverifyAll();\n+  122: \t\tassertThat( result ).contains( \"bar\" );\n+  123: \t}\n+  124: \n+  125: \t@Test\n+  126: \tpublic void error_missingBackend_emptyName() {\n+  127: \t\tresetAll();\n+  128: \t\tEasyMock.expect( configurationSourceMock.get( \"backends.myBackend.type\" ) )\n",
        "uniqueId": "be929dfca72583e5cef800af2f97727e75184a76_53_128_91_96_53_123",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Inline Method",
        "description": "Inline Method\tprivate doMassIndexingWithBook2GetIdFailure(sessionFactory SessionFactory) : void inlined to public getId() : void in class org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
                "startLine": 101,
                "endLine": 121,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
                "startLine": 101,
                "endLine": 140,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
                "startLine": 346,
                "endLine": 367,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void doMassIndexingWithBook2GetIdFailure(SessionFactory sessionFactory) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\t}",
        "filePathBefore": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
        "isPureRefactoring": true,
        "commitId": "d876cc12f196d470e7db696de0f569d8ee39c49b",
        "packageNameBefore": "org.hibernate.search.integrationtest.mapper.orm.massindexing",
        "classNameBefore": "org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithBook2GetIdFailure",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithFailure\n methodBody: private void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\nBook.failOnBook2GetId.set(ExecutionExpectation.FAIL.equals(book2GetIdExpectation));\nBook.failOnBook2GetTitle.set(ExecutionExpectation.FAIL.equals(book2GetTitleExpectation));\nAssertionError assertionError=null;\ntryOrmUtils.withinSession(sessionFactory,session -> {\n  SearchSession searchSession=Search.session(session);\n  MassIndexer indexer=indexerProducer.apply(searchSession);\n  MassIndexingFailureHandler massIndexingFailureHandler=getMassIndexingFailureHandler();\n  if (massIndexingFailureHandler != null) {\n    indexer.failureHandler(massIndexingFailureHandler);\n  }\n  for (  Runnable expectationSetter : expectationSetters) {\n    expectationSetter.run();\n  }\n  Runnable runnable=() -> {\n    try {\n      indexer.startAndWait();\n    }\n catch (    InterruptedException e) {\n      fail(\"Unexpected InterruptedException: \" + e.getMessage());\n    }\n  }\n;\n  if (thrownExpectation == null) {\n    runnable.run();\n  }\n else {\n    SubTest.expectException(runnable).assertThrown().satisfies(thrownExpectation);\n  }\n}\n);\nbackendMock.verifyExpectationsMet();\ncatch(AssertionError e)assertionError=e;\nthrow e;\nfinallyBook.failOnBook2GetId.set(false);\nBook.failOnBook2GetTitle.set(false);\nif(assertionError == null){switch(threadExpectation)case CREATED_AND_TERMINATED:Awaitility.await().untilAsserted(() -> assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isNotEmpty().allSatisfy(t -> assertThat(t).extracting(Thread::getState).isEqualTo(Thread.State.TERMINATED)));\nbreak;\ncase NOT_CREATED:assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isEmpty();\nbreak;\n}}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexScopeWork\n methodBody: private Runnable expectIndexScopeWork(StubIndexScopeWork.Type type, ExecutionExpectation executionExpectation) {\nreturn () -> {\nswitch (executionExpectation) {\ncase SUCCEED:    backendMock.expectIndexScopeWorks(Book.NAME).indexScopeWork(type);\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(type.name() + \" failure\"));\nbackendMock.expectIndexScopeWorks(Book.NAME).indexScopeWork(type,failingFuture);\nbreak;\ncase SKIP:break;\n}\n}\n;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexingWorks\n methodBody: private Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\nreturn () -> {\nswitch (workTwoExecutionExpectation) {\ncase SUCCEED:    backendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"2\",b -> b.field(\"title\",TITLE_2).field(\"author\",AUTHOR_2)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(\"Indexing failure\"));\nbackendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\nbackendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"2\",b -> b.field(\"title\",TITLE_2).field(\"author\",AUTHOR_2)).processedThenExecuted(failingFuture);\nbreak;\ncase SKIP:backendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\nbreak;\n}\n}\n;\n}",
        "classSignatureBefore": "public abstract class AbstractMassIndexingFailureIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithBook2GetIdFailure"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT"
        ],
        "classSignatureBeforeSet": [
            "public abstract class AbstractMassIndexingFailureIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.massindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Fail.fail;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.cfg.EngineSettings;\nimport org.hibernate.search.engine.cfg.spi.EngineSpiSettings;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.automaticindexing.AutomaticIndexingStrategyName;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexer;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexingFailureHandler;\nimport org.hibernate.search.mapper.orm.session.SearchSession;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.ThreadSpy;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubIndexScopeWork;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils;\nimport org.hibernate.search.util.impl.test.SubTest;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.assertj.core.api.InstanceOfAssertFactories;\nimport org.awaitility.Awaitility;\n\npublic abstract class AbstractMassIndexingFailureIT {\n\n\tpublic static final String TITLE_1 = \"Oliver Twist\";\n\tpublic static final String AUTHOR_1 = \"Charles Dickens\";\n\tpublic static final String TITLE_2 = \"Ulysses\";\n\tpublic static final String AUTHOR_2 = \"James Joyce\";\n\tpublic static final String TITLE_3 = \"Frankenstein\";\n\tpublic static final String AUTHOR_3 = \"Mary Shelley\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock );\n\n\t@Rule\n\tpublic ThreadSpy threadSpy = new ThreadSpy();\n\n\t@Test\n\tpublic void indexing() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"Indexing failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getId() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getId failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithBook2GetIdFailure( sessionFactory );\n\n\t\tassertEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getTitle() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getTitle failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithBook2GetTitleFailure( sessionFactory );\n\n\t\tassertEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void purge() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"PURGE failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsBefore() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsAfter() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer().mergeSegmentsOnFinish( true ),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void flush() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"FLUSH failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void indexingAndFlush() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString failingEntityIndexingExceptionMessage = \"Indexing failure\";\n\t\tString failingEntityIndexingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\t\tString failingMassIndexerOperationExceptionMessage = \"FLUSH failure\";\n\t\tString failingMassIndexerOperationAsString = \"MassIndexer operation\";\n\n\t\texpectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( failingMassIndexerOperationExceptionMessage )\n\t\t\t\t\t\t// Indexing failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\t\tfailingEntityIndexingExceptionMessage\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\t}\n\n\tprotected abstract String getBackgroundFailureHandlerReference();\n\n\tprotected abstract MassIndexingFailureHandler getMassIndexingFailureHandler();\n\n\tprotected void assertBeforeSetup() {\n\t}\n\n\tprotected void assertAfterSetup() {\n\t}\n\n\tprotected abstract void expectEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectMassIndexerOperationFailureHandling(\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertMassIndexerOperationFailureHandling(\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprotected abstract void assertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tindexerProducer,\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithBook2GetIdFailure(SessionFactory sessionFactory) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithBook2GetTitleFailure(SessionFactory sessionFactory) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tBook.failOnBook2GetId.set( ExecutionExpectation.FAIL.equals( book2GetIdExpectation ) );\n\t\tBook.failOnBook2GetTitle.set( ExecutionExpectation.FAIL.equals( book2GetTitleExpectation ) );\n\t\tAssertionError assertionError = null;\n\t\ttry {\n\t\t\tOrmUtils.withinSession( sessionFactory, session -> {\n\t\t\t\tSearchSession searchSession = Search.session( session );\n\t\t\t\tMassIndexer indexer = indexerProducer.apply( searchSession );\n\n\t\t\t\tMassIndexingFailureHandler massIndexingFailureHandler = getMassIndexingFailureHandler();\n\t\t\t\tif ( massIndexingFailureHandler != null ) {\n\t\t\t\t\tindexer.failureHandler( massIndexingFailureHandler );\n\t\t\t\t}\n\n\t\t\t\tfor ( Runnable expectationSetter : expectationSetters ) {\n\t\t\t\t\texpectationSetter.run();\n\t\t\t\t}\n\n\t\t\t\t// TODO HSEARCH-3728 simplify this when even indexing exceptions are propagated\n\t\t\t\tRunnable runnable = () -> {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tindexer.startAndWait();\n\t\t\t\t\t}\n\t\t\t\t\tcatch (InterruptedException e) {\n\t\t\t\t\t\tfail( \"Unexpected InterruptedException: \" + e.getMessage() );\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t\tif ( thrownExpectation == null ) {\n\t\t\t\t\trunnable.run();\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tSubTest.expectException( runnable )\n\t\t\t\t\t\t\t.assertThrown()\n\t\t\t\t\t\t\t.satisfies( thrownExpectation );\n\t\t\t\t}\n\t\t\t} );\n\t\t\tbackendMock.verifyExpectationsMet();\n\t\t}\n\t\tcatch (AssertionError e) {\n\t\t\tassertionError = e;\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tBook.failOnBook2GetId.set( false );\n\t\t\tBook.failOnBook2GetTitle.set( false );\n\n\t\t\tif ( assertionError == null ) {\n\t\t\t\tswitch ( threadExpectation ) {\n\t\t\t\t\tcase CREATED_AND_TERMINATED:\n\t\t\t\t\t\tAwaitility.await().untilAsserted(\n\t\t\t\t\t\t\t\t() -> assertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t\t\t.isNotEmpty()\n\t\t\t\t\t\t\t\t\t\t.allSatisfy( t -> assertThat( t )\n\t\t\t\t\t\t\t\t\t\t\t\t.extracting( Thread::getState )\n\t\t\t\t\t\t\t\t\t\t\t\t.isEqualTo( Thread.State.TERMINATED )\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase NOT_CREATED:\n\t\t\t\t\t\tassertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t.isEmpty();\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate Runnable expectIndexScopeWork(StubIndexScopeWork.Type type, ExecutionExpectation executionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( executionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectIndexScopeWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScopeWork( type );\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\t\t\tbackendMock.expectIndexScopeWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScopeWork( type, failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( workTwoExecutionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( \"Indexing failure\" ) );\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted( failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate SessionFactory setup() {\n\t\tassertBeforeSetup();\n\n\t\tbackendMock.expectAnySchema( Book.NAME );\n\n\t\tSessionFactory sessionFactory = ormSetupHelper.start()\n\t\t\t\t.withPropertyRadical( HibernateOrmMapperSettings.Radicals.AUTOMATIC_INDEXING_STRATEGY, AutomaticIndexingStrategyName.NONE )\n\t\t\t\t.withPropertyRadical( EngineSettings.Radicals.BACKGROUND_FAILURE_HANDLER, getBackgroundFailureHandlerReference() )\n\t\t\t\t.withPropertyRadical( EngineSpiSettings.Radicals.THREAD_PROVIDER, threadSpy.getThreadProvider() )\n\t\t\t\t.setup( Book.class );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tOrmUtils.withinTransaction( sessionFactory, session -> {\n\t\t\tsession.persist( new Book( 1, TITLE_1, AUTHOR_1 ) );\n\t\t\tsession.persist( new Book( 2, TITLE_2, AUTHOR_2 ) );\n\t\t\tsession.persist( new Book( 3, TITLE_3, AUTHOR_3 ) );\n\t\t} );\n\n\t\tassertAfterSetup();\n\n\t\treturn sessionFactory;\n\t}\n\n\tprivate enum ExecutionExpectation {\n\t\tSUCCEED,\n\t\tFAIL,\n\t\tSKIP;\n\t}\n\n\tprivate enum ThreadExpectation {\n\t\tCREATED_AND_TERMINATED,\n\t\tNOT_CREATED;\n\t}\n\n\t@Entity(name = Book.NAME)\n\t@Indexed(index = Book.NAME)\n\tpublic static class Book {\n\n\t\tpublic static final String NAME = \"Book\";\n\n\t\tprivate static final AtomicBoolean failOnBook2GetId = new AtomicBoolean( false );\n\t\tprivate static final AtomicBoolean failOnBook2GetTitle = new AtomicBoolean( false );\n\n\t\tprivate Integer id;\n\n\t\tprivate String title;\n\n\t\tprivate String author;\n\n\t\tpublic Book() {\n\t\t}\n\n\t\tpublic Book(Integer id, String title, String author) {\n\t\t\tthis.id = id;\n\t\t\tthis.title = title;\n\t\t\tthis.author = author;\n\t\t}\n\n\t\t@Id // This must be on the getter, so that Hibernate Search uses getters instead of direct field access\n\t\tpublic Integer getId() {\n\t\t\tif ( id == 2 && failOnBook2GetId.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getId failure\" );\n\t\t\t}\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getTitle() {\n\t\t\tif ( id == 2 && failOnBook2GetTitle.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getTitle failure\" );\n\t\t\t}\n\t\t\treturn title;\n\t\t}\n\n\t\tpublic void setTitle(String title) {\n\t\t\tthis.title = title;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getAuthor() {\n\t\t\treturn author;\n\t\t}\n\n\t\tpublic void setAuthor(String author) {\n\t\t\tthis.author = author;\n\t\t}\n\t}\n\n\tprotected static class SimulatedFailure extends RuntimeException {\n\t\tSimulatedFailure(String message) {\n\t\t\tsuper( message );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.massindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Fail.fail;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.cfg.EngineSettings;\nimport org.hibernate.search.engine.cfg.spi.EngineSpiSettings;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.automaticindexing.AutomaticIndexingStrategyName;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexer;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexingFailureHandler;\nimport org.hibernate.search.mapper.orm.session.SearchSession;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.ThreadSpy;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubIndexScopeWork;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils;\nimport org.hibernate.search.util.impl.test.SubTest;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.assertj.core.api.InstanceOfAssertFactories;\nimport org.awaitility.Awaitility;\n\npublic abstract class AbstractMassIndexingFailureIT {\n\n\tpublic static final String TITLE_1 = \"Oliver Twist\";\n\tpublic static final String AUTHOR_1 = \"Charles Dickens\";\n\tpublic static final String TITLE_2 = \"Ulysses\";\n\tpublic static final String AUTHOR_2 = \"James Joyce\";\n\tpublic static final String TITLE_3 = \"Frankenstein\";\n\tpublic static final String AUTHOR_3 = \"Mary Shelley\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock );\n\n\t@Rule\n\tpublic ThreadSpy threadSpy = new ThreadSpy();\n\n\t@Test\n\tpublic void indexing() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"Indexing failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getId() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getId failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getTitle() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getTitle failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void purge() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"PURGE failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsBefore() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsAfter() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer().mergeSegmentsOnFinish( true ),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void flush() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"FLUSH failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void indexingAndFlush() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString failingEntityIndexingExceptionMessage = \"Indexing failure\";\n\t\tString failingEntityIndexingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\t\tString failingMassIndexerOperationExceptionMessage = \"FLUSH failure\";\n\t\tString failingMassIndexerOperationAsString = \"MassIndexer operation\";\n\n\t\texpectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( failingMassIndexerOperationExceptionMessage )\n\t\t\t\t\t\t// Indexing failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\t\tfailingEntityIndexingExceptionMessage\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\t}\n\n\tprotected abstract String getBackgroundFailureHandlerReference();\n\n\tprotected abstract MassIndexingFailureHandler getMassIndexingFailureHandler();\n\n\tprotected void assertBeforeSetup() {\n\t}\n\n\tprotected void assertAfterSetup() {\n\t}\n\n\tprotected abstract void expectEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectMassIndexerOperationFailureHandling(\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertMassIndexerOperationFailureHandling(\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprotected abstract void assertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tindexerProducer,\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tBook.failOnBook2GetId.set( ExecutionExpectation.FAIL.equals( book2GetIdExpectation ) );\n\t\tBook.failOnBook2GetTitle.set( ExecutionExpectation.FAIL.equals( book2GetTitleExpectation ) );\n\t\tAssertionError assertionError = null;\n\t\ttry {\n\t\t\tOrmUtils.withinSession( sessionFactory, session -> {\n\t\t\t\tSearchSession searchSession = Search.session( session );\n\t\t\t\tMassIndexer indexer = indexerProducer.apply( searchSession );\n\n\t\t\t\tMassIndexingFailureHandler massIndexingFailureHandler = getMassIndexingFailureHandler();\n\t\t\t\tif ( massIndexingFailureHandler != null ) {\n\t\t\t\t\tindexer.failureHandler( massIndexingFailureHandler );\n\t\t\t\t}\n\n\t\t\t\tfor ( Runnable expectationSetter : expectationSetters ) {\n\t\t\t\t\texpectationSetter.run();\n\t\t\t\t}\n\n\t\t\t\t// TODO HSEARCH-3728 simplify this when even indexing exceptions are propagated\n\t\t\t\tRunnable runnable = () -> {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tindexer.startAndWait();\n\t\t\t\t\t}\n\t\t\t\t\tcatch (InterruptedException e) {\n\t\t\t\t\t\tfail( \"Unexpected InterruptedException: \" + e.getMessage() );\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t\tif ( thrownExpectation == null ) {\n\t\t\t\t\trunnable.run();\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tSubTest.expectException( runnable )\n\t\t\t\t\t\t\t.assertThrown()\n\t\t\t\t\t\t\t.satisfies( thrownExpectation );\n\t\t\t\t}\n\t\t\t} );\n\t\t\tbackendMock.verifyExpectationsMet();\n\t\t}\n\t\tcatch (AssertionError e) {\n\t\t\tassertionError = e;\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tBook.failOnBook2GetId.set( false );\n\t\t\tBook.failOnBook2GetTitle.set( false );\n\n\t\t\tif ( assertionError == null ) {\n\t\t\t\tswitch ( threadExpectation ) {\n\t\t\t\t\tcase CREATED_AND_TERMINATED:\n\t\t\t\t\t\tAwaitility.await().untilAsserted(\n\t\t\t\t\t\t\t\t() -> assertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t\t\t.isNotEmpty()\n\t\t\t\t\t\t\t\t\t\t.allSatisfy( t -> assertThat( t )\n\t\t\t\t\t\t\t\t\t\t\t\t.extracting( Thread::getState )\n\t\t\t\t\t\t\t\t\t\t\t\t.isEqualTo( Thread.State.TERMINATED )\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase NOT_CREATED:\n\t\t\t\t\t\tassertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t.isEmpty();\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate Runnable expectIndexScopeWork(StubIndexScopeWork.Type type, ExecutionExpectation executionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( executionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectIndexScopeWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScopeWork( type );\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\t\t\tbackendMock.expectIndexScopeWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScopeWork( type, failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( workTwoExecutionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( \"Indexing failure\" ) );\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted( failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate SessionFactory setup() {\n\t\tassertBeforeSetup();\n\n\t\tbackendMock.expectAnySchema( Book.NAME );\n\n\t\tSessionFactory sessionFactory = ormSetupHelper.start()\n\t\t\t\t.withPropertyRadical( HibernateOrmMapperSettings.Radicals.AUTOMATIC_INDEXING_STRATEGY, AutomaticIndexingStrategyName.NONE )\n\t\t\t\t.withPropertyRadical( EngineSettings.Radicals.BACKGROUND_FAILURE_HANDLER, getBackgroundFailureHandlerReference() )\n\t\t\t\t.withPropertyRadical( EngineSpiSettings.Radicals.THREAD_PROVIDER, threadSpy.getThreadProvider() )\n\t\t\t\t.setup( Book.class );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tOrmUtils.withinTransaction( sessionFactory, session -> {\n\t\t\tsession.persist( new Book( 1, TITLE_1, AUTHOR_1 ) );\n\t\t\tsession.persist( new Book( 2, TITLE_2, AUTHOR_2 ) );\n\t\t\tsession.persist( new Book( 3, TITLE_3, AUTHOR_3 ) );\n\t\t} );\n\n\t\tassertAfterSetup();\n\n\t\treturn sessionFactory;\n\t}\n\n\tprivate enum ExecutionExpectation {\n\t\tSUCCEED,\n\t\tFAIL,\n\t\tSKIP;\n\t}\n\n\tprivate enum ThreadExpectation {\n\t\tCREATED_AND_TERMINATED,\n\t\tNOT_CREATED;\n\t}\n\n\t@Entity(name = Book.NAME)\n\t@Indexed(index = Book.NAME)\n\tpublic static class Book {\n\n\t\tpublic static final String NAME = \"Book\";\n\n\t\tprivate static final AtomicBoolean failOnBook2GetId = new AtomicBoolean( false );\n\t\tprivate static final AtomicBoolean failOnBook2GetTitle = new AtomicBoolean( false );\n\n\t\tprivate Integer id;\n\n\t\tprivate String title;\n\n\t\tprivate String author;\n\n\t\tpublic Book() {\n\t\t}\n\n\t\tpublic Book(Integer id, String title, String author) {\n\t\t\tthis.id = id;\n\t\t\tthis.title = title;\n\t\t\tthis.author = author;\n\t\t}\n\n\t\t@Id // This must be on the getter, so that Hibernate Search uses getters instead of direct field access\n\t\tpublic Integer getId() {\n\t\t\tif ( id == 2 && failOnBook2GetId.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getId failure\" );\n\t\t\t}\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getTitle() {\n\t\t\tif ( id == 2 && failOnBook2GetTitle.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getTitle failure\" );\n\t\t\t}\n\t\t\treturn title;\n\t\t}\n\n\t\tpublic void setTitle(String title) {\n\t\t\tthis.title = title;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getAuthor() {\n\t\t\treturn author;\n\t\t}\n\n\t\tpublic void setAuthor(String author) {\n\t\t\tthis.author = author;\n\t\t}\n\t}\n\n\tprotected static class SimulatedFailure extends RuntimeException {\n\t\tSimulatedFailure(String message) {\n\t\t\tsuper( message );\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithFailure\n methodBody: private void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\nBook.failOnBook2GetId.set(ExecutionExpectation.FAIL.equals(book2GetIdExpectation));\nBook.failOnBook2GetTitle.set(ExecutionExpectation.FAIL.equals(book2GetTitleExpectation));\nAssertionError assertionError=null;\ntryOrmUtils.withinSession(sessionFactory,session -> {\n  SearchSession searchSession=Search.session(session);\n  MassIndexer indexer=indexerProducer.apply(searchSession);\n  MassIndexingFailureHandler massIndexingFailureHandler=getMassIndexingFailureHandler();\n  if (massIndexingFailureHandler != null) {\n    indexer.failureHandler(massIndexingFailureHandler);\n  }\n  for (  Runnable expectationSetter : expectationSetters) {\n    expectationSetter.run();\n  }\n  Runnable runnable=() -> {\n    try {\n      indexer.startAndWait();\n    }\n catch (    InterruptedException e) {\n      fail(\"Unexpected InterruptedException: \" + e.getMessage());\n    }\n  }\n;\n  if (thrownExpectation == null) {\n    runnable.run();\n  }\n else {\n    SubTest.expectException(runnable).assertThrown().satisfies(thrownExpectation);\n  }\n}\n);\nbackendMock.verifyExpectationsMet();\ncatch(AssertionError e)assertionError=e;\nthrow e;\nfinallyBook.failOnBook2GetId.set(false);\nBook.failOnBook2GetTitle.set(false);\nif(assertionError == null){switch(threadExpectation)case CREATED_AND_TERMINATED:Awaitility.await().untilAsserted(() -> assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isNotEmpty().allSatisfy(t -> assertThat(t).extracting(Thread::getState).isEqualTo(Thread.State.TERMINATED)));\nbreak;\ncase NOT_CREATED:assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isEmpty();\nbreak;\n}}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexScopeWork\n methodBody: private Runnable expectIndexScopeWork(StubIndexScopeWork.Type type, ExecutionExpectation executionExpectation) {\nreturn () -> {\nswitch (executionExpectation) {\ncase SUCCEED:    backendMock.expectIndexScopeWorks(Book.NAME).indexScopeWork(type);\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(type.name() + \" failure\"));\nbackendMock.expectIndexScopeWorks(Book.NAME).indexScopeWork(type,failingFuture);\nbreak;\ncase SKIP:break;\n}\n}\n;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexingWorks\n methodBody: private Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\nreturn () -> {\nswitch (workTwoExecutionExpectation) {\ncase SUCCEED:    backendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"2\",b -> b.field(\"title\",TITLE_2).field(\"author\",AUTHOR_2)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(\"Indexing failure\"));\nbackendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\nbackendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"2\",b -> b.field(\"title\",TITLE_2).field(\"author\",AUTHOR_2)).processedThenExecuted(failingFuture);\nbreak;\ncase SKIP:backendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\nbreak;\n}\n}\n;\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void getId() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getId failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}",
        "diffSourceCode": "   101: \t@Test\n   102: \tpublic void getId() {\n   103: \t\tSessionFactory sessionFactory = setup();\n   104: \n   105: \t\tString entityName = Book.NAME;\n   106: \t\tString entityReferenceAsString = Book.NAME + \"#2\";\n   107: \t\tString exceptionMessage = \"getId failure\";\n   108: \t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n   109: \n   110: \t\texpectEntityGetterFailureHandling(\n   111: \t\t\t\tentityName, entityReferenceAsString,\n   112: \t\t\t\texceptionMessage, failingOperationAsString\n   113: \t\t);\n   114: \n-  115: \t\tdoMassIndexingWithBook2GetIdFailure( sessionFactory );\n-  116: \n-  117: \t\tassertEntityGetterFailureHandling(\n-  118: \t\t\t\tentityName, entityReferenceAsString,\n-  119: \t\t\t\texceptionMessage, failingOperationAsString\n-  120: \t\t);\n-  121: \t}\n-  122: \n-  123: \t@Test\n-  124: \tpublic void getTitle() {\n-  125: \t\tSessionFactory sessionFactory = setup();\n-  126: \n-  127: \t\tString entityName = Book.NAME;\n-  128: \t\tString entityReferenceAsString = Book.NAME + \"#2\";\n-  129: \t\tString exceptionMessage = \"getTitle failure\";\n-  130: \t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n-  131: \n-  132: \t\texpectEntityGetterFailureHandling(\n-  133: \t\t\t\tentityName, entityReferenceAsString,\n-  134: \t\t\t\texceptionMessage, failingOperationAsString\n-  135: \t\t);\n-  136: \n-  137: \t\tdoMassIndexingWithBook2GetTitleFailure( sessionFactory );\n-  138: \n-  139: \t\tassertEntityGetterFailureHandling(\n-  140: \t\t\t\tentityName, entityReferenceAsString,\n-  346: \tprivate void doMassIndexingWithBook2GetIdFailure(SessionFactory sessionFactory) {\n-  347: \t\tdoMassIndexingWithFailure(\n-  348: \t\t\t\tsessionFactory,\n-  349: \t\t\t\tsearchSession -> searchSession.massIndexer(),\n-  350: \t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n-  351: \t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n-  352: \t\t\t\t\t\t.hasMessageContainingAll(\n-  353: \t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n-  354: \t\t\t\t\t\t\t\t\"See the logs for details.\",\n-  355: \t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n-  356: \t\t\t\t\t\t\t\t\"Exception while invoking\"\n-  357: \t\t\t\t\t\t)\n-  358: \t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n-  359: \t\t\t\t\t\t.isInstanceOf( SearchException.class )\n-  360: \t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n-  361: \t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n-  362: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n-  363: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n-  364: \t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n-  365: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n+  115: \t\tdoMassIndexingWithFailure(\n+  116: \t\t\t\tsessionFactory,\n+  117: \t\t\t\tsearchSession -> searchSession.massIndexer(),\n+  118: \t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n+  119: \t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n+  120: \t\t\t\t\t\t.hasMessageContainingAll(\n+  121: \t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n+  122: \t\t\t\t\t\t\t\t\"See the logs for details.\",\n+  123: \t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n+  124: \t\t\t\t\t\t\t\t\"Exception while invoking\"\n+  125: \t\t\t\t\t\t)\n+  126: \t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n+  127: \t\t\t\t\t\t.isInstanceOf( SearchException.class )\n+  128: \t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n+  129: \t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n+  130: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n+  131: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n+  132: \t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n+  133: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n+  134: \t\t);\n+  135: \n+  136: \t\tassertEntityGetterFailureHandling(\n+  137: \t\t\t\tentityName, entityReferenceAsString,\n+  138: \t\t\t\texceptionMessage, failingOperationAsString\n+  139: \t\t);\n+  140: \t}\n+  346: \tprotected abstract void expectEntityIndexingAndMassIndexerOperationFailureHandling(\n+  347: \t\t\tString entityName, String entityReferenceAsString,\n+  348: \t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n+  349: \t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n+  350: \n+  351: \tprotected abstract void assertEntityIndexingAndMassIndexerOperationFailureHandling(\n+  352: \t\t\tString entityName, String entityReferenceAsString,\n+  353: \t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n+  354: \t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n+  355: \n+  356: \tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n+  357: \t\t\tThreadExpectation threadExpectation,\n+  358: \t\t\tConsumer<Throwable> thrownExpectation,\n+  359: \t\t\tRunnable ... expectationSetters) {\n+  360: \t\tdoMassIndexingWithFailure(\n+  361: \t\t\t\tsessionFactory,\n+  362: \t\t\t\tsearchSession -> searchSession.massIndexer(),\n+  363: \t\t\t\tthreadExpectation,\n+  364: \t\t\t\tthrownExpectation,\n+  365: \t\t\t\texpectationSetters\n   366: \t\t);\n   367: \t}\n",
        "uniqueId": "d876cc12f196d470e7db696de0f569d8ee39c49b_101_121__101_140_346_367",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate doMassIndexingWithFailure(sessionFactory SessionFactory, indexerProducer Function<SearchSession,MassIndexer>, threadExpectation ThreadExpectation, thrownExpectation Consumer<Throwable>, expectationSetters Runnable...) : void extracted from private doMassIndexingWithFailure(sessionFactory SessionFactory, threadExpectation ThreadExpectation, thrownExpectation Consumer<Throwable>, expectationSetters Runnable...) : void in class org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/MassIndexingFailureIT.java",
                "startLine": 526,
                "endLine": 537,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/MassIndexingFailureIT.java",
                "startLine": 523,
                "endLine": 534,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/MassIndexingFailureIT.java",
                "startLine": 536,
                "endLine": 549,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n\t\t\t\texpectationSetters\n\t\t);\n\t}",
        "filePathBefore": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/MassIndexingFailureIT.java",
        "isPureRefactoring": true,
        "commitId": "cbb08b0ad36186a75ae72b07dcc5842c0f6dcdd6",
        "packageNameBefore": "org.hibernate.search.integrationtest.mapper.orm.massindexing",
        "classNameBefore": "org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureIT#doMassIndexingWithFailure",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureIT#doMassIndexingWithFailure\n methodBody: private void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\nBook.failOnBook2GetId.set(ExecutionExpectation.FAIL.equals(book2GetIdExpectation));\nBook.failOnBook2GetTitle.set(ExecutionExpectation.FAIL.equals(book2GetTitleExpectation));\nAssertionError assertionError=null;\ntryOrmUtils.withinSession(sessionFactory,session -> {\n  SearchSession searchSession=Search.session(session);\n  MassIndexer indexer=searchSession.massIndexer();\n  for (  Runnable expectationSetter : expectationSetters) {\n    expectationSetter.run();\n  }\n  Runnable runnable=() -> {\n    try {\n      indexer.startAndWait();\n    }\n catch (    InterruptedException e) {\n      fail(\"Unexpected InterruptedException: \" + e.getMessage());\n    }\n  }\n;\n  if (thrownExpectation == null) {\n    runnable.run();\n  }\n else {\n    SubTest.expectException(runnable).assertThrown().satisfies(thrownExpectation);\n  }\n}\n);\nbackendMock.verifyExpectationsMet();\ncatch(AssertionError e)assertionError=e;\nthrow e;\nfinallyBook.failOnBook2GetId.set(false);\nBook.failOnBook2GetTitle.set(false);\nif(assertionError == null){switch(threadExpectation)case CREATED_AND_TERMINATED:Awaitility.await().untilAsserted(() -> assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isNotEmpty().allSatisfy(t -> assertThat(t).extracting(Thread::getState).isEqualTo(Thread.State.TERMINATED)));\nbreak;\ncase NOT_CREATED:assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isEmpty();\nbreak;\n}}",
        "classSignatureBefore": "public class MassIndexingFailureIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureIT#doMassIndexingWithFailure"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureIT"
        ],
        "classSignatureBeforeSet": [
            "public class MassIndexingFailureIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Overlapped refactoring - can be identical by undoing the overlapped refactoring\n- Add Parameter-",
                "description": "Parametrization or Add Parameter on top of the extract method - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.massindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Fail.fail;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.function.Consumer;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.cfg.EngineSettings;\nimport org.hibernate.search.engine.cfg.spi.EngineSpiSettings;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.automaticindexing.AutomaticIndexingStrategyName;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexer;\nimport org.hibernate.search.mapper.orm.session.SearchSession;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.ThreadSpy;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.StubFailureHandler;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubIndexScopeWork;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils;\nimport org.hibernate.search.util.impl.test.ExceptionMatcherBuilder;\nimport org.hibernate.search.util.impl.test.SubTest;\nimport org.hibernate.search.util.impl.test.rule.ExpectedLog4jLog;\nimport org.hibernate.search.util.impl.test.rule.StaticCounters;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.apache.log4j.Level;\nimport org.assertj.core.api.InstanceOfAssertFactories;\nimport org.awaitility.Awaitility;\n\npublic class MassIndexingFailureIT {\n\n\tpublic static final String TITLE_1 = \"Oliver Twist\";\n\tpublic static final String AUTHOR_1 = \"Charles Dickens\";\n\tpublic static final String TITLE_2 = \"Ulysses\";\n\tpublic static final String AUTHOR_2 = \"James Joyce\";\n\tpublic static final String TITLE_3 = \"Frankenstein\";\n\tpublic static final String AUTHOR_3 = \"Mary Shelley\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock );\n\n\t@Rule\n\tpublic ExpectedLog4jLog logged = ExpectedLog4jLog.create();\n\n\t@Rule\n\tpublic StaticCounters staticCounters = new StaticCounters();\n\n\t@Rule\n\tpublic ThreadSpy threadSpy = new ThreadSpy();\n\n\t@Test\n\tpublic void indexing_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"Indexing failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"Indexing instance of entity '\" + Book.NAME + \"'\",\n\t\t\t\t\"Entities that could not be indexed correctly:\",\n\t\t\t\tBook.NAME + \"#2\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Indexing failure\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\t}\n\n\t@Test\n\tpublic void indexing_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Indexing failure\"\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void getId_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SearchException.class )\n\t\t\t\t\t\t.withMessage( \"Exception while invoking\" )\n\t\t\t\t\t\t.causedBy( SimulatedFailure.class )\n\t\t\t\t\t\t\t\t.withMessage( \"getId failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"Indexing instance of entity '\" + Book.NAME + \"'\",\n\t\t\t\t\"Entities that could not be indexed correctly:\",\n\t\t\t\tBook.NAME + \"#2\"\n\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithBook2GetIdFailure( sessionFactory );\n\t}\n\n\t@Test\n\tpublic void getId_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithBook2GetIdFailure( sessionFactory );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void getTitle_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SearchException.class )\n\t\t\t\t\t\t.withMessage( \"Exception while invoking\" )\n\t\t\t\t\t\t.causedBy( SimulatedFailure.class )\n\t\t\t\t\t\t\t\t.withMessage( \"getTitle failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"Indexing instance of entity '\" + Book.NAME + \"'\",\n\t\t\t\t\"Entities that could not be indexed correctly:\",\n\t\t\t\tBook.NAME + \"#2\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithBook2GetTitleFailure( sessionFactory );\n\t}\n\n\t@Test\n\tpublic void getTitle_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithBook2GetTitleFailure( sessionFactory );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void purge_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"PURGE failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"MassIndexer operation\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"PURGE failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.FAIL )\n\t\t);\n\t}\n\n\t@Test\n\tpublic void purge_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"PURGE failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void optimizeBefore_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"OPTIMIZE failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"MassIndexer operation\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"OPTIMIZE failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.FAIL )\n\t\t);\n\t}\n\n\t@Test\n\tpublic void optimizeBefore_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"OPTIMIZE failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void optimizeAfter_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"OPTIMIZE failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"MassIndexer operation\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"OPTIMIZE failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.FAIL )\n\t\t);\n\t}\n\n\t@Test\n\tpublic void optimizeAfter_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"OPTIMIZE failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void flush_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"FLUSH failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"MassIndexer operation\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"FLUSH failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\t}\n\n\t@Test\n\tpublic void flush_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"FLUSH failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void indexingAndFlush_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"Indexing failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"Indexing instance of entity '\" + Book.NAME + \"'\",\n\t\t\t\t\"Entities that could not be indexed correctly:\",\n\t\t\t\tBook.NAME + \"#2\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"FLUSH failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"MassIndexer operation\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"FLUSH failure\" )\n\t\t\t\t\t\t// Indexing failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\t\t\"Indexing failure\"\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\t}\n\n\t@Test\n\tpublic void indexingAndFlush_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"FLUSH failure\" )\n\t\t\t\t\t\t// Indexing failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\t\t\"Indexing failure\"\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithBook2GetIdFailure(SessionFactory sessionFactory) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithBook2GetTitleFailure(SessionFactory sessionFactory) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tBook.failOnBook2GetId.set( ExecutionExpectation.FAIL.equals( book2GetIdExpectation ) );\n\t\tBook.failOnBook2GetTitle.set( ExecutionExpectation.FAIL.equals( book2GetTitleExpectation ) );\n\t\tAssertionError assertionError = null;\n\t\ttry {\n\t\t\tOrmUtils.withinSession( sessionFactory, session -> {\n\t\t\t\tSearchSession searchSession = Search.session( session );\n\t\t\t\tMassIndexer indexer = searchSession.massIndexer();\n\n\t\t\t\tfor ( Runnable expectationSetter : expectationSetters ) {\n\t\t\t\t\texpectationSetter.run();\n\t\t\t\t}\n\n\t\t\t\t// TODO HSEARCH-3728 simplify this when even indexing exceptions are propagated\n\t\t\t\tRunnable runnable = () -> {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tindexer.startAndWait();\n\t\t\t\t\t}\n\t\t\t\t\tcatch (InterruptedException e) {\n\t\t\t\t\t\tfail( \"Unexpected InterruptedException: \" + e.getMessage() );\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t\tif ( thrownExpectation == null ) {\n\t\t\t\t\trunnable.run();\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tSubTest.expectException( runnable )\n\t\t\t\t\t\t\t.assertThrown()\n\t\t\t\t\t\t\t.satisfies( thrownExpectation );\n\t\t\t\t}\n\t\t\t} );\n\t\t\tbackendMock.verifyExpectationsMet();\n\t\t}\n\t\tcatch (AssertionError e) {\n\t\t\tassertionError = e;\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tBook.failOnBook2GetId.set( false );\n\t\t\tBook.failOnBook2GetTitle.set( false );\n\n\t\t\tif ( assertionError == null ) {\n\t\t\t\tswitch ( threadExpectation ) {\n\t\t\t\t\tcase CREATED_AND_TERMINATED:\n\t\t\t\t\t\tAwaitility.await().untilAsserted(\n\t\t\t\t\t\t\t\t() -> assertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t\t\t.isNotEmpty()\n\t\t\t\t\t\t\t\t\t\t.allSatisfy( t -> assertThat( t )\n\t\t\t\t\t\t\t\t\t\t\t\t.extracting( Thread::getState )\n\t\t\t\t\t\t\t\t\t\t\t\t.isEqualTo( Thread.State.TERMINATED )\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase NOT_CREATED:\n\t\t\t\t\t\tassertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t.isEmpty();\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate Runnable expectIndexScopeWork(StubIndexScopeWork.Type type, ExecutionExpectation executionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( executionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectIndexScopeWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScopeWork( type );\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\t\t\tbackendMock.expectIndexScopeWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScopeWork( type, failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( workTwoExecutionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( \"Indexing failure\" ) );\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted( failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate SessionFactory setup(String failureHandler) {\n\t\tbackendMock.expectAnySchema( Book.NAME );\n\n\t\tSessionFactory sessionFactory = ormSetupHelper.start()\n\t\t\t\t.withPropertyRadical( HibernateOrmMapperSettings.Radicals.AUTOMATIC_INDEXING_STRATEGY, AutomaticIndexingStrategyName.NONE )\n\t\t\t\t.withPropertyRadical( EngineSettings.Radicals.BACKGROUND_FAILURE_HANDLER, failureHandler )\n\t\t\t\t.withPropertyRadical( EngineSpiSettings.Radicals.THREAD_PROVIDER, threadSpy.getThreadProvider() )\n\t\t\t\t.setup( Book.class );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tOrmUtils.withinTransaction( sessionFactory, session -> {\n\t\t\tsession.persist( new Book( 1, TITLE_1, AUTHOR_1 ) );\n\t\t\tsession.persist( new Book( 2, TITLE_2, AUTHOR_2 ) );\n\t\t\tsession.persist( new Book( 3, TITLE_3, AUTHOR_3 ) );\n\t\t} );\n\n\t\treturn sessionFactory;\n\t}\n\n\tprivate enum ExecutionExpectation {\n\t\tSUCCEED,\n\t\tFAIL,\n\t\tSKIP;\n\t}\n\n\tprivate enum ThreadExpectation {\n\t\tCREATED_AND_TERMINATED,\n\t\tNOT_CREATED;\n\t}\n\n\t@Entity(name = Book.NAME)\n\t@Indexed(index = Book.NAME)\n\tpublic static class Book {\n\n\t\tpublic static final String NAME = \"Book\";\n\n\t\tprivate static final AtomicBoolean failOnBook2GetId = new AtomicBoolean( false );\n\t\tprivate static final AtomicBoolean failOnBook2GetTitle = new AtomicBoolean( false );\n\n\t\tprivate Integer id;\n\n\t\tprivate String title;\n\n\t\tprivate String author;\n\n\t\tpublic Book() {\n\t\t}\n\n\t\tpublic Book(Integer id, String title, String author) {\n\t\t\tthis.id = id;\n\t\t\tthis.title = title;\n\t\t\tthis.author = author;\n\t\t}\n\n\t\t@Id // This must be on the getter, so that Hibernate Search uses getters instead of direct field access\n\t\tpublic Integer getId() {\n\t\t\tif ( id == 2 && failOnBook2GetId.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getId failure\" );\n\t\t\t}\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getTitle() {\n\t\t\tif ( id == 2 && failOnBook2GetTitle.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getTitle failure\" );\n\t\t\t}\n\t\t\treturn title;\n\t\t}\n\n\t\tpublic void setTitle(String title) {\n\t\t\tthis.title = title;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getAuthor() {\n\t\t\treturn author;\n\t\t}\n\n\t\tpublic void setAuthor(String author) {\n\t\t\tthis.author = author;\n\t\t}\n\t}\n\n\tprivate static class SimulatedFailure extends RuntimeException {\n\t\tSimulatedFailure(String message) {\n\t\t\tsuper( message );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/MassIndexingFailureIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.massindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Fail.fail;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.cfg.EngineSettings;\nimport org.hibernate.search.engine.cfg.spi.EngineSpiSettings;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.automaticindexing.AutomaticIndexingStrategyName;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexer;\nimport org.hibernate.search.mapper.orm.session.SearchSession;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.ThreadSpy;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.StubFailureHandler;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubIndexScopeWork;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils;\nimport org.hibernate.search.util.impl.test.ExceptionMatcherBuilder;\nimport org.hibernate.search.util.impl.test.SubTest;\nimport org.hibernate.search.util.impl.test.rule.ExpectedLog4jLog;\nimport org.hibernate.search.util.impl.test.rule.StaticCounters;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.apache.log4j.Level;\nimport org.assertj.core.api.InstanceOfAssertFactories;\nimport org.awaitility.Awaitility;\n\npublic class MassIndexingFailureIT {\n\n\tpublic static final String TITLE_1 = \"Oliver Twist\";\n\tpublic static final String AUTHOR_1 = \"Charles Dickens\";\n\tpublic static final String TITLE_2 = \"Ulysses\";\n\tpublic static final String AUTHOR_2 = \"James Joyce\";\n\tpublic static final String TITLE_3 = \"Frankenstein\";\n\tpublic static final String AUTHOR_3 = \"Mary Shelley\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock );\n\n\t@Rule\n\tpublic ExpectedLog4jLog logged = ExpectedLog4jLog.create();\n\n\t@Rule\n\tpublic StaticCounters staticCounters = new StaticCounters();\n\n\t@Rule\n\tpublic ThreadSpy threadSpy = new ThreadSpy();\n\n\t@Test\n\tpublic void indexing_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"Indexing failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"Indexing instance of entity '\" + Book.NAME + \"'\",\n\t\t\t\t\"Entities that could not be indexed correctly:\",\n\t\t\t\tBook.NAME + \"#2\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Indexing failure\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\t}\n\n\t@Test\n\tpublic void indexing_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Indexing failure\"\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void getId_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SearchException.class )\n\t\t\t\t\t\t.withMessage( \"Exception while invoking\" )\n\t\t\t\t\t\t.causedBy( SimulatedFailure.class )\n\t\t\t\t\t\t\t\t.withMessage( \"getId failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"Indexing instance of entity '\" + Book.NAME + \"'\",\n\t\t\t\t\"Entities that could not be indexed correctly:\",\n\t\t\t\tBook.NAME + \"#2\"\n\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithBook2GetIdFailure( sessionFactory );\n\t}\n\n\t@Test\n\tpublic void getId_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithBook2GetIdFailure( sessionFactory );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void getTitle_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SearchException.class )\n\t\t\t\t\t\t.withMessage( \"Exception while invoking\" )\n\t\t\t\t\t\t.causedBy( SimulatedFailure.class )\n\t\t\t\t\t\t\t\t.withMessage( \"getTitle failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"Indexing instance of entity '\" + Book.NAME + \"'\",\n\t\t\t\t\"Entities that could not be indexed correctly:\",\n\t\t\t\tBook.NAME + \"#2\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithBook2GetTitleFailure( sessionFactory );\n\t}\n\n\t@Test\n\tpublic void getTitle_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithBook2GetTitleFailure( sessionFactory );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void purge_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"PURGE failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"MassIndexer operation\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"PURGE failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.FAIL )\n\t\t);\n\t}\n\n\t@Test\n\tpublic void purge_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"PURGE failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void optimizeBefore_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"OPTIMIZE failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"MassIndexer operation\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"OPTIMIZE failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.FAIL )\n\t\t);\n\t}\n\n\t@Test\n\tpublic void optimizeBefore_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"OPTIMIZE failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void optimizeAfter_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"OPTIMIZE failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"MassIndexer operation\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer().optimizeOnFinish( true ),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"OPTIMIZE failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.FAIL )\n\t\t);\n\t}\n\n\t@Test\n\tpublic void optimizeAfter_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer().optimizeOnFinish( true ),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"OPTIMIZE failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void flush_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"FLUSH failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"MassIndexer operation\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"FLUSH failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\t}\n\n\t@Test\n\tpublic void flush_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"FLUSH failure\" ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\t@Test\n\tpublic void indexingAndFlush_defaultHandler() {\n\t\tSessionFactory sessionFactory = setup( null );\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"Indexing failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"Indexing instance of entity '\" + Book.NAME + \"'\",\n\t\t\t\t\"Entities that could not be indexed correctly:\",\n\t\t\t\tBook.NAME + \"#2\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tlogged.expectEvent(\n\t\t\t\tLevel.ERROR,\n\t\t\t\tExceptionMatcherBuilder.isException( SimulatedFailure.class )\n\t\t\t\t\t\t.withMessage( \"FLUSH failure\" )\n\t\t\t\t\t\t.build(),\n\t\t\t\t\"MassIndexer operation\"\n\t\t)\n\t\t\t\t.once();\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"FLUSH failure\" )\n\t\t\t\t\t\t// Indexing failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\t\t\"Indexing failure\"\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\t}\n\n\t@Test\n\tpublic void indexingAndFlush_customHandler() {\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tSessionFactory sessionFactory = setup( StubFailureHandler.class.getName() );\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 0 );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( \"FLUSH failure\" )\n\t\t\t\t\t\t// Indexing failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\t\t\"Indexing failure\"\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertThat( staticCounters.get( StubFailureHandler.CREATE ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_INDEX_CONTEXT ) ).isEqualTo( 0 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_GENERIC_CONTEXT ) ).isEqualTo( 1 );\n\t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 1 );\n\t}\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tindexerProducer,\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithBook2GetIdFailure(SessionFactory sessionFactory) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithBook2GetTitleFailure(SessionFactory sessionFactory) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.OPTIMIZE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tBook.failOnBook2GetId.set( ExecutionExpectation.FAIL.equals( book2GetIdExpectation ) );\n\t\tBook.failOnBook2GetTitle.set( ExecutionExpectation.FAIL.equals( book2GetTitleExpectation ) );\n\t\tAssertionError assertionError = null;\n\t\ttry {\n\t\t\tOrmUtils.withinSession( sessionFactory, session -> {\n\t\t\t\tSearchSession searchSession = Search.session( session );\n\t\t\t\tMassIndexer indexer = indexerProducer.apply( searchSession );\n\n\t\t\t\tfor ( Runnable expectationSetter : expectationSetters ) {\n\t\t\t\t\texpectationSetter.run();\n\t\t\t\t}\n\n\t\t\t\t// TODO HSEARCH-3728 simplify this when even indexing exceptions are propagated\n\t\t\t\tRunnable runnable = () -> {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tindexer.startAndWait();\n\t\t\t\t\t}\n\t\t\t\t\tcatch (InterruptedException e) {\n\t\t\t\t\t\tfail( \"Unexpected InterruptedException: \" + e.getMessage() );\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t\tif ( thrownExpectation == null ) {\n\t\t\t\t\trunnable.run();\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tSubTest.expectException( runnable )\n\t\t\t\t\t\t\t.assertThrown()\n\t\t\t\t\t\t\t.satisfies( thrownExpectation );\n\t\t\t\t}\n\t\t\t} );\n\t\t\tbackendMock.verifyExpectationsMet();\n\t\t}\n\t\tcatch (AssertionError e) {\n\t\t\tassertionError = e;\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tBook.failOnBook2GetId.set( false );\n\t\t\tBook.failOnBook2GetTitle.set( false );\n\n\t\t\tif ( assertionError == null ) {\n\t\t\t\tswitch ( threadExpectation ) {\n\t\t\t\t\tcase CREATED_AND_TERMINATED:\n\t\t\t\t\t\tAwaitility.await().untilAsserted(\n\t\t\t\t\t\t\t\t() -> assertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t\t\t.isNotEmpty()\n\t\t\t\t\t\t\t\t\t\t.allSatisfy( t -> assertThat( t )\n\t\t\t\t\t\t\t\t\t\t\t\t.extracting( Thread::getState )\n\t\t\t\t\t\t\t\t\t\t\t\t.isEqualTo( Thread.State.TERMINATED )\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase NOT_CREATED:\n\t\t\t\t\t\tassertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t.isEmpty();\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate Runnable expectIndexScopeWork(StubIndexScopeWork.Type type, ExecutionExpectation executionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( executionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectIndexScopeWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScopeWork( type );\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\t\t\tbackendMock.expectIndexScopeWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScopeWork( type, failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( workTwoExecutionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( \"Indexing failure\" ) );\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted( failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate SessionFactory setup(String failureHandler) {\n\t\tbackendMock.expectAnySchema( Book.NAME );\n\n\t\tSessionFactory sessionFactory = ormSetupHelper.start()\n\t\t\t\t.withPropertyRadical( HibernateOrmMapperSettings.Radicals.AUTOMATIC_INDEXING_STRATEGY, AutomaticIndexingStrategyName.NONE )\n\t\t\t\t.withPropertyRadical( EngineSettings.Radicals.BACKGROUND_FAILURE_HANDLER, failureHandler )\n\t\t\t\t.withPropertyRadical( EngineSpiSettings.Radicals.THREAD_PROVIDER, threadSpy.getThreadProvider() )\n\t\t\t\t.setup( Book.class );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tOrmUtils.withinTransaction( sessionFactory, session -> {\n\t\t\tsession.persist( new Book( 1, TITLE_1, AUTHOR_1 ) );\n\t\t\tsession.persist( new Book( 2, TITLE_2, AUTHOR_2 ) );\n\t\t\tsession.persist( new Book( 3, TITLE_3, AUTHOR_3 ) );\n\t\t} );\n\n\t\treturn sessionFactory;\n\t}\n\n\tprivate enum ExecutionExpectation {\n\t\tSUCCEED,\n\t\tFAIL,\n\t\tSKIP;\n\t}\n\n\tprivate enum ThreadExpectation {\n\t\tCREATED_AND_TERMINATED,\n\t\tNOT_CREATED;\n\t}\n\n\t@Entity(name = Book.NAME)\n\t@Indexed(index = Book.NAME)\n\tpublic static class Book {\n\n\t\tpublic static final String NAME = \"Book\";\n\n\t\tprivate static final AtomicBoolean failOnBook2GetId = new AtomicBoolean( false );\n\t\tprivate static final AtomicBoolean failOnBook2GetTitle = new AtomicBoolean( false );\n\n\t\tprivate Integer id;\n\n\t\tprivate String title;\n\n\t\tprivate String author;\n\n\t\tpublic Book() {\n\t\t}\n\n\t\tpublic Book(Integer id, String title, String author) {\n\t\t\tthis.id = id;\n\t\t\tthis.title = title;\n\t\t\tthis.author = author;\n\t\t}\n\n\t\t@Id // This must be on the getter, so that Hibernate Search uses getters instead of direct field access\n\t\tpublic Integer getId() {\n\t\t\tif ( id == 2 && failOnBook2GetId.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getId failure\" );\n\t\t\t}\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getTitle() {\n\t\t\tif ( id == 2 && failOnBook2GetTitle.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getTitle failure\" );\n\t\t\t}\n\t\t\treturn title;\n\t\t}\n\n\t\tpublic void setTitle(String title) {\n\t\t\tthis.title = title;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getAuthor() {\n\t\t\treturn author;\n\t\t}\n\n\t\tpublic void setAuthor(String author) {\n\t\t\tthis.author = author;\n\t\t}\n\t}\n\n\tprivate static class SimulatedFailure extends RuntimeException {\n\t\tSimulatedFailure(String message) {\n\t\t\tsuper( message );\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tindexerProducer,\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n\t\t\t\texpectationSetters\n\t\t);\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureIT#doMassIndexingWithFailure\n methodBody: private void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\nBook.failOnBook2GetId.set(ExecutionExpectation.FAIL.equals(book2GetIdExpectation));\nBook.failOnBook2GetTitle.set(ExecutionExpectation.FAIL.equals(book2GetTitleExpectation));\nAssertionError assertionError=null;\ntryOrmUtils.withinSession(sessionFactory,session -> {\n  SearchSession searchSession=Search.session(session);\n  MassIndexer indexer=searchSession.massIndexer();\n  for (  Runnable expectationSetter : expectationSetters) {\n    expectationSetter.run();\n  }\n  Runnable runnable=() -> {\n    try {\n      indexer.startAndWait();\n    }\n catch (    InterruptedException e) {\n      fail(\"Unexpected InterruptedException: \" + e.getMessage());\n    }\n  }\n;\n  if (thrownExpectation == null) {\n    runnable.run();\n  }\n else {\n    SubTest.expectException(runnable).assertThrown().satisfies(thrownExpectation);\n  }\n}\n);\nbackendMock.verifyExpectationsMet();\ncatch(AssertionError e)assertionError=e;\nthrow e;\nfinallyBook.failOnBook2GetId.set(false);\nBook.failOnBook2GetTitle.set(false);\nif(assertionError == null){switch(threadExpectation)case CREATED_AND_TERMINATED:Awaitility.await().untilAsserted(() -> assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isNotEmpty().allSatisfy(t -> assertThat(t).extracting(Thread::getState).isEqualTo(Thread.State.TERMINATED)));\nbreak;\ncase NOT_CREATED:assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isEmpty();\nbreak;\n}}"
        ],
        "sourceCodeAfterRefactoring": "private void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\nprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tindexerProducer,\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n\t\t\t\texpectationSetters\n\t\t);\n\t}",
        "diffSourceCode": "-  523: \t\tassertThat( staticCounters.get( StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT ) ).isEqualTo( 1 );\n-  524: \t}\n-  525: \n-  526: \tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n-  527: \t\t\tThreadExpectation threadExpectation,\n-  528: \t\t\tConsumer<Throwable> thrownExpectation,\n-  529: \t\t\tRunnable ... expectationSetters) {\n-  530: \t\tdoMassIndexingWithFailure(\n-  531: \t\t\t\tsessionFactory,\n-  532: \t\t\t\tthreadExpectation,\n-  533: \t\t\t\tthrownExpectation,\n-  534: \t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n-  535: \t\t\t\texpectationSetters\n-  536: \t\t);\n-  537: \t}\n-  538: \n-  539: \tprivate void doMassIndexingWithBook2GetIdFailure(SessionFactory sessionFactory) {\n-  540: \t\tdoMassIndexingWithFailure(\n-  541: \t\t\t\tsessionFactory,\n-  542: \t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n-  543: \t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n-  544: \t\t\t\t\t\t.hasMessageContainingAll(\n-  545: \t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n-  546: \t\t\t\t\t\t\t\t\"See the logs for details.\",\n-  547: \t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n-  548: \t\t\t\t\t\t\t\t\"Exception while invoking\"\n-  549: \t\t\t\t\t\t)\n+  523: \tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n+  524: \t\t\tThreadExpectation threadExpectation,\n+  525: \t\t\tConsumer<Throwable> thrownExpectation,\n+  526: \t\t\tRunnable ... expectationSetters) {\n+  527: \t\tdoMassIndexingWithFailure(\n+  528: \t\t\t\tsessionFactory,\n+  529: \t\t\t\tsearchSession -> searchSession.massIndexer(),\n+  530: \t\t\t\tthreadExpectation,\n+  531: \t\t\t\tthrownExpectation,\n+  532: \t\t\t\texpectationSetters\n+  533: \t\t);\n+  534: \t}\n+  535: \n+  536: \tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n+  537: \t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n+  538: \t\t\tThreadExpectation threadExpectation,\n+  539: \t\t\tConsumer<Throwable> thrownExpectation,\n+  540: \t\t\tRunnable ... expectationSetters) {\n+  541: \t\tdoMassIndexingWithFailure(\n+  542: \t\t\t\tsessionFactory,\n+  543: \t\t\t\tindexerProducer,\n+  544: \t\t\t\tthreadExpectation,\n+  545: \t\t\t\tthrownExpectation,\n+  546: \t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n+  547: \t\t\t\texpectationSetters\n+  548: \t\t);\n+  549: \t}\n",
        "uniqueId": "cbb08b0ad36186a75ae72b07dcc5842c0f6dcdd6_526_537_536_549_523_534",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate indexDataSet(indexMapping IndexMapping, indexManager StubMappingIndexManager) : void extracted from private initData() : void in class org.hibernate.search.integrationtest.backend.lucene.LuceneExtensionIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/LuceneExtensionIT.java",
                "startLine": 707,
                "endLine": 773,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/LuceneExtensionIT.java",
                "startLine": 708,
                "endLine": 726,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/LuceneExtensionIT.java",
                "startLine": 728,
                "endLine": 783,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void initData() {\n\t\tIndexIndexingPlan<? extends DocumentElement> plan = indexManager.createIndexingPlan();\n\t\tplan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.string, \"text 1\" );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 37 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 37 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 37 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"a\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.integer, 2 );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 78 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 78 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 78 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"a\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 40.12, -71.34 ) );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 13 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 13 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 13 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"a\" );\n\t\t} );\n\t\tplan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.nativeField, 89 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 89 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 89 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tdocument.addValue( indexMapping.string, \"text 2\" );\n\t\t\tdocument.addValue( indexMapping.integer, 1 );\n\t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 45.12, -75.34 ) );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"zz\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"zz\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"zz\" );\n\t\t} );\n\n\t\tplan.execute().join();\n\n\t\t// Check that all documents are searchable\n\t\tStubMappingScope scope = indexManager.createScope();\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\t\tassertThat( query ).hasDocRefHitsAnyOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}",
        "filePathBefore": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/LuceneExtensionIT.java",
        "isPureRefactoring": true,
        "commitId": "339a22f55f9fc6d9ff92ac27157f790083bf62e1",
        "packageNameBefore": "org.hibernate.search.integrationtest.backend.lucene",
        "classNameBefore": "org.hibernate.search.integrationtest.backend.lucene.LuceneExtensionIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.backend.lucene.LuceneExtensionIT#initData",
        "invokedMethod": "methodSignature: org.hibernate.search.backend.lucene.work.impl.LuceneExplainWork#execute\n methodBody: public Explanation execute(LuceneReadWorkExecutionContext context) {\ntryIndexSearcher indexSearcher=new IndexSearcher(context.getIndexReader());\nint luceneDocId=getLuceneDocId(indexSearcher);\nreturn searcher.explain(indexSearcher,luceneDocId);\ncatch(IOException e)throw log.ioExceptionOnQueryExecution(searcher.getLuceneQueryForExceptions(),context.getEventContext(),e);\n}\nmethodSignature: org.hibernate.search.integrationtest.backend.lucene.LuceneExtensionIT#query\n methodBody: public void query() {\nStubMappingScope scope=indexManager.createScope();\nSearchQuery<DocumentReference> genericQuery=scope.query().predicate(f -> f.matchAll()).toQuery();\nLuceneSearchQuery<DocumentReference> query=genericQuery.extension(LuceneExtension.get());\nLuceneSearchResult<DocumentReference> result=query.fetchAll();\nassertThat(result).fromQuery(query).hasDocRefHitsAnyOrder(INDEX_NAME,FIRST_ID,SECOND_ID,THIRD_ID,FOURTH_ID,FIFTH_ID).hasTotalHitCount(5);\nSubTest.expectException(() -> query.extension((SearchQuery<DocumentReference> original,LoadingContext<?,?> loadingContext) -> Optional.empty())).assertThrown().isInstanceOf(SearchException.class);\n}",
        "classSignatureBefore": "public class LuceneExtensionIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.lucene.LuceneExtensionIT#initData"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.lucene.LuceneExtensionIT"
        ],
        "classSignatureBeforeSet": [
            "public class LuceneExtensionIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.lucene;\n\nimport static org.hibernate.search.integrationtest.backend.lucene.testsupport.util.DocumentAssert.containsDocument;\nimport static org.hibernate.search.util.impl.integrationtest.common.assertion.SearchResultAssert.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMapperUtils.referenceProvider;\n\nimport java.util.List;\nimport java.util.Optional;\nimport java.util.function.Consumer;\nimport java.util.stream.Collectors;\n\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field.Store;\nimport org.apache.lucene.document.IntPoint;\nimport org.apache.lucene.document.LatLonPoint;\nimport org.apache.lucene.document.NumericDocValuesField;\nimport org.apache.lucene.document.StringField;\nimport org.apache.lucene.index.IndexableField;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.Explanation;\nimport org.apache.lucene.search.MatchAllDocsQuery;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.search.SortField.Type;\nimport org.apache.lucene.search.TermQuery;\nimport org.assertj.core.api.Assertions;\n\nimport org.hibernate.search.backend.lucene.LuceneBackend;\nimport org.hibernate.search.backend.lucene.index.LuceneIndexManager;\nimport org.hibernate.search.backend.lucene.search.query.dsl.LuceneSearchQueryOptionsStep;\nimport org.hibernate.search.backend.lucene.search.query.dsl.LuceneSearchQueryPredicateStep;\nimport org.hibernate.search.backend.lucene.search.query.dsl.LuceneSearchQueryHitTypeStep;\nimport org.hibernate.search.backend.lucene.search.query.LuceneSearchQuery;\nimport org.hibernate.search.backend.lucene.search.query.LuceneSearchResult;\nimport org.hibernate.search.backend.lucene.util.impl.LuceneFields;\nimport org.hibernate.search.engine.backend.Backend;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldReference;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.types.Projectable;\nimport org.hibernate.search.engine.backend.types.Sortable;\nimport org.hibernate.search.engine.backend.index.IndexManager;\nimport org.hibernate.search.engine.backend.work.execution.spi.IndexIndexingPlan;\nimport org.hibernate.search.engine.common.spi.SearchIntegration;\nimport org.hibernate.search.engine.search.common.ValueConvert;\nimport org.hibernate.search.engine.search.projection.SearchProjection;\nimport org.hibernate.search.engine.search.loading.context.spi.LoadingContext;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.ValueWrapper;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMappingIndexManager;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMappingScope;\nimport org.hibernate.search.backend.lucene.LuceneExtension;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.rule.SearchSetupHelper;\nimport org.hibernate.search.engine.reporting.spi.EventContexts;\nimport org.hibernate.search.engine.backend.common.DocumentReference;\nimport org.hibernate.search.engine.search.predicate.SearchPredicate;\nimport org.hibernate.search.engine.search.query.SearchQuery;\nimport org.hibernate.search.engine.search.sort.SearchSort;\nimport org.hibernate.search.engine.spatial.GeoPoint;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.FailureReportUtils;\nimport org.hibernate.search.util.impl.test.SubTest;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\n\npublic class LuceneExtensionIT {\n\n\tprivate static final String BACKEND_NAME = \"myLuceneBackend\";\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\tprivate static final String OTHER_INDEX_NAME = \"OtherIndexName\";\n\n\tprivate static final String FIRST_ID = \"1\";\n\tprivate static final String SECOND_ID = \"2\";\n\tprivate static final String THIRD_ID = \"3\";\n\tprivate static final String FOURTH_ID = \"4\";\n\tprivate static final String FIFTH_ID = \"5\";\n\n\t@Rule\n\tpublic SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic ExpectedException thrown = ExpectedException.none();\n\n\tprivate SearchIntegration integration;\n\n\tprivate IndexMapping indexMapping;\n\tprivate StubMappingIndexManager indexManager;\n\n\tprivate StubMappingIndexManager otherIndexManager;\n\n\t@Before\n\tpublic void setup() {\n\t\tthis.integration = setupHelper.start( BACKEND_NAME )\n\t\t\t\t.withIndex(\n\t\t\t\t\t\tINDEX_NAME,\n\t\t\t\t\t\tctx -> this.indexMapping = new IndexMapping( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.indexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.withIndex(\n\t\t\t\t\t\tOTHER_INDEX_NAME,\n\t\t\t\t\t\tctx -> new IndexMapping( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.otherIndexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.setup();\n\n\t\tinitData();\n\t}\n\n\t@Test\n\t@SuppressWarnings(\"unused\")\n\tpublic void queryContext() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\t// Put intermediary contexts into variables to check they have the right type\n\t\tLuceneSearchQueryHitTypeStep<DocumentReference, DocumentReference> context1 =\n\t\t\t\tscope.query().extension( LuceneExtension.get() );\n\t\tLuceneSearchQueryPredicateStep<DocumentReference> context2 = context1.asProjection(\n\t\t\t\tf -> f.composite(\n\t\t\t\t\t\t// We don't care about the document, it's just to test that the factory context allows Lucene-specific projection\n\t\t\t\t\t\t(docRef, document) -> docRef,\n\t\t\t\t\t\tf.documentReference(), f.document()\n\t\t\t\t)\n\t\t);\n\t\t// Note we can use Lucene-specific predicates immediately\n\t\tLuceneSearchQueryOptionsStep<DocumentReference> context3 =\n\t\t\t\tcontext2.predicate( f -> f.fromLuceneQuery( new MatchAllDocsQuery() ) );\n\t\t// Note we can use Lucene-specific sorts immediately\n\t\tLuceneSearchQueryOptionsStep<DocumentReference> context4 =\n\t\t\t\tcontext3.sort( f -> f.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) ) );\n\n\t\t// Put the query and result into variables to check they have the right type\n\t\tLuceneSearchQuery<DocumentReference> query = context4.toQuery();\n\t\tLuceneSearchResult<DocumentReference> result = query.fetchAll();\n\n\t\tassertThat( result ).fromQuery( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID )\n\t\t\t\t.hasTotalHitCount( 5 );\n\n\t\t// Also check (at compile time) the context type for other asXXX() methods, since we need to override each method explicitly\n\t\tLuceneSearchQueryPredicateStep<DocumentReference> asReferenceContext =\n\t\t\t\tscope.query().extension( LuceneExtension.get() ).asEntityReference();\n\t\tLuceneSearchQueryPredicateStep<DocumentReference> asEntityContext =\n\t\t\t\tscope.query().extension( LuceneExtension.get() ).asEntity();\n\t\tSearchProjection<DocumentReference> projection = scope.projection().documentReference().toProjection();\n\t\tLuceneSearchQueryPredicateStep<DocumentReference> asProjectionContext =\n\t\t\t\tscope.query().extension( LuceneExtension.get() ).asProjection( projection );\n\t\tLuceneSearchQueryPredicateStep<List<?>> asProjectionsContext =\n\t\t\t\tscope.query().extension( LuceneExtension.get() ).asProjections( projection, projection );\n\t\tLuceneSearchQueryOptionsStep<DocumentReference> defaultResultContext =\n\t\t\t\tscope.query().extension( LuceneExtension.get() )\n\t\t\t\t\t\t.predicate( f -> f.fromLuceneQuery( new MatchAllDocsQuery() ) );\n\t}\n\n\t@Test\n\tpublic void query() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<DocumentReference> genericQuery = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\n\t\t// Put the query and result into variables to check they have the right type\n\t\tLuceneSearchQuery<DocumentReference> query = genericQuery.extension( LuceneExtension.get() );\n\t\tLuceneSearchResult<DocumentReference> result = query.fetchAll();\n\t\tassertThat( result ).fromQuery( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID )\n\t\t\t\t.hasTotalHitCount( 5 );\n\n\t\t// Unsupported extension\n\t\tSubTest.expectException(\n\t\t\t\t() -> query.extension( (SearchQuery<DocumentReference> original, LoadingContext<?, ?> loadingContext) -> Optional.empty() )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class );\n\t}\n\n\t@Test\n\tpublic void query_explain_singleIndex() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tLuceneSearchQuery<DocumentReference> query = scope.query().extension( LuceneExtension.get() )\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\t// Matching document\n\t\tAssertions.assertThat( query.explain( FIRST_ID ) )\n\t\t\t\t.extracting( Object::toString ).asString()\n\t\t\t\t.contains( LuceneFields.idFieldName() );\n\n\t\t// Non-matching document\n\t\tAssertions.assertThat( query.explain( FIFTH_ID ) )\n\t\t\t\t.extracting( Object::toString ).asString()\n\t\t\t\t.contains( LuceneFields.idFieldName() );\n\t}\n\n\t@Test\n\tpublic void query_explain_singleIndex_invalidId() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tLuceneSearchQuery<DocumentReference> query = scope.query().extension( LuceneExtension.get() )\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\t// Non-existing document\n\t\tSubTest.expectException(\n\t\t\t\t() -> query.explain( \"InvalidId\" )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining(\n\t\t\t\t\t\t\"Document with id 'InvalidId' does not exist in index '\" + INDEX_NAME + \"'\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void query_explain_multipleIndexes() {\n\t\tStubMappingScope scope = indexManager.createScope( otherIndexManager );\n\n\t\tLuceneSearchQuery<DocumentReference> query = scope.query().extension( LuceneExtension.get() )\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\t// Matching document\n\t\tAssertions.assertThat( query.explain( INDEX_NAME, FIRST_ID ) )\n\t\t\t\t.extracting( Object::toString ).asString()\n\t\t\t\t.contains( LuceneFields.idFieldName() );\n\n\t\t// Non-matching document\n\t\tAssertions.assertThat( query.explain( INDEX_NAME, FIFTH_ID ) )\n\t\t\t\t.extracting( Object::toString ).asString()\n\t\t\t\t.contains( LuceneFields.idFieldName() );\n\t}\n\n\t@Test\n\tpublic void query_explain_multipleIndexes_missingIndexName() {\n\t\tStubMappingScope scope = indexManager.createScope( otherIndexManager );\n\n\t\tLuceneSearchQuery<DocumentReference> query = scope.query().extension( LuceneExtension.get() )\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\tSubTest.expectException(\n\t\t\t\t() -> query.explain( FIRST_ID )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"explain(String id) cannot be used when the query targets multiple indexes\" )\n\t\t\t\t.hasMessageContaining(\n\t\t\t\t\t\t\"pass one of [\" + INDEX_NAME + \", \" + OTHER_INDEX_NAME + \"]\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void query_explain_multipleIndexes_invalidIndexName() {\n\t\tStubMappingScope scope = indexManager.createScope( otherIndexManager );\n\n\t\tLuceneSearchQuery<DocumentReference> query = scope.query().extension( LuceneExtension.get() )\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\tSubTest.expectException(\n\t\t\t\t() -> query.explain( \"NotAnIndexName\", FIRST_ID )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining(\n\t\t\t\t\t\t\"index name 'NotAnIndexName' is not among the indexes targeted by this query: [\"\n\t\t\t\t\t\t\t\t+ INDEX_NAME + \", \" + OTHER_INDEX_NAME + \"]\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.bool()\n\t\t\t\t\t\t.should( f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.should( f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.should( f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) )\n\t\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.toQuery();\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasTotalHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery_separatePredicate() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchPredicate predicate1 = scope.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) ).toPredicate();\n\t\tSearchPredicate predicate2 = scope.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) ).toPredicate();\n\t\tSearchPredicate predicate3 = scope.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) ).toPredicate();\n\t\tSearchPredicate booleanPredicate = scope.predicate().bool()\n\t\t\t\t.should( predicate1 )\n\t\t\t\t.should( predicate2 )\n\t\t\t\t.should( predicate3 )\n\t\t\t\t.toPredicate();\n\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( booleanPredicate )\n\t\t\t\t.toQuery();\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasTotalHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( f -> f\n\t\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.toQuery();\n\t\tassertThat( query ).hasDocRefHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\n\t\tquery = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( f -> f\n\t\t\t\t\t\t.extension().ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSort( new Sort(\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.orElseFail()\n\t\t\t\t)\n\t\t\t\t.toQuery();\n\t\tassertThat( query ).hasDocRefHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tTHIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField_separateSort() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchSort sort1 = scope.sort().extension()\n\t\t\t\t\t\t.ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.orElseFail()\n\t\t\t\t.toSort();\n\t\tSearchSort sort2 = scope.sort().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t.toSort();\n\t\tSearchSort sort3 = scope.sort().extension()\n\t\t\t\t.ifSupported(\n\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.orElseFail()\n\t\t\t\t.toSort();\n\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( f -> f.composite().add( sort1 ).add( sort2 ).add( sort3 ) )\n\t\t\t\t.toQuery();\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID );\n\n\t\tSearchSort sort = scope.sort()\n\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneSort( new Sort(\n\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.toSort();\n\n\t\tquery = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( sort )\n\t\t\t\t.toQuery();\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsExactOrder( INDEX_NAME, THIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"match() predicate on unsupported native field\",\n\t\t\t\t() -> scope.query()\n\t\t\t\t\t\t.predicate( f -> f.match().field( \"nativeField\" ).matching( \"37\" ) )\n\t\t\t\t\t\t.toQuery()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining predicates with the DSL: use the Lucene extension and a native query.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_fromLuceneQuery() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"nativeField\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.toQuery();\n\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_exists() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"exists() predicate on unsupported native field\",\n\t\t\t\t() -> scope.predicate().exists().field( \"nativeField\" )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining predicates with the DSL: use the Lucene extension and a native query.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void sort_nativeField() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"sort on unsupported native field\",\n\t\t\t\t() -> scope.query()\n\t\t\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t\t\t.sort( f -> f.field( \"nativeField\" ) )\n\t\t\t\t\t\t.toQuery()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining sorts with the DSL: use the Lucene extension and a native sort.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void sort_nativeField_fromLuceneSortField() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( f -> f.extension( LuceneExtension.get() ).fromLuceneSortField( new SortField( \"nativeField\", Type.LONG ) ) )\n\t\t\t\t.toQuery();\n\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsExactOrder( INDEX_NAME, FIFTH_ID, THIRD_ID, FIRST_ID, SECOND_ID, FOURTH_ID );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<Integer> query = scope.query()\n\t\t\t\t.asProjection( f -> f.field( \"nativeField\", Integer.class ) )\n\t\t\t\t.predicate( f -> f.match().field( \"string\" ).matching( \"text 1\" ) )\n\t\t\t\t.toQuery();\n\n\t\tassertThat( query ).hasHitsAnyOrder( 37 );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField_withProjectionConverters_enabled() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<ValueWrapper> query = scope.query()\n\t\t\t\t.asProjection( f -> f.field( \"nativeField_converted\", ValueWrapper.class ) )\n\t\t\t\t.predicate( f -> f.match().field( \"string\" ).matching( \"text 1\" ) )\n\t\t\t\t.toQuery();\n\n\t\tassertThat( query ).hasHitsAnyOrder( new ValueWrapper<>( 37 ) );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField_withProjectionConverters_disabled() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<Integer> query = scope.query()\n\t\t\t\t.asProjection( f -> f.field( \"nativeField_converted\", Integer.class, ValueConvert.NO ) )\n\t\t\t\t.predicate( f -> f.match().field( \"string\" ).matching( \"text 1\" ) )\n\t\t\t\t.toQuery();\n\n\t\tassertThat( query ).hasHitsAnyOrder( 37 );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField_unsupportedProjection() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\t// let's check that it's possible to query the field beforehand\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"nativeField_unsupportedProjection\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.toQuery();\n\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\n\t\t// now, let's check that projecting on the field throws an exception\n\t\tSubTest.expectException(\n\t\t\t\t\"projection on native field not supporting projections\",\n\t\t\t\t() -> scope.projection().field( \"nativeField_unsupportedProjection\", Integer.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Projections are not enabled for field\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField_unsupportedProjection\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void projection_document() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<Document> query = scope.query()\n\t\t\t\t.asProjection(\n\t\t\t\t\t\tf -> f.extension( LuceneExtension.get() ).document()\n\t\t\t\t)\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\n\t\tList<Document> result = query.fetchAll().getHits();\n\t\tAssertions.assertThat( result )\n\t\t\t\t.hasSize( 5 )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"string\", \"text 1\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField\", \"37\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_converted\", \"37\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"37\" )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"integer\", 2 )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField\", \"78\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_converted\", \"78\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"78\" )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"nativeField\", \"13\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_converted\", \"13\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"13\" )\n\t\t\t\t\t\t\t\t// Geo points are stored as two internal fields\n\t\t\t\t\t\t\t\t.hasInternalField( \"geoPoint_latitude\", 40.12 )\n\t\t\t\t\t\t\t\t.hasInternalField( \"geoPoint_longitude\", -71.34 )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"nativeField\", \"89\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_converted\", \"89\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"89\" )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"string\", \"text 2\" )\n\t\t\t\t\t\t\t\t.hasField( \"integer\", 1 )\n\t\t\t\t\t\t\t\t// Geo points are stored as two internal fields\n\t\t\t\t\t\t\t\t.hasInternalField( \"geoPoint_latitude\", 45.12 )\n\t\t\t\t\t\t\t\t.hasInternalField( \"geoPoint_longitude\", -75.34 )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) );\n\t}\n\n\t/**\n\t * Check that the projection on a document includes all fields,\n\t * even if there is a field projection, which would usually trigger document filtering.\n\t */\n\t@Test\n\tpublic void projection_documentAndField() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<List<?>> query = scope.query()\n\t\t\t\t.asProjection( f ->\n\t\t\t\t\t\tf.composite(\n\t\t\t\t\t\t\t\tf.extension( LuceneExtension.get() ).document(),\n\t\t\t\t\t\t\t\tf.field( \"string\" )\n\t\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\tList<Document> result = query.fetchAll().getHits().stream()\n\t\t\t\t.map( list -> (Document) list.get( 0 ) )\n\t\t\t\t.collect( Collectors.toList() );\n\t\tAssertions.assertThat( result )\n\t\t\t\t.hasSize( 1 )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"string\", \"text 1\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField\", \"37\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_converted\", \"37\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"37\" )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void projection_explanation() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<Explanation> query = scope.query()\n\t\t\t\t.asProjection( f -> f.extension( LuceneExtension.get() ).explanation() )\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\tList<Explanation> result = query.fetchAll().getHits();\n\t\tAssertions.assertThat( result ).hasSize( 1 );\n\t\tAssertions.assertThat( result.get( 0 ) )\n\t\t\t\t.isInstanceOf( Explanation.class )\n\t\t\t\t.extracting( Object::toString ).asString()\n\t\t\t\t.contains( LuceneFields.idFieldName() );\n\t}\n\n\t@Test\n\tpublic void nativeField_invalidFieldPath() {\n\t\tIndexIndexingPlan<? extends DocumentElement> plan = indexManager.createIndexingPlan();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"native field contributing field with invalid field path\",\n\t\t\t\t() -> plan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\t\t\tdocument.addValue( indexMapping.nativeField_invalidFieldPath, 45 );\n\t\t\t\t} ) )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Invalid field path; expected path 'nativeField_invalidFieldPath', got 'not the expected path'.\" );\n\t}\n\n\t@Test\n\tpublic void backend_unwrap() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tAssertions.assertThat( backend.unwrap( LuceneBackend.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void backend_unwrap_error_unknownType() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene backend to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this backend can only be unwrapped to '\" + LuceneBackend.class.getName() + \"'\" );\n\n\t\tbackend.unwrap( String.class );\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\t\tAssertions.assertThat( indexManager.unwrap( LuceneIndexManager.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap_error_unknownType() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene index manager to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this index manager can only be unwrapped to '\" + LuceneIndexManager.class.getName() + \"'\" );\n\n\t\tindexManager.unwrap( String.class );\n\t}\n\n\tprivate void initData() {\n\t\tIndexIndexingPlan<? extends DocumentElement> plan = indexManager.createIndexingPlan();\n\t\tplan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.string, \"text 1\" );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 37 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 37 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 37 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"a\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.integer, 2 );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 78 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 78 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 78 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"a\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 40.12, -71.34 ) );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 13 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 13 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 13 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"a\" );\n\t\t} );\n\t\tplan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.nativeField, 89 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 89 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 89 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tdocument.addValue( indexMapping.string, \"text 2\" );\n\t\t\tdocument.addValue( indexMapping.integer, 1 );\n\t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 45.12, -75.34 ) );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"zz\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"zz\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"zz\" );\n\t\t} );\n\n\t\tplan.execute().join();\n\n\t\t// Check that all documents are searchable\n\t\tStubMappingScope scope = indexManager.createScope();\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\t\tassertThat( query ).hasDocRefHitsAnyOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}\n\n\tprivate static class IndexMapping {\n\t\tfinal IndexFieldReference<Integer> integer;\n\t\tfinal IndexFieldReference<String> string;\n\t\tfinal IndexFieldReference<GeoPoint> geoPoint;\n\t\tfinal IndexFieldReference<Integer> nativeField;\n\t\tfinal IndexFieldReference<Integer> nativeField_converted;\n\t\tfinal IndexFieldReference<Integer> nativeField_unsupportedProjection;\n\t\tfinal IndexFieldReference<Integer> nativeField_invalidFieldPath;\n\n\t\tfinal IndexFieldReference<String> sort1;\n\t\tfinal IndexFieldReference<String> sort2;\n\t\tfinal IndexFieldReference<String> sort3;\n\n\t\tIndexMapping(IndexSchemaElement root) {\n\t\t\tinteger = root.field(\n\t\t\t\t\t\"integer\",\n\t\t\t\t\tf -> f.asInteger().projectable( Projectable.YES )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tstring = root.field(\n\t\t\t\t\t\"string\",\n\t\t\t\t\tf -> f.asString().projectable( Projectable.YES )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tgeoPoint = root.field(\n\t\t\t\t\t\"geoPoint\",\n\t\t\t\t\tf -> f.asGeoPoint().projectable( Projectable.YES )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tnativeField = root.field(\n\t\t\t\t\t\"nativeField\",\n\t\t\t\t\tf -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.asNative( Integer.class, LuceneExtensionIT::contributeNativeField, LuceneExtensionIT::fromNativeField )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tnativeField_converted = root.field(\n\t\t\t\t\t\"nativeField_converted\",\n\t\t\t\t\tf -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.asNative( Integer.class, LuceneExtensionIT::contributeNativeField, LuceneExtensionIT::fromNativeField )\n\t\t\t\t\t\t\t.projectionConverter( ValueWrapper.class, ValueWrapper.fromIndexFieldConverter() )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tnativeField_unsupportedProjection = root.field(\n\t\t\t\t\t\"nativeField_unsupportedProjection\",\n\t\t\t\t\tf -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.asNative( Integer.class, LuceneExtensionIT::contributeNativeField )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tnativeField_invalidFieldPath = root.field(\n\t\t\t\t\t\"nativeField_invalidFieldPath\",\n\t\t\t\t\tf -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.asNative( Integer.class, LuceneExtensionIT::contributeNativeFieldInvalidFieldPath )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\n\t\t\tsort1 = root.field( \"sort1\", f -> f.asString().sortable( Sortable.YES ) )\n\t\t\t\t\t.toReference();\n\t\t\tsort2 = root.field( \"sort2\", f -> f.asString().sortable( Sortable.YES ) )\n\t\t\t\t\t.toReference();\n\t\t\tsort3 = root.field( \"sort3\", f -> f.asString().sortable( Sortable.YES ) )\n\t\t\t\t\t.toReference();\n\t\t}\n\t}\n\n\tprivate static void contributeNativeField(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( absoluteFieldPath, value.toString(), Store.YES ) );\n\t\tcollector.accept( new NumericDocValuesField( absoluteFieldPath, value.longValue() ) );\n\t}\n\n\tprivate static Integer fromNativeField(IndexableField field) {\n\t\treturn Integer.parseInt( field.stringValue() );\n\t}\n\n\tprivate static void contributeNativeFieldInvalidFieldPath(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( \"not the expected path\", value.toString(), Store.YES ) );\n\t}\n}\n",
        "filePathAfter": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/LuceneExtensionIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.lucene;\n\nimport static org.hibernate.search.integrationtest.backend.lucene.testsupport.util.DocumentAssert.containsDocument;\nimport static org.hibernate.search.util.impl.integrationtest.common.assertion.SearchResultAssert.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMapperUtils.referenceProvider;\n\nimport java.util.List;\nimport java.util.Optional;\nimport java.util.function.Consumer;\nimport java.util.stream.Collectors;\n\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field.Store;\nimport org.apache.lucene.document.IntPoint;\nimport org.apache.lucene.document.LatLonPoint;\nimport org.apache.lucene.document.NumericDocValuesField;\nimport org.apache.lucene.document.StringField;\nimport org.apache.lucene.index.IndexableField;\nimport org.apache.lucene.index.Term;\nimport org.apache.lucene.search.Explanation;\nimport org.apache.lucene.search.MatchAllDocsQuery;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.search.SortField.Type;\nimport org.apache.lucene.search.TermQuery;\nimport org.assertj.core.api.Assertions;\n\nimport org.hibernate.search.backend.lucene.LuceneBackend;\nimport org.hibernate.search.backend.lucene.index.LuceneIndexManager;\nimport org.hibernate.search.backend.lucene.search.query.dsl.LuceneSearchQueryOptionsStep;\nimport org.hibernate.search.backend.lucene.search.query.dsl.LuceneSearchQueryPredicateStep;\nimport org.hibernate.search.backend.lucene.search.query.dsl.LuceneSearchQueryHitTypeStep;\nimport org.hibernate.search.backend.lucene.search.query.LuceneSearchQuery;\nimport org.hibernate.search.backend.lucene.search.query.LuceneSearchResult;\nimport org.hibernate.search.backend.lucene.util.impl.LuceneFields;\nimport org.hibernate.search.engine.backend.Backend;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldReference;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.types.Projectable;\nimport org.hibernate.search.engine.backend.types.Sortable;\nimport org.hibernate.search.engine.backend.index.IndexManager;\nimport org.hibernate.search.engine.backend.work.execution.spi.IndexIndexingPlan;\nimport org.hibernate.search.engine.common.spi.SearchIntegration;\nimport org.hibernate.search.engine.search.common.ValueConvert;\nimport org.hibernate.search.engine.search.projection.SearchProjection;\nimport org.hibernate.search.engine.search.loading.context.spi.LoadingContext;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.ValueWrapper;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMappingIndexManager;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.mapper.StubMappingScope;\nimport org.hibernate.search.backend.lucene.LuceneExtension;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.rule.SearchSetupHelper;\nimport org.hibernate.search.engine.reporting.spi.EventContexts;\nimport org.hibernate.search.engine.backend.common.DocumentReference;\nimport org.hibernate.search.engine.search.predicate.SearchPredicate;\nimport org.hibernate.search.engine.search.query.SearchQuery;\nimport org.hibernate.search.engine.search.sort.SearchSort;\nimport org.hibernate.search.engine.spatial.GeoPoint;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.FailureReportUtils;\nimport org.hibernate.search.util.impl.test.SubTest;\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.rules.ExpectedException;\n\npublic class LuceneExtensionIT {\n\n\tprivate static final String BACKEND_NAME = \"myLuceneBackend\";\n\tprivate static final String INDEX_NAME = \"IndexName\";\n\tprivate static final String OTHER_INDEX_NAME = \"OtherIndexName\";\n\n\tprivate static final String FIRST_ID = \"1\";\n\tprivate static final String SECOND_ID = \"2\";\n\tprivate static final String THIRD_ID = \"3\";\n\tprivate static final String FOURTH_ID = \"4\";\n\tprivate static final String FIFTH_ID = \"5\";\n\n\t@Rule\n\tpublic SearchSetupHelper setupHelper = new SearchSetupHelper();\n\n\t@Rule\n\tpublic ExpectedException thrown = ExpectedException.none();\n\n\tprivate SearchIntegration integration;\n\n\tprivate IndexMapping indexMapping;\n\tprivate StubMappingIndexManager indexManager;\n\n\tprivate IndexMapping otherIndexMapping;\n\tprivate StubMappingIndexManager otherIndexManager;\n\n\t@Before\n\tpublic void setup() {\n\t\tthis.integration = setupHelper.start( BACKEND_NAME )\n\t\t\t\t.withIndex(\n\t\t\t\t\t\tINDEX_NAME,\n\t\t\t\t\t\tctx -> this.indexMapping = new IndexMapping( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.indexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.withIndex(\n\t\t\t\t\t\tOTHER_INDEX_NAME,\n\t\t\t\t\t\tctx -> this.otherIndexMapping = new IndexMapping( ctx.getSchemaElement() ),\n\t\t\t\t\t\tindexManager -> this.otherIndexManager = indexManager\n\t\t\t\t)\n\t\t\t\t.setup();\n\n\t\tinitData();\n\t}\n\n\t@Test\n\t@SuppressWarnings(\"unused\")\n\tpublic void queryContext() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\t// Put intermediary contexts into variables to check they have the right type\n\t\tLuceneSearchQueryHitTypeStep<DocumentReference, DocumentReference> context1 =\n\t\t\t\tscope.query().extension( LuceneExtension.get() );\n\t\tLuceneSearchQueryPredicateStep<DocumentReference> context2 = context1.asProjection(\n\t\t\t\tf -> f.composite(\n\t\t\t\t\t\t// We don't care about the document, it's just to test that the factory context allows Lucene-specific projection\n\t\t\t\t\t\t(docRef, document) -> docRef,\n\t\t\t\t\t\tf.documentReference(), f.document()\n\t\t\t\t)\n\t\t);\n\t\t// Note we can use Lucene-specific predicates immediately\n\t\tLuceneSearchQueryOptionsStep<DocumentReference> context3 =\n\t\t\t\tcontext2.predicate( f -> f.fromLuceneQuery( new MatchAllDocsQuery() ) );\n\t\t// Note we can use Lucene-specific sorts immediately\n\t\tLuceneSearchQueryOptionsStep<DocumentReference> context4 =\n\t\t\t\tcontext3.sort( f -> f.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) ) );\n\n\t\t// Put the query and result into variables to check they have the right type\n\t\tLuceneSearchQuery<DocumentReference> query = context4.toQuery();\n\t\tLuceneSearchResult<DocumentReference> result = query.fetchAll();\n\n\t\tassertThat( result ).fromQuery( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID )\n\t\t\t\t.hasTotalHitCount( 5 );\n\n\t\t// Also check (at compile time) the context type for other asXXX() methods, since we need to override each method explicitly\n\t\tLuceneSearchQueryPredicateStep<DocumentReference> asReferenceContext =\n\t\t\t\tscope.query().extension( LuceneExtension.get() ).asEntityReference();\n\t\tLuceneSearchQueryPredicateStep<DocumentReference> asEntityContext =\n\t\t\t\tscope.query().extension( LuceneExtension.get() ).asEntity();\n\t\tSearchProjection<DocumentReference> projection = scope.projection().documentReference().toProjection();\n\t\tLuceneSearchQueryPredicateStep<DocumentReference> asProjectionContext =\n\t\t\t\tscope.query().extension( LuceneExtension.get() ).asProjection( projection );\n\t\tLuceneSearchQueryPredicateStep<List<?>> asProjectionsContext =\n\t\t\t\tscope.query().extension( LuceneExtension.get() ).asProjections( projection, projection );\n\t\tLuceneSearchQueryOptionsStep<DocumentReference> defaultResultContext =\n\t\t\t\tscope.query().extension( LuceneExtension.get() )\n\t\t\t\t\t\t.predicate( f -> f.fromLuceneQuery( new MatchAllDocsQuery() ) );\n\t}\n\n\t@Test\n\tpublic void query() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<DocumentReference> genericQuery = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\n\t\t// Put the query and result into variables to check they have the right type\n\t\tLuceneSearchQuery<DocumentReference> query = genericQuery.extension( LuceneExtension.get() );\n\t\tLuceneSearchResult<DocumentReference> result = query.fetchAll();\n\t\tassertThat( result ).fromQuery( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID )\n\t\t\t\t.hasTotalHitCount( 5 );\n\n\t\t// Unsupported extension\n\t\tSubTest.expectException(\n\t\t\t\t() -> query.extension( (SearchQuery<DocumentReference> original, LoadingContext<?, ?> loadingContext) -> Optional.empty() )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class );\n\t}\n\n\t@Test\n\tpublic void query_explain_singleIndex() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tLuceneSearchQuery<DocumentReference> query = scope.query().extension( LuceneExtension.get() )\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\t// Matching document\n\t\tAssertions.assertThat( query.explain( FIRST_ID ) )\n\t\t\t\t.extracting( Object::toString ).asString()\n\t\t\t\t.contains( LuceneFields.idFieldName() );\n\n\t\t// Non-matching document\n\t\tAssertions.assertThat( query.explain( FIFTH_ID ) )\n\t\t\t\t.extracting( Object::toString ).asString()\n\t\t\t\t.contains( LuceneFields.idFieldName() );\n\t}\n\n\t@Test\n\tpublic void query_explain_singleIndex_invalidId() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tLuceneSearchQuery<DocumentReference> query = scope.query().extension( LuceneExtension.get() )\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\t// Non-existing document\n\t\tSubTest.expectException(\n\t\t\t\t() -> query.explain( \"InvalidId\" )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining(\n\t\t\t\t\t\t\"Document with id 'InvalidId' does not exist in index '\" + INDEX_NAME + \"'\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void query_explain_multipleIndexes() {\n\t\tStubMappingScope scope = indexManager.createScope( otherIndexManager );\n\n\t\tLuceneSearchQuery<DocumentReference> query = scope.query().extension( LuceneExtension.get() )\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\t// Matching document\n\t\tAssertions.assertThat( query.explain( INDEX_NAME, FIRST_ID ) )\n\t\t\t\t.extracting( Object::toString ).asString()\n\t\t\t\t.contains( LuceneFields.idFieldName() );\n\n\t\t// Non-matching document\n\t\tAssertions.assertThat( query.explain( INDEX_NAME, FIFTH_ID ) )\n\t\t\t\t.extracting( Object::toString ).asString()\n\t\t\t\t.contains( LuceneFields.idFieldName() );\n\t}\n\n\t@Test\n\tpublic void query_explain_multipleIndexes_missingIndexName() {\n\t\tStubMappingScope scope = indexManager.createScope( otherIndexManager );\n\n\t\tLuceneSearchQuery<DocumentReference> query = scope.query().extension( LuceneExtension.get() )\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\tSubTest.expectException(\n\t\t\t\t() -> query.explain( FIRST_ID )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"explain(String id) cannot be used when the query targets multiple indexes\" )\n\t\t\t\t.hasMessageContaining(\n\t\t\t\t\t\t\"pass one of [\" + INDEX_NAME + \", \" + OTHER_INDEX_NAME + \"]\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void query_explain_multipleIndexes_invalidIndexName() {\n\t\tStubMappingScope scope = indexManager.createScope( otherIndexManager );\n\n\t\tLuceneSearchQuery<DocumentReference> query = scope.query().extension( LuceneExtension.get() )\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\tSubTest.expectException(\n\t\t\t\t() -> query.explain( \"NotAnIndexName\", FIRST_ID )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining(\n\t\t\t\t\t\t\"index name 'NotAnIndexName' is not among the indexes targeted by this query: [\"\n\t\t\t\t\t\t\t\t+ INDEX_NAME + \", \" + OTHER_INDEX_NAME + \"]\"\n\t\t\t\t);\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.bool()\n\t\t\t\t\t\t.should( f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.should( f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.should( f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) )\n\t\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.toQuery();\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasTotalHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void predicate_fromLuceneQuery_separatePredicate() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchPredicate predicate1 = scope.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"string\", \"text 1\" ) ) ).toPredicate();\n\t\tSearchPredicate predicate2 = scope.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( IntPoint.newExactQuery( \"integer\", 2 ) ).toPredicate();\n\t\tSearchPredicate predicate3 = scope.predicate().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneQuery( LatLonPoint.newDistanceQuery( \"geoPoint\", 40, -70, 200_000 ) ).toPredicate();\n\t\tSearchPredicate booleanPredicate = scope.predicate().bool()\n\t\t\t\t.should( predicate1 )\n\t\t\t\t.should( predicate2 )\n\t\t\t\t.should( predicate3 )\n\t\t\t\t.toPredicate();\n\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( booleanPredicate )\n\t\t\t\t.toQuery();\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID )\n\t\t\t\t.hasTotalHitCount( 3 );\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( f -> f\n\t\t\t\t\t\t.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t\t\t.then().extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t\t.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.toQuery();\n\t\tassertThat( query ).hasDocRefHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\n\t\tquery = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( f -> f\n\t\t\t\t\t\t.extension().ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSort( new Sort(\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.orElseFail()\n\t\t\t\t)\n\t\t\t\t.toQuery();\n\t\tassertThat( query ).hasDocRefHitsExactOrder(\n\t\t\t\tINDEX_NAME,\n\t\t\t\tTHIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID\n\t\t);\n\t}\n\n\t@Test\n\tpublic void sort_fromLuceneSortField_separateSort() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchSort sort1 = scope.sort().extension()\n\t\t\t\t\t\t.ifSupported(\n\t\t\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort1\", Type.STRING ) )\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.orElseFail()\n\t\t\t\t.toSort();\n\t\tSearchSort sort2 = scope.sort().extension( LuceneExtension.get() )\n\t\t\t\t.fromLuceneSortField( new SortField( \"sort2\", Type.STRING ) )\n\t\t\t\t.toSort();\n\t\tSearchSort sort3 = scope.sort().extension()\n\t\t\t\t.ifSupported(\n\t\t\t\t\t\tLuceneExtension.get(),\n\t\t\t\t\t\tc2 -> c2.fromLuceneSortField( new SortField( \"sort3\", Type.STRING ) )\n\t\t\t\t)\n\t\t\t\t.orElseFail()\n\t\t\t\t.toSort();\n\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( f -> f.composite().add( sort1 ).add( sort2 ).add( sort3 ) )\n\t\t\t\t.toQuery();\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsExactOrder( INDEX_NAME, FIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID );\n\n\t\tSearchSort sort = scope.sort()\n\t\t\t\t.extension( LuceneExtension.get() ).fromLuceneSort( new Sort(\n\t\t\t\t\t\tnew SortField( \"sort3\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort2\", Type.STRING ),\n\t\t\t\t\t\tnew SortField( \"sort1\", Type.STRING )\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.toSort();\n\n\t\tquery = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( sort )\n\t\t\t\t.toQuery();\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsExactOrder( INDEX_NAME, THIRD_ID, SECOND_ID, FIRST_ID, FOURTH_ID, FIFTH_ID );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"match() predicate on unsupported native field\",\n\t\t\t\t() -> scope.query()\n\t\t\t\t\t\t.predicate( f -> f.match().field( \"nativeField\" ).matching( \"37\" ) )\n\t\t\t\t\t\t.toQuery()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining predicates with the DSL: use the Lucene extension and a native query.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_fromLuceneQuery() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"nativeField\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.toQuery();\n\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\t}\n\n\t@Test\n\tpublic void predicate_nativeField_exists() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"exists() predicate on unsupported native field\",\n\t\t\t\t() -> scope.predicate().exists().field( \"nativeField\" )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining predicates with the DSL: use the Lucene extension and a native query.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void sort_nativeField() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"sort on unsupported native field\",\n\t\t\t\t() -> scope.query()\n\t\t\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t\t\t.sort( f -> f.field( \"nativeField\" ) )\n\t\t\t\t\t\t.toQuery()\n\t\t\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Native fields do not support defining sorts with the DSL: use the Lucene extension and a native sort.\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void sort_nativeField_fromLuceneSortField() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.sort( f -> f.extension( LuceneExtension.get() ).fromLuceneSortField( new SortField( \"nativeField\", Type.LONG ) ) )\n\t\t\t\t.toQuery();\n\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsExactOrder( INDEX_NAME, FIFTH_ID, THIRD_ID, FIRST_ID, SECOND_ID, FOURTH_ID );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<Integer> query = scope.query()\n\t\t\t\t.asProjection( f -> f.field( \"nativeField\", Integer.class ) )\n\t\t\t\t.predicate( f -> f.match().field( \"string\" ).matching( \"text 1\" ) )\n\t\t\t\t.toQuery();\n\n\t\tassertThat( query ).hasHitsAnyOrder( 37 );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField_withProjectionConverters_enabled() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<ValueWrapper> query = scope.query()\n\t\t\t\t.asProjection( f -> f.field( \"nativeField_converted\", ValueWrapper.class ) )\n\t\t\t\t.predicate( f -> f.match().field( \"string\" ).matching( \"text 1\" ) )\n\t\t\t\t.toQuery();\n\n\t\tassertThat( query ).hasHitsAnyOrder( new ValueWrapper<>( 37 ) );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField_withProjectionConverters_disabled() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<Integer> query = scope.query()\n\t\t\t\t.asProjection( f -> f.field( \"nativeField_converted\", Integer.class, ValueConvert.NO ) )\n\t\t\t\t.predicate( f -> f.match().field( \"string\" ).matching( \"text 1\" ) )\n\t\t\t\t.toQuery();\n\n\t\tassertThat( query ).hasHitsAnyOrder( 37 );\n\t}\n\n\t@Test\n\tpublic void projection_nativeField_unsupportedProjection() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\t// let's check that it's possible to query the field beforehand\n\t\tSearchQuery<DocumentReference> query = scope.query()\n\t\t\t\t.predicate( f -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t.fromLuceneQuery( new TermQuery( new Term( \"nativeField_unsupportedProjection\", \"37\" ) ) )\n\t\t\t\t)\n\t\t\t\t.toQuery();\n\n\t\tassertThat( query )\n\t\t\t\t.hasDocRefHitsAnyOrder( INDEX_NAME, FIRST_ID );\n\n\t\t// now, let's check that projecting on the field throws an exception\n\t\tSubTest.expectException(\n\t\t\t\t\"projection on native field not supporting projections\",\n\t\t\t\t() -> scope.projection().field( \"nativeField_unsupportedProjection\", Integer.class )\n\t\t)\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Projections are not enabled for field\" )\n\t\t\t\t.satisfies( FailureReportUtils.hasContext(\n\t\t\t\t\t\tEventContexts.fromIndexFieldAbsolutePath( \"nativeField_unsupportedProjection\" )\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void projection_document() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<Document> query = scope.query()\n\t\t\t\t.asProjection(\n\t\t\t\t\t\tf -> f.extension( LuceneExtension.get() ).document()\n\t\t\t\t)\n\t\t\t\t.predicate( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\n\t\tList<Document> result = query.fetchAll().getHits();\n\t\tAssertions.assertThat( result )\n\t\t\t\t.hasSize( 5 )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"string\", \"text 1\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField\", \"37\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_converted\", \"37\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"37\" )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"integer\", 2 )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField\", \"78\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_converted\", \"78\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"78\" )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"nativeField\", \"13\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_converted\", \"13\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"13\" )\n\t\t\t\t\t\t\t\t// Geo points are stored as two internal fields\n\t\t\t\t\t\t\t\t.hasInternalField( \"geoPoint_latitude\", 40.12 )\n\t\t\t\t\t\t\t\t.hasInternalField( \"geoPoint_longitude\", -71.34 )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"nativeField\", \"89\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_converted\", \"89\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"89\" )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"string\", \"text 2\" )\n\t\t\t\t\t\t\t\t.hasField( \"integer\", 1 )\n\t\t\t\t\t\t\t\t// Geo points are stored as two internal fields\n\t\t\t\t\t\t\t\t.hasInternalField( \"geoPoint_latitude\", 45.12 )\n\t\t\t\t\t\t\t\t.hasInternalField( \"geoPoint_longitude\", -75.34 )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) );\n\t}\n\n\t/**\n\t * Check that the projection on a document includes all fields,\n\t * even if there is a field projection, which would usually trigger document filtering.\n\t */\n\t@Test\n\tpublic void projection_documentAndField() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<List<?>> query = scope.query()\n\t\t\t\t.asProjection( f ->\n\t\t\t\t\t\tf.composite(\n\t\t\t\t\t\t\t\tf.extension( LuceneExtension.get() ).document(),\n\t\t\t\t\t\t\t\tf.field( \"string\" )\n\t\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\tList<Document> result = query.fetchAll().getHits().stream()\n\t\t\t\t.map( list -> (Document) list.get( 0 ) )\n\t\t\t\t.collect( Collectors.toList() );\n\t\tAssertions.assertThat( result )\n\t\t\t\t.hasSize( 1 )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"string\", \"text 1\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField\", \"37\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_converted\", \"37\" )\n\t\t\t\t\t\t\t\t.hasField( \"nativeField_unsupportedProjection\", \"37\" )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tpublic void projection_explanation() {\n\t\tStubMappingScope scope = indexManager.createScope();\n\n\t\tSearchQuery<Explanation> query = scope.query()\n\t\t\t\t.asProjection( f -> f.extension( LuceneExtension.get() ).explanation() )\n\t\t\t\t.predicate( f -> f.id().matching( FIRST_ID ) )\n\t\t\t\t.toQuery();\n\n\t\tList<Explanation> result = query.fetchAll().getHits();\n\t\tAssertions.assertThat( result ).hasSize( 1 );\n\t\tAssertions.assertThat( result.get( 0 ) )\n\t\t\t\t.isInstanceOf( Explanation.class )\n\t\t\t\t.extracting( Object::toString ).asString()\n\t\t\t\t.contains( LuceneFields.idFieldName() );\n\t}\n\n\t@Test\n\tpublic void nativeField_invalidFieldPath() {\n\t\tIndexIndexingPlan<? extends DocumentElement> plan = indexManager.createIndexingPlan();\n\n\t\tSubTest.expectException(\n\t\t\t\t\"native field contributing field with invalid field path\",\n\t\t\t\t() -> plan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\t\t\tdocument.addValue( indexMapping.nativeField_invalidFieldPath, 45 );\n\t\t\t\t} ) )\n\t\t\t\t.assertThrown()\n\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t.hasMessageContaining( \"Invalid field path; expected path 'nativeField_invalidFieldPath', got 'not the expected path'.\" );\n\t}\n\n\t@Test\n\tpublic void backend_unwrap() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\t\tAssertions.assertThat( backend.unwrap( LuceneBackend.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void backend_unwrap_error_unknownType() {\n\t\tBackend backend = integration.getBackend( BACKEND_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene backend to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this backend can only be unwrapped to '\" + LuceneBackend.class.getName() + \"'\" );\n\n\t\tbackend.unwrap( String.class );\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\t\tAssertions.assertThat( indexManager.unwrap( LuceneIndexManager.class ) )\n\t\t\t\t.isNotNull();\n\t}\n\n\t@Test\n\tpublic void indexManager_unwrap_error_unknownType() {\n\t\tIndexManager indexManager = integration.getIndexManager( INDEX_NAME );\n\n\t\tthrown.expect( SearchException.class );\n\t\tthrown.expectMessage( \"Attempt to unwrap a Lucene index manager to '\" + String.class.getName() + \"'\" );\n\t\tthrown.expectMessage( \"this index manager can only be unwrapped to '\" + LuceneIndexManager.class.getName() + \"'\" );\n\n\t\tindexManager.unwrap( String.class );\n\t}\n\n\tprivate void initData() {\n\t\tindexDataSet( indexMapping, indexManager );\n\n\t\t// Use the same IDs and dataset for otherIndexMapping to trigger\n\t\t// a failure in explain() tests if index selection doesn't work correctly.\n\t\tindexDataSet( otherIndexMapping, otherIndexManager );\n\n\t\t// Check that all documents are searchable\n\t\tassertThat( indexManager.createScope().query().predicate( f -> f.matchAll() ).toQuery() )\n\t\t\t\t.hasDocRefHitsAnyOrder(\n\t\t\t\t\t\tINDEX_NAME,\n\t\t\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t\t\t);\n\t\tassertThat( otherIndexManager.createScope().query().predicate( f -> f.matchAll() ).toQuery() )\n\t\t\t\t.hasDocRefHitsAnyOrder(\n\t\t\t\t\t\tOTHER_INDEX_NAME,\n\t\t\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t\t\t);\n\t}\n\n\tprivate static void indexDataSet(IndexMapping indexMapping, StubMappingIndexManager indexManager) {\n\t\tIndexIndexingPlan<? extends DocumentElement> plan = indexManager.createIndexingPlan();\n\t\tplan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.string, \"text 1\" );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 37 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 37 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 37 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"a\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.integer, 2 );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 78 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 78 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 78 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"a\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 40.12, -71.34 ) );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 13 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 13 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 13 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"a\" );\n\t\t} );\n\t\tplan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.nativeField, 89 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 89 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 89 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tdocument.addValue( indexMapping.string, \"text 2\" );\n\t\t\tdocument.addValue( indexMapping.integer, 1 );\n\t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 45.12, -75.34 ) );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"zz\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"zz\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"zz\" );\n\t\t} );\n\t\tplan.execute().join();\n\t}\n\n\tprivate static class IndexMapping {\n\t\tfinal IndexFieldReference<Integer> integer;\n\t\tfinal IndexFieldReference<String> string;\n\t\tfinal IndexFieldReference<GeoPoint> geoPoint;\n\t\tfinal IndexFieldReference<Integer> nativeField;\n\t\tfinal IndexFieldReference<Integer> nativeField_converted;\n\t\tfinal IndexFieldReference<Integer> nativeField_unsupportedProjection;\n\t\tfinal IndexFieldReference<Integer> nativeField_invalidFieldPath;\n\n\t\tfinal IndexFieldReference<String> sort1;\n\t\tfinal IndexFieldReference<String> sort2;\n\t\tfinal IndexFieldReference<String> sort3;\n\n\t\tIndexMapping(IndexSchemaElement root) {\n\t\t\tinteger = root.field(\n\t\t\t\t\t\"integer\",\n\t\t\t\t\tf -> f.asInteger().projectable( Projectable.YES )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tstring = root.field(\n\t\t\t\t\t\"string\",\n\t\t\t\t\tf -> f.asString().projectable( Projectable.YES )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tgeoPoint = root.field(\n\t\t\t\t\t\"geoPoint\",\n\t\t\t\t\tf -> f.asGeoPoint().projectable( Projectable.YES )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tnativeField = root.field(\n\t\t\t\t\t\"nativeField\",\n\t\t\t\t\tf -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.asNative( Integer.class, LuceneExtensionIT::contributeNativeField, LuceneExtensionIT::fromNativeField )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tnativeField_converted = root.field(\n\t\t\t\t\t\"nativeField_converted\",\n\t\t\t\t\tf -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.asNative( Integer.class, LuceneExtensionIT::contributeNativeField, LuceneExtensionIT::fromNativeField )\n\t\t\t\t\t\t\t.projectionConverter( ValueWrapper.class, ValueWrapper.fromIndexFieldConverter() )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tnativeField_unsupportedProjection = root.field(\n\t\t\t\t\t\"nativeField_unsupportedProjection\",\n\t\t\t\t\tf -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.asNative( Integer.class, LuceneExtensionIT::contributeNativeField )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tnativeField_invalidFieldPath = root.field(\n\t\t\t\t\t\"nativeField_invalidFieldPath\",\n\t\t\t\t\tf -> f.extension( LuceneExtension.get() )\n\t\t\t\t\t\t\t.asNative( Integer.class, LuceneExtensionIT::contributeNativeFieldInvalidFieldPath )\n\t\t\t)\n\t\t\t\t\t.toReference();\n\n\t\t\tsort1 = root.field( \"sort1\", f -> f.asString().sortable( Sortable.YES ) )\n\t\t\t\t\t.toReference();\n\t\t\tsort2 = root.field( \"sort2\", f -> f.asString().sortable( Sortable.YES ) )\n\t\t\t\t\t.toReference();\n\t\t\tsort3 = root.field( \"sort3\", f -> f.asString().sortable( Sortable.YES ) )\n\t\t\t\t\t.toReference();\n\t\t}\n\t}\n\n\tprivate static void contributeNativeField(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( absoluteFieldPath, value.toString(), Store.YES ) );\n\t\tcollector.accept( new NumericDocValuesField( absoluteFieldPath, value.longValue() ) );\n\t}\n\n\tprivate static Integer fromNativeField(IndexableField field) {\n\t\treturn Integer.parseInt( field.stringValue() );\n\t}\n\n\tprivate static void contributeNativeFieldInvalidFieldPath(String absoluteFieldPath, Integer value, Consumer<IndexableField> collector) {\n\t\tcollector.accept( new StringField( \"not the expected path\", value.toString(), Store.YES ) );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private static void indexDataSet(IndexMapping indexMapping, StubMappingIndexManager indexManager) {\n\t\tIndexIndexingPlan<? extends DocumentElement> plan = indexManager.createIndexingPlan();\n\t\tplan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.string, \"text 1\" );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 37 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 37 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 37 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"a\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.integer, 2 );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 78 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 78 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 78 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"a\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 40.12, -71.34 ) );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 13 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 13 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 13 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"a\" );\n\t\t} );\n\t\tplan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.nativeField, 89 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 89 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 89 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tdocument.addValue( indexMapping.string, \"text 2\" );\n\t\t\tdocument.addValue( indexMapping.integer, 1 );\n\t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 45.12, -75.34 ) );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"zz\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"zz\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"zz\" );\n\t\t} );\n\t\tplan.execute().join();\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.backend.lucene.work.impl.LuceneExplainWork#execute\n methodBody: public Explanation execute(LuceneReadWorkExecutionContext context) {\ntryIndexSearcher indexSearcher=new IndexSearcher(context.getIndexReader());\nint luceneDocId=getLuceneDocId(indexSearcher);\nreturn searcher.explain(indexSearcher,luceneDocId);\ncatch(IOException e)throw log.ioExceptionOnQueryExecution(searcher.getLuceneQueryForExceptions(),context.getEventContext(),e);\n}",
            "methodSignature: org.hibernate.search.integrationtest.backend.lucene.LuceneExtensionIT#query\n methodBody: public void query() {\nStubMappingScope scope=indexManager.createScope();\nSearchQuery<DocumentReference> genericQuery=scope.query().predicate(f -> f.matchAll()).toQuery();\nLuceneSearchQuery<DocumentReference> query=genericQuery.extension(LuceneExtension.get());\nLuceneSearchResult<DocumentReference> result=query.fetchAll();\nassertThat(result).fromQuery(query).hasDocRefHitsAnyOrder(INDEX_NAME,FIRST_ID,SECOND_ID,THIRD_ID,FOURTH_ID,FIFTH_ID).hasTotalHitCount(5);\nSubTest.expectException(() -> query.extension((SearchQuery<DocumentReference> original,LoadingContext<?,?> loadingContext) -> Optional.empty())).assertThrown().isInstanceOf(SearchException.class);\n}"
        ],
        "sourceCodeAfterRefactoring": "private void initData() {\n\t\tindexDataSet( indexMapping, indexManager );\n\n\t\t// Use the same IDs and dataset for otherIndexMapping to trigger\n\t\t// a failure in explain() tests if index selection doesn't work correctly.\n\t\tindexDataSet( otherIndexMapping, otherIndexManager );\n\n\t\t// Check that all documents are searchable\n\t\tassertThat( indexManager.createScope().query().predicate( f -> f.matchAll() ).toQuery() )\n\t\t\t\t.hasDocRefHitsAnyOrder(\n\t\t\t\t\t\tINDEX_NAME,\n\t\t\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t\t\t);\n\t\tassertThat( otherIndexManager.createScope().query().predicate( f -> f.matchAll() ).toQuery() )\n\t\t\t\t.hasDocRefHitsAnyOrder(\n\t\t\t\t\t\tOTHER_INDEX_NAME,\n\t\t\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n\t\t\t\t);\n\t}\nprivate static void indexDataSet(IndexMapping indexMapping, StubMappingIndexManager indexManager) {\n\t\tIndexIndexingPlan<? extends DocumentElement> plan = indexManager.createIndexingPlan();\n\t\tplan.add( referenceProvider( FIRST_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.string, \"text 1\" );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 37 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 37 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 37 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"a\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( SECOND_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.integer, 2 );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 78 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 78 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 78 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"a\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( THIRD_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 40.12, -71.34 ) );\n\n\t\t\tdocument.addValue( indexMapping.nativeField, 13 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 13 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 13 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"a\" );\n\t\t} );\n\t\tplan.add( referenceProvider( FOURTH_ID ), document -> {\n\t\t\tdocument.addValue( indexMapping.nativeField, 89 );\n\t\t\tdocument.addValue( indexMapping.nativeField_converted, 89 );\n\t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 89 );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n\t\t} );\n\t\tplan.add( referenceProvider( FIFTH_ID ), document -> {\n\t\t\t// This document should not match any query\n\t\t\tdocument.addValue( indexMapping.string, \"text 2\" );\n\t\t\tdocument.addValue( indexMapping.integer, 1 );\n\t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 45.12, -75.34 ) );\n\n\t\t\tdocument.addValue( indexMapping.sort1, \"zz\" );\n\t\t\tdocument.addValue( indexMapping.sort2, \"zz\" );\n\t\t\tdocument.addValue( indexMapping.sort3, \"zz\" );\n\t\t} );\n\t\tplan.execute().join();\n\t}",
        "diffSourceCode": "-  707: \tprivate void initData() {\n-  708: \t\tIndexIndexingPlan<? extends DocumentElement> plan = indexManager.createIndexingPlan();\n-  709: \t\tplan.add( referenceProvider( FIRST_ID ), document -> {\n-  710: \t\t\tdocument.addValue( indexMapping.string, \"text 1\" );\n-  711: \n-  712: \t\t\tdocument.addValue( indexMapping.nativeField, 37 );\n-  713: \t\t\tdocument.addValue( indexMapping.nativeField_converted, 37 );\n-  714: \t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 37 );\n-  715: \n-  716: \t\t\tdocument.addValue( indexMapping.sort1, \"a\" );\n-  717: \t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n-  718: \t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n-  719: \t\t} );\n-  720: \t\tplan.add( referenceProvider( SECOND_ID ), document -> {\n-  721: \t\t\tdocument.addValue( indexMapping.integer, 2 );\n-  722: \n-  723: \t\t\tdocument.addValue( indexMapping.nativeField, 78 );\n-  724: \t\t\tdocument.addValue( indexMapping.nativeField_converted, 78 );\n-  725: \t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 78 );\n-  726: \n-  727: \t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n-  728: \t\t\tdocument.addValue( indexMapping.sort2, \"a\" );\n-  729: \t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n-  730: \t\t} );\n-  731: \t\tplan.add( referenceProvider( THIRD_ID ), document -> {\n-  732: \t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 40.12, -71.34 ) );\n-  733: \n-  734: \t\t\tdocument.addValue( indexMapping.nativeField, 13 );\n-  735: \t\t\tdocument.addValue( indexMapping.nativeField_converted, 13 );\n-  736: \t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 13 );\n-  737: \n-  738: \t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n-  739: \t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n-  740: \t\t\tdocument.addValue( indexMapping.sort3, \"a\" );\n-  741: \t\t} );\n-  742: \t\tplan.add( referenceProvider( FOURTH_ID ), document -> {\n-  743: \t\t\tdocument.addValue( indexMapping.nativeField, 89 );\n-  744: \t\t\tdocument.addValue( indexMapping.nativeField_converted, 89 );\n-  745: \t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 89 );\n-  746: \n-  747: \t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n-  748: \t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n-  749: \t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n-  750: \t\t} );\n-  751: \t\tplan.add( referenceProvider( FIFTH_ID ), document -> {\n-  752: \t\t\t// This document should not match any query\n-  753: \t\t\tdocument.addValue( indexMapping.string, \"text 2\" );\n-  754: \t\t\tdocument.addValue( indexMapping.integer, 1 );\n-  755: \t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 45.12, -75.34 ) );\n-  756: \n-  757: \t\t\tdocument.addValue( indexMapping.sort1, \"zz\" );\n-  758: \t\t\tdocument.addValue( indexMapping.sort2, \"zz\" );\n-  759: \t\t\tdocument.addValue( indexMapping.sort3, \"zz\" );\n-  760: \t\t} );\n-  761: \n-  762: \t\tplan.execute().join();\n-  763: \n-  764: \t\t// Check that all documents are searchable\n-  765: \t\tStubMappingScope scope = indexManager.createScope();\n-  766: \t\tSearchQuery<DocumentReference> query = scope.query()\n-  767: \t\t\t\t.predicate( f -> f.matchAll() )\n-  768: \t\t\t\t.toQuery();\n-  769: \t\tassertThat( query ).hasDocRefHitsAnyOrder(\n-  770: \t\t\t\tINDEX_NAME,\n-  771: \t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n-  772: \t\t);\n-  773: \t}\n-  774: \n-  775: \tprivate static class IndexMapping {\n-  776: \t\tfinal IndexFieldReference<Integer> integer;\n-  777: \t\tfinal IndexFieldReference<String> string;\n-  778: \t\tfinal IndexFieldReference<GeoPoint> geoPoint;\n-  779: \t\tfinal IndexFieldReference<Integer> nativeField;\n-  780: \t\tfinal IndexFieldReference<Integer> nativeField_converted;\n-  781: \t\tfinal IndexFieldReference<Integer> nativeField_unsupportedProjection;\n-  782: \t\tfinal IndexFieldReference<Integer> nativeField_invalidFieldPath;\n-  783: \n+  707: \n+  708: \tprivate void initData() {\n+  709: \t\tindexDataSet( indexMapping, indexManager );\n+  710: \n+  711: \t\t// Use the same IDs and dataset for otherIndexMapping to trigger\n+  712: \t\t// a failure in explain() tests if index selection doesn't work correctly.\n+  713: \t\tindexDataSet( otherIndexMapping, otherIndexManager );\n+  714: \n+  715: \t\t// Check that all documents are searchable\n+  716: \t\tassertThat( indexManager.createScope().query().predicate( f -> f.matchAll() ).toQuery() )\n+  717: \t\t\t\t.hasDocRefHitsAnyOrder(\n+  718: \t\t\t\t\t\tINDEX_NAME,\n+  719: \t\t\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n+  720: \t\t\t\t);\n+  721: \t\tassertThat( otherIndexManager.createScope().query().predicate( f -> f.matchAll() ).toQuery() )\n+  722: \t\t\t\t.hasDocRefHitsAnyOrder(\n+  723: \t\t\t\t\t\tOTHER_INDEX_NAME,\n+  724: \t\t\t\t\t\tFIRST_ID, SECOND_ID, THIRD_ID, FOURTH_ID, FIFTH_ID\n+  725: \t\t\t\t);\n+  726: \t}\n+  727: \n+  728: \tprivate static void indexDataSet(IndexMapping indexMapping, StubMappingIndexManager indexManager) {\n+  729: \t\tIndexIndexingPlan<? extends DocumentElement> plan = indexManager.createIndexingPlan();\n+  730: \t\tplan.add( referenceProvider( FIRST_ID ), document -> {\n+  731: \t\t\tdocument.addValue( indexMapping.string, \"text 1\" );\n+  732: \n+  733: \t\t\tdocument.addValue( indexMapping.nativeField, 37 );\n+  734: \t\t\tdocument.addValue( indexMapping.nativeField_converted, 37 );\n+  735: \t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 37 );\n+  736: \n+  737: \t\t\tdocument.addValue( indexMapping.sort1, \"a\" );\n+  738: \t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n+  739: \t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n+  740: \t\t} );\n+  741: \t\tplan.add( referenceProvider( SECOND_ID ), document -> {\n+  742: \t\t\tdocument.addValue( indexMapping.integer, 2 );\n+  743: \n+  744: \t\t\tdocument.addValue( indexMapping.nativeField, 78 );\n+  745: \t\t\tdocument.addValue( indexMapping.nativeField_converted, 78 );\n+  746: \t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 78 );\n+  747: \n+  748: \t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n+  749: \t\t\tdocument.addValue( indexMapping.sort2, \"a\" );\n+  750: \t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n+  751: \t\t} );\n+  752: \t\tplan.add( referenceProvider( THIRD_ID ), document -> {\n+  753: \t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 40.12, -71.34 ) );\n+  754: \n+  755: \t\t\tdocument.addValue( indexMapping.nativeField, 13 );\n+  756: \t\t\tdocument.addValue( indexMapping.nativeField_converted, 13 );\n+  757: \t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 13 );\n+  758: \n+  759: \t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n+  760: \t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n+  761: \t\t\tdocument.addValue( indexMapping.sort3, \"a\" );\n+  762: \t\t} );\n+  763: \t\tplan.add( referenceProvider( FOURTH_ID ), document -> {\n+  764: \t\t\tdocument.addValue( indexMapping.nativeField, 89 );\n+  765: \t\t\tdocument.addValue( indexMapping.nativeField_converted, 89 );\n+  766: \t\t\tdocument.addValue( indexMapping.nativeField_unsupportedProjection, 89 );\n+  767: \n+  768: \t\t\tdocument.addValue( indexMapping.sort1, \"z\" );\n+  769: \t\t\tdocument.addValue( indexMapping.sort2, \"z\" );\n+  770: \t\t\tdocument.addValue( indexMapping.sort3, \"z\" );\n+  771: \t\t} );\n+  772: \t\tplan.add( referenceProvider( FIFTH_ID ), document -> {\n+  773: \t\t\t// This document should not match any query\n+  774: \t\t\tdocument.addValue( indexMapping.string, \"text 2\" );\n+  775: \t\t\tdocument.addValue( indexMapping.integer, 1 );\n+  776: \t\t\tdocument.addValue( indexMapping.geoPoint, GeoPoint.of( 45.12, -75.34 ) );\n+  777: \n+  778: \t\t\tdocument.addValue( indexMapping.sort1, \"zz\" );\n+  779: \t\t\tdocument.addValue( indexMapping.sort2, \"zz\" );\n+  780: \t\t\tdocument.addValue( indexMapping.sort3, \"zz\" );\n+  781: \t\t} );\n+  782: \t\tplan.execute().join();\n+  783: \t}\n",
        "uniqueId": "339a22f55f9fc6d9ff92ac27157f790083bf62e1_707_773_728_783_708_726",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tpublic builder() : Builder extracted from public submitTo(processor LuceneWriteWorkProcessor) : void in class org.hibernate.search.backend.lucene.work.execution.impl.LuceneIndexingPlanWriteWorkSet & moved to class org.hibernate.search.engine.reporting.IndexFailureContext",
        "diffLocations": [
            {
                "filePath": "backend/lucene/src/main/java/org/hibernate/search/backend/lucene/work/execution/impl/LuceneIndexingPlanWriteWorkSet.java",
                "startLine": 40,
                "endLine": 90,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/lucene/src/main/java/org/hibernate/search/backend/lucene/work/execution/impl/LuceneIndexingPlanWriteWorkSet.java",
                "startLine": 40,
                "endLine": 90,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/lucene/src/main/java/org/hibernate/search/backend/lucene/work/execution/impl/LuceneIndexingPlanWriteWorkSet.java",
                "startLine": 18,
                "endLine": 20,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Override\n\tpublic void submitTo(LuceneWriteWorkProcessor processor) {\n\t\tIndexIndexingPlanExecutionReport.Builder reportBuilder = IndexIndexingPlanExecutionReport.builder();\n\n\t\tprocessor.beforeWorkSet( commitStrategy, refreshStrategy );\n\n\t\tThrowable throwable = null;\n\t\tObject failingOperation = null;\n\n\t\tfor ( LuceneWriteWork<?> work : works ) {\n\t\t\ttry {\n\t\t\t\tprocessor.submit( work );\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\treportBuilder.throwable( e );\n\t\t\t\tthrowable = e;\n\t\t\t\tfailingOperation = work.getInfo();\n\t\t\t\tbreak; // Don't even try to submit the next works\n\t\t\t}\n\t\t}\n\n\t\tif ( throwable == null ) {\n\t\t\ttry {\n\t\t\t\tprocessor.afterSuccessfulWorkSet();\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\treportBuilder.throwable( e );\n\t\t\t\tthrowable = e;\n\t\t\t\tfailingOperation = \"Commit after a set of index works\";\n\t\t\t}\n\t\t}\n\n\n\t\tif ( throwable == null ) {\n\t\t\tindexingPlanFuture.complete( reportBuilder.build() );\n\t\t}\n\t\telse {\n\t\t\t// FIXME HSEARCH-3735 This is temporary and should be removed when all failures are reported to the mapper directly\n\t\t\tIndexFailureContextImpl.Builder failureContextBuilder = new IndexFailureContextImpl.Builder();\n\t\t\tfailureContextBuilder.throwable( throwable );\n\t\t\tfailureContextBuilder.failingOperation( failingOperation );\n\t\t\t// Even if some works succeeded, there's no guarantee they were actually committed to the index.\n\t\t\t// Report all works as uncommitted.\n\t\t\tfor ( LuceneSingleDocumentWriteWork<?> work : works ) {\n\t\t\t\treportBuilder.failingDocument( new LuceneDocumentReference( indexName, work.getDocumentId() ) );\n\t\t\t\tfailureContextBuilder.uncommittedOperation( work.getInfo() );\n\t\t\t}\n\t\t\tindexingPlanFuture.complete( reportBuilder.build() );\n\t\t\tprocessor.getFailureHandler().handle( failureContextBuilder.build() );\n\t\t}\n\t}",
        "filePathBefore": "backend/lucene/src/main/java/org/hibernate/search/backend/lucene/work/execution/impl/LuceneIndexingPlanWriteWorkSet.java",
        "isPureRefactoring": true,
        "commitId": "daaa9ff20cae0f7751636f16bcaf0cf2512c16a4",
        "packageNameBefore": "org.hibernate.search.backend.lucene.work.execution.impl",
        "classNameBefore": "org.hibernate.search.backend.lucene.work.execution.impl.LuceneIndexingPlanWriteWorkSet",
        "methodNameBefore": "org.hibernate.search.backend.lucene.work.execution.impl.LuceneIndexingPlanWriteWorkSet#submitTo",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.reporting.spi.IndexFailureContextImpl.Builder#build\n methodBody: public IndexFailureContext build() {\nreturn new IndexFailureContextImpl(this);\n}\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#beforeWorkSet\n methodBody: public void beforeWorkSet(DocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\nworkSetForcesCommit=DocumentCommitStrategy.FORCE.equals(commitStrategy) || DocumentRefreshStrategy.FORCE.equals(refreshStrategy);\nworkSetUncommittedWorks.clear();\nworkSetHasFailure=false;\n}\nmethodSignature: org.hibernate.search.engine.reporting.spi.IndexFailureContextImpl.Builder#uncommittedOperation\n methodBody: public void uncommittedOperation(Object uncommittedOperation) {\nif(uncommittedOperations == null){uncommittedOperations=new ArrayList<>();\n}uncommittedOperations.add(uncommittedOperation);\n}\nmethodSignature: org.hibernate.search.mapper.orm.massindexing.impl.FailureHandledRunnable#getFailureHandler\n methodBody: protected FailureHandler getFailureHandler() {\nreturn failureHandler;\n}\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#getFailureHandler\n methodBody: public FailureHandler getFailureHandler() {\nreturn failureHandler;\n}\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#afterSuccessfulWorkSet\n methodBody: public void afterSuccessfulWorkSet() {\nif(workSetForcesCommit){trycommit();\ncatch(RuntimeException e)cleanUpAfterFailure(e,\"Commit after a set of index works\");\nthrow e;\nfinallypreviousWorkSetsUncommittedWorks.clear();\nworkSetUncommittedWorks.clear();\n}{previousWorkSetsUncommittedWorks.addAll(workSetUncommittedWorks);\nworkSetUncommittedWorks.clear();\n}}\nmethodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#build\n methodBody: public CompletableFuture<Void> build() {\nfinal SequenceContext sequenceContext=currentlyBuildingSequenceContext;\nreturn Futures.whenCompleteExecute(currentlyBuildingSequenceTail,() -> sequenceContext.executionContext.executePendingRefreshes().whenComplete(Futures.copyHandler(sequenceContext.refreshFuture))).exceptionally(Futures.handler(t -> {\n  sequenceContext.notifySequenceFailed(t);\n  return null;\n}\n));\n}\nmethodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#build\n methodBody: public FailureContext build() {\nreturn new FailureContextImpl(this);\n}\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#submit\n methodBody: public void submit(W workset) throws InterruptedException {\nif(executorService == null){throw new AssertionFailure(\"Attempt to submit a workset to executor '\" + name + \"', which is stopped\" + \" There is probably a bug in Hibernate Search, please report it.\");\n}workQueue.put(workset);\nensureProcessingScheduled();\n}\nmethodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#failingOperation\n methodBody: public void failingOperation(Object failingOperation) {\nthis.failingOperation=failingOperation;\n}\nmethodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#throwable\n methodBody: public void throwable(Throwable th) {\nthis.throwable=th;\n}\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#submit\n methodBody: public <T> T submit(LuceneWriteWork<T> work) {\nif(workSetHasFailure){throw new AssertionFailure(\"A work was submitted to the processor after a failure occurred in the current workset.\" + \" There is a bug in Hibernate Search, please report it.\");\n}tryworkSetUncommittedWorks.add(work);\nreturn work.execute(context);\ncatch(RuntimeException e)cleanUpAfterFailure(e,work.getInfo());\nthrow e;\n}",
        "classSignatureBefore": "class LuceneIndexingPlanWriteWorkSet implements LuceneWriteWorkSet ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.lucene.work.execution.impl.LuceneIndexingPlanWriteWorkSet#submitTo"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.lucene.work.execution.impl.LuceneIndexingPlanWriteWorkSet"
        ],
        "classSignatureBeforeSet": [
            "class LuceneIndexingPlanWriteWorkSet implements LuceneWriteWorkSet "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Tolerable changes in the body\n",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.lucene.work.execution.impl;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.CompletableFuture;\n\nimport org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor;\nimport org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkSet;\nimport org.hibernate.search.backend.lucene.search.impl.LuceneDocumentReference;\nimport org.hibernate.search.backend.lucene.work.impl.LuceneSingleDocumentWriteWork;\nimport org.hibernate.search.backend.lucene.work.impl.LuceneWriteWork;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.backend.work.execution.spi.IndexIndexingPlanExecutionReport;\nimport org.hibernate.search.engine.reporting.spi.IndexFailureContextImpl;\n\nclass LuceneIndexingPlanWriteWorkSet implements LuceneWriteWorkSet {\n\tprivate final String indexName;\n\tprivate final List<LuceneSingleDocumentWriteWork<?>> works;\n\tprivate final CompletableFuture<IndexIndexingPlanExecutionReport> indexingPlanFuture;\n\tprivate final DocumentCommitStrategy commitStrategy;\n\tprivate final DocumentRefreshStrategy refreshStrategy;\n\n\tLuceneIndexingPlanWriteWorkSet(String indexName, List<LuceneSingleDocumentWriteWork<?>> works,\n\t\t\tCompletableFuture<IndexIndexingPlanExecutionReport> indexingPlanFuture,\n\t\t\tDocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\n\t\tthis.indexName = indexName;\n\t\tthis.works = new ArrayList<>( works );\n\t\tthis.indexingPlanFuture = indexingPlanFuture;\n\t\tthis.commitStrategy = commitStrategy;\n\t\tthis.refreshStrategy = refreshStrategy;\n\t}\n\n\t@Override\n\tpublic void submitTo(LuceneWriteWorkProcessor processor) {\n\t\tIndexIndexingPlanExecutionReport.Builder reportBuilder = IndexIndexingPlanExecutionReport.builder();\n\n\t\tprocessor.beforeWorkSet( commitStrategy, refreshStrategy );\n\n\t\tThrowable throwable = null;\n\t\tObject failingOperation = null;\n\n\t\tfor ( LuceneWriteWork<?> work : works ) {\n\t\t\ttry {\n\t\t\t\tprocessor.submit( work );\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\treportBuilder.throwable( e );\n\t\t\t\tthrowable = e;\n\t\t\t\tfailingOperation = work.getInfo();\n\t\t\t\tbreak; // Don't even try to submit the next works\n\t\t\t}\n\t\t}\n\n\t\tif ( throwable == null ) {\n\t\t\ttry {\n\t\t\t\tprocessor.afterSuccessfulWorkSet();\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\treportBuilder.throwable( e );\n\t\t\t\tthrowable = e;\n\t\t\t\tfailingOperation = \"Commit after a set of index works\";\n\t\t\t}\n\t\t}\n\n\n\t\tif ( throwable == null ) {\n\t\t\tindexingPlanFuture.complete( reportBuilder.build() );\n\t\t}\n\t\telse {\n\t\t\t// FIXME HSEARCH-3735 This is temporary and should be removed when all failures are reported to the mapper directly\n\t\t\tIndexFailureContextImpl.Builder failureContextBuilder = new IndexFailureContextImpl.Builder();\n\t\t\tfailureContextBuilder.throwable( throwable );\n\t\t\tfailureContextBuilder.failingOperation( failingOperation );\n\t\t\t// Even if some works succeeded, there's no guarantee they were actually committed to the index.\n\t\t\t// Report all works as uncommitted.\n\t\t\tfor ( LuceneSingleDocumentWriteWork<?> work : works ) {\n\t\t\t\treportBuilder.failingDocument( new LuceneDocumentReference( indexName, work.getDocumentId() ) );\n\t\t\t\tfailureContextBuilder.uncommittedOperation( work.getInfo() );\n\t\t\t}\n\t\t\tindexingPlanFuture.complete( reportBuilder.build() );\n\t\t\tprocessor.getFailureHandler().handle( failureContextBuilder.build() );\n\t\t}\n\t}\n\n\t@Override\n\tpublic void markAsFailed(Throwable t) {\n\t\tindexingPlanFuture.completeExceptionally( t );\n\t}\n}\n",
        "filePathAfter": "backend/lucene/src/main/java/org/hibernate/search/backend/lucene/work/execution/impl/LuceneIndexingPlanWriteWorkSet.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.lucene.work.execution.impl;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.CompletableFuture;\n\nimport org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor;\nimport org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkSet;\nimport org.hibernate.search.backend.lucene.search.impl.LuceneDocumentReference;\nimport org.hibernate.search.backend.lucene.work.impl.LuceneSingleDocumentWriteWork;\nimport org.hibernate.search.backend.lucene.work.impl.LuceneWriteWork;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.backend.work.execution.spi.IndexIndexingPlanExecutionReport;\nimport org.hibernate.search.engine.reporting.IndexFailureContext;\n\nclass LuceneIndexingPlanWriteWorkSet implements LuceneWriteWorkSet {\n\tprivate final String indexName;\n\tprivate final List<LuceneSingleDocumentWriteWork<?>> works;\n\tprivate final CompletableFuture<IndexIndexingPlanExecutionReport> indexingPlanFuture;\n\tprivate final DocumentCommitStrategy commitStrategy;\n\tprivate final DocumentRefreshStrategy refreshStrategy;\n\n\tLuceneIndexingPlanWriteWorkSet(String indexName, List<LuceneSingleDocumentWriteWork<?>> works,\n\t\t\tCompletableFuture<IndexIndexingPlanExecutionReport> indexingPlanFuture,\n\t\t\tDocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\n\t\tthis.indexName = indexName;\n\t\tthis.works = new ArrayList<>( works );\n\t\tthis.indexingPlanFuture = indexingPlanFuture;\n\t\tthis.commitStrategy = commitStrategy;\n\t\tthis.refreshStrategy = refreshStrategy;\n\t}\n\n\t@Override\n\tpublic void submitTo(LuceneWriteWorkProcessor processor) {\n\t\tIndexIndexingPlanExecutionReport.Builder reportBuilder = IndexIndexingPlanExecutionReport.builder();\n\n\t\tprocessor.beforeWorkSet( commitStrategy, refreshStrategy );\n\n\t\tThrowable throwable = null;\n\t\tObject failingOperation = null;\n\n\t\tfor ( LuceneWriteWork<?> work : works ) {\n\t\t\ttry {\n\t\t\t\tprocessor.submit( work );\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\treportBuilder.throwable( e );\n\t\t\t\tthrowable = e;\n\t\t\t\tfailingOperation = work.getInfo();\n\t\t\t\tbreak; // Don't even try to submit the next works\n\t\t\t}\n\t\t}\n\n\t\tif ( throwable == null ) {\n\t\t\ttry {\n\t\t\t\tprocessor.afterSuccessfulWorkSet();\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\treportBuilder.throwable( e );\n\t\t\t\tthrowable = e;\n\t\t\t\tfailingOperation = \"Commit after a set of index works\";\n\t\t\t}\n\t\t}\n\n\n\t\tif ( throwable == null ) {\n\t\t\tindexingPlanFuture.complete( reportBuilder.build() );\n\t\t}\n\t\telse {\n\t\t\t// FIXME HSEARCH-3735 This is temporary and should be removed when all failures are reported to the mapper directly\n\t\t\tIndexFailureContext.Builder failureContextBuilder = IndexFailureContext.builder();\n\t\t\tfailureContextBuilder.throwable( throwable );\n\t\t\tfailureContextBuilder.failingOperation( failingOperation );\n\t\t\t// Even if some works succeeded, there's no guarantee they were actually committed to the index.\n\t\t\t// Report all works as uncommitted.\n\t\t\tfor ( LuceneSingleDocumentWriteWork<?> work : works ) {\n\t\t\t\treportBuilder.failingDocument( new LuceneDocumentReference( indexName, work.getDocumentId() ) );\n\t\t\t\tfailureContextBuilder.uncommittedOperation( work.getInfo() );\n\t\t\t}\n\t\t\tindexingPlanFuture.complete( reportBuilder.build() );\n\t\t\tprocessor.getFailureHandler().handle( failureContextBuilder.build() );\n\t\t}\n\t}\n\n\t@Override\n\tpublic void markAsFailed(Throwable t) {\n\t\tindexingPlanFuture.completeExceptionally( t );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "import org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.backend.work.execution.spi.IndexIndexingPlanExecutionReport;"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.reporting.spi.IndexFailureContextImpl.Builder#build\n methodBody: public IndexFailureContext build() {\nreturn new IndexFailureContextImpl(this);\n}",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#beforeWorkSet\n methodBody: public void beforeWorkSet(DocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\nworkSetForcesCommit=DocumentCommitStrategy.FORCE.equals(commitStrategy) || DocumentRefreshStrategy.FORCE.equals(refreshStrategy);\nworkSetUncommittedWorks.clear();\nworkSetHasFailure=false;\n}",
            "methodSignature: org.hibernate.search.engine.reporting.spi.IndexFailureContextImpl.Builder#uncommittedOperation\n methodBody: public void uncommittedOperation(Object uncommittedOperation) {\nif(uncommittedOperations == null){uncommittedOperations=new ArrayList<>();\n}uncommittedOperations.add(uncommittedOperation);\n}",
            "methodSignature: org.hibernate.search.mapper.orm.massindexing.impl.FailureHandledRunnable#getFailureHandler\n methodBody: protected FailureHandler getFailureHandler() {\nreturn failureHandler;\n}",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#getFailureHandler\n methodBody: public FailureHandler getFailureHandler() {\nreturn failureHandler;\n}",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#afterSuccessfulWorkSet\n methodBody: public void afterSuccessfulWorkSet() {\nif(workSetForcesCommit){trycommit();\ncatch(RuntimeException e)cleanUpAfterFailure(e,\"Commit after a set of index works\");\nthrow e;\nfinallypreviousWorkSetsUncommittedWorks.clear();\nworkSetUncommittedWorks.clear();\n}{previousWorkSetsUncommittedWorks.addAll(workSetUncommittedWorks);\nworkSetUncommittedWorks.clear();\n}}",
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#build\n methodBody: public CompletableFuture<Void> build() {\nfinal SequenceContext sequenceContext=currentlyBuildingSequenceContext;\nreturn Futures.whenCompleteExecute(currentlyBuildingSequenceTail,() -> sequenceContext.executionContext.executePendingRefreshes().whenComplete(Futures.copyHandler(sequenceContext.refreshFuture))).exceptionally(Futures.handler(t -> {\n  sequenceContext.notifySequenceFailed(t);\n  return null;\n}\n));\n}",
            "methodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#build\n methodBody: public FailureContext build() {\nreturn new FailureContextImpl(this);\n}",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#submit\n methodBody: public void submit(W workset) throws InterruptedException {\nif(executorService == null){throw new AssertionFailure(\"Attempt to submit a workset to executor '\" + name + \"', which is stopped\" + \" There is probably a bug in Hibernate Search, please report it.\");\n}workQueue.put(workset);\nensureProcessingScheduled();\n}",
            "methodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#failingOperation\n methodBody: public void failingOperation(Object failingOperation) {\nthis.failingOperation=failingOperation;\n}",
            "methodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#throwable\n methodBody: public void throwable(Throwable th) {\nthis.throwable=th;\n}",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#submit\n methodBody: public <T> T submit(LuceneWriteWork<T> work) {\nif(workSetHasFailure){throw new AssertionFailure(\"A work was submitted to the processor after a failure occurred in the current workset.\" + \" There is a bug in Hibernate Search, please report it.\");\n}tryworkSetUncommittedWorks.add(work);\nreturn work.execute(context);\ncatch(RuntimeException e)cleanUpAfterFailure(e,work.getInfo());\nthrow e;\n}"
        ],
        "sourceCodeAfterRefactoring": "@Override\n\tpublic void submitTo(LuceneWriteWorkProcessor processor) {\n\t\tIndexIndexingPlanExecutionReport.Builder reportBuilder = IndexIndexingPlanExecutionReport.builder();\n\n\t\tprocessor.beforeWorkSet( commitStrategy, refreshStrategy );\n\n\t\tThrowable throwable = null;\n\t\tObject failingOperation = null;\n\n\t\tfor ( LuceneWriteWork<?> work : works ) {\n\t\t\ttry {\n\t\t\t\tprocessor.submit( work );\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\treportBuilder.throwable( e );\n\t\t\t\tthrowable = e;\n\t\t\t\tfailingOperation = work.getInfo();\n\t\t\t\tbreak; // Don't even try to submit the next works\n\t\t\t}\n\t\t}\n\n\t\tif ( throwable == null ) {\n\t\t\ttry {\n\t\t\t\tprocessor.afterSuccessfulWorkSet();\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\treportBuilder.throwable( e );\n\t\t\t\tthrowable = e;\n\t\t\t\tfailingOperation = \"Commit after a set of index works\";\n\t\t\t}\n\t\t}\n\n\n\t\tif ( throwable == null ) {\n\t\t\tindexingPlanFuture.complete( reportBuilder.build() );\n\t\t}\n\t\telse {\n\t\t\t// FIXME HSEARCH-3735 This is temporary and should be removed when all failures are reported to the mapper directly\n\t\t\tIndexFailureContext.Builder failureContextBuilder = IndexFailureContext.builder();\n\t\t\tfailureContextBuilder.throwable( throwable );\n\t\t\tfailureContextBuilder.failingOperation( failingOperation );\n\t\t\t// Even if some works succeeded, there's no guarantee they were actually committed to the index.\n\t\t\t// Report all works as uncommitted.\n\t\t\tfor ( LuceneSingleDocumentWriteWork<?> work : works ) {\n\t\t\t\treportBuilder.failingDocument( new LuceneDocumentReference( indexName, work.getDocumentId() ) );\n\t\t\t\tfailureContextBuilder.uncommittedOperation( work.getInfo() );\n\t\t\t}\n\t\t\tindexingPlanFuture.complete( reportBuilder.build() );\n\t\t\tprocessor.getFailureHandler().handle( failureContextBuilder.build() );\n\t\t}\n\t}\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.backend.work.execution.spi.IndexIndexingPlanExecutionReport;",
        "diffSourceCode": "    18: import org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\n    19: import org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\n    20: import org.hibernate.search.engine.backend.work.execution.spi.IndexIndexingPlanExecutionReport;\n    40: \t@Override\n    41: \tpublic void submitTo(LuceneWriteWorkProcessor processor) {\n    42: \t\tIndexIndexingPlanExecutionReport.Builder reportBuilder = IndexIndexingPlanExecutionReport.builder();\n    43: \n    44: \t\tprocessor.beforeWorkSet( commitStrategy, refreshStrategy );\n    45: \n    46: \t\tThrowable throwable = null;\n    47: \t\tObject failingOperation = null;\n    48: \n    49: \t\tfor ( LuceneWriteWork<?> work : works ) {\n    50: \t\t\ttry {\n    51: \t\t\t\tprocessor.submit( work );\n    52: \t\t\t}\n    53: \t\t\tcatch (RuntimeException e) {\n    54: \t\t\t\treportBuilder.throwable( e );\n    55: \t\t\t\tthrowable = e;\n    56: \t\t\t\tfailingOperation = work.getInfo();\n    57: \t\t\t\tbreak; // Don't even try to submit the next works\n    58: \t\t\t}\n    59: \t\t}\n    60: \n    61: \t\tif ( throwable == null ) {\n    62: \t\t\ttry {\n    63: \t\t\t\tprocessor.afterSuccessfulWorkSet();\n    64: \t\t\t}\n    65: \t\t\tcatch (RuntimeException e) {\n    66: \t\t\t\treportBuilder.throwable( e );\n    67: \t\t\t\tthrowable = e;\n    68: \t\t\t\tfailingOperation = \"Commit after a set of index works\";\n    69: \t\t\t}\n    70: \t\t}\n    71: \n    72: \n    73: \t\tif ( throwable == null ) {\n    74: \t\t\tindexingPlanFuture.complete( reportBuilder.build() );\n    75: \t\t}\n    76: \t\telse {\n    77: \t\t\t// FIXME HSEARCH-3735 This is temporary and should be removed when all failures are reported to the mapper directly\n-   78: \t\t\tIndexFailureContextImpl.Builder failureContextBuilder = new IndexFailureContextImpl.Builder();\n+   78: \t\t\tIndexFailureContext.Builder failureContextBuilder = IndexFailureContext.builder();\n    79: \t\t\tfailureContextBuilder.throwable( throwable );\n    80: \t\t\tfailureContextBuilder.failingOperation( failingOperation );\n    81: \t\t\t// Even if some works succeeded, there's no guarantee they were actually committed to the index.\n    82: \t\t\t// Report all works as uncommitted.\n    83: \t\t\tfor ( LuceneSingleDocumentWriteWork<?> work : works ) {\n    84: \t\t\t\treportBuilder.failingDocument( new LuceneDocumentReference( indexName, work.getDocumentId() ) );\n    85: \t\t\t\tfailureContextBuilder.uncommittedOperation( work.getInfo() );\n    86: \t\t\t}\n    87: \t\t\tindexingPlanFuture.complete( reportBuilder.build() );\n    88: \t\t\tprocessor.getFailureHandler().handle( failureContextBuilder.build() );\n    89: \t\t}\n    90: \t}\n",
        "uniqueId": "daaa9ff20cae0f7751636f16bcaf0cf2512c16a4_40_90_18_20_40_90",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 111
            },
            "BRANCH": {
                "missed": 0,
                "covered": 8
            },
            "LINE": {
                "missed": 0,
                "covered": 32
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 5
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tpublic builder() : Builder extracted from private cleanUpAfterFailure(throwable Throwable, failingOperation Object) : void in class org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor & moved to class org.hibernate.search.engine.reporting.IndexFailureContext",
        "diffLocations": [
            {
                "filePath": "backend/lucene/src/main/java/org/hibernate/search/backend/lucene/orchestration/impl/LuceneWriteWorkProcessor.java",
                "startLine": 159,
                "endLine": 190,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/lucene/src/main/java/org/hibernate/search/backend/lucene/orchestration/impl/LuceneWriteWorkProcessor.java",
                "startLine": 158,
                "endLine": 189,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/lucene/src/main/java/org/hibernate/search/backend/lucene/orchestration/impl/LuceneWriteWorkProcessor.java",
                "startLine": 18,
                "endLine": 20,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void cleanUpAfterFailure(Throwable throwable, Object failingOperation) {\n\t\ttry {\n\t\t\t/*\n\t\t\t * Note this will close the index writer,\n\t\t\t * which with the default settings will trigger a commit.\n\t\t\t */\n\t\t\tindexWriterDelegator.forceLockRelease();\n\t\t}\n\t\tcatch (RuntimeException | IOException e) {\n\t\t\tthrowable.addSuppressed( log.unableToCleanUpAfterError( indexEventContext, e ) );\n\t\t}\n\n\t\tif ( previousWorkSetsUncommittedWorks.isEmpty() ) {\n\t\t\t// The failure will be reported elsewhere with all the necessary context.\n\t\t\treturn;\n\t\t}\n\n\t\t/*\n\t\t * The failure will be reported elsewhere,\n\t\t * but that report will not mention that some works from previous worksets may have been affected too.\n\t\t * Report the failure again, just to warn about previous worksets potentially being affected.\n\t\t */\n\t\tIndexFailureContextImpl.Builder failureContextBuilder = new IndexFailureContextImpl.Builder();\n\t\tfailureContextBuilder.throwable( throwable );\n\t\tfailureContextBuilder.failingOperation( failingOperation );\n\t\tfor ( LuceneWriteWork<?> work : previousWorkSetsUncommittedWorks ) {\n\t\t\tfailureContextBuilder.uncommittedOperation( work.getInfo() );\n\t\t}\n\t\tpreviousWorkSetsUncommittedWorks.clear();\n\t\tIndexFailureContext failureContext = failureContextBuilder.build();\n\t\tfailureHandler.handle( failureContext );\n\t}",
        "filePathBefore": "backend/lucene/src/main/java/org/hibernate/search/backend/lucene/orchestration/impl/LuceneWriteWorkProcessor.java",
        "isPureRefactoring": true,
        "commitId": "daaa9ff20cae0f7751636f16bcaf0cf2512c16a4",
        "packageNameBefore": "org.hibernate.search.backend.lucene.orchestration.impl",
        "classNameBefore": "org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor",
        "methodNameBefore": "org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#cleanUpAfterFailure",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.reporting.spi.IndexFailureContextImpl.Builder#build\n methodBody: public IndexFailureContext build() {\nreturn new IndexFailureContextImpl(this);\n}\nmethodSignature: org.hibernate.search.engine.reporting.spi.IndexFailureContextImpl.Builder#uncommittedOperation\n methodBody: public void uncommittedOperation(Object uncommittedOperation) {\nif(uncommittedOperations == null){uncommittedOperations=new ArrayList<>();\n}uncommittedOperations.add(uncommittedOperation);\n}\nmethodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#build\n methodBody: public CompletableFuture<Void> build() {\nfinal SequenceContext sequenceContext=currentlyBuildingSequenceContext;\nreturn Futures.whenCompleteExecute(currentlyBuildingSequenceTail,() -> sequenceContext.executionContext.executePendingRefreshes().whenComplete(Futures.copyHandler(sequenceContext.refreshFuture))).exceptionally(Futures.handler(t -> {\n  sequenceContext.notifySequenceFailed(t);\n  return null;\n}\n));\n}\nmethodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#build\n methodBody: public FailureContext build() {\nreturn new FailureContextImpl(this);\n}\nmethodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#failingOperation\n methodBody: public void failingOperation(Object failingOperation) {\nthis.failingOperation=failingOperation;\n}\nmethodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#throwable\n methodBody: public void throwable(Throwable th) {\nthis.throwable=th;\n}",
        "classSignatureBefore": "public class LuceneWriteWorkProcessor implements BatchingExecutor.WorkProcessor ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#cleanUpAfterFailure"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor"
        ],
        "classSignatureBeforeSet": [
            "public class LuceneWriteWorkProcessor implements BatchingExecutor.WorkProcessor "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Tolerable changes in the body\n",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.lucene.orchestration.impl;\n\nimport java.io.IOException;\nimport java.lang.invoke.MethodHandles;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.CompletableFuture;\n\nimport org.hibernate.search.backend.lucene.logging.impl.Log;\nimport org.hibernate.search.backend.lucene.lowlevel.writer.impl.IndexWriterDelegator;\nimport org.hibernate.search.backend.lucene.work.impl.LuceneWriteWork;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor;\nimport org.hibernate.search.engine.reporting.IndexFailureContext;\nimport org.hibernate.search.engine.reporting.FailureHandler;\nimport org.hibernate.search.engine.reporting.spi.IndexFailureContextImpl;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\nimport org.hibernate.search.util.common.reporting.EventContext;\n\n/**\n * A thread-unsafe component responsible for applying write works to an index writer.\n * <p>\n * Ported from Search 5's LuceneBackendQueueTask, in particular.\n */\npublic class LuceneWriteWorkProcessor implements BatchingExecutor.WorkProcessor {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate final EventContext indexEventContext;\n\tprivate final IndexWriterDelegator indexWriterDelegator;\n\tprivate final LuceneWriteWorkExecutionContextImpl context;\n\tprivate final FailureHandler failureHandler;\n\n\tprivate List<LuceneWriteWork<?>> previousWorkSetsUncommittedWorks = new ArrayList<>();\n\n\tprivate boolean workSetForcesCommit;\n\tprivate List<LuceneWriteWork<?>> workSetUncommittedWorks = new ArrayList<>();\n\tprivate boolean workSetHasFailure;\n\n\tpublic LuceneWriteWorkProcessor(EventContext indexEventContext, IndexWriterDelegator indexWriterDelegator,\n\t\t\tFailureHandler failureHandler) {\n\t\tthis.indexEventContext = indexEventContext;\n\t\tthis.indexWriterDelegator = indexWriterDelegator;\n\t\tthis.context = new LuceneWriteWorkExecutionContextImpl( indexEventContext, indexWriterDelegator );\n\t\tthis.failureHandler = failureHandler;\n\t}\n\n\t// FIXME HSEARCH-3735 This is temporary and should be removed when failures are reported to the mapper directly\n\tpublic FailureHandler getFailureHandler() {\n\t\treturn failureHandler;\n\t}\n\n\t@Override\n\tpublic void beginBatch() {\n\t\t// Nothing to do\n\t}\n\n\t@Override\n\tpublic CompletableFuture<?> endBatch() {\n\t\tif ( !previousWorkSetsUncommittedWorks.isEmpty() ) {\n\t\t\ttry {\n\t\t\t\tcommit();\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\tcleanUpAfterFailure( e, \"Commit after a batch of index works\" );\n\t\t\t\t// The exception was reported to the failure handler, no need to propagate it.\n\t\t\t}\n\t\t\tfinally {\n\t\t\t\t// Only clear the lists after the commit succeeds or failures are reported.\n\t\t\t\tpreviousWorkSetsUncommittedWorks.clear();\n\t\t\t}\n\t\t}\n\t\t// Everything was already executed, so just return a completed future.\n\t\treturn CompletableFuture.completedFuture( null );\n\t}\n\n\tpublic void beforeWorkSet(DocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\n\t\tworkSetForcesCommit = DocumentCommitStrategy.FORCE.equals( commitStrategy )\n\t\t\t\t// We need to commit in order to make the changes visible\n\t\t\t\t// TODO HSEARCH-3117 this may not be true with the NRT implementation from Search 5\n\t\t\t\t|| DocumentRefreshStrategy.FORCE.equals( refreshStrategy );\n\t\tworkSetUncommittedWorks.clear();\n\t\tworkSetHasFailure = false;\n\t}\n\n\t/**\n\t * This bypasses the normal {@link #submit(LuceneWriteWork)} method in order\n\t * to avoid setting {@link #hasUncommittedWorks} to {@code true},\n\t * so that we skip the end-of-batch commit and thus avoid the creation of an IndexWriter,\n\t * which would be pointless in this case.\n\t */\n\tvoid ensureIndexExists() {\n\t\ttry {\n\t\t\tindexWriterDelegator.ensureIndexExists();\n\t\t}\n\t\tcatch (IOException | RuntimeException e) {\n\t\t\tthrow log.unableToInitializeIndexDirectory(\n\t\t\t\t\te.getMessage(), indexEventContext, e\n\t\t\t);\n\t\t}\n\t}\n\n\tpublic <T> T submit(LuceneWriteWork<T> work) {\n\t\tif ( workSetHasFailure ) {\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"A work was submitted to the processor after a failure occurred in the current workset.\"\n\t\t\t\t\t\t\t+ \" There is a bug in Hibernate Search, please report it.\"\n\t\t\t);\n\t\t}\n\t\ttry {\n\t\t\tworkSetUncommittedWorks.add( work );\n\t\t\treturn work.execute( context );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tcleanUpAfterFailure( e, work.getInfo() );\n\t\t\tthrow e;\n\t\t}\n\t}\n\n\tpublic void afterSuccessfulWorkSet() {\n\t\tif ( workSetForcesCommit ) {\n\t\t\ttry {\n\t\t\t\tcommit();\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\tcleanUpAfterFailure( e, \"Commit after a set of index works\" );\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t\tfinally {\n\t\t\t\t// Only clear the lists after the commit succeeds or failures are reported.\n\t\t\t\tpreviousWorkSetsUncommittedWorks.clear();\n\t\t\t\tworkSetUncommittedWorks.clear();\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tpreviousWorkSetsUncommittedWorks.addAll( workSetUncommittedWorks );\n\t\t\tworkSetUncommittedWorks.clear();\n\t\t}\n\t}\n\n\tprivate void commit() {\n\t\ttry {\n\t\t\t// TODO HSEARCH-3117 restore the commit policy feature to allow scheduled commits?\n\t\t\tindexWriterDelegator.commit();\n\t\t}\n\t\tcatch (RuntimeException | IOException e) {\n\t\t\tthrow log.unableToCommitIndex( indexEventContext, e );\n\t\t}\n\t}\n\n\tprivate void cleanUpAfterFailure(Throwable throwable, Object failingOperation) {\n\t\ttry {\n\t\t\t/*\n\t\t\t * Note this will close the index writer,\n\t\t\t * which with the default settings will trigger a commit.\n\t\t\t */\n\t\t\tindexWriterDelegator.forceLockRelease();\n\t\t}\n\t\tcatch (RuntimeException | IOException e) {\n\t\t\tthrowable.addSuppressed( log.unableToCleanUpAfterError( indexEventContext, e ) );\n\t\t}\n\n\t\tif ( previousWorkSetsUncommittedWorks.isEmpty() ) {\n\t\t\t// The failure will be reported elsewhere with all the necessary context.\n\t\t\treturn;\n\t\t}\n\n\t\t/*\n\t\t * The failure will be reported elsewhere,\n\t\t * but that report will not mention that some works from previous worksets may have been affected too.\n\t\t * Report the failure again, just to warn about previous worksets potentially being affected.\n\t\t */\n\t\tIndexFailureContextImpl.Builder failureContextBuilder = new IndexFailureContextImpl.Builder();\n\t\tfailureContextBuilder.throwable( throwable );\n\t\tfailureContextBuilder.failingOperation( failingOperation );\n\t\tfor ( LuceneWriteWork<?> work : previousWorkSetsUncommittedWorks ) {\n\t\t\tfailureContextBuilder.uncommittedOperation( work.getInfo() );\n\t\t}\n\t\tpreviousWorkSetsUncommittedWorks.clear();\n\t\tIndexFailureContext failureContext = failureContextBuilder.build();\n\t\tfailureHandler.handle( failureContext );\n\t}\n}\n",
        "filePathAfter": "backend/lucene/src/main/java/org/hibernate/search/backend/lucene/orchestration/impl/LuceneWriteWorkProcessor.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.lucene.orchestration.impl;\n\nimport java.io.IOException;\nimport java.lang.invoke.MethodHandles;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.CompletableFuture;\n\nimport org.hibernate.search.backend.lucene.logging.impl.Log;\nimport org.hibernate.search.backend.lucene.lowlevel.writer.impl.IndexWriterDelegator;\nimport org.hibernate.search.backend.lucene.work.impl.LuceneWriteWork;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor;\nimport org.hibernate.search.engine.reporting.IndexFailureContext;\nimport org.hibernate.search.engine.reporting.FailureHandler;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\nimport org.hibernate.search.util.common.reporting.EventContext;\n\n/**\n * A thread-unsafe component responsible for applying write works to an index writer.\n * <p>\n * Ported from Search 5's LuceneBackendQueueTask, in particular.\n */\npublic class LuceneWriteWorkProcessor implements BatchingExecutor.WorkProcessor {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate final EventContext indexEventContext;\n\tprivate final IndexWriterDelegator indexWriterDelegator;\n\tprivate final LuceneWriteWorkExecutionContextImpl context;\n\tprivate final FailureHandler failureHandler;\n\n\tprivate List<LuceneWriteWork<?>> previousWorkSetsUncommittedWorks = new ArrayList<>();\n\n\tprivate boolean workSetForcesCommit;\n\tprivate List<LuceneWriteWork<?>> workSetUncommittedWorks = new ArrayList<>();\n\tprivate boolean workSetHasFailure;\n\n\tpublic LuceneWriteWorkProcessor(EventContext indexEventContext, IndexWriterDelegator indexWriterDelegator,\n\t\t\tFailureHandler failureHandler) {\n\t\tthis.indexEventContext = indexEventContext;\n\t\tthis.indexWriterDelegator = indexWriterDelegator;\n\t\tthis.context = new LuceneWriteWorkExecutionContextImpl( indexEventContext, indexWriterDelegator );\n\t\tthis.failureHandler = failureHandler;\n\t}\n\n\t// FIXME HSEARCH-3735 This is temporary and should be removed when failures are reported to the mapper directly\n\tpublic FailureHandler getFailureHandler() {\n\t\treturn failureHandler;\n\t}\n\n\t@Override\n\tpublic void beginBatch() {\n\t\t// Nothing to do\n\t}\n\n\t@Override\n\tpublic CompletableFuture<?> endBatch() {\n\t\tif ( !previousWorkSetsUncommittedWorks.isEmpty() ) {\n\t\t\ttry {\n\t\t\t\tcommit();\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\tcleanUpAfterFailure( e, \"Commit after a batch of index works\" );\n\t\t\t\t// The exception was reported to the failure handler, no need to propagate it.\n\t\t\t}\n\t\t\tfinally {\n\t\t\t\t// Only clear the lists after the commit succeeds or failures are reported.\n\t\t\t\tpreviousWorkSetsUncommittedWorks.clear();\n\t\t\t}\n\t\t}\n\t\t// Everything was already executed, so just return a completed future.\n\t\treturn CompletableFuture.completedFuture( null );\n\t}\n\n\tpublic void beforeWorkSet(DocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\n\t\tworkSetForcesCommit = DocumentCommitStrategy.FORCE.equals( commitStrategy )\n\t\t\t\t// We need to commit in order to make the changes visible\n\t\t\t\t// TODO HSEARCH-3117 this may not be true with the NRT implementation from Search 5\n\t\t\t\t|| DocumentRefreshStrategy.FORCE.equals( refreshStrategy );\n\t\tworkSetUncommittedWorks.clear();\n\t\tworkSetHasFailure = false;\n\t}\n\n\t/**\n\t * This bypasses the normal {@link #submit(LuceneWriteWork)} method in order\n\t * to avoid setting {@link #hasUncommittedWorks} to {@code true},\n\t * so that we skip the end-of-batch commit and thus avoid the creation of an IndexWriter,\n\t * which would be pointless in this case.\n\t */\n\tvoid ensureIndexExists() {\n\t\ttry {\n\t\t\tindexWriterDelegator.ensureIndexExists();\n\t\t}\n\t\tcatch (IOException | RuntimeException e) {\n\t\t\tthrow log.unableToInitializeIndexDirectory(\n\t\t\t\t\te.getMessage(), indexEventContext, e\n\t\t\t);\n\t\t}\n\t}\n\n\tpublic <T> T submit(LuceneWriteWork<T> work) {\n\t\tif ( workSetHasFailure ) {\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"A work was submitted to the processor after a failure occurred in the current workset.\"\n\t\t\t\t\t\t\t+ \" There is a bug in Hibernate Search, please report it.\"\n\t\t\t);\n\t\t}\n\t\ttry {\n\t\t\tworkSetUncommittedWorks.add( work );\n\t\t\treturn work.execute( context );\n\t\t}\n\t\tcatch (RuntimeException e) {\n\t\t\tcleanUpAfterFailure( e, work.getInfo() );\n\t\t\tthrow e;\n\t\t}\n\t}\n\n\tpublic void afterSuccessfulWorkSet() {\n\t\tif ( workSetForcesCommit ) {\n\t\t\ttry {\n\t\t\t\tcommit();\n\t\t\t}\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\tcleanUpAfterFailure( e, \"Commit after a set of index works\" );\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t\tfinally {\n\t\t\t\t// Only clear the lists after the commit succeeds or failures are reported.\n\t\t\t\tpreviousWorkSetsUncommittedWorks.clear();\n\t\t\t\tworkSetUncommittedWorks.clear();\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tpreviousWorkSetsUncommittedWorks.addAll( workSetUncommittedWorks );\n\t\t\tworkSetUncommittedWorks.clear();\n\t\t}\n\t}\n\n\tprivate void commit() {\n\t\ttry {\n\t\t\t// TODO HSEARCH-3117 restore the commit policy feature to allow scheduled commits?\n\t\t\tindexWriterDelegator.commit();\n\t\t}\n\t\tcatch (RuntimeException | IOException e) {\n\t\t\tthrow log.unableToCommitIndex( indexEventContext, e );\n\t\t}\n\t}\n\n\tprivate void cleanUpAfterFailure(Throwable throwable, Object failingOperation) {\n\t\ttry {\n\t\t\t/*\n\t\t\t * Note this will close the index writer,\n\t\t\t * which with the default settings will trigger a commit.\n\t\t\t */\n\t\t\tindexWriterDelegator.forceLockRelease();\n\t\t}\n\t\tcatch (RuntimeException | IOException e) {\n\t\t\tthrowable.addSuppressed( log.unableToCleanUpAfterError( indexEventContext, e ) );\n\t\t}\n\n\t\tif ( previousWorkSetsUncommittedWorks.isEmpty() ) {\n\t\t\t// The failure will be reported elsewhere with all the necessary context.\n\t\t\treturn;\n\t\t}\n\n\t\t/*\n\t\t * The failure will be reported elsewhere,\n\t\t * but that report will not mention that some works from previous worksets may have been affected too.\n\t\t * Report the failure again, just to warn about previous worksets potentially being affected.\n\t\t */\n\t\tIndexFailureContext.Builder failureContextBuilder = IndexFailureContext.builder();\n\t\tfailureContextBuilder.throwable( throwable );\n\t\tfailureContextBuilder.failingOperation( failingOperation );\n\t\tfor ( LuceneWriteWork<?> work : previousWorkSetsUncommittedWorks ) {\n\t\t\tfailureContextBuilder.uncommittedOperation( work.getInfo() );\n\t\t}\n\t\tpreviousWorkSetsUncommittedWorks.clear();\n\t\tIndexFailureContext failureContext = failureContextBuilder.build();\n\t\tfailureHandler.handle( failureContext );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "import org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor;"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.reporting.spi.IndexFailureContextImpl.Builder#build\n methodBody: public IndexFailureContext build() {\nreturn new IndexFailureContextImpl(this);\n}",
            "methodSignature: org.hibernate.search.engine.reporting.spi.IndexFailureContextImpl.Builder#uncommittedOperation\n methodBody: public void uncommittedOperation(Object uncommittedOperation) {\nif(uncommittedOperations == null){uncommittedOperations=new ArrayList<>();\n}uncommittedOperations.add(uncommittedOperation);\n}",
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#build\n methodBody: public CompletableFuture<Void> build() {\nfinal SequenceContext sequenceContext=currentlyBuildingSequenceContext;\nreturn Futures.whenCompleteExecute(currentlyBuildingSequenceTail,() -> sequenceContext.executionContext.executePendingRefreshes().whenComplete(Futures.copyHandler(sequenceContext.refreshFuture))).exceptionally(Futures.handler(t -> {\n  sequenceContext.notifySequenceFailed(t);\n  return null;\n}\n));\n}",
            "methodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#build\n methodBody: public FailureContext build() {\nreturn new FailureContextImpl(this);\n}",
            "methodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#failingOperation\n methodBody: public void failingOperation(Object failingOperation) {\nthis.failingOperation=failingOperation;\n}",
            "methodSignature: org.hibernate.search.engine.reporting.spi.FailureContextImpl.Builder#throwable\n methodBody: public void throwable(Throwable th) {\nthis.throwable=th;\n}"
        ],
        "sourceCodeAfterRefactoring": "private void cleanUpAfterFailure(Throwable throwable, Object failingOperation) {\n\t\ttry {\n\t\t\t/*\n\t\t\t * Note this will close the index writer,\n\t\t\t * which with the default settings will trigger a commit.\n\t\t\t */\n\t\t\tindexWriterDelegator.forceLockRelease();\n\t\t}\n\t\tcatch (RuntimeException | IOException e) {\n\t\t\tthrowable.addSuppressed( log.unableToCleanUpAfterError( indexEventContext, e ) );\n\t\t}\n\n\t\tif ( previousWorkSetsUncommittedWorks.isEmpty() ) {\n\t\t\t// The failure will be reported elsewhere with all the necessary context.\n\t\t\treturn;\n\t\t}\n\n\t\t/*\n\t\t * The failure will be reported elsewhere,\n\t\t * but that report will not mention that some works from previous worksets may have been affected too.\n\t\t * Report the failure again, just to warn about previous worksets potentially being affected.\n\t\t */\n\t\tIndexFailureContext.Builder failureContextBuilder = IndexFailureContext.builder();\n\t\tfailureContextBuilder.throwable( throwable );\n\t\tfailureContextBuilder.failingOperation( failingOperation );\n\t\tfor ( LuceneWriteWork<?> work : previousWorkSetsUncommittedWorks ) {\n\t\t\tfailureContextBuilder.uncommittedOperation( work.getInfo() );\n\t\t}\n\t\tpreviousWorkSetsUncommittedWorks.clear();\n\t\tIndexFailureContext failureContext = failureContextBuilder.build();\n\t\tfailureHandler.handle( failureContext );\n\t}\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor;",
        "diffSourceCode": "    18: import org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\n    19: import org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\n    20: import org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor;\n-  158: \n-  159: \tprivate void cleanUpAfterFailure(Throwable throwable, Object failingOperation) {\n-  160: \t\ttry {\n-  161: \t\t\t/*\n-  162: \t\t\t * Note this will close the index writer,\n-  163: \t\t\t * which with the default settings will trigger a commit.\n-  164: \t\t\t */\n-  165: \t\t\tindexWriterDelegator.forceLockRelease();\n-  166: \t\t}\n-  167: \t\tcatch (RuntimeException | IOException e) {\n-  168: \t\t\tthrowable.addSuppressed( log.unableToCleanUpAfterError( indexEventContext, e ) );\n-  169: \t\t}\n-  170: \n-  171: \t\tif ( previousWorkSetsUncommittedWorks.isEmpty() ) {\n-  172: \t\t\t// The failure will be reported elsewhere with all the necessary context.\n-  173: \t\t\treturn;\n-  174: \t\t}\n-  175: \n-  176: \t\t/*\n-  177: \t\t * The failure will be reported elsewhere,\n-  178: \t\t * but that report will not mention that some works from previous worksets may have been affected too.\n-  179: \t\t * Report the failure again, just to warn about previous worksets potentially being affected.\n-  180: \t\t */\n-  181: \t\tIndexFailureContextImpl.Builder failureContextBuilder = new IndexFailureContextImpl.Builder();\n-  182: \t\tfailureContextBuilder.throwable( throwable );\n-  183: \t\tfailureContextBuilder.failingOperation( failingOperation );\n-  184: \t\tfor ( LuceneWriteWork<?> work : previousWorkSetsUncommittedWorks ) {\n-  185: \t\t\tfailureContextBuilder.uncommittedOperation( work.getInfo() );\n-  186: \t\t}\n-  187: \t\tpreviousWorkSetsUncommittedWorks.clear();\n-  188: \t\tIndexFailureContext failureContext = failureContextBuilder.build();\n-  189: \t\tfailureHandler.handle( failureContext );\n-  190: \t}\n+  158: \tprivate void cleanUpAfterFailure(Throwable throwable, Object failingOperation) {\n+  159: \t\ttry {\n+  160: \t\t\t/*\n+  161: \t\t\t * Note this will close the index writer,\n+  162: \t\t\t * which with the default settings will trigger a commit.\n+  163: \t\t\t */\n+  164: \t\t\tindexWriterDelegator.forceLockRelease();\n+  165: \t\t}\n+  166: \t\tcatch (RuntimeException | IOException e) {\n+  167: \t\t\tthrowable.addSuppressed( log.unableToCleanUpAfterError( indexEventContext, e ) );\n+  168: \t\t}\n+  169: \n+  170: \t\tif ( previousWorkSetsUncommittedWorks.isEmpty() ) {\n+  171: \t\t\t// The failure will be reported elsewhere with all the necessary context.\n+  172: \t\t\treturn;\n+  173: \t\t}\n+  174: \n+  175: \t\t/*\n+  176: \t\t * The failure will be reported elsewhere,\n+  177: \t\t * but that report will not mention that some works from previous worksets may have been affected too.\n+  178: \t\t * Report the failure again, just to warn about previous worksets potentially being affected.\n+  179: \t\t */\n+  180: \t\tIndexFailureContext.Builder failureContextBuilder = IndexFailureContext.builder();\n+  181: \t\tfailureContextBuilder.throwable( throwable );\n+  182: \t\tfailureContextBuilder.failingOperation( failingOperation );\n+  183: \t\tfor ( LuceneWriteWork<?> work : previousWorkSetsUncommittedWorks ) {\n+  184: \t\t\tfailureContextBuilder.uncommittedOperation( work.getInfo() );\n+  185: \t\t}\n+  186: \t\tpreviousWorkSetsUncommittedWorks.clear();\n+  187: \t\tIndexFailureContext failureContext = failureContextBuilder.build();\n+  188: \t\tfailureHandler.handle( failureContext );\n+  189: \t}\n+  190: }\n",
        "uniqueId": "daaa9ff20cae0f7751636f16bcaf0cf2512c16a4_159_190_18_20_158_189",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 1,
                "covered": 55
            },
            "BRANCH": {
                "missed": 1,
                "covered": 3
            },
            "LINE": {
                "missed": 1,
                "covered": 16
            },
            "COMPLEXITY": {
                "missed": 1,
                "covered": 2
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Move And Rename Method",
        "description": "Move And Rename Method\tprivate findClass(clazz Class<T>, key String, value String) : List<T> from class org.hibernate.search.jsr352.massindexing.BatchIndexingJobIT to public findIndexedResults(emf EntityManagerFactory, clazz Class<T>, key String, value String) : List<T> from class org.hibernate.search.jsr352.test.util.JobTestUtil",
        "diffLocations": [
            {
                "filePath": "jsr352/core/src/test/java/org/hibernate/search/jsr352/massindexing/BatchIndexingJobIT.java",
                "startLine": 263,
                "endLine": 274,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "jsr352/core/src/test/java/org/hibernate/search/jsr352/test/util/JobTestUtil.java",
                "startLine": 57,
                "endLine": 68,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private <T> List<T> findClass(Class<T> clazz, String key, String value) {\n\t\tEntityManager em = emf.createEntityManager();\n\t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n\t\tQuery luceneQuery = ftem.getSearchFactory().buildQueryBuilder()\n\t\t\t\t.forEntity( clazz ).get()\n\t\t\t\t.keyword().onField( key ).matching( value )\n\t\t\t\t.createQuery();\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tList<T> result = ftem.createFullTextQuery( luceneQuery ).getResultList();\n\t\tem.close();\n\t\treturn result;\n\t}",
        "filePathBefore": "jsr352/core/src/test/java/org/hibernate/search/jsr352/massindexing/BatchIndexingJobIT.java",
        "isPureRefactoring": true,
        "commitId": "4c0595551f48dcca939fc3f7d3ea5bcf86dcbe7d",
        "packageNameBefore": "org.hibernate.search.jsr352.massindexing",
        "classNameBefore": "org.hibernate.search.jsr352.massindexing.BatchIndexingJobIT",
        "methodNameBefore": "org.hibernate.search.jsr352.massindexing.BatchIndexingJobIT#findClass",
        "invokedMethod": "methodSignature: org.hibernate.search.jsr352.massindexing.MassIndexingJob.ParametersBuilderInitialStep#forEntity\n methodBody: public ParametersBuilder forEntity(Class<?> rootEntity) {\nreturn new ParametersBuilder(rootEntity);\n}",
        "classSignatureBefore": "public class BatchIndexingJobIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.jsr352.massindexing.BatchIndexingJobIT#findClass"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.jsr352.massindexing.BatchIndexingJobIT"
        ],
        "classSignatureBeforeSet": [
            "public class BatchIndexingJobIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.jsr352.massindexing;\n\nimport static org.junit.Assert.assertEquals;\n\nimport java.io.IOException;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\nimport javax.batch.operations.JobOperator;\nimport javax.batch.runtime.BatchStatus;\nimport javax.batch.runtime.JobExecution;\nimport javax.batch.runtime.Metric;\nimport javax.batch.runtime.Metric.MetricType;\nimport javax.batch.runtime.StepExecution;\nimport javax.persistence.EntityManager;\nimport javax.persistence.EntityManagerFactory;\nimport javax.persistence.Persistence;\n\nimport org.apache.lucene.search.Query;\nimport org.hibernate.criterion.Restrictions;\nimport org.hibernate.search.jpa.FullTextEntityManager;\nimport org.hibernate.search.jpa.Search;\nimport org.hibernate.search.jsr352.massindexing.test.entity.Company;\nimport org.hibernate.search.jsr352.massindexing.test.entity.Person;\nimport org.hibernate.search.jsr352.massindexing.test.entity.WhoAmI;\nimport org.hibernate.search.jsr352.test.util.JobFactory;\nimport org.hibernate.search.jsr352.test.util.JobTestUtil;\nimport org.jboss.logging.Logger;\nimport org.junit.After;\nimport org.junit.Before;\nimport org.junit.Test;\n\n/**\n * @author Mincong Huang\n */\npublic class BatchIndexingJobIT {\n\n\tprivate static final Logger LOGGER = Logger.getLogger( BatchIndexingJobIT.class );\n\n\tprivate static final String PERSISTENCE_UNIT_NAME = \"h2\";\n\tprivate static final String SESSION_FACTORY_NAME = \"h2-entityManagerFactory\";\n\n\tprivate static final int JOB_TIMEOUT_MS = 10_000;\n\n\t// example dataset\n\tprivate static final long DB_COMP_ROWS = 3;\n\tprivate static final long DB_PERS_ROWS = 3;\n\tprivate static final long DB_WHOS_ROWS = 3;\n\tprivate static final long DB_TOTAL_ROWS = DB_COMP_ROWS + DB_PERS_ROWS + DB_WHOS_ROWS;\n\n\tprivate JobOperator jobOperator;\n\tprivate EntityManagerFactory emf;\n\n\t@Before\n\tpublic void setup() {\n\t\tjobOperator = JobFactory.getJobOperator();\n\n\t\tList<Company> companies = Arrays.asList(\n\t\t\t\tnew Company( \"Google\" ),\n\t\t\t\tnew Company( \"Red Hat\" ),\n\t\t\t\tnew Company( \"Microsoft\" ) );\n\t\tList<Person> people = Arrays.asList(\n\t\t\t\tnew Person( \"BG\", \"Bill\", \"Gates\" ),\n\t\t\t\tnew Person( \"LT\", \"Linus\", \"Torvalds\" ),\n\t\t\t\tnew Person( \"SJ\", \"Steven\", \"Jobs\" ) );\n\t\tList<WhoAmI> whos = Arrays.asList(\n\t\t\t\tnew WhoAmI( \"cid01\", \"id01\", \"uid01\" ),\n\t\t\t\tnew WhoAmI( \"cid02\", \"id02\", \"uid02\" ),\n\t\t\t\tnew WhoAmI( \"cid03\", \"id03\", \"uid03\" ) );\n\t\tEntityManager em = null;\n\n\t\ttry {\n\t\t\temf = Persistence.createEntityManagerFactory( PERSISTENCE_UNIT_NAME );\n\t\t\tem = emf.createEntityManager();\n\t\t\tem.getTransaction().begin();\n\t\t\tfor ( Company c : companies ) {\n\t\t\t\tem.persist( c );\n\t\t\t}\n\t\t\tfor ( Person p : people ) {\n\t\t\t\tem.persist( p );\n\t\t\t}\n\t\t\tfor ( WhoAmI w : whos ) {\n\t\t\t\tem.persist( w );\n\t\t\t}\n\t\t\tem.getTransaction().commit();\n\t\t}\n\t\tfinally {\n\t\t\ttry {\n\t\t\t\tem.close();\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\tLOGGER.error( e );\n\t\t\t}\n\t\t}\n\t}\n\n\t@Test\n\tpublic void simple() throws InterruptedException,\n\t\t\tIOException {\n\t\tEntityManager em = emf.createEntityManager();\n\t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n\t\tftem.purgeAll( Person.class );\n\t\tftem.purgeAll( Company.class );\n\t\tftem.purgeAll( WhoAmI.class );\n\t\tftem.flushToIndexes();\n\t\tem.close();\n\t\tList<Company> companies = findClass( Company.class, \"name\", \"Google\" );\n\t\tList<Person> people = findClass( Person.class, \"firstName\", \"Linus\" );\n\t\tList<WhoAmI> whos = findClass( WhoAmI.class, \"id\", \"id01\" );\n\t\tassertEquals( 0, companies.size() );\n\t\tassertEquals( 0, people.size() );\n\t\tassertEquals( 0, whos.size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntities( Company.class, Person.class, WhoAmI.class )\n\t\t\t\t\t\t.entityManagerFactoryReference( PERSISTENCE_UNIT_NAME )\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tjobExecution = JobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\t\tList<StepExecution> stepExecutions = jobOperator.getStepExecutions( executionId );\n\t\tfor ( StepExecution stepExecution : stepExecutions ) {\n\t\t\tLOGGER.infof( \"step %s executed.\", stepExecution.getStepName() );\n\t\t\ttestBatchStatus( stepExecution );\n\t\t}\n\n\t\tcompanies = findClass( Company.class, \"name\", \"Google\" );\n\t\tpeople = findClass( Person.class, \"firstName\", \"Linus\" );\n\t\twhos = findClass( WhoAmI.class, \"id\", \"id01\" );\n\t\tassertEquals( 1, companies.size() );\n\t\tassertEquals( 1, people.size() );\n\t\tassertEquals( 1, whos.size() );\n\t}\n\n\t@Test\n\tpublic void entityManagerFactoryScope_persistenceUnitName() throws InterruptedException,\n\t\t\tIOException {\n\t\tEntityManager em = emf.createEntityManager();\n\t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n\t\tftem.purgeAll( Company.class );\n\t\tList<Company> companies = findClass( Company.class, \"name\", \"Google\" );\n\t\tassertEquals( 0, companies.size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntity( Company.class )\n\t\t\t\t\t\t.entityManagerFactoryScope( \"persistence-unit-name\" )\n\t\t\t\t\t\t.entityManagerFactoryReference( PERSISTENCE_UNIT_NAME )\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tjobExecution = JobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\n\t\tcompanies = findClass( Company.class, \"name\", \"Google\" );\n\t\tassertEquals( 1, companies.size() );\n\t}\n\n\t@Test\n\tpublic void entityManagerFactoryScope_sessionFactoryName() throws InterruptedException,\n\t\t\tIOException {\n\t\tEntityManager em = emf.createEntityManager();\n\t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n\t\tftem.purgeAll( Company.class );\n\t\tList<Company> companies = findClass( Company.class, \"name\", \"Google\" );\n\t\tassertEquals( 0, companies.size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntity( Company.class )\n\t\t\t\t\t\t.entityManagerFactoryScope( \"session-factory-name\" )\n\t\t\t\t\t\t.entityManagerFactoryReference( SESSION_FACTORY_NAME )\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tjobExecution = JobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\n\t\tcompanies = findClass( Company.class, \"name\", \"Google\" );\n\t\tassertEquals( 1, companies.size() );\n\t}\n\n\t@Test\n\tpublic void criteria() throws InterruptedException,\n\t\t\tIOException {\n\n\t\t// purge all before start\n\t\t// TODO Can the creation of a new EM and FTEM be avoided?\n\t\tEntityManager em = emf.createEntityManager();\n\t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n\t\tftem.purgeAll( Person.class );\n\t\tftem.purgeAll( Company.class );\n\t\tftem.flushToIndexes();\n\t\tem.close();\n\n\t\t// searches before mass index,\n\t\t// expected no results for each search\n\t\tassertEquals( 0, findClass( Company.class, \"name\", \"Google\" ).size() );\n\t\tassertEquals( 0, findClass( Company.class, \"name\", \"Red Hat\" ).size() );\n\t\tassertEquals( 0, findClass( Company.class, \"name\", \"Microsoft\" ).size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntity( Company.class )\n\t\t\t\t\t\t.restrictedBy( Restrictions.in( \"name\", \"Google\", \"Red Hat\" ) )\n\t\t\t\t\t\t.entityManagerFactoryReference( PERSISTENCE_UNIT_NAME )\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tjobExecution = JobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\n\t\tassertEquals( 1, findClass( Company.class, \"name\", \"Google\" ).size() );\n\t\tassertEquals( 1, findClass( Company.class, \"name\", \"Red Hat\" ).size() );\n\t\tassertEquals( 0, findClass( Company.class, \"name\", \"Microsoft\" ).size() );\n\t}\n\n\t@Test\n\tpublic void hql() throws InterruptedException,\n\t\t\tIOException {\n\n\t\t// purge all before start\n\t\t// TODO Can the creation of a new EM and FTEM be avoided?\n\t\tEntityManager em = emf.createEntityManager();\n\t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n\t\tftem.purgeAll( Person.class );\n\t\tftem.purgeAll( Company.class );\n\t\tftem.flushToIndexes();\n\t\tem.close();\n\n\t\t// searches before mass index,\n\t\t// expected no results for each search\n\t\tassertEquals( 0, findClass( Company.class, \"name\", \"Google\" ).size() );\n\t\tassertEquals( 0, findClass( Company.class, \"name\", \"Red Hat\" ).size() );\n\t\tassertEquals( 0, findClass( Company.class, \"name\", \"Microsoft\" ).size() );\n\n\t\tlong executionId = jobOperator.start(\n\t\t\t\tMassIndexingJob.NAME,\n\t\t\t\tMassIndexingJob.parameters()\n\t\t\t\t\t\t.forEntity( Company.class )\n\t\t\t\t\t\t.restrictedBy( \"select c from Company c where c.name in ( 'Google', 'Red Hat' )\" )\n\t\t\t\t\t\t.entityManagerFactoryReference( PERSISTENCE_UNIT_NAME )\n\t\t\t\t\t\t.build()\n\t\t\t\t);\n\t\tJobExecution jobExecution = jobOperator.getJobExecution( executionId );\n\t\tjobExecution = JobTestUtil.waitForTermination( jobOperator, jobExecution, JOB_TIMEOUT_MS );\n\n\t\tassertEquals( 1, findClass( Company.class, \"name\", \"Google\" ).size() );\n\t\tassertEquals( 1, findClass( Company.class, \"name\", \"Red Hat\" ).size() );\n\t\tassertEquals( 0, findClass( Company.class, \"name\", \"Microsoft\" ).size() );\n\t}\n\n\tprivate <T> List<T> findClass(Class<T> clazz, String key, String value) {\n\t\tEntityManager em = emf.createEntityManager();\n\t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n\t\tQuery luceneQuery = ftem.getSearchFactory().buildQueryBuilder()\n\t\t\t\t.forEntity( clazz ).get()\n\t\t\t\t.keyword().onField( key ).matching( value )\n\t\t\t\t.createQuery();\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tList<T> result = ftem.createFullTextQuery( luceneQuery ).getResultList();\n\t\tem.close();\n\t\treturn result;\n\t}\n\n\tprivate void testBatchStatus(StepExecution stepExecution) {\n\t\tBatchStatus batchStatus = stepExecution.getBatchStatus();\n\t\tswitch ( stepExecution.getStepName() ) {\n\t\t\tcase \"produceLuceneDoc\":\n\t\t\t\tfor ( Metric m : stepExecution.getMetrics() ) {\n\t\t\t\t\tif ( m.getType().equals( MetricType.READ_COUNT ) ) {\n\t\t\t\t\t\tassertEquals( DB_TOTAL_ROWS, m.getValue() );\n\t\t\t\t\t}\n\t\t\t\t\telse if ( m.getType().equals( MetricType.WRITE_COUNT ) ) {\n\t\t\t\t\t\tassertEquals( DB_TOTAL_ROWS, m.getValue() );\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tassertEquals( BatchStatus.COMPLETED, batchStatus );\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\t/**\n\t * Convert the Metric array contained in StepExecution to a key-value map for easy access to Metric parameters.\n\t *\n\t * @param metrics a Metric array contained in StepExecution.\n\t * @return a map view of the metrics array.\n\t */\n\tpublic Map<Metric.MetricType, Long> getMetricsMap(Metric[] metrics) {\n\t\tMap<Metric.MetricType, Long> metricsMap = new HashMap<>();\n\t\tfor ( Metric metric : metrics ) {\n\t\t\tmetricsMap.put( metric.getType(), metric.getValue() );\n\t\t}\n\t\treturn metricsMap;\n\t}\n\n\t@After\n\tpublic void shutdown() {\n\t\temf.close();\n\t}\n}\n",
        "filePathAfter": "jsr352/core/src/test/java/org/hibernate/search/jsr352/test/util/JobTestUtil.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.jsr352.test.util;\n\nimport java.util.List;\nimport javax.batch.operations.JobOperator;\nimport javax.batch.runtime.BatchStatus;\nimport javax.batch.runtime.JobExecution;\nimport javax.persistence.EntityManager;\nimport javax.persistence.EntityManagerFactory;\n\nimport org.hibernate.search.jpa.FullTextEntityManager;\nimport org.hibernate.search.jpa.Search;\n\nimport org.jboss.logging.Logger;\n\nimport org.apache.lucene.search.Query;\n\n/**\n * @author Yoann Rodiere\n */\npublic final class JobTestUtil {\n\n\tprivate static final Logger LOGGER = Logger.getLogger( JobTestUtil.class );\n\n\tprivate static final int THREAD_SLEEP = 1000;\n\n\tprivate JobTestUtil() {\n\t}\n\n\tpublic static JobExecution waitForTermination(JobOperator jobOperator, JobExecution jobExecution, int timeoutInMs)\n\t\t\tthrows InterruptedException {\n\t\tlong endTime = System.currentTimeMillis() + timeoutInMs;\n\n\t\twhile ( !jobExecution.getBatchStatus().equals( BatchStatus.COMPLETED )\n\t\t\t\t&& !jobExecution.getBatchStatus().equals( BatchStatus.STOPPED )\n\t\t\t\t&& !jobExecution.getBatchStatus().equals( BatchStatus.FAILED )\n\t\t\t\t&& System.currentTimeMillis() < endTime ) {\n\n\t\t\tlong executionId = jobExecution.getExecutionId();\n\t\t\tLOGGER.infof(\n\t\t\t\t\t\"Job execution (id=%d) has status %s. Thread sleeps %d ms...\",\n\t\t\t\t\texecutionId,\n\t\t\t\t\tjobExecution.getBatchStatus(),\n\t\t\t\t\tTHREAD_SLEEP );\n\t\t\tThread.sleep( THREAD_SLEEP );\n\t\t\tjobExecution = jobOperator.getJobExecution( executionId );\n\t\t}\n\n\t\treturn jobExecution;\n\t}\n\n\tpublic static <T> List<T> findIndexedResults(EntityManagerFactory emf, Class<T> clazz, String key, String value) {\n\t\tEntityManager em = emf.createEntityManager();\n\t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n\t\tQuery luceneQuery = ftem.getSearchFactory().buildQueryBuilder()\n\t\t\t\t.forEntity( clazz ).get()\n\t\t\t\t.keyword().onField( key ).matching( value )\n\t\t\t\t.createQuery();\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tList<T> result = ftem.createFullTextQuery( luceneQuery ).getResultList();\n\t\tem.close();\n\t\treturn result;\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.jsr352.massindexing.MassIndexingJob.ParametersBuilderInitialStep#forEntity\n methodBody: public ParametersBuilder forEntity(Class<?> rootEntity) {\nreturn new ParametersBuilder(rootEntity);\n}"
        ],
        "sourceCodeAfterRefactoring": "public static <T> List<T> findIndexedResults(EntityManagerFactory emf, Class<T> clazz, String key, String value) {\n\t\tEntityManager em = emf.createEntityManager();\n\t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n\t\tQuery luceneQuery = ftem.getSearchFactory().buildQueryBuilder()\n\t\t\t\t.forEntity( clazz ).get()\n\t\t\t\t.keyword().onField( key ).matching( value )\n\t\t\t\t.createQuery();\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tList<T> result = ftem.createFullTextQuery( luceneQuery ).getResultList();\n\t\tem.close();\n\t\treturn result;\n\t}",
        "diffSourceCode": "-   57: \tprivate static final long DB_TOTAL_ROWS = DB_COMP_ROWS + DB_PERS_ROWS + DB_WHOS_ROWS;\n-   58: \n-   59: \tprivate JobOperator jobOperator;\n-   60: \tprivate EntityManagerFactory emf;\n-   61: \n-   62: \t@Before\n-   63: \tpublic void setup() {\n-   64: \t\tjobOperator = JobFactory.getJobOperator();\n-   65: \n-   66: \t\tList<Company> companies = Arrays.asList(\n-   67: \t\t\t\tnew Company( \"Google\" ),\n-   68: \t\t\t\tnew Company( \"Red Hat\" ),\n-  263: \tprivate <T> List<T> findClass(Class<T> clazz, String key, String value) {\n-  264: \t\tEntityManager em = emf.createEntityManager();\n-  265: \t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n-  266: \t\tQuery luceneQuery = ftem.getSearchFactory().buildQueryBuilder()\n-  267: \t\t\t\t.forEntity( clazz ).get()\n-  268: \t\t\t\t.keyword().onField( key ).matching( value )\n-  269: \t\t\t\t.createQuery();\n-  270: \t\t@SuppressWarnings(\"unchecked\")\n-  271: \t\tList<T> result = ftem.createFullTextQuery( luceneQuery ).getResultList();\n-  272: \t\tem.close();\n-  273: \t\treturn result;\n-  274: \t}\n+   57: \tpublic static <T> List<T> findIndexedResults(EntityManagerFactory emf, Class<T> clazz, String key, String value) {\n+   58: \t\tEntityManager em = emf.createEntityManager();\n+   59: \t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n+   60: \t\tQuery luceneQuery = ftem.getSearchFactory().buildQueryBuilder()\n+   61: \t\t\t\t.forEntity( clazz ).get()\n+   62: \t\t\t\t.keyword().onField( key ).matching( value )\n+   63: \t\t\t\t.createQuery();\n+   64: \t\t@SuppressWarnings(\"unchecked\")\n+   65: \t\tList<T> result = ftem.createFullTextQuery( luceneQuery ).getResultList();\n+   66: \t\tem.close();\n+   67: \t\treturn result;\n+   68: \t}\n",
        "uniqueId": "4c0595551f48dcca939fc3f7d3ea5bcf86dcbe7d_263_274__57_68",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Inline Method",
        "description": "Inline Method\tprivate createAndStartJob(jobOperator JobOperator) : long inlined to public testJob() : void in class org.hibernate.search.jsr352.RestartIT",
        "diffLocations": [
            {
                "filePath": "jsr352/integrationtest/javaee-wildfly/src/test/java/org/hibernate/search/jsr352/RestartIT.java",
                "startLine": 81,
                "endLine": 115,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "jsr352/integrationtest/wildfly/src/test/java/org/hibernate/search/jsr352/RestartIT.java",
                "startLine": 88,
                "endLine": 109,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "jsr352/integrationtest/wildfly/src/test/java/org/hibernate/search/jsr352/RestartIT.java",
                "startLine": 139,
                "endLine": 150,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private long createAndStartJob(JobOperator jobOperator) {\n\t\tMassIndexer massIndexer = new MassIndexer()\n\t\t\t\t.fetchSize( JOB_FETCH_SIZE )\n\t\t\t\t.maxResults( JOB_MAX_RESULTS )\n\t\t\t\t.maxThreads( JOB_MAX_THREADS )\n\t\t\t\t.purgeAtStart( JOB_PURGE_AT_START )\n\t\t\t\t.rowsPerPartition( JOB_ROWS_PER_PARTITION )\n\t\t\t\t.jobOperator( jobOperator )\n\t\t\t\t.addRootEntities( Company.class, Person.class );\n\t\tlong executionId = massIndexer.start();\n\t\treturn executionId;\n\t}",
        "filePathBefore": "jsr352/integrationtest/javaee-wildfly/src/test/java/org/hibernate/search/jsr352/RestartIT.java",
        "isPureRefactoring": true,
        "commitId": "fee3e4f8d370622571556c29cb60adf5f4c5e420",
        "packageNameBefore": "org.hibernate.search.jsr352",
        "classNameBefore": "org.hibernate.search.jsr352.RestartIT",
        "methodNameBefore": "org.hibernate.search.jsr352.RestartIT#createAndStartJob",
        "invokedMethod": "methodSignature: org.hibernate.search.jsr352.MassIndexer#jobOperator\n methodBody: public MassIndexer jobOperator(JobOperator jobOperatorInJavaSE) {\nthis.jobOperator=jobOperatorInJavaSE;\nreturn this;\n}\nmethodSignature: org.hibernate.search.jsr352.MassIndexer#maxThreads\n methodBody: public MassIndexer maxThreads(int maxThreads) {\nif(maxThreads < 1){throw new IllegalArgumentException(\"threads must be at least 1.\");\n}this.maxThreads=maxThreads;\nreturn this;\n}\nmethodSignature: org.hibernate.search.jsr352.MassIndexer#start\n methodBody: public long start() {\nif(rootEntities == null){throw new NullPointerException(\"rootEntities cannot be null\");\n}if(isJavaSE){if(emf == null){throw new NullPointerException(\"You're under a Java SE environment. \" + \"Please assign the EntityManagerFactory before the job start.\");\n}if(jobOperator == null){throw new NullPointerException(\"You're under a Java SE environment. \" + \"Please assign the jobOperator before the job start.\");\n}JobSEEnvironment.setEntityManagerFactory(emf);\n}{if(emf != null){throw new IllegalStateException(\"You're under a Java EE environmant. \" + \"Please do not assign the EntityManagerFactory. \" + \"If you're under Java SE, set isJavaSE( true );\");\n}jobOperator=BatchRuntime.getJobOperator();\n}Properties jobParams=new Properties();\njobParams.put(\"cacheable\",String.valueOf(cacheable));\njobParams.put(\"fetchSize\",String.valueOf(fetchSize));\njobParams.put(\"isJavaSE\",String.valueOf(isJavaSE));\njobParams.put(\"itemCount\",String.valueOf(itemCount));\njobParams.put(\"maxResults\",String.valueOf(maxResults));\njobParams.put(\"maxThreads\",String.valueOf(maxThreads));\njobParams.put(\"optimizeAfterPurge\",String.valueOf(optimizeAfterPurge));\njobParams.put(\"optimizeAtEnd\",String.valueOf(optimizeAtEnd));\njobParams.put(\"purgeAtStart\",String.valueOf(purgeAtStart));\njobParams.put(\"rootEntities\",getRootEntitiesAsString());\njobParams.put(\"rowsPerPartition\",String.valueOf(rowsPerPartition));\nexecutionId=jobOperator.start(JOB_NAME,jobParams);\nreturn executionId;\n}\nmethodSignature: org.hibernate.search.jsr352.MassIndexer#purgeAtStart\n methodBody: public MassIndexer purgeAtStart(boolean purgeAtStart) {\nthis.purgeAtStart=purgeAtStart;\nreturn this;\n}\nmethodSignature: org.hibernate.search.jsr352.MassIndexer#maxResults\n methodBody: public MassIndexer maxResults(int maxResults) {\nif(maxResults < 1){throw new IllegalArgumentException(\"maxResults must be at least 1\");\n}this.maxResults=maxResults;\nreturn this;\n}\nmethodSignature: org.hibernate.search.jsr352.MassIndexer#rowsPerPartition\n methodBody: public MassIndexer rowsPerPartition(int rowsPerPartition) {\nif(rowsPerPartition < 1){throw new IllegalArgumentException(\"rowsPerPartition must be at least 1\");\n}this.rowsPerPartition=rowsPerPartition;\nreturn this;\n}\nmethodSignature: org.hibernate.search.jsr352.MassIndexer#fetchSize\n methodBody: public MassIndexer fetchSize(int fetchSize) {\nif(fetchSize < 1){throw new IllegalArgumentException(\"fetchSize must be at least 1\");\n}this.fetchSize=fetchSize;\nreturn this;\n}\nmethodSignature: org.hibernate.search.jsr352.MassIndexer#addRootEntities\n methodBody: public MassIndexer addRootEntities(Class<?>... rootEntities) {\nif(rootEntities == null){throw new NullPointerException(\"rootEntities cannot be NULL.\");\n}if(rootEntities.length == 0){throw new IllegalStateException(\"rootEntities must have at least 1 element.\");\n}this.rootEntities.addAll(Arrays.asList(rootEntities));\nreturn this;\n}",
        "classSignatureBefore": "public class RestartIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.jsr352.RestartIT#createAndStartJob"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.jsr352.RestartIT"
        ],
        "classSignatureBeforeSet": [
            "public class RestartIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Overlapped refactoring - can be identical by undoing the overlapped refactoring\n- Inline Variable-",
                "description": "Return statements added",
                "mappingState": 2
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.jsr352;\n\nimport static org.junit.Assert.assertEquals;\n\nimport java.io.Serializable;\nimport java.util.ArrayList;\nimport java.util.Date;\nimport java.util.List;\n\nimport javax.batch.operations.JobOperator;\nimport javax.batch.runtime.BatchRuntime;\nimport javax.batch.runtime.BatchStatus;\nimport javax.batch.runtime.JobExecution;\nimport javax.inject.Inject;\n\nimport org.hibernate.search.jsr352.test.entity.Company;\nimport org.hibernate.search.jsr352.test.entity.CompanyManager;\nimport org.hibernate.search.jsr352.test.entity.Person;\nimport org.hibernate.search.jsr352.test.entity.PersonManager;\nimport org.jboss.arquillian.container.test.api.Deployment;\nimport org.jboss.arquillian.junit.Arquillian;\nimport org.jboss.logging.Logger;\nimport org.jboss.shrinkwrap.api.ShrinkWrap;\nimport org.jboss.shrinkwrap.api.asset.EmptyAsset;\nimport org.jboss.shrinkwrap.api.spec.WebArchive;\nimport org.junit.Ignore;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\n\n/**\n * This integration test (IT) aims to test the restartability of the job\n * execution mass-indexer under Java EE environment, with step partitioning\n * (parallelism). We need to prove that the job restart from the checkpoint\n * where it was stopped, but not from the very beginning.\n *\n * @author Mincong Huang\n */\n@Ignore(\"No need to run another restart test under Java EE\")\n@RunWith(Arquillian.class)\npublic class RestartIT {\n\n\tprivate static final Logger LOGGER = Logger.getLogger( RestartIT.class );\n\n\tprivate final boolean JOB_PURGE_AT_START = true;\n\tprivate final int JOB_FETCH_SIZE = 100 * 1000;\n\tprivate final int JOB_MAX_RESULTS = 200 * 1000;\n\tprivate final int JOB_MAX_THREADS = 3;\n\tprivate final int JOB_ROWS_PER_PARTITION = 1000;\n\n\tprivate final long DB_COMP_ROWS = 2500;\n\tprivate final long DB_PERS_ROWS = 2600;\n\n\tprivate final int MAX_TRIES = 40;\n\tprivate final int THREAD_SLEEP = 1000;\n\n\t@Inject\n\tprivate CompanyManager companyManager;\n\n\t@Inject\n\tprivate PersonManager personManager;\n\n\t@Deployment\n\tpublic static WebArchive createDeployment() {\n\t\tWebArchive war = ShrinkWrap.create( WebArchive.class )\n\t\t\t\t.addAsResource( \"META-INF/persistence.xml\" )\n\t\t\t\t.addAsResource( \"META-INF/batch-jobs/mass-index.xml\" )\n\t\t\t\t.addAsWebInfResource( EmptyAsset.INSTANCE, \"beans.xml\" )\n\t\t\t\t.addClasses( Serializable.class, Date.class )\n\t\t\t\t.addPackages( true, \"org.hibernate.search.annotations\" )\n\t\t\t\t.addPackages( true, \"org.hibernate.search.jsr352\" )\n\t\t\t\t.addPackages( true, \"javax.persistence\" );\n\t\treturn war;\n\t}\n\n\t@Test\n\tpublic void testJob() throws InterruptedException {\n\n\t\tfinal String google = \"google\";\n\t\tfinal String googleCEO = \"Sundar\";\n\n\t\tinsertData();\n\t\tList<Company> googles = companyManager.findCompanyByName( google );\n\t\tList<Person> googleCEOs = personManager.findPerson( googleCEO );\n\t\tassertEquals( 0, googles.size() );\n\t\tassertEquals( 0, googleCEOs.size() );\n\n\t\t// Start the job. This is the 1st execution.\n\t\t// Keep the execution alive and wait Byteman to stop the job\n\t\tJobOperator jobOperator = BatchRuntime.getJobOperator();\n\t\tlong execId1 = createAndStartJob( jobOperator );\n\t\tJobExecution jobExec1 = jobOperator.getJobExecution( execId1 );\n\t\tjobExec1 = keepTestAlive( jobExec1 );\n\n\t\t// Restart the job. This is the 2nd execution.\n\t\tlong execId2 = jobOperator.restart( execId1, null );\n\t\tJobExecution jobExec2 = jobOperator.getJobExecution( execId2 );\n\t\tjobExec2 = keepTestAlive( jobExec2 );\n\t\tassertEquals( BatchStatus.COMPLETED, jobExec2.getBatchStatus() );\n\n\t\tgoogles = companyManager.findCompanyByName( google );\n\t\tgoogleCEOs = personManager.findPerson( googleCEO );\n\t\tassertEquals( DB_COMP_ROWS / 5, googles.size() );\n\t\tassertEquals( DB_PERS_ROWS / 5, googleCEOs.size() );\n\n\t\t// TODO this method should not belong to company manager\n\t\t// but how to create an all context query ?\n\t\tint totalDocs = companyManager.findAll().size();\n\t\tassertEquals( (int) ( DB_COMP_ROWS + DB_PERS_ROWS ), totalDocs );\n\t}\n\n\tprivate void insertData() {\n\t\tfinal String[][] str = new String[][]{\n\t\t\t\t{ \"Google\", \"Sundar\", \"Pichai\" },\n\t\t\t\t{ \"Red Hat\", \"James\", \"M. Whitehurst\" },\n\t\t\t\t{ \"Microsoft\", \"Satya\", \"Nadella\" },\n\t\t\t\t{ \"Facebook\", \"Mark\", \"Zuckerberg\" },\n\t\t\t\t{ \"Amazon\", \"Jeff\", \"Bezos\" }\n\t\t};\n\t\tList<Person> people = new ArrayList<>( (int) DB_PERS_ROWS );\n\t\tList<Company> companies = new ArrayList<>( (int) DB_COMP_ROWS );\n\t\tfor ( int i = 0; i < DB_PERS_ROWS; i++ ) {\n\t\t\tPerson p = new Person( i, str[i % 5][1], str[i % 5][2] );\n\t\t\tpeople.add( p );\n\t\t}\n\t\tfor ( int i = 0; i < DB_COMP_ROWS; i++ ) {\n\t\t\tCompany c = new Company( str[i % 5][0] );\n\t\t\tcompanies.add( c );\n\t\t}\n\t\tpersonManager.persist( people );\n\t\tcompanyManager.persist( companies );\n\t}\n\n\tprivate long createAndStartJob(JobOperator jobOperator) {\n\t\tMassIndexer massIndexer = new MassIndexer()\n\t\t\t\t.fetchSize( JOB_FETCH_SIZE )\n\t\t\t\t.maxResults( JOB_MAX_RESULTS )\n\t\t\t\t.maxThreads( JOB_MAX_THREADS )\n\t\t\t\t.purgeAtStart( JOB_PURGE_AT_START )\n\t\t\t\t.rowsPerPartition( JOB_ROWS_PER_PARTITION )\n\t\t\t\t.jobOperator( jobOperator )\n\t\t\t\t.addRootEntities( Company.class, Person.class );\n\t\tlong executionId = massIndexer.start();\n\t\treturn executionId;\n\t}\n\n\tprivate JobExecution keepTestAlive(JobExecution jobExecution)\n\t\t\tthrows InterruptedException {\n\n\t\tint tries = 0;\n\t\tJobOperator jobOperator = BatchRuntime.getJobOperator();\n\t\twhile ( !jobExecution.getBatchStatus().equals( BatchStatus.COMPLETED )\n\t\t\t\t&& !jobExecution.getBatchStatus().equals( BatchStatus.STOPPED )\n\t\t\t\t&& !jobExecution.getBatchStatus().equals( BatchStatus.FAILED )\n\t\t\t\t&& tries < MAX_TRIES) {\n\n\t\t\tlong executionId = jobExecution.getExecutionId();\n\t\t\tLOGGER.infof(\n\t\t\t\t\t\"Job execution (id=%d) has status %s. Thread sleeps %d ms...\",\n\t\t\t\t\texecutionId,\n\t\t\t\t\tjobExecution.getBatchStatus(),\n\t\t\t\t\tTHREAD_SLEEP );\n\t\t\tThread.sleep( THREAD_SLEEP );\n\t\t\tjobExecution = jobOperator.getJobExecution( executionId );\n\t\t\ttries++;\n\t\t}\n\t\treturn jobExecution;\n\t}\n}\n",
        "filePathAfter": "jsr352/integrationtest/wildfly/src/test/java/org/hibernate/search/jsr352/RestartIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.jsr352;\n\nimport static org.junit.Assert.assertEquals;\n\nimport java.io.IOException;\nimport java.text.ParseException;\nimport java.text.SimpleDateFormat;\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport javax.batch.runtime.BatchRuntime;\nimport javax.batch.runtime.BatchStatus;\nimport javax.batch.runtime.JobExecution;\nimport javax.inject.Inject;\nimport javax.persistence.EntityManager;\nimport javax.persistence.EntityManagerFactory;\nimport javax.persistence.PersistenceUnit;\n\nimport org.hibernate.criterion.Restrictions;\nimport org.hibernate.search.jpa.FullTextEntityManager;\nimport org.hibernate.search.jpa.Search;\nimport org.hibernate.search.jsr352.test.Message;\nimport org.hibernate.search.jsr352.test.MessageManager;\nimport org.jboss.arquillian.container.test.api.Deployment;\nimport org.jboss.arquillian.junit.Arquillian;\nimport org.jboss.logging.Logger;\nimport org.jboss.shrinkwrap.api.ShrinkWrap;\nimport org.jboss.shrinkwrap.api.asset.EmptyAsset;\nimport org.jboss.shrinkwrap.api.spec.WebArchive;\nimport org.junit.FixMethodOrder;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.junit.runners.MethodSorters;\n\n/**\n * This integration test (IT) aims to test the restartability of the job execution mass-indexer under Java EE\n * environment, with step partitioning (parallelism). We need to prove that the job restart from the checkpoint where it\n * was stopped, but not from the very beginning.\n *\n * @author Mincong Huang\n */\n@RunWith(Arquillian.class)\n@FixMethodOrder(MethodSorters.NAME_ASCENDING)\npublic class RestartIT {\n\n\tprivate static final Logger LOGGER = Logger.getLogger( RestartIT.class );\n\tprivate static final SimpleDateFormat SDF = new SimpleDateFormat( \"dd/MM/yyyy\" );\n\tprivate static final int DB_DAY1_ROWS = 2000;\n\tprivate static final int DB_DAY2_ROWS = 3000;\n\tprivate static final int MAX_TRIES = 40;\n\tprivate static final int THREAD_SLEEP = 1000;\n\n\t@Inject\n\tprivate MessageManager messageManager;\n\n\t@PersistenceUnit(unitName = \"h2\")\n\tprivate EntityManagerFactory emf;\n\n\t@Deployment\n\tpublic static WebArchive createDeployment() {\n\t\tWebArchive war = ShrinkWrap\n\t\t\t\t.create( WebArchive.class, RestartIT.class.getSimpleName() + \".war\" )\n\t\t\t\t.addAsResource( \"META-INF/persistence.xml\" )\n\t\t\t\t.addAsResource( \"META-INF/batch-jobs/make-deployment-as-batch-app.xml\" ) // WFLY-7000\n\t\t\t\t.addAsWebInfResource( \"jboss-deployment-structure.xml\" )\n\t\t\t\t.addAsWebInfResource( EmptyAsset.INSTANCE, \"beans.xml\" )\n\t\t\t\t.addPackage( Message.class.getPackage() );\n\t\treturn war;\n\t}\n\n\tpublic void insertData() throws ParseException {\n\t\tList<Message> messages = new ArrayList<>( DB_DAY1_ROWS + DB_DAY2_ROWS );\n\t\tfor ( int i = 0; i < DB_DAY1_ROWS; i++ ) {\n\t\t\tmessages.add( new Message( String.valueOf( i ), SDF.parse( \"31/08/2016\" ) ) );\n\t\t}\n\t\tfor ( int i = 0; i < DB_DAY2_ROWS; i++ ) {\n\t\t\tmessages.add( new Message( String.valueOf( i ), SDF.parse( \"01/09/2016\" ) ) );\n\t\t}\n\t\tmessageManager.persist( messages );\n\t}\n\n\t@Test\n\tpublic void testJob() throws InterruptedException, IOException, ParseException {\n\n\t\tinsertData();\n\n\t\tassertEquals( 0, messageManager.findMessagesFor( SDF.parse( \"31/08/2016\" ) ).size() );\n\t\tassertEquals( 0, messageManager.findMessagesFor( SDF.parse( \"01/09/2016\" ) ).size() );\n\n\t\t// The 1st execution. Keep it alive and wait Byteman to stop it\n\t\tlong execId1 = BatchIndexingJob.forEntity( Message.class ).start();\n\t\tJobExecution jobExec1 = BatchRuntime.getJobOperator().getJobExecution( execId1 );\n\t\tjobExec1 = keepTestAlive( jobExec1 );\n\n\t\t// Restart the job. This is the 2nd execution.\n\t\tlong execId2 = BatchIndexingJob.restart( execId1 );\n\t\tJobExecution jobExec2 = BatchRuntime.getJobOperator().getJobExecution( execId2 );\n\t\tjobExec2 = keepTestAlive( jobExec2 );\n\n\t\tassertEquals( BatchStatus.COMPLETED, jobExec2.getBatchStatus() );\n\t\tassertEquals( DB_DAY1_ROWS, messageManager.findMessagesFor( SDF.parse( \"31/08/2016\" ) ).size() );\n\t\tassertEquals( DB_DAY2_ROWS, messageManager.findMessagesFor( SDF.parse( \"01/09/2016\" ) ).size() );\n\t}\n\n\t@Test\n\tpublic void testJob_usingCriteria() throws InterruptedException, IOException, ParseException {\n\n\t\t// purge all before start\n\t\t// TODO Can the creation of a new EM and FTEM be avoided?\n\t\tEntityManager em = emf.createEntityManager();\n\t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n\t\tftem.purgeAll( Message.class );\n\t\tftem.flushToIndexes();\n\t\tem.close();\n\n\t\tassertEquals( 0, messageManager.findMessagesFor( SDF.parse( \"31/08/2016\" ) ).size() );\n\t\tassertEquals( 0, messageManager.findMessagesFor( SDF.parse( \"01/09/2016\" ) ).size() );\n\n\t\t// The 1st execution. Keep it alive and wait Byteman to stop it\n\t\tlong execId1 = BatchIndexingJob.forEntity( Message.class )\n\t\t\t\t.restrictedBy( Restrictions.ge( \"date\", SDF.parse( \"01/09/2016\" ) ) )\n\t\t\t\t.start();\n\t\tJobExecution jobExec1 = BatchRuntime.getJobOperator().getJobExecution( execId1 );\n\t\tjobExec1 = keepTestAlive( jobExec1 );\n\n\t\t// Restart the job. This is the 2nd execution.\n\t\tlong execId2 = BatchIndexingJob.restart( execId1 );\n\t\tJobExecution jobExec2 = BatchRuntime.getJobOperator().getJobExecution( execId2 );\n\t\tjobExec2 = keepTestAlive( jobExec2 );\n\n\t\tassertEquals( BatchStatus.COMPLETED, jobExec2.getBatchStatus() );\n\t\tassertEquals( 0, messageManager.findMessagesFor( SDF.parse( \"31/08/2016\" ) ).size() );\n\t\tassertEquals( DB_DAY2_ROWS, messageManager.findMessagesFor( SDF.parse( \"01/09/2016\" ) ).size() );\n\t}\n\n\t@Test\n\tpublic void testJob_usingHQL() throws InterruptedException, IOException, ParseException {\n\n\t\t// purge all before start\n\t\t// TODO Can the creation of a new EM and FTEM be avoided?\n\t\tEntityManager em = emf.createEntityManager();\n\t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n\t\tftem.purgeAll( Message.class );\n\t\tftem.flushToIndexes();\n\t\tem.close();\n\n\t\tassertEquals( 0, messageManager.findMessagesFor( SDF.parse( \"31/08/2016\" ) ).size() );\n\t\tassertEquals( 0, messageManager.findMessagesFor( SDF.parse( \"01/09/2016\" ) ).size() );\n\n\t\tlong execId1 = BatchIndexingJob.forEntity( Message.class )\n\t\t\t\t.restrictedBy( \"select m from Message m where day( m.date ) = 31\" )\n\t\t\t\t.start();\n\t\tJobExecution jobExec1 = BatchRuntime.getJobOperator().getJobExecution( execId1 );\n\t\tjobExec1 = keepTestAlive( jobExec1 );\n\n\t\tassertEquals( BatchStatus.COMPLETED, jobExec1.getBatchStatus() );\n\t\tassertEquals( DB_DAY1_ROWS, messageManager.findMessagesFor( SDF.parse( \"31/08/2016\" ) ).size() );\n\t\tassertEquals( 0, messageManager.findMessagesFor( SDF.parse( \"01/09/2016\" ) ).size() );\n\t}\n\n\tprivate JobExecution keepTestAlive(JobExecution jobExecution)\n\t\t\tthrows InterruptedException {\n\n\t\tint tries = 0;\n\t\twhile ( !jobExecution.getBatchStatus().equals( BatchStatus.COMPLETED )\n\t\t\t\t&& !jobExecution.getBatchStatus().equals( BatchStatus.STOPPED )\n\t\t\t\t&& !jobExecution.getBatchStatus().equals( BatchStatus.FAILED )\n\t\t\t\t&& tries < MAX_TRIES ) {\n\n\t\t\tlong executionId = jobExecution.getExecutionId();\n\t\t\tLOGGER.infof(\n\t\t\t\t\t\"Job execution (id=%d) has status %s. Thread sleeps %d ms...\",\n\t\t\t\t\texecutionId,\n\t\t\t\t\tjobExecution.getBatchStatus(),\n\t\t\t\t\tTHREAD_SLEEP );\n\t\t\tThread.sleep( THREAD_SLEEP );\n\t\t\tjobExecution = BatchRuntime.getJobOperator().getJobExecution( executionId );\n\t\t\ttries++;\n\t\t}\n\t\treturn jobExecution;\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.jsr352.MassIndexer#jobOperator\n methodBody: public MassIndexer jobOperator(JobOperator jobOperatorInJavaSE) {\nthis.jobOperator=jobOperatorInJavaSE;\nreturn this;\n}",
            "methodSignature: org.hibernate.search.jsr352.MassIndexer#maxThreads\n methodBody: public MassIndexer maxThreads(int maxThreads) {\nif(maxThreads < 1){throw new IllegalArgumentException(\"threads must be at least 1.\");\n}this.maxThreads=maxThreads;\nreturn this;\n}",
            "methodSignature: org.hibernate.search.jsr352.MassIndexer#start\n methodBody: public long start() {\nif(rootEntities == null){throw new NullPointerException(\"rootEntities cannot be null\");\n}if(isJavaSE){if(emf == null){throw new NullPointerException(\"You're under a Java SE environment. \" + \"Please assign the EntityManagerFactory before the job start.\");\n}if(jobOperator == null){throw new NullPointerException(\"You're under a Java SE environment. \" + \"Please assign the jobOperator before the job start.\");\n}JobSEEnvironment.setEntityManagerFactory(emf);\n}{if(emf != null){throw new IllegalStateException(\"You're under a Java EE environmant. \" + \"Please do not assign the EntityManagerFactory. \" + \"If you're under Java SE, set isJavaSE( true );\");\n}jobOperator=BatchRuntime.getJobOperator();\n}Properties jobParams=new Properties();\njobParams.put(\"cacheable\",String.valueOf(cacheable));\njobParams.put(\"fetchSize\",String.valueOf(fetchSize));\njobParams.put(\"isJavaSE\",String.valueOf(isJavaSE));\njobParams.put(\"itemCount\",String.valueOf(itemCount));\njobParams.put(\"maxResults\",String.valueOf(maxResults));\njobParams.put(\"maxThreads\",String.valueOf(maxThreads));\njobParams.put(\"optimizeAfterPurge\",String.valueOf(optimizeAfterPurge));\njobParams.put(\"optimizeAtEnd\",String.valueOf(optimizeAtEnd));\njobParams.put(\"purgeAtStart\",String.valueOf(purgeAtStart));\njobParams.put(\"rootEntities\",getRootEntitiesAsString());\njobParams.put(\"rowsPerPartition\",String.valueOf(rowsPerPartition));\nexecutionId=jobOperator.start(JOB_NAME,jobParams);\nreturn executionId;\n}",
            "methodSignature: org.hibernate.search.jsr352.MassIndexer#purgeAtStart\n methodBody: public MassIndexer purgeAtStart(boolean purgeAtStart) {\nthis.purgeAtStart=purgeAtStart;\nreturn this;\n}",
            "methodSignature: org.hibernate.search.jsr352.MassIndexer#maxResults\n methodBody: public MassIndexer maxResults(int maxResults) {\nif(maxResults < 1){throw new IllegalArgumentException(\"maxResults must be at least 1\");\n}this.maxResults=maxResults;\nreturn this;\n}",
            "methodSignature: org.hibernate.search.jsr352.MassIndexer#rowsPerPartition\n methodBody: public MassIndexer rowsPerPartition(int rowsPerPartition) {\nif(rowsPerPartition < 1){throw new IllegalArgumentException(\"rowsPerPartition must be at least 1\");\n}this.rowsPerPartition=rowsPerPartition;\nreturn this;\n}",
            "methodSignature: org.hibernate.search.jsr352.MassIndexer#fetchSize\n methodBody: public MassIndexer fetchSize(int fetchSize) {\nif(fetchSize < 1){throw new IllegalArgumentException(\"fetchSize must be at least 1\");\n}this.fetchSize=fetchSize;\nreturn this;\n}",
            "methodSignature: org.hibernate.search.jsr352.MassIndexer#addRootEntities\n methodBody: public MassIndexer addRootEntities(Class<?>... rootEntities) {\nif(rootEntities == null){throw new NullPointerException(\"rootEntities cannot be NULL.\");\n}if(rootEntities.length == 0){throw new IllegalStateException(\"rootEntities must have at least 1 element.\");\n}this.rootEntities.addAll(Arrays.asList(rootEntities));\nreturn this;\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void testJob() throws InterruptedException, IOException, ParseException {\n\n\t\tinsertData();\n\n\t\tassertEquals( 0, messageManager.findMessagesFor( SDF.parse( \"31/08/2016\" ) ).size() );\n\t\tassertEquals( 0, messageManager.findMessagesFor( SDF.parse( \"01/09/2016\" ) ).size() );\n\n\t\t// The 1st execution. Keep it alive and wait Byteman to stop it\n\t\tlong execId1 = BatchIndexingJob.forEntity( Message.class ).start();\n\t\tJobExecution jobExec1 = BatchRuntime.getJobOperator().getJobExecution( execId1 );\n\t\tjobExec1 = keepTestAlive( jobExec1 );\n\n\t\t// Restart the job. This is the 2nd execution.\n\t\tlong execId2 = BatchIndexingJob.restart( execId1 );\n\t\tJobExecution jobExec2 = BatchRuntime.getJobOperator().getJobExecution( execId2 );\n\t\tjobExec2 = keepTestAlive( jobExec2 );\n\n\t\tassertEquals( BatchStatus.COMPLETED, jobExec2.getBatchStatus() );\n\t\tassertEquals( DB_DAY1_ROWS, messageManager.findMessagesFor( SDF.parse( \"31/08/2016\" ) ).size() );\n\t\tassertEquals( DB_DAY2_ROWS, messageManager.findMessagesFor( SDF.parse( \"01/09/2016\" ) ).size() );\n\t}",
        "diffSourceCode": "-   81: \t@Test\n-   82: \tpublic void testJob() throws InterruptedException {\n-   83: \n-   84: \t\tfinal String google = \"google\";\n-   85: \t\tfinal String googleCEO = \"Sundar\";\n-   86: \n-   87: \t\tinsertData();\n-   88: \t\tList<Company> googles = companyManager.findCompanyByName( google );\n-   89: \t\tList<Person> googleCEOs = personManager.findPerson( googleCEO );\n-   90: \t\tassertEquals( 0, googles.size() );\n-   91: \t\tassertEquals( 0, googleCEOs.size() );\n+   81: \t\t}\n+   82: \t\tfor ( int i = 0; i < DB_DAY2_ROWS; i++ ) {\n+   83: \t\t\tmessages.add( new Message( String.valueOf( i ), SDF.parse( \"01/09/2016\" ) ) );\n+   84: \t\t}\n+   85: \t\tmessageManager.persist( messages );\n+   86: \t}\n+   87: \n+   88: \t@Test\n+   89: \tpublic void testJob() throws InterruptedException, IOException, ParseException {\n+   90: \n+   91: \t\tinsertData();\n    92: \n-   93: \t\t// Start the job. This is the 1st execution.\n-   94: \t\t// Keep the execution alive and wait Byteman to stop the job\n-   95: \t\tJobOperator jobOperator = BatchRuntime.getJobOperator();\n-   96: \t\tlong execId1 = createAndStartJob( jobOperator );\n-   97: \t\tJobExecution jobExec1 = jobOperator.getJobExecution( execId1 );\n-   98: \t\tjobExec1 = keepTestAlive( jobExec1 );\n-   99: \n-  100: \t\t// Restart the job. This is the 2nd execution.\n-  101: \t\tlong execId2 = jobOperator.restart( execId1, null );\n-  102: \t\tJobExecution jobExec2 = jobOperator.getJobExecution( execId2 );\n-  103: \t\tjobExec2 = keepTestAlive( jobExec2 );\n-  104: \t\tassertEquals( BatchStatus.COMPLETED, jobExec2.getBatchStatus() );\n+   93: \t\tassertEquals( 0, messageManager.findMessagesFor( SDF.parse( \"31/08/2016\" ) ).size() );\n+   94: \t\tassertEquals( 0, messageManager.findMessagesFor( SDF.parse( \"01/09/2016\" ) ).size() );\n+   95: \n+   96: \t\t// The 1st execution. Keep it alive and wait Byteman to stop it\n+   97: \t\tlong execId1 = BatchIndexingJob.forEntity( Message.class ).start();\n+   98: \t\tJobExecution jobExec1 = BatchRuntime.getJobOperator().getJobExecution( execId1 );\n+   99: \t\tjobExec1 = keepTestAlive( jobExec1 );\n+  100: \n+  101: \t\t// Restart the job. This is the 2nd execution.\n+  102: \t\tlong execId2 = BatchIndexingJob.restart( execId1 );\n+  103: \t\tJobExecution jobExec2 = BatchRuntime.getJobOperator().getJobExecution( execId2 );\n+  104: \t\tjobExec2 = keepTestAlive( jobExec2 );\n   105: \n-  106: \t\tgoogles = companyManager.findCompanyByName( google );\n-  107: \t\tgoogleCEOs = personManager.findPerson( googleCEO );\n-  108: \t\tassertEquals( DB_COMP_ROWS / 5, googles.size() );\n-  109: \t\tassertEquals( DB_PERS_ROWS / 5, googleCEOs.size() );\n+  106: \t\tassertEquals( BatchStatus.COMPLETED, jobExec2.getBatchStatus() );\n+  107: \t\tassertEquals( DB_DAY1_ROWS, messageManager.findMessagesFor( SDF.parse( \"31/08/2016\" ) ).size() );\n+  108: \t\tassertEquals( DB_DAY2_ROWS, messageManager.findMessagesFor( SDF.parse( \"01/09/2016\" ) ).size() );\n+  109: \t}\n   110: \n-  111: \t\t// TODO this method should not belong to company manager\n-  112: \t\t// but how to create an all context query ?\n-  113: \t\tint totalDocs = companyManager.findAll().size();\n-  114: \t\tassertEquals( (int) ( DB_COMP_ROWS + DB_PERS_ROWS ), totalDocs );\n-  115: \t}\n-  139: \tprivate long createAndStartJob(JobOperator jobOperator) {\n-  140: \t\tMassIndexer massIndexer = new MassIndexer()\n-  141: \t\t\t\t.fetchSize( JOB_FETCH_SIZE )\n-  142: \t\t\t\t.maxResults( JOB_MAX_RESULTS )\n-  143: \t\t\t\t.maxThreads( JOB_MAX_THREADS )\n-  144: \t\t\t\t.purgeAtStart( JOB_PURGE_AT_START )\n-  145: \t\t\t\t.rowsPerPartition( JOB_ROWS_PER_PARTITION )\n-  146: \t\t\t\t.jobOperator( jobOperator )\n-  147: \t\t\t\t.addRootEntities( Company.class, Person.class );\n-  148: \t\tlong executionId = massIndexer.start();\n-  149: \t\treturn executionId;\n-  150: \t}\n+  111: \t@Test\n+  112: \tpublic void testJob_usingCriteria() throws InterruptedException, IOException, ParseException {\n+  113: \n+  114: \t\t// purge all before start\n+  115: \t\t// TODO Can the creation of a new EM and FTEM be avoided?\n+  139: \t\tassertEquals( DB_DAY2_ROWS, messageManager.findMessagesFor( SDF.parse( \"01/09/2016\" ) ).size() );\n+  140: \t}\n+  141: \n+  142: \t@Test\n+  143: \tpublic void testJob_usingHQL() throws InterruptedException, IOException, ParseException {\n+  144: \n+  145: \t\t// purge all before start\n+  146: \t\t// TODO Can the creation of a new EM and FTEM be avoided?\n+  147: \t\tEntityManager em = emf.createEntityManager();\n+  148: \t\tFullTextEntityManager ftem = Search.getFullTextEntityManager( em );\n+  149: \t\tftem.purgeAll( Message.class );\n+  150: \t\tftem.flushToIndexes();\n",
        "uniqueId": "fee3e4f8d370622571556c29cb60adf5f4c5e420_81_115__88_109_139_150",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8
    },
    {
        "type": "Move And Rename Method",
        "description": "Move And Rename Method\tprivate createSearchMapping() : SearchMapping from class org.hibernate.search.test.analyzer.DuplicatedAnalyzerDefinitionTest to public create() : SearchMapping from class org.hibernate.search.test.analyzer.DuplicatedAnalyzerDefinitionTest.ProgrammaticMappingWithDuplicateAnalyzerDefinitions",
        "diffLocations": [
            {
                "filePath": "orm/src/test/java/org/hibernate/search/test/analyzer/DuplicatedAnalyzerDefinitionTest.java",
                "startLine": 73,
                "endLine": 88,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "orm/src/test/java/org/hibernate/search/test/analyzer/DuplicatedAnalyzerDefinitionTest.java",
                "startLine": 93,
                "endLine": 108,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private SearchMapping createSearchMapping() {\n\t\tSearchMapping mapping = new SearchMapping();\n\n\t\tmapping.analyzerDef( \"english\", StandardTokenizerFactory.class )\n\t\t\t\t.filter( LowerCaseFilterFactory.class )\n\t\t\t\t.filter( SnowballPorterFilterFactory.class )\n\t\t\t\t.analyzerDef(\n\t\t\t\t\t\t\"english\", StandardTokenizerFactory.class\n\t\t\t\t) // ups duplicate name here - this should throw an exception\n\t\t\t\t.filter( LowerCaseFilterFactory.class )\n\t\t\t\t.filter( GermanStemFilterFactory.class )\n\t\t\t\t.entity( BlogEntry.class )\n\t\t\t\t.indexed()\n\t\t\t\t.property( \"title\", ElementType.METHOD );\n\t\treturn mapping;\n\t}",
        "filePathBefore": "orm/src/test/java/org/hibernate/search/test/analyzer/DuplicatedAnalyzerDefinitionTest.java",
        "isPureRefactoring": true,
        "commitId": "6b55b85d003d11848939dbdf2640fcb479397186",
        "packageNameBefore": "org.hibernate.search.test.analyzer",
        "classNameBefore": "org.hibernate.search.test.analyzer.DuplicatedAnalyzerDefinitionTest",
        "methodNameBefore": "org.hibernate.search.test.analyzer.DuplicatedAnalyzerDefinitionTest#createSearchMapping",
        "classSignatureBefore": "public class DuplicatedAnalyzerDefinitionTest extends SearchTestBase ",
        "methodNameBeforeSet": [
            "org.hibernate.search.test.analyzer.DuplicatedAnalyzerDefinitionTest#createSearchMapping"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.test.analyzer.DuplicatedAnalyzerDefinitionTest"
        ],
        "classSignatureBeforeSet": [
            "public class DuplicatedAnalyzerDefinitionTest extends SearchTestBase "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Overlapped refactoring - can be identical by undoing the overlapped refactoring\n- Rename Variable-",
                "description": "Rename Variable on top of the moved method - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\n\npackage org.hibernate.search.test.analyzer;\n\nimport java.lang.annotation.ElementType;\n\nimport org.apache.lucene.analysis.core.LowerCaseFilterFactory;\nimport org.apache.lucene.analysis.de.GermanStemFilterFactory;\nimport org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory;\nimport org.apache.lucene.analysis.standard.StandardTokenizerFactory;\nimport org.hibernate.cfg.Configuration;\nimport org.hibernate.search.cfg.Environment;\nimport org.hibernate.search.exception.SearchException;\nimport org.hibernate.search.cfg.SearchMapping;\nimport org.hibernate.search.test.SearchTestBase;\nimport org.junit.Test;\n\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.fail;\n\n/**\n * Tests for HSEARCH-569.\n *\n * @author Hardy Ferentschik\n */\npublic class DuplicatedAnalyzerDefinitionTest extends SearchTestBase {\n\n\t@Override\n\tpublic Class<?>[] getAnnotatedClasses() {\n\t\treturn new Class[] { };\n\t}\n\n\t@Test\n\tpublic void testDuplicatedAnalyzerDefinitionThrowsException() throws Exception {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.addAnnotatedClass( Entity1.class );\n\t\tconfig.addAnnotatedClass( Entity2.class );\n\t\tconfig.setProperty( \"hibernate.search.default.directory_provider\", \"local-heap\" );\n\t\ttry {\n\t\t\tconfig.buildSessionFactory();\n\t\t\tfail( \"Session creation should have failed due to duplicate analyzer definition\" );\n\t\t}\n\t\tcatch (SearchException e) {\n\t\t\tassertEquals(\n\t\t\t\t\t\"HSEARCH000330: Multiple analyzer definitions with the same name: 'my-analyzer'.\",\n\t\t\t\t\te.getMessage()\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\tpublic void testDuplicatedProgrammaticAnalyzerDefinitionThrowsException() throws Exception {\n\t\tConfiguration config = new Configuration();\n\t\tconfig.getProperties().put( Environment.MODEL_MAPPING, createSearchMapping() );\n\t\tconfig.setProperty( \"hibernate.search.default.directory_provider\", \"local-heap\" );\n\t\ttry {\n\t\t\tconfig.buildSessionFactory();\n\t\t\tfail( \"Session creation should have failed due to duplicate analyzer definition\" );\n\t\t}\n\t\tcatch (SearchException e) {\n\t\t\tassertEquals(\n\t\t\t\t\t\"HSEARCH000330: Multiple analyzer definitions with the same name: 'english'.\",\n\t\t\t\t\te.getMessage()\n\t\t\t);\n\t\t}\n\t}\n\n\tprivate SearchMapping createSearchMapping() {\n\t\tSearchMapping mapping = new SearchMapping();\n\n\t\tmapping.analyzerDef( \"english\", StandardTokenizerFactory.class )\n\t\t\t\t.filter( LowerCaseFilterFactory.class )\n\t\t\t\t.filter( SnowballPorterFilterFactory.class )\n\t\t\t\t.analyzerDef(\n\t\t\t\t\t\t\"english\", StandardTokenizerFactory.class\n\t\t\t\t) // ups duplicate name here - this should throw an exception\n\t\t\t\t.filter( LowerCaseFilterFactory.class )\n\t\t\t\t.filter( GermanStemFilterFactory.class )\n\t\t\t\t.entity( BlogEntry.class )\n\t\t\t\t.indexed()\n\t\t\t\t.property( \"title\", ElementType.METHOD );\n\t\treturn mapping;\n\t}\n}\n\n\n",
        "filePathAfter": "orm/src/test/java/org/hibernate/search/test/analyzer/DuplicatedAnalyzerDefinitionTest.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\n\npackage org.hibernate.search.test.analyzer;\n\nimport java.lang.annotation.ElementType;\n\nimport org.hibernate.search.annotations.Factory;\nimport org.hibernate.search.cfg.Environment;\nimport org.hibernate.search.cfg.SearchMapping;\nimport org.hibernate.search.exception.SearchException;\nimport org.hibernate.search.testsupport.junit.SearchIntegratorResource;\nimport org.hibernate.search.testsupport.setup.SearchConfigurationForTest;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.apache.lucene.analysis.core.LowerCaseFilterFactory;\nimport org.apache.lucene.analysis.de.GermanStemFilterFactory;\nimport org.apache.lucene.analysis.snowball.SnowballPorterFilterFactory;\nimport org.apache.lucene.analysis.standard.StandardTokenizerFactory;\n\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertTrue;\nimport static org.junit.Assert.fail;\n\n/**\n * Tests for HSEARCH-569.\n *\n * @author Hardy Ferentschik\n */\npublic class DuplicatedAnalyzerDefinitionTest {\n\n\t@Rule\n\tpublic SearchIntegratorResource integratorResource = new SearchIntegratorResource();\n\n\t@Test\n\tpublic void testDuplicatedAnalyzerDefinitionThrowsException() throws Exception {\n\t\tSearchConfigurationForTest config = new SearchConfigurationForTest()\n\t\t\t\t.addClasses( Entity1.class, Entity2.class )\n\t\t\t\t.addProperty( \"hibernate.search.default.directory_provider\", \"local-heap\" );\n\t\ttry {\n\t\t\tintegratorResource.create( config );\n\t\t\tfail( \"Integrator creation should have failed due to duplicate analyzer definition\" );\n\t\t}\n\t\tcatch (SearchException e) {\n\t\t\tassertEquals(\n\t\t\t\t\t\"HSEARCH000330: Multiple analyzer definitions with the same name: 'my-analyzer'.\",\n\t\t\t\t\te.getMessage()\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\tpublic void testDuplicatedProgrammaticAnalyzerDefinitionThrowsException() throws Exception {\n\t\tSearchConfigurationForTest config = new SearchConfigurationForTest()\n\t\t\t\t.addProperty( Environment.MODEL_MAPPING, ProgrammaticMappingWithDuplicateAnalyzerDefinitions.class.getName() )\n\t\t\t\t.addProperty( \"hibernate.search.default.directory_provider\", \"local-heap\" );\n\t\ttry {\n\t\t\tintegratorResource.create( config );\n\t\t\tfail( \"Integrator creation should have failed due to duplicate analyzer definition\" );\n\t\t}\n\t\tcatch (SearchException e) {\n\t\t\tassertEquals(\n\t\t\t\t\t\"HSEARCH000330: Multiple analyzer definitions with the same name: 'english'.\",\n\t\t\t\t\te.getMessage()\n\t\t\t);\n\t\t}\n\t}\n\n\t@Test\n\tpublic void testDuplicatedAnalyzerDiscriminatorDefinitions() throws Exception {\n\t\tSearchConfigurationForTest config = new SearchConfigurationForTest()\n\t\t\t\t.addClass( BlogEntry.class )\n\t\t\t\t.addProperty( \"hibernate.search.default.directory_provider\", \"local-heap\" );\n\t\ttry {\n\t\t\tintegratorResource.create( config );\n\t\t\tfail( \"Integrator creation should have failed due to duplicate analyzer definition\" );\n\t\t}\n\t\tcatch (SearchException e) {\n\t\t\tassertTrue(\n\t\t\t\t\t\"Wrong error message\",\n\t\t\t\t\te.getMessage().startsWith( \"Multiple AnalyzerDiscriminator defined in the same class hierarchy\" )\n\t\t\t);\n\t\t}\n\t}\n\n\tpublic static class ProgrammaticMappingWithDuplicateAnalyzerDefinitions {\n\t\t@Factory\n\t\tpublic SearchMapping create() {\n\t\t\tSearchMapping searchMapping = new SearchMapping();\n\t\t\tsearchMapping.analyzerDef( \"english\", StandardTokenizerFactory.class )\n\t\t\t\t\t.filter( LowerCaseFilterFactory.class )\n\t\t\t\t\t.filter( SnowballPorterFilterFactory.class )\n\t\t\t\t\t.analyzerDef(\n\t\t\t\t\t\t\t\"english\", StandardTokenizerFactory.class\n\t\t\t\t\t) // ups duplicate name here - this should throw an exception\n\t\t\t\t\t.filter( LowerCaseFilterFactory.class )\n\t\t\t\t\t.filter( GermanStemFilterFactory.class )\n\t\t\t\t\t.entity( BlogEntry.class )\n\t\t\t\t\t.indexed()\n\t\t\t\t\t.property( \"title\", ElementType.METHOD );\n\t\t\treturn searchMapping;\n\t\t}\n\t}\n}\n\n\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "@Factory\n\t\tpublic SearchMapping create() {\n\t\t\tSearchMapping searchMapping = new SearchMapping();\n\t\t\tsearchMapping.analyzerDef( \"english\", StandardTokenizerFactory.class )\n\t\t\t\t\t.filter( LowerCaseFilterFactory.class )\n\t\t\t\t\t.filter( SnowballPorterFilterFactory.class )\n\t\t\t\t\t.analyzerDef(\n\t\t\t\t\t\t\t\"english\", StandardTokenizerFactory.class\n\t\t\t\t\t) // ups duplicate name here - this should throw an exception\n\t\t\t\t\t.filter( LowerCaseFilterFactory.class )\n\t\t\t\t\t.filter( GermanStemFilterFactory.class )\n\t\t\t\t\t.entity( BlogEntry.class )\n\t\t\t\t\t.indexed()\n\t\t\t\t\t.property( \"title\", ElementType.METHOD );\n\t\t\treturn searchMapping;\n\t\t}",
        "diffSourceCode": "-   73: \tprivate SearchMapping createSearchMapping() {\n-   74: \t\tSearchMapping mapping = new SearchMapping();\n-   75: \n-   76: \t\tmapping.analyzerDef( \"english\", StandardTokenizerFactory.class )\n-   77: \t\t\t\t.filter( LowerCaseFilterFactory.class )\n-   78: \t\t\t\t.filter( SnowballPorterFilterFactory.class )\n-   79: \t\t\t\t.analyzerDef(\n-   80: \t\t\t\t\t\t\"english\", StandardTokenizerFactory.class\n-   81: \t\t\t\t) // ups duplicate name here - this should throw an exception\n-   82: \t\t\t\t.filter( LowerCaseFilterFactory.class )\n-   83: \t\t\t\t.filter( GermanStemFilterFactory.class )\n-   84: \t\t\t\t.entity( BlogEntry.class )\n-   85: \t\t\t\t.indexed()\n-   86: \t\t\t\t.property( \"title\", ElementType.METHOD );\n-   87: \t\treturn mapping;\n-   88: \t}\n+   73: \t}\n+   74: \n+   75: \t@Test\n+   76: \tpublic void testDuplicatedAnalyzerDiscriminatorDefinitions() throws Exception {\n+   77: \t\tSearchConfigurationForTest config = new SearchConfigurationForTest()\n+   78: \t\t\t\t.addClass( BlogEntry.class )\n+   79: \t\t\t\t.addProperty( \"hibernate.search.default.directory_provider\", \"local-heap\" );\n+   80: \t\ttry {\n+   81: \t\t\tintegratorResource.create( config );\n+   82: \t\t\tfail( \"Integrator creation should have failed due to duplicate analyzer definition\" );\n+   83: \t\t}\n+   84: \t\tcatch (SearchException e) {\n+   85: \t\t\tassertTrue(\n+   86: \t\t\t\t\t\"Wrong error message\",\n+   87: \t\t\t\t\te.getMessage().startsWith( \"Multiple AnalyzerDiscriminator defined in the same class hierarchy\" )\n+   88: \t\t\t);\n+   93: \t\t@Factory\n+   94: \t\tpublic SearchMapping create() {\n+   95: \t\t\tSearchMapping searchMapping = new SearchMapping();\n+   96: \t\t\tsearchMapping.analyzerDef( \"english\", StandardTokenizerFactory.class )\n+   97: \t\t\t\t\t.filter( LowerCaseFilterFactory.class )\n+   98: \t\t\t\t\t.filter( SnowballPorterFilterFactory.class )\n+   99: \t\t\t\t\t.analyzerDef(\n+  100: \t\t\t\t\t\t\t\"english\", StandardTokenizerFactory.class\n+  101: \t\t\t\t\t) // ups duplicate name here - this should throw an exception\n+  102: \t\t\t\t\t.filter( LowerCaseFilterFactory.class )\n+  103: \t\t\t\t\t.filter( GermanStemFilterFactory.class )\n+  104: \t\t\t\t\t.entity( BlogEntry.class )\n+  105: \t\t\t\t\t.indexed()\n+  106: \t\t\t\t\t.property( \"title\", ElementType.METHOD );\n+  107: \t\t\treturn searchMapping;\n+  108: \t\t}\n",
        "uniqueId": "6b55b85d003d11848939dbdf2640fcb479397186_73_88__93_108",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 1.8
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tpublic throwable() : Throwable extracted from public getThrowable() : Throwable in class org.hibernate.search.engine.reporting.FailureContext",
        "diffLocations": [
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/reporting/FailureContext.java",
                "startLine": 45,
                "endLine": 51,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/reporting/FailureContext.java",
                "startLine": 53,
                "endLine": 61,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/reporting/FailureContext.java",
                "startLine": 45,
                "endLine": 51,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "/**\n\t * @return The {@link Exception} or {@link Error} thrown when the operation failed.\n\t * Never {@code null}.\n\t */\n\tpublic Throwable getThrowable() {\n\t\treturn this.throwable;\n\t}",
        "filePathBefore": "engine/src/main/java/org/hibernate/search/engine/reporting/FailureContext.java",
        "isPureRefactoring": true,
        "commitId": "0f189bd436357445911ad3f67721e652e5b627dc",
        "packageNameBefore": "org.hibernate.search.engine.reporting",
        "classNameBefore": "org.hibernate.search.engine.reporting.FailureContext",
        "methodNameBefore": "org.hibernate.search.engine.reporting.FailureContext#getThrowable",
        "classSignatureBefore": "public class FailureContext ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.reporting.FailureContext#getThrowable"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.reporting.FailureContext"
        ],
        "classSignatureBeforeSet": [
            "public class FailureContext "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Tolerable changes in the body\n",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.reporting;\n\nimport org.hibernate.search.util.common.AssertionFailure;\n\n/**\n * Contextual information about a failing background operation.\n */\npublic class FailureContext {\n\n\t/**\n\t * @return A new {@link FailureContext} builder.\n\t */\n\tpublic static Builder builder() {\n\t\treturn new Builder();\n\t}\n\n\tprivate final Throwable throwable;\n\n\tprivate final Object failingOperation;\n\n\tFailureContext(Builder builder) {\n\t\t/*\n\t\t * Avoid nulls: they should not happen, and they are most likely bugs in Hibernate Search,\n\t\t * but we don't want user-implemented failure handlers to fail because of that\n\t\t * (they would throw an NPE which may produce disastrous results such as killing background threads).\n\t\t */\n\t\tthis.throwable = builder.throwable == null\n\t\t\t\t? new AssertionFailure(\n\t\t\t\t\"Unknown throwable: missing throwable when reporting the failure.\"\n\t\t\t\t\t\t+ \" There is probably a bug in Hibernate Search, please report it.\"\n\t\t)\n\t\t\t\t: builder.throwable;\n\t\tthis.failingOperation = builder.failingOperation == null\n\t\t\t\t? \"Unknown operation: missing operation when reporting the failure.\"\n\t\t\t\t+ \" There is probably a bug in Hibernate Search, please report it.\"\n\t\t\t\t: builder.failingOperation;\n\t}\n\n\t/**\n\t * @return The {@link Exception} or {@link Error} thrown when the operation failed.\n\t * Never {@code null}.\n\t */\n\tpublic Throwable getThrowable() {\n\t\treturn this.throwable;\n\t}\n\n\t/**\n\t * @return The operation that triggered the failure.\n\t * Never {@code null}.\n\t * Use {@link Object#toString()} to get a textual representation.\n\t */\n\tpublic Object getFailingOperation() {\n\t\treturn this.failingOperation;\n\t}\n\n\tpublic static class Builder {\n\n\t\tprivate Throwable throwable;\n\t\tprivate Object failingOperation;\n\n\t\tBuilder() {\n\t\t}\n\n\t\tpublic void throwable(Throwable th) {\n\t\t\tthis.throwable = th;\n\t\t}\n\n\t\tpublic void failingOperation(Object failingOperation) {\n\t\t\tthis.failingOperation = failingOperation;\n\t\t}\n\n\t\tpublic FailureContext build() {\n\t\t\treturn new FailureContext( this );\n\t\t}\n\t}\n\n}\n",
        "filePathAfter": "engine/src/main/java/org/hibernate/search/engine/reporting/FailureContext.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.reporting;\n\nimport org.hibernate.search.util.common.AssertionFailure;\n\n/**\n * Contextual information about a failing background operation.\n */\npublic class FailureContext {\n\n\t/**\n\t * @return A new {@link FailureContext} builder.\n\t */\n\tpublic static Builder builder() {\n\t\treturn new Builder();\n\t}\n\n\tprivate final Throwable throwable;\n\n\tprivate final Object failingOperation;\n\n\tFailureContext(Builder builder) {\n\t\t/*\n\t\t * Avoid nulls: they should not happen, and they are most likely bugs in Hibernate Search,\n\t\t * but we don't want user-implemented failure handlers to fail because of that\n\t\t * (they would throw an NPE which may produce disastrous results such as killing background threads).\n\t\t */\n\t\tthis.throwable = builder.throwable == null\n\t\t\t\t? new AssertionFailure(\n\t\t\t\t\"Unknown throwable: missing throwable when reporting the failure.\"\n\t\t\t\t\t\t+ \" There is probably a bug in Hibernate Search, please report it.\"\n\t\t)\n\t\t\t\t: builder.throwable;\n\t\tthis.failingOperation = builder.failingOperation == null\n\t\t\t\t? \"Unknown operation: missing operation when reporting the failure.\"\n\t\t\t\t+ \" There is probably a bug in Hibernate Search, please report it.\"\n\t\t\t\t: builder.failingOperation;\n\t}\n\n\t/**\n\t * @return The {@link Exception} or {@link Error} thrown when the operation failed.\n\t * Never {@code null}.\n\t */\n\tpublic Throwable throwable() {\n\t\treturn throwable;\n\t}\n\n\t/**\n\t * @return The {@link Exception} or {@link Error} thrown when the operation failed.\n\t * Never {@code null}.\n\t * @deprecated Use {@link #throwable} instead.\n\t */\n\t@Deprecated\n\tpublic Throwable getThrowable() {\n\t\treturn throwable();\n\t}\n\n\t/**\n\t * @return The operation that triggered the failure.\n\t * Never {@code null}.\n\t * Use {@link Object#toString()} to get a textual representation.\n\t */\n\tpublic Object failingOperation() {\n\t\treturn this.failingOperation;\n\t}\n\n\t/**\n\t * @return The operation that triggered the failure.\n\t * Never {@code null}.\n\t * Use {@link Object#toString()} to get a textual representation.\n\t * @deprecated Use {@link #failingOperation()} instead.\n\t */\n\t@Deprecated\n\tpublic Object getFailingOperation() {\n\t\treturn this.failingOperation;\n\t}\n\n\tpublic static class Builder {\n\n\t\tprivate Throwable throwable;\n\t\tprivate Object failingOperation;\n\n\t\tBuilder() {\n\t\t}\n\n\t\tpublic void throwable(Throwable th) {\n\t\t\tthis.throwable = th;\n\t\t}\n\n\t\tpublic void failingOperation(Object failingOperation) {\n\t\t\tthis.failingOperation = failingOperation;\n\t\t}\n\n\t\tpublic FailureContext build() {\n\t\t\treturn new FailureContext( this );\n\t\t}\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "/**\n\t * @return The {@link Exception} or {@link Error} thrown when the operation failed.\n\t * Never {@code null}.\n\t */\n\tpublic Throwable throwable() {\n\t\treturn throwable;\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "/**\n\t * @return The {@link Exception} or {@link Error} thrown when the operation failed.\n\t * Never {@code null}.\n\t * @deprecated Use {@link #throwable} instead.\n\t */\n\t@Deprecated\n\tpublic Throwable getThrowable() {\n\t\treturn throwable();\n\t}\n/**\n\t * @return The {@link Exception} or {@link Error} thrown when the operation failed.\n\t * Never {@code null}.\n\t */\n\tpublic Throwable throwable() {\n\t\treturn throwable;\n\t}",
        "diffSourceCode": "    45: \t/**\n    46: \t * @return The {@link Exception} or {@link Error} thrown when the operation failed.\n    47: \t * Never {@code null}.\n    48: \t */\n-   49: \tpublic Throwable getThrowable() {\n-   50: \t\treturn this.throwable;\n+   49: \tpublic Throwable throwable() {\n+   50: \t\treturn throwable;\n    51: \t}\n    53: \t/**\n-   54: \t * @return The operation that triggered the failure.\n+   54: \t * @return The {@link Exception} or {@link Error} thrown when the operation failed.\n    55: \t * Never {@code null}.\n-   56: \t * Use {@link Object#toString()} to get a textual representation.\n+   56: \t * @deprecated Use {@link #throwable} instead.\n    57: \t */\n-   58: \tpublic Object getFailingOperation() {\n-   59: \t\treturn this.failingOperation;\n-   60: \t}\n-   61: \n+   58: \t@Deprecated\n+   59: \tpublic Throwable getThrowable() {\n+   60: \t\treturn throwable();\n+   61: \t}\n",
        "uniqueId": "0f189bd436357445911ad3f67721e652e5b627dc_45_51_45_51_53_61",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 3
            },
            "LINE": {
                "missed": 0,
                "covered": 1
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate getOrmPropertyMetadataFromThisType(propertyName String) : HibernateOrmBasicClassPropertyMetadata extracted from private findOrmPropertyMetadata(propertyName String) : HibernateOrmBasicClassPropertyMetadata in class org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel",
        "diffLocations": [
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
                "startLine": 148,
                "endLine": 157,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
                "startLine": 144,
                "endLine": 150,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
                "startLine": 175,
                "endLine": 182,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private HibernateOrmBasicClassPropertyMetadata findOrmPropertyMetadata(String propertyName) {\n\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadata = null;\n\t\tif ( ormTypeMetadata != null ) {\n\t\t\tpropertyMetadata = ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n\t\t}\n\t\tif ( propertyMetadata == null ) {\n\t\t\tpropertyMetadata = getOrmPropertyMetadataFromParentTypes( propertyName );\n\t\t}\n\t\treturn propertyMetadata;\n\t}",
        "filePathBefore": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
        "isPureRefactoring": true,
        "commitId": "c46066ddc29fe1069994465ef58bd442c40eb0b0",
        "packageNameBefore": "org.hibernate.search.mapper.orm.model.impl",
        "classNameBefore": "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel",
        "methodNameBefore": "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#findOrmPropertyMetadata",
        "invokedMethod": "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getOrmPropertyMetadataFromParentTypes\n methodBody: private HibernateOrmBasicClassPropertyMetadata getOrmPropertyMetadataFromParentTypes(String propertyName) {\nreturn getAscendingSuperTypes().skip(1).map(type -> type.getPropertyOrNull(propertyName)).filter(Objects::nonNull).findFirst().map(HibernateOrmClassPropertyModel::getOrmPropertyMetadata).orElse(null);\n}",
        "classSignatureBefore": "public class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> ",
        "methodNameBeforeSet": [
            "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#findOrmPropertyMetadata"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel"
        ],
        "classSignatureBeforeSet": [
            "public class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": " Severe changes",
                "description": "Just an empty block - with non-mapped leaves",
                "mappingState": 3
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.orm.model.impl;\n\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport org.hibernate.annotations.common.reflection.XProperty;\nimport org.hibernate.search.engine.mapper.model.spi.MappableTypeModel;\nimport org.hibernate.search.mapper.pojo.model.hcann.spi.PojoCommonsAnnotationsHelper;\nimport org.hibernate.search.mapper.pojo.model.spi.GenericContextAwarePojoGenericTypeModel.RawTypeDeclaringContext;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeIdentifier;\n\npublic class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> {\n\n\tprivate final HibernateOrmBasicClassTypeMetadata ormTypeMetadata;\n\tprivate final RawTypeDeclaringContext<T> rawTypeDeclaringContext;\n\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> ascendingSuperTypesCache;\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> descendingSuperTypesCache;\n\n\tprivate final Map<String, HibernateOrmClassPropertyModel<?>> propertyModelCache = new HashMap<>();\n\n\tprivate Map<String, XProperty> declaredFieldAccessXPropertiesByName;\n\tprivate Map<String, XProperty> declaredMethodAccessXPropertiesByName;\n\n\tHibernateOrmClassRawTypeModel(HibernateOrmBootstrapIntrospector introspector,\n\t\t\tPojoRawTypeIdentifier<T> typeIdentifier,\n\t\t\tHibernateOrmBasicClassTypeMetadata ormTypeMetadata, RawTypeDeclaringContext<T> rawTypeDeclaringContext) {\n\t\tsuper( introspector, typeIdentifier );\n\t\tthis.ormTypeMetadata = ormTypeMetadata;\n\t\tthis.rawTypeDeclaringContext = rawTypeDeclaringContext;\n\t}\n\n\t@Override\n\tpublic boolean isAbstract() {\n\t\treturn xClass.isAbstract();\n\t}\n\n\t@Override\n\tpublic boolean isSubTypeOf(MappableTypeModel superTypeCandidate) {\n\t\treturn superTypeCandidate instanceof HibernateOrmClassRawTypeModel\n\t\t\t\t&& ( (HibernateOrmClassRawTypeModel<?>) superTypeCandidate ).xClass.isAssignableFrom( xClass );\n\t}\n\n\t@Override\n\t@SuppressWarnings(\"unchecked\") // xClass represents T, so its supertypes represent ? super T\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> getAscendingSuperTypes() {\n\t\tif ( ascendingSuperTypesCache == null ) {\n\t\t\tascendingSuperTypesCache =\n\t\t\t\t\t( (Stream<HibernateOrmClassRawTypeModel<? super T>>) introspector.getAscendingSuperTypes( xClass ) )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn ascendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\t@SuppressWarnings(\"unchecked\") // xClass represents T, so its supertypes represent ? super T\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> getDescendingSuperTypes() {\n\t\tif ( descendingSuperTypesCache == null ) {\n\t\t\tdescendingSuperTypesCache =\n\t\t\t\t\t( (Stream<HibernateOrmClassRawTypeModel<? super T>>) introspector.getDescendingSuperTypes( xClass ) )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn descendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\tpublic Stream<Annotation> getAnnotations() {\n\t\treturn introspector.getAnnotations( xClass );\n\t}\n\n\t@Override\n\tStream<String> getDeclaredPropertyNames() {\n\t\treturn Stream.concat(\n\t\t\t\tgetDeclaredFieldAccessXPropertiesByName().keySet().stream(),\n\t\t\t\tgetDeclaredMethodAccessXPropertiesByName().keySet().stream()\n\t\t)\n\t\t\t\t.distinct();\n\t}\n\n\t@Override\n\tHibernateOrmClassPropertyModel<?> getPropertyOrNull(String propertyName) {\n\t\treturn propertyModelCache.computeIfAbsent( propertyName, this::createPropertyModel );\n\t}\n\n\tRawTypeDeclaringContext<T> getRawTypeDeclaringContext() {\n\t\treturn rawTypeDeclaringContext;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredFieldAccessXPropertiesByName() {\n\t\tif ( declaredFieldAccessXPropertiesByName == null ) {\n\t\t\tdeclaredFieldAccessXPropertiesByName =\n\t\t\t\t\tintrospector.getDeclaredFieldAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredFieldAccessXPropertiesByName;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredMethodAccessXPropertiesByName() {\n\t\tif ( declaredMethodAccessXPropertiesByName == null ) {\n\t\t\tdeclaredMethodAccessXPropertiesByName =\n\t\t\t\t\tintrospector.getDeclaredMethodAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredMethodAccessXPropertiesByName;\n\t}\n\n\tprivate HibernateOrmClassPropertyModel<?> createPropertyModel(String propertyName) {\n\t\tList<XProperty> declaredXProperties = new ArrayList<>( 2 );\n\t\t// Add the method first on purpose: the first XProperty may be used as a default to create the value accessor handle\n\t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( methodAccessXProperty );\n\t\t}\n\t\tXProperty fieldAccessXProperty = getDeclaredFieldAccessXPropertiesByName().get( propertyName );\n\t\tif ( fieldAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( fieldAccessXProperty );\n\t\t}\n\n\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata = findOrmPropertyMetadata( propertyName );\n\n\t\tMember member = findPropertyMember(\n\t\t\t\tpropertyName, methodAccessXProperty, fieldAccessXProperty, ormPropertyMetadata\n\t\t);\n\n\t\tif ( member == null ) {\n\t\t\treturn null;\n\t\t}\n\n\t\treturn new HibernateOrmClassPropertyModel<>(\n\t\t\t\tintrospector, this, propertyName,\n\t\t\t\tdeclaredXProperties, ormPropertyMetadata, member\n\t\t);\n\t}\n\n\tprivate HibernateOrmBasicClassPropertyMetadata findOrmPropertyMetadata(String propertyName) {\n\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadata = null;\n\t\tif ( ormTypeMetadata != null ) {\n\t\t\tpropertyMetadata = ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n\t\t}\n\t\tif ( propertyMetadata == null ) {\n\t\t\tpropertyMetadata = getOrmPropertyMetadataFromParentTypes( propertyName );\n\t\t}\n\t\treturn propertyMetadata;\n\t}\n\n\tprivate Member findPropertyMember(String propertyName,\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tMember result;\n\t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n\t\t\t// Hibernate ORM has metadata for this property (the property is persisted).\n\t\t\t// Use ORM metadata to find the corresponding member (field/method).\n\t\t\tresult = getPropertyMemberUsingHibernateOrmMetadataFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty, propertyMetadataFromHibernateOrmMetamodel\n\t\t\t);\n\t\t}\n\t\telse {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n\t\t\t// Use reflection to find the corresponding member (field/method).\n\t\t\tresult = getPropertyMemberUsingReflectionFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty\n\t\t\t);\n\t\t}\n\n\t\tif ( result == null ) {\n\t\t\t// There is no member for this property on the current type.\n\t\t\t// Try to find one in the closest supertype.\n\t\t\tresult = getPropertyMemberFromParentTypes( propertyName );\n\t\t}\n\n\t\treturn result;\n\t}\n\n\tprivate Member getPropertyMemberUsingHibernateOrmMetadataFromThisType(XProperty methodAccessXProperty,\n\t\t\tXProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n\t\t/*\n\t\t * Hibernate ORM has metadata for this property,\n\t\t * which means this property is persisted.\n\t\t *\n\t\t * Hibernate ORM might return us the member as declared in a supertype,\n\t\t * in which case the type of that member will not be up-to-date.\n\t\t * Thus we try to get the overridden member declared in the current type,\n\t\t * and failing that we look for the member in supertypes.\n\t\t *\n\t\t * We still try to comply with JPA's configured access type,\n\t\t * which explains the two if/else branches below.\n\t\t */\n\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n\t\t\treturn methodAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n\t\t\treturn fieldAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\treturn null;\n\t\t}\n\t}\n\n\t/*\n\t * Hibernate ORM doesn't have any metadata for this property,\n\t * which means this property is transient.\n\t * We don't need to worry about JPA's access type.\n\t */\n\tprivate Member getPropertyMemberUsingReflectionFromThisType(\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty) {\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\t// Method access is available. Get values from the getter.\n\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( fieldAccessXProperty != null ) {\n\t\t\t// Method access is not available, but field access is. Get values directly from the field.\n\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\t// Neither method access nor field access is available.\n\t\t\t// The property is not declared in this type.\n\t\t\treturn null;\n\t\t}\n\t}\n\n\tprivate HibernateOrmBasicClassPropertyMetadata getOrmPropertyMetadataFromParentTypes(String propertyName) {\n\t\t// TODO HSEARCH-3056 remove lambdas if possible\n\t\treturn getAscendingSuperTypes()\n\t\t\t\t.skip( 1 ) // Ignore self\n\t\t\t\t.map( type -> type.getPropertyOrNull( propertyName ) )\n\t\t\t\t.filter( Objects::nonNull )\n\t\t\t\t.findFirst()\n\t\t\t\t.map( HibernateOrmClassPropertyModel::getOrmPropertyMetadata )\n\t\t\t\t.orElse( null );\n\t}\n\n\tprivate Member getPropertyMemberFromParentTypes(String propertyName) {\n\t\t// TODO HSEARCH-3056 remove lambdas if possible\n\t\treturn getAscendingSuperTypes()\n\t\t\t\t.skip( 1 ) // Ignore self\n\t\t\t\t.map( type -> type.getPropertyOrNull( propertyName ) )\n\t\t\t\t.filter( Objects::nonNull )\n\t\t\t\t.findFirst()\n\t\t\t\t.map( HibernateOrmClassPropertyModel::getMember )\n\t\t\t\t.orElse( null );\n\t}\n}\n",
        "filePathAfter": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.orm.model.impl;\n\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport org.hibernate.annotations.common.reflection.XProperty;\nimport org.hibernate.search.engine.mapper.model.spi.MappableTypeModel;\nimport org.hibernate.search.mapper.pojo.model.hcann.spi.PojoCommonsAnnotationsHelper;\nimport org.hibernate.search.mapper.pojo.model.spi.GenericContextAwarePojoGenericTypeModel.RawTypeDeclaringContext;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeIdentifier;\n\npublic class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> {\n\n\tprivate final HibernateOrmBasicClassTypeMetadata ormTypeMetadata;\n\tprivate final RawTypeDeclaringContext<T> rawTypeDeclaringContext;\n\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> ascendingSuperTypesCache;\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> descendingSuperTypesCache;\n\n\tprivate final Map<String, HibernateOrmClassPropertyModel<?>> propertyModelCache = new HashMap<>();\n\n\tprivate Map<String, XProperty> declaredFieldAccessXPropertiesByName;\n\tprivate Map<String, XProperty> declaredMethodAccessXPropertiesByName;\n\n\tHibernateOrmClassRawTypeModel(HibernateOrmBootstrapIntrospector introspector,\n\t\t\tPojoRawTypeIdentifier<T> typeIdentifier,\n\t\t\tHibernateOrmBasicClassTypeMetadata ormTypeMetadata, RawTypeDeclaringContext<T> rawTypeDeclaringContext) {\n\t\tsuper( introspector, typeIdentifier );\n\t\tthis.ormTypeMetadata = ormTypeMetadata;\n\t\tthis.rawTypeDeclaringContext = rawTypeDeclaringContext;\n\t}\n\n\t@Override\n\tpublic boolean isAbstract() {\n\t\treturn xClass.isAbstract();\n\t}\n\n\t@Override\n\tpublic boolean isSubTypeOf(MappableTypeModel superTypeCandidate) {\n\t\treturn superTypeCandidate instanceof HibernateOrmClassRawTypeModel\n\t\t\t\t&& ( (HibernateOrmClassRawTypeModel<?>) superTypeCandidate ).xClass.isAssignableFrom( xClass );\n\t}\n\n\t@Override\n\t@SuppressWarnings(\"unchecked\") // xClass represents T, so its supertypes represent ? super T\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> getAscendingSuperTypes() {\n\t\tif ( ascendingSuperTypesCache == null ) {\n\t\t\tascendingSuperTypesCache =\n\t\t\t\t\t( (Stream<HibernateOrmClassRawTypeModel<? super T>>) introspector.getAscendingSuperTypes( xClass ) )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn ascendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\t@SuppressWarnings(\"unchecked\") // xClass represents T, so its supertypes represent ? super T\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> getDescendingSuperTypes() {\n\t\tif ( descendingSuperTypesCache == null ) {\n\t\t\tdescendingSuperTypesCache =\n\t\t\t\t\t( (Stream<HibernateOrmClassRawTypeModel<? super T>>) introspector.getDescendingSuperTypes( xClass ) )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn descendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\tpublic Stream<Annotation> getAnnotations() {\n\t\treturn introspector.getAnnotations( xClass );\n\t}\n\n\t@Override\n\tStream<String> getDeclaredPropertyNames() {\n\t\treturn Stream.concat(\n\t\t\t\tgetDeclaredFieldAccessXPropertiesByName().keySet().stream(),\n\t\t\t\tgetDeclaredMethodAccessXPropertiesByName().keySet().stream()\n\t\t)\n\t\t\t\t.distinct();\n\t}\n\n\t@Override\n\tHibernateOrmClassPropertyModel<?> getPropertyOrNull(String propertyName) {\n\t\treturn propertyModelCache.computeIfAbsent( propertyName, this::createPropertyModel );\n\t}\n\n\tRawTypeDeclaringContext<T> getRawTypeDeclaringContext() {\n\t\treturn rawTypeDeclaringContext;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredFieldAccessXPropertiesByName() {\n\t\tif ( declaredFieldAccessXPropertiesByName == null ) {\n\t\t\tdeclaredFieldAccessXPropertiesByName =\n\t\t\t\t\tintrospector.getDeclaredFieldAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredFieldAccessXPropertiesByName;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredMethodAccessXPropertiesByName() {\n\t\tif ( declaredMethodAccessXPropertiesByName == null ) {\n\t\t\tdeclaredMethodAccessXPropertiesByName =\n\t\t\t\t\tintrospector.getDeclaredMethodAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredMethodAccessXPropertiesByName;\n\t}\n\n\tprivate HibernateOrmClassPropertyModel<?> createPropertyModel(String propertyName) {\n\t\tList<XProperty> declaredXProperties = new ArrayList<>( 2 );\n\t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( methodAccessXProperty );\n\t\t}\n\t\tXProperty fieldAccessXProperty = getDeclaredFieldAccessXPropertiesByName().get( propertyName );\n\t\tif ( fieldAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( fieldAccessXProperty );\n\t\t}\n\n\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata = findOrmPropertyMetadata( propertyName );\n\t\tMember member = findPropertyMember( propertyName, ormPropertyMetadata );\n\n\t\tif ( member == null ) {\n\t\t\treturn null;\n\t\t}\n\n\t\treturn new HibernateOrmClassPropertyModel<>(\n\t\t\t\tintrospector, this, propertyName,\n\t\t\t\tdeclaredXProperties, ormPropertyMetadata, member\n\t\t);\n\t}\n\n\tprivate HibernateOrmBasicClassPropertyMetadata findOrmPropertyMetadata(String propertyName) {\n\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadata = getOrmPropertyMetadataFromThisType( propertyName );\n\t\tif ( propertyMetadata == null ) {\n\t\t\tpropertyMetadata = getOrmPropertyMetadataFromParentTypes( propertyName );\n\t\t}\n\t\treturn propertyMetadata;\n\t}\n\n\tprivate Member findPropertyMember(String propertyName,\n\t\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata) {\n\t\tMember result = getPropertyMemberFromThisType( propertyName, ormPropertyMetadata );\n\n\t\tif ( result == null ) {\n\t\t\t// There is no member for this property on the current type.\n\t\t\t// Try to find one in the closest supertype.\n\t\t\tresult = getPropertyMemberFromParentTypes( propertyName, ormPropertyMetadata );\n\t\t}\n\n\t\treturn result;\n\t}\n\n\tprivate HibernateOrmBasicClassPropertyMetadata getOrmPropertyMetadataFromParentTypes(String propertyName) {\n\t\t// TODO HSEARCH-3056 remove lambdas if possible\n\t\treturn getAscendingSuperTypes()\n\t\t\t\t.skip( 1 ) // Ignore self\n\t\t\t\t.map( type -> type.getOrmPropertyMetadataFromThisType( propertyName ) )\n\t\t\t\t.filter( Objects::nonNull )\n\t\t\t\t.findFirst()\n\t\t\t\t.orElse( null );\n\t}\n\n\tprivate HibernateOrmBasicClassPropertyMetadata getOrmPropertyMetadataFromThisType(String propertyName) {\n\t\tif ( ormTypeMetadata != null ) {\n\t\t\treturn ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n\t\t}\n\t\telse {\n\t\t\treturn null;\n\t\t}\n\t}\n\n\tprivate Member getPropertyMemberFromParentTypes(String propertyName,\n\t\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata) {\n\t\t// TODO HSEARCH-3056 remove lambdas if possible\n\t\treturn getAscendingSuperTypes()\n\t\t\t\t.skip( 1 ) // Ignore self\n\t\t\t\t.map( type -> type.getPropertyMemberFromThisType( propertyName, ormPropertyMetadata ) )\n\t\t\t\t.filter( Objects::nonNull )\n\t\t\t\t.findFirst()\n\t\t\t\t.orElse( null );\n\t}\n\n\tprivate Member getPropertyMemberFromThisType(String propertyName,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n\t\tXProperty fieldAccessXProperty = getDeclaredFieldAccessXPropertiesByName().get( propertyName );\n\t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n\t\t\t// Hibernate ORM has metadata for this property (the property is persisted).\n\t\t\t// Use ORM metadata to find the corresponding member (field/method).\n\t\t\treturn getPropertyMemberUsingHibernateOrmMetadataFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty, propertyMetadataFromHibernateOrmMetamodel\n\t\t\t);\n\t\t}\n\t\telse {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n\t\t\t// Use reflection to find the corresponding member (field/method).\n\t\t\treturn getPropertyMemberUsingReflectionFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty\n\t\t\t);\n\t\t}\n\t}\n\n\tprivate Member getPropertyMemberUsingHibernateOrmMetadataFromThisType(XProperty methodAccessXProperty,\n\t\t\tXProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n\t\t/*\n\t\t * Hibernate ORM has metadata for this property,\n\t\t * which means this property is persisted.\n\t\t *\n\t\t * Hibernate ORM might return us the member as declared in a supertype,\n\t\t * in which case the type of that member will not be up-to-date.\n\t\t * Thus we try to get the overridden member declared in the current type,\n\t\t * and failing that we look for the member in supertypes.\n\t\t *\n\t\t * We still try to comply with JPA's configured access type,\n\t\t * which explains the two if/else branches below.\n\t\t */\n\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n\t\t\treturn methodAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n\t\t\treturn fieldAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\treturn null;\n\t\t}\n\t}\n\n\t/*\n\t * Hibernate ORM doesn't have any metadata for this property,\n\t * which means this property is transient.\n\t * We don't need to worry about JPA's access type.\n\t */\n\tprivate Member getPropertyMemberUsingReflectionFromThisType(\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty) {\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\t// Method access is available. Get values from the getter.\n\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( fieldAccessXProperty != null ) {\n\t\t\t// Method access is not available, but field access is. Get values directly from the field.\n\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\t// Neither method access nor field access is available.\n\t\t\t// The property is not declared in this type.\n\t\t\treturn null;\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private HibernateOrmBasicClassPropertyMetadata getOrmPropertyMetadataFromThisType(String propertyName) {\n\t\tif ( ormTypeMetadata != null ) {\n\t\t\treturn ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n\t\t}\n\t\telse {\n\t\t\treturn null;\n\t\t}\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getOrmPropertyMetadataFromParentTypes\n methodBody: private HibernateOrmBasicClassPropertyMetadata getOrmPropertyMetadataFromParentTypes(String propertyName) {\nreturn getAscendingSuperTypes().skip(1).map(type -> type.getPropertyOrNull(propertyName)).filter(Objects::nonNull).findFirst().map(HibernateOrmClassPropertyModel::getOrmPropertyMetadata).orElse(null);\n}"
        ],
        "sourceCodeAfterRefactoring": "private HibernateOrmBasicClassPropertyMetadata findOrmPropertyMetadata(String propertyName) {\n\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadata = getOrmPropertyMetadataFromThisType( propertyName );\n\t\tif ( propertyMetadata == null ) {\n\t\t\tpropertyMetadata = getOrmPropertyMetadataFromParentTypes( propertyName );\n\t\t}\n\t\treturn propertyMetadata;\n\t}\nprivate HibernateOrmBasicClassPropertyMetadata getOrmPropertyMetadataFromThisType(String propertyName) {\n\t\tif ( ormTypeMetadata != null ) {\n\t\t\treturn ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n\t\t}\n\t\telse {\n\t\t\treturn null;\n\t\t}\n\t}",
        "diffSourceCode": "-  144: \t\t\t\tdeclaredXProperties, ormPropertyMetadata, member\n-  145: \t\t);\n-  146: \t}\n-  147: \n-  148: \tprivate HibernateOrmBasicClassPropertyMetadata findOrmPropertyMetadata(String propertyName) {\n-  149: \t\tHibernateOrmBasicClassPropertyMetadata propertyMetadata = null;\n-  150: \t\tif ( ormTypeMetadata != null ) {\n-  151: \t\t\tpropertyMetadata = ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n-  152: \t\t}\n-  153: \t\tif ( propertyMetadata == null ) {\n-  154: \t\t\tpropertyMetadata = getOrmPropertyMetadataFromParentTypes( propertyName );\n-  155: \t\t}\n-  156: \t\treturn propertyMetadata;\n-  157: \t}\n-  175: \t\t\t);\n-  176: \t\t}\n-  177: \n-  178: \t\tif ( result == null ) {\n-  179: \t\t\t// There is no member for this property on the current type.\n-  180: \t\t\t// Try to find one in the closest supertype.\n-  181: \t\t\tresult = getPropertyMemberFromParentTypes( propertyName );\n-  182: \t\t}\n+  144: \tprivate HibernateOrmBasicClassPropertyMetadata findOrmPropertyMetadata(String propertyName) {\n+  145: \t\tHibernateOrmBasicClassPropertyMetadata propertyMetadata = getOrmPropertyMetadataFromThisType( propertyName );\n+  146: \t\tif ( propertyMetadata == null ) {\n+  147: \t\t\tpropertyMetadata = getOrmPropertyMetadataFromParentTypes( propertyName );\n+  148: \t\t}\n+  149: \t\treturn propertyMetadata;\n+  150: \t}\n+  151: \n+  152: \tprivate Member findPropertyMember(String propertyName,\n+  153: \t\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata) {\n+  154: \t\tMember result = getPropertyMemberFromThisType( propertyName, ormPropertyMetadata );\n+  155: \n+  156: \t\tif ( result == null ) {\n+  157: \t\t\t// There is no member for this property on the current type.\n+  175: \tprivate HibernateOrmBasicClassPropertyMetadata getOrmPropertyMetadataFromThisType(String propertyName) {\n+  176: \t\tif ( ormTypeMetadata != null ) {\n+  177: \t\t\treturn ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n+  178: \t\t}\n+  179: \t\telse {\n+  180: \t\t\treturn null;\n+  181: \t\t}\n+  182: \t}\n",
        "uniqueId": "c46066ddc29fe1069994465ef58bd442c40eb0b0_148_157_175_182_144_150",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 4,
                "covered": 14
            },
            "BRANCH": {
                "missed": 2,
                "covered": 2
            },
            "LINE": {
                "missed": 1,
                "covered": 5
            },
            "COMPLEXITY": {
                "missed": 2,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate updateTail(workFuture CompletableFuture<?>) : void extracted from public addNonBulkExecution(work NonBulkableWork<T>) : CompletableFuture<T> in class org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder",
        "diffLocations": [
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
                "startLine": 52,
                "endLine": 83,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
                "startLine": 50,
                "endLine": 74,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
                "startLine": 119,
                "endLine": 126,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "/**\n\t * Add a step to execute a new work.\n\t * <p>\n\t * A failure in the previous work will lead to the new work being marked as skipped,\n\t * and a failure during the new work will lead to the new work being marked\n\t * as failed.\n\t *\n\t * @param work The work to be executed\n\t */\n\t@Override\n\tpublic <T> CompletableFuture<T> addNonBulkExecution(NonBulkableWork<T> work) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tNonBulkedWorkExecutionState<T> workExecutionState =\n\t\t\t\tnew NonBulkedWorkExecutionState<>( sequenceContext, work );\n\n\t\t// If the previous work failed, then skip the new work and notify the caller and failure handler as necessary.\n\t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n\t\t\t\t.whenComplete( Futures.handler( workExecutionState::onPreviousWorkComplete ) )\n\t\t\t\t// If the previous work completed normally, then execute the new work\n\t\t\t\t.thenCompose( Futures.safeComposer( workExecutionState::onPreviousWorkSuccess ) );\n\n\t\t/*\n\t\t * Make sure that the sequence will only advance to the next work\n\t\t * after both the work and *all* the handlers are executed,\n\t\t * because otherwise failureHandler.handle() could be called before all failed/skipped works are reported.\n\t\t */\n\t\tcurrentlyBuildingSequenceTail = handledWorkExecutionFuture;\n\n\t\treturn workExecutionState.workFutureForCaller;\n\t}",
        "filePathBefore": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
        "isPureRefactoring": true,
        "commitId": "9cd4135b36bd04a0ff6d2d99c85e60a47cae25a0",
        "packageNameBefore": "org.hibernate.search.backend.elasticsearch.orchestration.impl",
        "classNameBefore": "org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder",
        "methodNameBefore": "org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#addNonBulkExecution",
        "invokedMethod": "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder.NonBulkedWorkExecutionState#onPreviousWorkSuccess\n methodBody: CompletableFuture<R> onPreviousWorkSuccess(Object ignored) {\nCompletableFuture<R> workExecutionFuture=work.execute(sequenceContext.executionContext);\nreturn addPostExecutionHandlers(workExecutionFuture);\n}\nmethodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder.NonBulkedWorkExecutionState#onPreviousWorkComplete\n methodBody: void onPreviousWorkComplete(Object ignored, Throwable throwable) {\nif(throwable != null){skip(throwable);\n}}",
        "classSignatureBefore": "class ElasticsearchDefaultWorkSequenceBuilder implements ElasticsearchWorkSequenceBuilder ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#addNonBulkExecution"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder"
        ],
        "classSignatureBeforeSet": [
            "class ElasticsearchDefaultWorkSequenceBuilder implements ElasticsearchWorkSequenceBuilder "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics\nOverlapped refactoring - can be identical by undoing the overlapped refactoring\n",
                "description": "One of the overlapping cases - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.orchestration.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\nimport java.util.function.BiFunction;\nimport java.util.function.Supplier;\n\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.backend.elasticsearch.work.impl.BulkableWork;\nimport org.hibernate.search.backend.elasticsearch.work.impl.ElasticsearchWorkExecutionContext;\nimport org.hibernate.search.backend.elasticsearch.work.impl.NonBulkableWork;\nimport org.hibernate.search.backend.elasticsearch.work.impl.ElasticsearchWork;\nimport org.hibernate.search.backend.elasticsearch.work.result.impl.BulkResult;\nimport org.hibernate.search.backend.elasticsearch.work.result.impl.BulkResultItemExtractor;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.common.impl.Throwables;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * A simple implementation of {@link ElasticsearchWorkSequenceBuilder}.\n */\nclass ElasticsearchDefaultWorkSequenceBuilder implements ElasticsearchWorkSequenceBuilder {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate final Supplier<ElasticsearchWorkExecutionContext> contextSupplier;\n\tprivate final BulkResultExtractionStepImpl bulkResultExtractionStep = new BulkResultExtractionStepImpl();\n\n\tprivate CompletableFuture<?> currentlyBuildingSequenceTail;\n\tprivate SequenceContext currentlyBuildingSequenceContext;\n\n\tElasticsearchDefaultWorkSequenceBuilder(Supplier<ElasticsearchWorkExecutionContext> contextSupplier) {\n\t\tthis.contextSupplier = contextSupplier;\n\t}\n\n\t@Override\n\tpublic void init(CompletableFuture<?> previous) {\n\t\t// We only use the previous stage to delay the execution of the sequence, but we ignore its result\n\t\tthis.currentlyBuildingSequenceTail = previous.handle( (ignoredResult, ignoredThrowable) -> null );\n\t\tthis.currentlyBuildingSequenceContext = new SequenceContext(\n\t\t\t\tcontextSupplier.get()\n\t\t);\n\t}\n\n\t/**\n\t * Add a step to execute a new work.\n\t * <p>\n\t * A failure in the previous work will lead to the new work being marked as skipped,\n\t * and a failure during the new work will lead to the new work being marked\n\t * as failed.\n\t *\n\t * @param work The work to be executed\n\t */\n\t@Override\n\tpublic <T> CompletableFuture<T> addNonBulkExecution(NonBulkableWork<T> work) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tNonBulkedWorkExecutionState<T> workExecutionState =\n\t\t\t\tnew NonBulkedWorkExecutionState<>( sequenceContext, work );\n\n\t\t// If the previous work failed, then skip the new work and notify the caller and failure handler as necessary.\n\t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n\t\t\t\t.whenComplete( Futures.handler( workExecutionState::onPreviousWorkComplete ) )\n\t\t\t\t// If the previous work completed normally, then execute the new work\n\t\t\t\t.thenCompose( Futures.safeComposer( workExecutionState::onPreviousWorkSuccess ) );\n\n\t\t/*\n\t\t * Make sure that the sequence will only advance to the next work\n\t\t * after both the work and *all* the handlers are executed,\n\t\t * because otherwise failureHandler.handle() could be called before all failed/skipped works are reported.\n\t\t */\n\t\tcurrentlyBuildingSequenceTail = handledWorkExecutionFuture;\n\n\t\treturn workExecutionState.workFutureForCaller;\n\t}\n\n\t/**\n\t * Add a step to execute a bulk work.\n\t * <p>\n\t * The bulk work won't be marked as skipped or failed, regardless of errors.\n\t * Only the bulked works will be marked (as skipped) if a previous work or the bulk work fails.\n\t *\n\t * @param workFuture The work to be executed\n\t */\n\t@Override\n\tpublic CompletableFuture<BulkResult> addBulkExecution(CompletableFuture<? extends NonBulkableWork<BulkResult>> workFuture) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext currentSequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tCompletableFuture<BulkResult> bulkWorkResultFuture =\n\t\t\t\t// When the previous work completes successfully *and* the bulk work is available...\n\t\t\t\tcurrentlyBuildingSequenceTail.thenCombine( workFuture, (ignored, work) -> work )\n\t\t\t\t// ... execute the bulk work\n\t\t\t\t.thenCompose( currentSequenceContext::execute );\n\t\t// Do not propagate the exception as is: we expect the exception to be handled by each bulked work separately.\n\t\t// ... but still propagate *something*, in case a *previous* work failed.\n\t\tcurrentlyBuildingSequenceTail = bulkWorkResultFuture.exceptionally( Futures.handler( throwable -> {\n\t\t\tthrow new PreviousWorkException( throwable );\n\t\t} ) );\n\t\treturn bulkWorkResultFuture;\n\t}\n\n\t@Override\n\tpublic BulkResultExtractionStep addBulkResultExtraction(CompletableFuture<BulkResult> bulkResultFuture) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext currentSequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tCompletableFuture<BulkResultItemExtractor> extractorFuture =\n\t\t\t\tbulkResultFuture.thenApply( currentSequenceContext::addContext );\n\t\tbulkResultExtractionStep.init( extractorFuture );\n\t\treturn bulkResultExtractionStep;\n\t}\n\n\t@Override\n\tpublic CompletableFuture<Void> build() {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = currentlyBuildingSequenceContext;\n\n\t\treturn currentlyBuildingSequenceTail.handle( Futures.handler(\n\t\t\t\t(BiFunction<Object, Throwable, Void>) sequenceContext::onSequenceFinished\n\t\t) );\n\t}\n\n\tprivate final class BulkResultExtractionStepImpl implements BulkResultExtractionStep {\n\n\t\tprivate CompletableFuture<BulkResultItemExtractor> extractorFuture;\n\n\t\tvoid init(CompletableFuture<BulkResultItemExtractor> extractorFuture) {\n\t\t\tthis.extractorFuture = extractorFuture;\n\t\t}\n\n\t\t@Override\n\t\tpublic <T> CompletableFuture<T> add(BulkableWork<T> bulkedWork, int index) {\n\t\t\t// Use local variables to make sure the lambdas won't be affected by a reset()\n\t\t\tfinal SequenceContext sequenceContext = ElasticsearchDefaultWorkSequenceBuilder.this.currentlyBuildingSequenceContext;\n\n\t\t\tBulkedWorkExecutionState<T> workExecutionState =\n\t\t\t\t\tnew BulkedWorkExecutionState<>( sequenceContext, bulkedWork, index );\n\n\t\t\t// If the bulk work fails, make sure to notify the caller and failure handler as necessary.\n\t\t\tCompletableFuture<T> handledWorkExecutionFuture = extractorFuture\n\t\t\t\t\t.whenComplete( Futures.handler( workExecutionState::onBulkWorkComplete ) )\n\t\t\t\t\t// If the bulk work succeeds, then extract the bulked work result and notify as necessary\n\t\t\t\t\t.thenCompose( workExecutionState::onBulkWorkSuccess );\n\n\t\t\t/*\n\t\t\t * Make sure that the sequence will only advance to the next work\n\t\t\t * after both the work and *all* the handlers are executed,\n\t\t\t * because otherwise failureHandler.handle(...) could be called before all failed/skipped works are reported.\n\t\t\t */\n\t\t\tcurrentlyBuildingSequenceTail = CompletableFuture.allOf(\n\t\t\t\t\tcurrentlyBuildingSequenceTail,\n\t\t\t\t\thandledWorkExecutionFuture\n\t\t\t);\n\n\t\t\treturn workExecutionState.workFutureForCaller;\n\t\t}\n\n\t}\n\n\tprivate static final class PreviousWorkException extends RuntimeException {\n\n\t\tpublic PreviousWorkException(Throwable cause) {\n\t\t\tsuper( cause );\n\t\t}\n\n\t}\n\n\t/**\n\t * Regroups all objects that may be shared among multiple steps in the same sequence.\n\t * <p>\n\t * This was introduced to make references to data from a previous sequence less likely;\n\t * see\n\t * org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilderTest#intertwinedSequenceExecution()\n\t * for an example of what can go wrong if we don't take care to avoid that.\n\t */\n\tprivate static final class SequenceContext {\n\t\tprivate final ElasticsearchWorkExecutionContext executionContext;\n\n\t\tSequenceContext(ElasticsearchWorkExecutionContext executionContext) {\n\t\t\tthis.executionContext = executionContext;\n\t\t}\n\n\t\t<T> CompletionStage<T> execute(NonBulkableWork<T> work) {\n\t\t\treturn work.execute( executionContext );\n\t\t}\n\n\t\tpublic BulkResultItemExtractor addContext(BulkResult bulkResult) {\n\t\t\treturn bulkResult.withContext( executionContext );\n\t\t}\n\n\t\t<T> T onSequenceFinished(Object ignored, Throwable throwable) {\n\t\t\tif ( throwable != null && !(throwable instanceof PreviousWorkException) ) {\n\t\t\t\tthrow Throwables.toRuntimeException( throwable );\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n\n\tprivate abstract static class AbstractWorkExecutionState<T, W extends ElasticsearchWork<T>> {\n\n\t\tprotected final SequenceContext sequenceContext;\n\n\t\tprotected final W work;\n\n\t\t/*\n\t\t * Use a different future for the caller than the one used in the sequence,\n\t\t * because we manipulate internal exceptions in the sequence\n\t\t * that should not be exposed to the caller.\n\t\t */\n\t\tfinal CompletableFuture<T> workFutureForCaller = new CompletableFuture<>();\n\n\t\tprivate AbstractWorkExecutionState(SequenceContext sequenceContext, W work) {\n\t\t\tthis.sequenceContext = sequenceContext;\n\t\t\tthis.work = work;\n\t\t}\n\n\t\tprotected CompletableFuture<T> addPostExecutionHandlers(CompletableFuture<T> workExecutionFuture) {\n\t\t\t/*\n\t\t\t * In case of success, propagate the result to the client.\n\t\t\t */\n\t\t\tworkExecutionFuture.whenComplete( Futures.copyHandler( workFutureForCaller ) );\n\t\t\t/*\n\t\t\t * In case of error, propagate the exception immediately to both the failure handler and the client.\n\t\t\t *\n\t\t\t * Also, make sure to re-throw an exception\n\t\t\t * so that execution of following works in the sequence will be skipped.\n\t\t\t *\n\t\t\t * Make sure to return the resulting stage, and not executedWorkStage,\n\t\t\t * so that exception handling happens before the end of the sequence,\n\t\t\t * meaning notifyWorkFailed() is guaranteed to be called before notifySequenceFailed().\n\t\t\t */\n\t\t\treturn workExecutionFuture.exceptionally( Futures.handler( this::fail ) );\n\t\t}\n\n\t\tprotected void skip(Throwable throwable) {\n\t\t\tThrowable skippingCause = throwable instanceof PreviousWorkException ? throwable.getCause() : throwable;\n\t\t\tworkFutureForCaller.completeExceptionally(\n\t\t\t\t\tlog.elasticsearchSkippedBecauseOfPreviousWork( skippingCause )\n\t\t\t);\n\t\t}\n\n\t\tprotected T fail(Throwable throwable) {\n\t\t\tworkFutureForCaller.completeExceptionally( throwable );\n\t\t\tthrow new PreviousWorkException( throwable );\n\t\t}\n\t}\n\n\tprivate static final class NonBulkedWorkExecutionState<R> extends AbstractWorkExecutionState<R, NonBulkableWork<R>> {\n\n\t\tprivate NonBulkedWorkExecutionState(SequenceContext sequenceContext, NonBulkableWork<R> work) {\n\t\t\tsuper( sequenceContext, work );\n\t\t}\n\n\t\tvoid onPreviousWorkComplete(Object ignored, Throwable throwable) {\n\t\t\tif ( throwable != null ) {\n\t\t\t\tskip( throwable );\n\t\t\t}\n\t\t}\n\n\t\tCompletableFuture<R> onPreviousWorkSuccess(Object ignored) {\n\t\t\tCompletableFuture<R> workExecutionFuture = work.execute( sequenceContext.executionContext );\n\t\t\treturn addPostExecutionHandlers( workExecutionFuture );\n\t\t}\n\t}\n\n\tprivate static final class BulkedWorkExecutionState<R> extends AbstractWorkExecutionState<R, BulkableWork<R>> {\n\n\t\tprivate final BulkableWork<R> bulkedWork;\n\n\t\tprivate final int index;\n\n\t\tprivate BulkResultItemExtractor extractor;\n\n\t\tprivate BulkedWorkExecutionState(SequenceContext sequenceContext,\n\t\t\t\tBulkableWork<R> bulkedWork, int index) {\n\t\t\tsuper( sequenceContext, bulkedWork );\n\t\t\tthis.bulkedWork = bulkedWork;\n\t\t\tthis.index = index;\n\t\t}\n\n\t\tvoid onBulkWorkComplete(BulkResultItemExtractor ignored, Throwable throwable) {\n\t\t\tif ( throwable == null ) {\n\t\t\t\t// No failure: nothing to handle.\n\t\t\t\treturn;\n\t\t\t}\n\t\t\telse if ( throwable instanceof PreviousWorkException ) {\n\t\t\t\t// The bulk work itself was skipped; mark the bulked work as skipped too\n\t\t\t\tskip( throwable );\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// The bulk work failed; mark the bulked work as failed too\n\t\t\t\tfailBecauseBulkFailed( throwable );\n\t\t\t}\n\t\t}\n\n\t\tCompletableFuture<R> onBulkWorkSuccess(BulkResultItemExtractor extractor) {\n\t\t\tthis.extractor = extractor;\n\t\t\t// Use Futures.create to catch any exception thrown by extractor.extract\n\t\t\tCompletableFuture<R> workExecutionFuture = Futures.create( this::extract );\n\t\t\treturn addPostExecutionHandlers( workExecutionFuture );\n\t\t}\n\n\t\tprivate CompletableFuture<R> extract() {\n\t\t\treturn CompletableFuture.completedFuture( extractor.extract( bulkedWork, index ) );\n\t\t}\n\n\t\tprivate void failBecauseBulkFailed(Throwable throwable) {\n\t\t\tfail( log.elasticsearchFailedBecauseOfBulkFailure( throwable ) );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.orchestration.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\nimport java.util.function.Supplier;\n\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.backend.elasticsearch.work.impl.BulkableWork;\nimport org.hibernate.search.backend.elasticsearch.work.impl.ElasticsearchWorkExecutionContext;\nimport org.hibernate.search.backend.elasticsearch.work.impl.NonBulkableWork;\nimport org.hibernate.search.backend.elasticsearch.work.impl.ElasticsearchWork;\nimport org.hibernate.search.backend.elasticsearch.work.result.impl.BulkResult;\nimport org.hibernate.search.backend.elasticsearch.work.result.impl.BulkResultItemExtractor;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * A simple implementation of {@link ElasticsearchWorkSequenceBuilder}.\n */\nclass ElasticsearchDefaultWorkSequenceBuilder implements ElasticsearchWorkSequenceBuilder {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate final Supplier<ElasticsearchWorkExecutionContext> contextSupplier;\n\tprivate final BulkResultExtractionStepImpl bulkResultExtractionStep = new BulkResultExtractionStepImpl();\n\n\tprivate CompletableFuture<Void> currentlyBuildingSequenceTail;\n\tprivate SequenceContext currentlyBuildingSequenceContext;\n\n\tElasticsearchDefaultWorkSequenceBuilder(Supplier<ElasticsearchWorkExecutionContext> contextSupplier) {\n\t\tthis.contextSupplier = contextSupplier;\n\t}\n\n\t@Override\n\tpublic void init(CompletableFuture<?> previous) {\n\t\t// We only use the previous stage to delay the execution of the sequence, but we ignore its result\n\t\tthis.currentlyBuildingSequenceTail = previous.handle( (ignoredResult, ignoredThrowable) -> null );\n\t\tthis.currentlyBuildingSequenceContext = new SequenceContext(\n\t\t\t\tcontextSupplier.get()\n\t\t);\n\t}\n\n\t/**\n\t * Add a step to execute a new work.\n\t * <p>\n\t * A failure in the previous work will lead to the new work being marked as skipped,\n\t * and a failure during the new work will lead to the new work being marked\n\t * as failed.\n\t *\n\t * @param work The work to be executed\n\t */\n\t@Override\n\tpublic <T> CompletableFuture<T> addNonBulkExecution(NonBulkableWork<T> work) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tNonBulkedWorkExecutionState<T> workExecutionState =\n\t\t\t\tnew NonBulkedWorkExecutionState<>( sequenceContext, work );\n\n\t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n\t\t\t\t// When the previous work completes, execute the new work and notify as necessary.\n\t\t\t\t.thenCompose( Futures.safeComposer( workExecutionState::onPreviousWorkComplete ) );\n\n\t\tupdateTail( handledWorkExecutionFuture );\n\n\t\treturn workExecutionState.workFutureForCaller;\n\t}\n\n\t/**\n\t * Add a step to execute a bulk work.\n\t * <p>\n\t * The bulk work won't be marked as skipped or failed, regardless of errors.\n\t * Only the bulked works will be marked (as skipped) if a previous work or the bulk work fails.\n\t *\n\t * @param workFuture The work to be executed\n\t */\n\t@Override\n\tpublic CompletableFuture<BulkResult> addBulkExecution(CompletableFuture<? extends NonBulkableWork<BulkResult>> workFuture) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext currentSequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tCompletableFuture<BulkResult> bulkWorkResultFuture =\n\t\t\t\t// When the previous work completes *and* the bulk work is available...\n\t\t\t\tcurrentlyBuildingSequenceTail.thenCombine( workFuture, (ignored, work) -> work )\n\t\t\t\t// ... execute the bulk work\n\t\t\t\t.thenCompose( currentSequenceContext::execute );\n\n\t\tupdateTail( bulkWorkResultFuture );\n\n\t\treturn bulkWorkResultFuture;\n\t}\n\n\t@Override\n\tpublic BulkResultExtractionStep addBulkResultExtraction(CompletableFuture<BulkResult> bulkResultFuture) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext currentSequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tCompletableFuture<BulkResultItemExtractor> extractorFuture =\n\t\t\t\t// Only start extraction after the previous work is complete, so as to comply with the sequence order.\n\t\t\t\tcurrentlyBuildingSequenceTail.thenCombine( bulkResultFuture, (ignored, bulkResult) -> bulkResult )\n\t\t\t\t.thenApply( currentSequenceContext::addContext );\n\n\t\tbulkResultExtractionStep.init( extractorFuture );\n\t\treturn bulkResultExtractionStep;\n\t}\n\n\t@Override\n\tpublic CompletableFuture<Void> build() {\n\t\treturn currentlyBuildingSequenceTail;\n\t}\n\n\tprivate void updateTail(CompletableFuture<?> workFuture) {\n\t\t// The result of the work is expected to be already reported when \"workFuture\" completes,\n\t\t// successfully or not.\n\t\t// Ignore any exception in following works in this sequence,\n\t\t// to make sure the following works will execute regardless of the failures in previous works\n\t\t// (but will still execute *after* previous works).\n\t\tcurrentlyBuildingSequenceTail = workFuture.handle( (ignoredResult, ignoredThrowable) -> null );\n\t}\n\n\tprivate final class BulkResultExtractionStepImpl implements BulkResultExtractionStep {\n\n\t\tprivate CompletableFuture<BulkResultItemExtractor> extractorFuture;\n\n\t\tvoid init(CompletableFuture<BulkResultItemExtractor> extractorFuture) {\n\t\t\tthis.extractorFuture = extractorFuture;\n\t\t}\n\n\t\t@Override\n\t\tpublic <T> CompletableFuture<T> add(BulkableWork<T> bulkedWork, int index) {\n\t\t\t// Use local variables to make sure the lambdas won't be affected by a reset()\n\t\t\tfinal SequenceContext sequenceContext = ElasticsearchDefaultWorkSequenceBuilder.this.currentlyBuildingSequenceContext;\n\n\t\t\tBulkedWorkExecutionState<T> workExecutionState =\n\t\t\t\t\tnew BulkedWorkExecutionState<>( sequenceContext, bulkedWork, index );\n\n\t\t\tCompletableFuture<T> handledWorkExecutionFuture = extractorFuture\n\t\t\t\t\t// If the bulk work fails, make sure to notify the caller as necessary.\n\t\t\t\t\t.whenComplete( Futures.handler( workExecutionState::onBulkWorkComplete ) )\n\t\t\t\t\t// If the bulk work succeeds, then extract the bulked work result and notify as necessary.\n\t\t\t\t\t.thenCompose( workExecutionState::onBulkWorkSuccess );\n\n\t\t\t// Note the bulk\n\t\t\tupdateTail( CompletableFuture.allOf( currentlyBuildingSequenceTail, handledWorkExecutionFuture ) );\n\n\t\t\treturn workExecutionState.workFutureForCaller;\n\t\t}\n\n\t}\n\n\tprivate static final class PreviousWorkException extends RuntimeException {\n\n\t\tpublic PreviousWorkException(Throwable cause) {\n\t\t\tsuper( cause );\n\t\t}\n\n\t}\n\n\t/**\n\t * Regroups all objects that may be shared among multiple steps in the same sequence.\n\t * <p>\n\t * This was introduced to make references to data from a previous sequence less likely;\n\t * see\n\t * org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilderTest#intertwinedSequenceExecution()\n\t * for an example of what can go wrong if we don't take care to avoid that.\n\t */\n\tprivate static final class SequenceContext {\n\t\tprivate final ElasticsearchWorkExecutionContext executionContext;\n\n\t\tSequenceContext(ElasticsearchWorkExecutionContext executionContext) {\n\t\t\tthis.executionContext = executionContext;\n\t\t}\n\n\t\t<T> CompletionStage<T> execute(NonBulkableWork<T> work) {\n\t\t\treturn work.execute( executionContext );\n\t\t}\n\n\t\tpublic BulkResultItemExtractor addContext(BulkResult bulkResult) {\n\t\t\treturn bulkResult.withContext( executionContext );\n\t\t}\n\t}\n\n\tprivate abstract static class AbstractWorkExecutionState<T, W extends ElasticsearchWork<T>> {\n\n\t\tprotected final SequenceContext sequenceContext;\n\n\t\tprotected final W work;\n\n\t\t/*\n\t\t * Use a different future for the caller than the one used in the sequence,\n\t\t * because we manipulate internal exceptions in the sequence\n\t\t * that should not be exposed to the caller.\n\t\t */\n\t\tfinal CompletableFuture<T> workFutureForCaller = new CompletableFuture<>();\n\n\t\tprivate AbstractWorkExecutionState(SequenceContext sequenceContext, W work) {\n\t\t\tthis.sequenceContext = sequenceContext;\n\t\t\tthis.work = work;\n\t\t}\n\n\t\tprotected CompletableFuture<T> addPostExecutionHandlers(CompletableFuture<T> workExecutionFuture) {\n\t\t\t/*\n\t\t\t * In case of success, propagate the result to the client.\n\t\t\t */\n\t\t\tworkExecutionFuture.whenComplete( Futures.copyHandler( workFutureForCaller ) );\n\t\t\t/*\n\t\t\t * In case of error, propagate the exception immediately to both the failure handler and the client.\n\t\t\t *\n\t\t\t * Also, make sure to re-throw an exception\n\t\t\t * so that execution of following works in the sequence will be skipped.\n\t\t\t *\n\t\t\t * Make sure to return the resulting stage, and not executedWorkStage,\n\t\t\t * so that exception handling happens before the end of the sequence,\n\t\t\t * meaning notifyWorkFailed() is guaranteed to be called before notifySequenceFailed().\n\t\t\t */\n\t\t\treturn workExecutionFuture.exceptionally( Futures.handler( this::fail ) );\n\t\t}\n\n\t\tprotected T fail(Throwable throwable) {\n\t\t\tworkFutureForCaller.completeExceptionally( throwable );\n\t\t\tthrow new PreviousWorkException( throwable );\n\t\t}\n\t}\n\n\tprivate static final class NonBulkedWorkExecutionState<R> extends AbstractWorkExecutionState<R, NonBulkableWork<R>> {\n\n\t\tprivate NonBulkedWorkExecutionState(SequenceContext sequenceContext, NonBulkableWork<R> work) {\n\t\t\tsuper( sequenceContext, work );\n\t\t}\n\n\t\tCompletableFuture<R> onPreviousWorkComplete(Object ignored) {\n\t\t\tCompletableFuture<R> workExecutionFuture = work.execute( sequenceContext.executionContext );\n\t\t\treturn addPostExecutionHandlers( workExecutionFuture );\n\t\t}\n\t}\n\n\tprivate static final class BulkedWorkExecutionState<R> extends AbstractWorkExecutionState<R, BulkableWork<R>> {\n\n\t\tprivate final BulkableWork<R> bulkedWork;\n\n\t\tprivate final int index;\n\n\t\tprivate BulkResultItemExtractor extractor;\n\n\t\tprivate BulkedWorkExecutionState(SequenceContext sequenceContext,\n\t\t\t\tBulkableWork<R> bulkedWork, int index) {\n\t\t\tsuper( sequenceContext, bulkedWork );\n\t\t\tthis.bulkedWork = bulkedWork;\n\t\t\tthis.index = index;\n\t\t}\n\n\t\tvoid onBulkWorkComplete(Object ignored, Throwable throwable) {\n\t\t\tif ( throwable != null ) {\n\t\t\t\t// The bulk work failed; mark the bulked work as failed too\n\t\t\t\tfail( log.elasticsearchFailedBecauseOfBulkFailure( throwable ) );\n\t\t\t}\n\t\t}\n\n\t\tCompletableFuture<R> onBulkWorkSuccess(BulkResultItemExtractor extractor) {\n\t\t\tthis.extractor = extractor;\n\t\t\t// Use Futures.create to catch any exception thrown by extractor.extract\n\t\t\tCompletableFuture<R> workExecutionFuture = Futures.create( this::extract );\n\t\t\treturn addPostExecutionHandlers( workExecutionFuture );\n\t\t}\n\n\t\tprivate CompletableFuture<R> extract() {\n\t\t\treturn CompletableFuture.completedFuture( extractor.extract( bulkedWork, index ) );\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private void updateTail(CompletableFuture<?> workFuture) {\n\t\t// The result of the work is expected to be already reported when \"workFuture\" completes,\n\t\t// successfully or not.\n\t\t// Ignore any exception in following works in this sequence,\n\t\t// to make sure the following works will execute regardless of the failures in previous works\n\t\t// (but will still execute *after* previous works).\n\t\tcurrentlyBuildingSequenceTail = workFuture.handle( (ignoredResult, ignoredThrowable) -> null );\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder.NonBulkedWorkExecutionState#onPreviousWorkSuccess\n methodBody: CompletableFuture<R> onPreviousWorkSuccess(Object ignored) {\nCompletableFuture<R> workExecutionFuture=work.execute(sequenceContext.executionContext);\nreturn addPostExecutionHandlers(workExecutionFuture);\n}",
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder.NonBulkedWorkExecutionState#onPreviousWorkComplete\n methodBody: void onPreviousWorkComplete(Object ignored, Throwable throwable) {\nif(throwable != null){skip(throwable);\n}}"
        ],
        "sourceCodeAfterRefactoring": "/**\n\t * Add a step to execute a new work.\n\t * <p>\n\t * A failure in the previous work will lead to the new work being marked as skipped,\n\t * and a failure during the new work will lead to the new work being marked\n\t * as failed.\n\t *\n\t * @param work The work to be executed\n\t */\n\t@Override\n\tpublic <T> CompletableFuture<T> addNonBulkExecution(NonBulkableWork<T> work) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tNonBulkedWorkExecutionState<T> workExecutionState =\n\t\t\t\tnew NonBulkedWorkExecutionState<>( sequenceContext, work );\n\n\t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n\t\t\t\t// When the previous work completes, execute the new work and notify as necessary.\n\t\t\t\t.thenCompose( Futures.safeComposer( workExecutionState::onPreviousWorkComplete ) );\n\n\t\tupdateTail( handledWorkExecutionFuture );\n\n\t\treturn workExecutionState.workFutureForCaller;\n\t}\nprivate void updateTail(CompletableFuture<?> workFuture) {\n\t\t// The result of the work is expected to be already reported when \"workFuture\" completes,\n\t\t// successfully or not.\n\t\t// Ignore any exception in following works in this sequence,\n\t\t// to make sure the following works will execute regardless of the failures in previous works\n\t\t// (but will still execute *after* previous works).\n\t\tcurrentlyBuildingSequenceTail = workFuture.handle( (ignoredResult, ignoredThrowable) -> null );\n\t}",
        "diffSourceCode": "-   50: \t}\n-   51: \n-   52: \t/**\n-   53: \t * Add a step to execute a new work.\n-   54: \t * <p>\n-   55: \t * A failure in the previous work will lead to the new work being marked as skipped,\n-   56: \t * and a failure during the new work will lead to the new work being marked\n-   57: \t * as failed.\n-   58: \t *\n-   59: \t * @param work The work to be executed\n-   60: \t */\n-   61: \t@Override\n-   62: \tpublic <T> CompletableFuture<T> addNonBulkExecution(NonBulkableWork<T> work) {\n-   63: \t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n-   64: \t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n-   65: \n-   66: \t\tNonBulkedWorkExecutionState<T> workExecutionState =\n-   67: \t\t\t\tnew NonBulkedWorkExecutionState<>( sequenceContext, work );\n-   68: \n-   69: \t\t// If the previous work failed, then skip the new work and notify the caller and failure handler as necessary.\n-   70: \t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n-   71: \t\t\t\t.whenComplete( Futures.handler( workExecutionState::onPreviousWorkComplete ) )\n-   72: \t\t\t\t// If the previous work completed normally, then execute the new work\n-   73: \t\t\t\t.thenCompose( Futures.safeComposer( workExecutionState::onPreviousWorkSuccess ) );\n-   74: \n-   75: \t\t/*\n-   76: \t\t * Make sure that the sequence will only advance to the next work\n-   77: \t\t * after both the work and *all* the handlers are executed,\n-   78: \t\t * because otherwise failureHandler.handle() could be called before all failed/skipped works are reported.\n-   79: \t\t */\n-   80: \t\tcurrentlyBuildingSequenceTail = handledWorkExecutionFuture;\n-   81: \n-   82: \t\treturn workExecutionState.workFutureForCaller;\n-   83: \t}\n-  119: \t\treturn bulkResultExtractionStep;\n-  120: \t}\n-  121: \n-  122: \t@Override\n-  123: \tpublic CompletableFuture<Void> build() {\n-  124: \t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n-  125: \t\tfinal SequenceContext sequenceContext = currentlyBuildingSequenceContext;\n-  126: \n+   50: \t/**\n+   51: \t * Add a step to execute a new work.\n+   52: \t * <p>\n+   53: \t * A failure in the previous work will lead to the new work being marked as skipped,\n+   54: \t * and a failure during the new work will lead to the new work being marked\n+   55: \t * as failed.\n+   56: \t *\n+   57: \t * @param work The work to be executed\n+   58: \t */\n+   59: \t@Override\n+   60: \tpublic <T> CompletableFuture<T> addNonBulkExecution(NonBulkableWork<T> work) {\n+   61: \t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n+   62: \t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n+   63: \n+   64: \t\tNonBulkedWorkExecutionState<T> workExecutionState =\n+   65: \t\t\t\tnew NonBulkedWorkExecutionState<>( sequenceContext, work );\n+   66: \n+   67: \t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n+   68: \t\t\t\t// When the previous work completes, execute the new work and notify as necessary.\n+   69: \t\t\t\t.thenCompose( Futures.safeComposer( workExecutionState::onPreviousWorkComplete ) );\n+   70: \n+   71: \t\tupdateTail( handledWorkExecutionFuture );\n+   72: \n+   73: \t\treturn workExecutionState.workFutureForCaller;\n+   74: \t}\n+   75: \n+   76: \t/**\n+   77: \t * Add a step to execute a bulk work.\n+   78: \t * <p>\n+   79: \t * The bulk work won't be marked as skipped or failed, regardless of errors.\n+   80: \t * Only the bulked works will be marked (as skipped) if a previous work or the bulk work fails.\n+   81: \t *\n+   82: \t * @param workFuture The work to be executed\n+   83: \t */\n+  119: \tprivate void updateTail(CompletableFuture<?> workFuture) {\n+  120: \t\t// The result of the work is expected to be already reported when \"workFuture\" completes,\n+  121: \t\t// successfully or not.\n+  122: \t\t// Ignore any exception in following works in this sequence,\n+  123: \t\t// to make sure the following works will execute regardless of the failures in previous works\n+  124: \t\t// (but will still execute *after* previous works).\n+  125: \t\tcurrentlyBuildingSequenceTail = workFuture.handle( (ignoredResult, ignoredThrowable) -> null );\n+  126: \t}\n",
        "uniqueId": "9cd4135b36bd04a0ff6d2d99c85e60a47cae25a0_52_83_119_126_50_74",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 33
            },
            "LINE": {
                "missed": 0,
                "covered": 7
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Move And Inline Method",
        "description": "Move And Inline Method\tpublic getDocumentId() : String moved from class org.hibernate.search.backend.lucene.work.impl.LuceneAddEntryWork to class org.hibernate.search.backend.lucene.work.execution.impl.LuceneIndexingPlanWriteWorkSetTest & inlined to private expectWorkGetInfo(ids int...) : void",
        "diffLocations": [
            {
                "filePath": "backend/lucene/src/test/java/org/hibernate/search/backend/lucene/work/execution/impl/LuceneIndexingPlanWriteWorkSetTest.java",
                "startLine": 179,
                "endLine": 185,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/lucene/src/test/java/org/hibernate/search/backend/lucene/work/execution/impl/LuceneIndexingPlanWriteWorkSetTest.java",
                "startLine": 186,
                "endLine": 195,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/lucene/src/test/java/org/hibernate/search/backend/lucene/work/execution/impl/LuceneIndexingPlanWriteWorkSetTest.java",
                "startLine": 57,
                "endLine": 60,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void doTestSuccess(DocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\n\t\tCompletableFuture<IndexIndexingPlanExecutionReport> workSetFuture = new CompletableFuture<>();",
        "filePathBefore": "backend/lucene/src/test/java/org/hibernate/search/backend/lucene/work/execution/impl/LuceneIndexingPlanWriteWorkSetTest.java",
        "isPureRefactoring": true,
        "commitId": "64d26592b1ac48943690f7690355d44821591f36",
        "packageNameBefore": "org.hibernate.search.backend.lucene.work.impl",
        "classNameBefore": "org.hibernate.search.backend.lucene.work.impl.LuceneAddEntryWork",
        "methodNameBefore": "org.hibernate.search.backend.lucene.work.impl.LuceneAddEntryWork#getDocumentId",
        "classSignatureBefore": "public class LuceneAddEntryWork extends AbstractLuceneWriteWork<Long>\n\t\timplements LuceneSingleDocumentWriteWork<Long> ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.lucene.work.impl.LuceneAddEntryWork#getDocumentId"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.lucene.work.impl.LuceneAddEntryWork"
        ],
        "classSignatureBeforeSet": [
            "public class LuceneAddEntryWork extends AbstractLuceneWriteWork<Long>\n\t\timplements LuceneSingleDocumentWriteWork<Long> "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Tolerable changes in the body\n",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.lucene.work.execution.impl;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.easymock.EasyMock.expect;\nimport static org.easymock.EasyMock.expectLastCall;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.CompletableFuture;\n\nimport org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor;\nimport org.hibernate.search.backend.lucene.search.impl.LuceneDocumentReference;\nimport org.hibernate.search.backend.lucene.work.impl.LuceneSingleDocumentWriteWork;\nimport org.hibernate.search.engine.backend.common.DocumentReference;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.backend.work.execution.spi.IndexIndexingPlanExecutionReport;\nimport org.hibernate.search.util.impl.test.FutureAssert;\n\nimport org.junit.Test;\n\nimport org.assertj.core.api.SoftAssertions;\nimport org.easymock.EasyMock;\nimport org.easymock.EasyMockSupport;\n\n/**\n * Test worksets produced by indexing plans.\n */\npublic class LuceneIndexingPlanWriteWorkSetTest extends EasyMockSupport {\n\n\tprivate static final String INDEX_NAME = \"SomeIndexName\";\n\n\tprivate LuceneWriteWorkProcessor processorMock = createStrictMock( LuceneWriteWorkProcessor.class );\n\n\tprivate List<LuceneSingleDocumentWriteWork<?>> workMocks = new ArrayList<>();\n\n\t@Test\n\tpublic void success_commitNone_refreshNone() {\n\t\tdoTestSuccess( DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE );\n\t}\n\n\t@Test\n\tpublic void success_commitForce_refreshNone() {\n\t\tdoTestSuccess( DocumentCommitStrategy.FORCE, DocumentRefreshStrategy.NONE );\n\t}\n\n\t@Test\n\tpublic void success_commitForce_refreshForce() {\n\t\tdoTestSuccess( DocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE );\n\t}\n\n\tprivate void doTestSuccess(DocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\n\t\tCompletableFuture<IndexIndexingPlanExecutionReport> workSetFuture = new CompletableFuture<>();\n\n\t\tLuceneIndexingPlanWriteWorkSet workSet = new LuceneIndexingPlanWriteWorkSet(\n\t\t\t\tINDEX_NAME, createWorkMocks( 3 ), workSetFuture,\n\t\t\t\tcommitStrategy, refreshStrategy\n\t\t);\n\n\t\tFutureAssert.assertThat( workSetFuture ).isPending();\n\n\t\tresetAll();\n\t\tprocessorMock.beforeWorkSet( commitStrategy, refreshStrategy );\n\t\texpect( processorMock.submit( workMocks.get( 0 ) ) ).andReturn( null );\n\t\texpect( processorMock.submit( workMocks.get( 1 ) ) ).andReturn( null );\n\t\texpect( processorMock.submit( workMocks.get( 2 ) ) ).andReturn( null );\n\t\tprocessorMock.afterSuccessfulWorkSet();\n\t\treplayAll();\n\t\tworkSet.submitTo( processorMock );\n\t\tverifyAll();\n\n\t\tFutureAssert.assertThat( workSetFuture ).isSuccessful( report -> {\n\t\t\tassertThat( report ).isNotNull();\n\t\t\tSoftAssertions.assertSoftly( softly -> {\n\t\t\t\tsoftly.assertThat( report.getThrowable() ).isEmpty();\n\t\t\t\tsoftly.assertThat( report.getFailingDocuments() ).isEmpty();\n\t\t\t} );\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void markAsFailed() {\n\t\tCompletableFuture<IndexIndexingPlanExecutionReport> workSetFuture = new CompletableFuture<>();\n\n\t\tLuceneIndexingPlanWriteWorkSet workSet = new LuceneIndexingPlanWriteWorkSet(\n\t\t\t\tINDEX_NAME, createWorkMocks( 3 ), workSetFuture,\n\t\t\t\tDocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t);\n\n\t\tFutureAssert.assertThat( workSetFuture ).isPending();\n\n\t\tThrowable throwable = new Throwable( \"Some message\" );\n\t\tresetAll();\n\t\t// Do not expect any call on the mocks\n\t\treplayAll();\n\t\tworkSet.markAsFailed( throwable );\n\t\tverifyAll();\n\n\t\tFutureAssert.assertThat( workSetFuture ).isFailed( throwable );\n\t}\n\n\t@Test\n\tpublic void failure_work() {\n\t\tCompletableFuture<IndexIndexingPlanExecutionReport> workSetFuture = new CompletableFuture<>();\n\n\t\tLuceneIndexingPlanWriteWorkSet workSet = new LuceneIndexingPlanWriteWorkSet(\n\t\t\t\tINDEX_NAME, createWorkMocks( 3 ), workSetFuture,\n\t\t\t\tDocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t);\n\n\t\tFutureAssert.assertThat( workSetFuture ).isPending();\n\n\t\tRuntimeException workException = new RuntimeException( \"Some message\" );\n\t\tresetAll();\n\t\tprocessorMock.beforeWorkSet( DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE );\n\t\texpect( processorMock.submit( workMocks.get( 0 ) ) ).andReturn( null );\n\t\texpect( processorMock.submit( workMocks.get( 1 ) ) ).andThrow( workException );\n\t\texpectWorkGetInfo( 0, 1, 2 );\n\t\treplayAll();\n\t\tworkSet.submitTo( processorMock );\n\t\tverifyAll();\n\n\t\tFutureAssert.assertThat( workSetFuture ).isSuccessful( report -> {\n\t\t\tassertThat( report ).isNotNull();\n\t\t\tSoftAssertions.assertSoftly( softly -> {\n\t\t\t\tsoftly.assertThat( report.getThrowable() ).containsSame( workException );\n\t\t\t\tsoftly.assertThat( report.getFailingDocuments() )\n\t\t\t\t\t\t.containsExactly(\n\t\t\t\t\t\t\t\t// All documents from the current workset, even ones from successful works\n\t\t\t\t\t\t\t\tdocReference( 0 ), docReference( 1 ), docReference( 2 )\n\t\t\t\t\t\t);\n\t\t\t} );\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void failure_commit() {\n\t\tCompletableFuture<IndexIndexingPlanExecutionReport> workSetFuture = new CompletableFuture<>();\n\n\t\tLuceneIndexingPlanWriteWorkSet workSet = new LuceneIndexingPlanWriteWorkSet(\n\t\t\t\tINDEX_NAME, createWorkMocks( 3 ), workSetFuture,\n\t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.NONE\n\t\t);\n\n\t\tFutureAssert.assertThat( workSetFuture ).isPending();\n\n\t\tRuntimeException commitException = new RuntimeException( \"Some message\" );\n\t\tresetAll();\n\t\tprocessorMock.beforeWorkSet( DocumentCommitStrategy.FORCE, DocumentRefreshStrategy.NONE );\n\t\texpect( processorMock.submit( workMocks.get( 0 ) ) ).andReturn( null );\n\t\texpect( processorMock.submit( workMocks.get( 1 ) ) ).andReturn( null );\n\t\texpect( processorMock.submit( workMocks.get( 2 ) ) ).andReturn( null );\n\t\tprocessorMock.afterSuccessfulWorkSet();\n\t\texpectLastCall().andThrow( commitException );\n\t\texpectWorkGetInfo( 0, 1, 2 );\n\t\treplayAll();\n\t\tworkSet.submitTo( processorMock );\n\t\tverifyAll();\n\n\t\tFutureAssert.assertThat( workSetFuture ).isSuccessful( report -> {\n\t\t\tassertThat( report ).isNotNull();\n\t\t\tSoftAssertions.assertSoftly( softly -> {\n\t\t\t\tsoftly.assertThat( report.getThrowable() ).containsSame( commitException );\n\t\t\t\tsoftly.assertThat( report.getFailingDocuments() )\n\t\t\t\t\t\t.containsExactly(\n\t\t\t\t\t\t\t\t// All documents from the current workset, even ones from successful works\n\t\t\t\t\t\t\t\tdocReference( 0 ), docReference( 1 ), docReference( 2 )\n\t\t\t\t\t\t);\n\t\t\t} );\n\t\t} );\n\t}\n\n\tprivate void expectWorkGetInfo(int ... ids) {\n\t\tfor ( int id : ids ) {\n\t\t\tLuceneSingleDocumentWriteWork<?> workMock = workMocks.get( id );\n\t\t\tEasyMock.expect( workMock.getInfo() ).andStubReturn( workInfo( id ) );\n\t\t\tEasyMock.expect( workMock.getDocumentId() ).andStubReturn( String.valueOf( id ) );\n\t\t}\n\t}\n\n\tprivate List<LuceneSingleDocumentWriteWork<?>> createWorkMocks(int count) {\n\t\tList<LuceneSingleDocumentWriteWork<?>> result = new ArrayList<>();\n\t\tfor ( int i = 0; i < count; i++ ) {\n\t\t\tresult.add( createWorkMock() );\n\t\t}\n\t\treturn result;\n\t}\n\n\tprivate <T> LuceneSingleDocumentWriteWork<T> createWorkMock() {\n\t\tString workName = workInfo( workMocks.size() );\n\t\tLuceneSingleDocumentWriteWork<T> workMock = createStrictMock( workName, LuceneSingleDocumentWriteWork.class );\n\t\tworkMocks.add( workMock );\n\t\treturn workMock;\n\t}\n\n\tprivate DocumentReference docReference(int id) {\n\t\treturn new LuceneDocumentReference( INDEX_NAME, String.valueOf( id ) );\n\t}\n\n\tprivate String workInfo(int index) {\n\t\treturn \"work_\" + index;\n\t}\n\n}",
        "filePathAfter": "backend/lucene/src/test/java/org/hibernate/search/backend/lucene/work/execution/impl/LuceneIndexingPlanWriteWorkSetTest.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.lucene.work.execution.impl;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.easymock.EasyMock.expect;\nimport static org.easymock.EasyMock.expectLastCall;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Objects;\nimport java.util.concurrent.CompletableFuture;\n\nimport org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor;\nimport org.hibernate.search.backend.lucene.work.impl.LuceneSingleDocumentWriteWork;\nimport org.hibernate.search.engine.backend.common.spi.EntityReferenceFactory;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.backend.work.execution.spi.IndexIndexingPlanExecutionReport;\nimport org.hibernate.search.util.impl.test.FutureAssert;\n\nimport org.junit.Test;\n\nimport org.assertj.core.api.SoftAssertions;\nimport org.easymock.EasyMock;\nimport org.easymock.EasyMockSupport;\n\n/**\n * Test worksets produced by indexing plans.\n */\npublic class LuceneIndexingPlanWriteWorkSetTest extends EasyMockSupport {\n\n\tprivate static final String TYPE_NAME = \"SomeTypeName\";\n\n\tprivate LuceneWriteWorkProcessor processorMock = createStrictMock( LuceneWriteWorkProcessor.class );\n\n\tprivate EntityReferenceFactory<StubEntityReference> entityReferenceFactoryMock =\n\t\t\tcreateStrictMock( EntityReferenceFactory.class );\n\n\tprivate List<LuceneSingleDocumentWriteWork<?>> workMocks = new ArrayList<>();\n\n\t@Test\n\tpublic void success_commitNone_refreshNone() {\n\t\tdoTestSuccess( DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE );\n\t}\n\n\t@Test\n\tpublic void success_commitForce_refreshNone() {\n\t\tdoTestSuccess( DocumentCommitStrategy.FORCE, DocumentRefreshStrategy.NONE );\n\t}\n\n\t@Test\n\tpublic void success_commitForce_refreshForce() {\n\t\tdoTestSuccess( DocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE );\n\t}\n\n\tprivate void doTestSuccess(DocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\n\t\tCompletableFuture<IndexIndexingPlanExecutionReport<StubEntityReference>> workSetFuture =\n\t\t\t\tnew CompletableFuture<>();\n\n\t\tLuceneIndexingPlanWriteWorkSet<StubEntityReference> workSet = new LuceneIndexingPlanWriteWorkSet<>(\n\t\t\t\tcreateWorkMocks( 3 ), entityReferenceFactoryMock, workSetFuture,\n\t\t\t\tcommitStrategy, refreshStrategy\n\t\t);\n\n\t\tFutureAssert.assertThat( workSetFuture ).isPending();\n\n\t\tresetAll();\n\t\tprocessorMock.beforeWorkSet( commitStrategy, refreshStrategy );\n\t\texpect( processorMock.submit( workMocks.get( 0 ) ) ).andReturn( null );\n\t\texpect( processorMock.submit( workMocks.get( 1 ) ) ).andReturn( null );\n\t\texpect( processorMock.submit( workMocks.get( 2 ) ) ).andReturn( null );\n\t\tprocessorMock.afterSuccessfulWorkSet();\n\t\treplayAll();\n\t\tworkSet.submitTo( processorMock );\n\t\tverifyAll();\n\n\t\tFutureAssert.assertThat( workSetFuture ).isSuccessful( report -> {\n\t\t\tassertThat( report ).isNotNull();\n\t\t\tSoftAssertions.assertSoftly( softly -> {\n\t\t\t\tsoftly.assertThat( report.getThrowable() ).isEmpty();\n\t\t\t\tsoftly.assertThat( report.getFailingEntityReferences() ).isEmpty();\n\t\t\t} );\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void markAsFailed() {\n\t\tCompletableFuture<IndexIndexingPlanExecutionReport<StubEntityReference>> workSetFuture =\n\t\t\t\tnew CompletableFuture<>();\n\n\t\tLuceneIndexingPlanWriteWorkSet<StubEntityReference> workSet = new LuceneIndexingPlanWriteWorkSet<>(\n\t\t\t\tcreateWorkMocks( 3 ), entityReferenceFactoryMock, workSetFuture,\n\t\t\t\tDocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t);\n\n\t\tFutureAssert.assertThat( workSetFuture ).isPending();\n\n\t\tThrowable throwable = new Throwable( \"Some message\" );\n\t\tresetAll();\n\t\t// Do not expect any call on the mocks\n\t\treplayAll();\n\t\tworkSet.markAsFailed( throwable );\n\t\tverifyAll();\n\n\t\tFutureAssert.assertThat( workSetFuture ).isFailed( throwable );\n\t}\n\n\t@Test\n\tpublic void failure_work() {\n\t\tCompletableFuture<IndexIndexingPlanExecutionReport<StubEntityReference>> workSetFuture =\n\t\t\t\tnew CompletableFuture<>();\n\n\t\tLuceneIndexingPlanWriteWorkSet<StubEntityReference> workSet = new LuceneIndexingPlanWriteWorkSet<>(\n\t\t\t\tcreateWorkMocks( 3 ), entityReferenceFactoryMock, workSetFuture,\n\t\t\t\tDocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t);\n\n\t\tFutureAssert.assertThat( workSetFuture ).isPending();\n\n\t\tRuntimeException workException = new RuntimeException( \"Some message\" );\n\t\tresetAll();\n\t\tprocessorMock.beforeWorkSet( DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE );\n\t\texpect( processorMock.submit( workMocks.get( 0 ) ) ).andReturn( null );\n\t\texpect( processorMock.submit( workMocks.get( 1 ) ) ).andThrow( workException );\n\t\texpectWorkGetInfo( 0, 1, 2 );\n\t\treplayAll();\n\t\tworkSet.submitTo( processorMock );\n\t\tverifyAll();\n\n\t\tFutureAssert.assertThat( workSetFuture ).isSuccessful( report -> {\n\t\t\tassertThat( report ).isNotNull();\n\t\t\tSoftAssertions.assertSoftly( softly -> {\n\t\t\t\tsoftly.assertThat( report.getThrowable() ).containsSame( workException );\n\t\t\t\tsoftly.assertThat( report.getFailingEntityReferences() )\n\t\t\t\t\t\t.containsExactly(\n\t\t\t\t\t\t\t\t// All documents from the current workset, even ones from successful works\n\t\t\t\t\t\t\t\tentityReference( 0 ), entityReference( 1 ), entityReference( 2 )\n\t\t\t\t\t\t);\n\t\t\t} );\n\t\t} );\n\t}\n\n\t@Test\n\tpublic void failure_commit() {\n\t\tCompletableFuture<IndexIndexingPlanExecutionReport<StubEntityReference>> workSetFuture =\n\t\t\t\tnew CompletableFuture<>();\n\n\t\tLuceneIndexingPlanWriteWorkSet<StubEntityReference> workSet = new LuceneIndexingPlanWriteWorkSet<>(\n\t\t\t\tcreateWorkMocks( 3 ), entityReferenceFactoryMock, workSetFuture,\n\t\t\t\tDocumentCommitStrategy.FORCE, DocumentRefreshStrategy.NONE\n\t\t);\n\n\t\tFutureAssert.assertThat( workSetFuture ).isPending();\n\n\t\tRuntimeException commitException = new RuntimeException( \"Some message\" );\n\t\tresetAll();\n\t\tprocessorMock.beforeWorkSet( DocumentCommitStrategy.FORCE, DocumentRefreshStrategy.NONE );\n\t\texpect( processorMock.submit( workMocks.get( 0 ) ) ).andReturn( null );\n\t\texpect( processorMock.submit( workMocks.get( 1 ) ) ).andReturn( null );\n\t\texpect( processorMock.submit( workMocks.get( 2 ) ) ).andReturn( null );\n\t\tprocessorMock.afterSuccessfulWorkSet();\n\t\texpectLastCall().andThrow( commitException );\n\t\texpectWorkGetInfo( 0, 1, 2 );\n\t\treplayAll();\n\t\tworkSet.submitTo( processorMock );\n\t\tverifyAll();\n\n\t\tFutureAssert.assertThat( workSetFuture ).isSuccessful( report -> {\n\t\t\tassertThat( report ).isNotNull();\n\t\t\tSoftAssertions.assertSoftly( softly -> {\n\t\t\t\tsoftly.assertThat( report.getThrowable() ).containsSame( commitException );\n\t\t\t\tsoftly.assertThat( report.getFailingEntityReferences() )\n\t\t\t\t\t\t.containsExactly(\n\t\t\t\t\t\t\t\t// All documents from the current workset, even ones from successful works\n\t\t\t\t\t\t\t\tentityReference( 0 ), entityReference( 1 ), entityReference( 2 )\n\t\t\t\t\t\t);\n\t\t\t} );\n\t\t} );\n\t}\n\n\tprivate void expectWorkGetInfo(int ... ids) {\n\t\tfor ( int id : ids ) {\n\t\t\tLuceneSingleDocumentWriteWork<?> workMock = workMocks.get( id );\n\t\t\tEasyMock.expect( workMock.getInfo() ).andStubReturn( workInfo( id ) );\n\t\t\tEasyMock.expect( workMock.getEntityTypeName() ).andStubReturn( TYPE_NAME );\n\t\t\tEasyMock.expect( workMock.getEntityIdentifier() ).andStubReturn( id );\n\t\t\tEasyMock.expect( entityReferenceFactoryMock.createEntityReference( TYPE_NAME, id ) )\n\t\t\t\t\t.andReturn( entityReference( id ) );\n\t\t}\n\t}\n\n\tprivate List<LuceneSingleDocumentWriteWork<?>> createWorkMocks(int count) {\n\t\tList<LuceneSingleDocumentWriteWork<?>> result = new ArrayList<>();\n\t\tfor ( int i = 0; i < count; i++ ) {\n\t\t\tresult.add( createWorkMock() );\n\t\t}\n\t\treturn result;\n\t}\n\n\tprivate <T> LuceneSingleDocumentWriteWork<T> createWorkMock() {\n\t\tString workName = workInfo( workMocks.size() );\n\t\tLuceneSingleDocumentWriteWork<T> workMock = createStrictMock( workName, LuceneSingleDocumentWriteWork.class );\n\t\tworkMocks.add( workMock );\n\t\treturn workMock;\n\t}\n\n\tprivate StubEntityReference entityReference(int id) {\n\t\treturn new StubEntityReference( TYPE_NAME, id );\n\t}\n\n\tprivate String workInfo(int index) {\n\t\treturn \"work_\" + index;\n\t}\n\n\tprivate static class StubEntityReference {\n\t\tprivate final String typeName;\n\t\tprivate final Object identifier;\n\n\t\tprivate StubEntityReference(String typeName, Object identifier) {\n\t\t\tthis.typeName = typeName;\n\t\t\tthis.identifier = identifier;\n\t\t}\n\n\t\t@Override\n\t\tpublic String toString() {\n\t\t\treturn \"StubEntityReference{\" +\n\t\t\t\t\t\"typeName='\" + typeName + '\\'' +\n\t\t\t\t\t\", identifier=\" + identifier +\n\t\t\t\t\t'}';\n\t\t}\n\n\t\t@Override\n\t\tpublic boolean equals(Object o) {\n\t\t\tif ( this == o ) {\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\tif ( o == null || getClass() != o.getClass() ) {\n\t\t\t\treturn false;\n\t\t\t}\n\t\t\tStubEntityReference that = (StubEntityReference) o;\n\t\t\treturn Objects.equals( typeName, that.typeName ) &&\n\t\t\t\t\tObjects.equals( identifier, that.identifier );\n\t\t}\n\n\t\t@Override\n\t\tpublic int hashCode() {\n\t\t\treturn Objects.hash( typeName, identifier );\n\t\t}\n\t}\n}",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "private void expectWorkGetInfo(int ... ids) {\n\t\tfor ( int id : ids ) {\n\t\t\tLuceneSingleDocumentWriteWork<?> workMock = workMocks.get( id );\n\t\t\tEasyMock.expect( workMock.getInfo() ).andStubReturn( workInfo( id ) );\n\t\t\tEasyMock.expect( workMock.getEntityTypeName() ).andStubReturn( TYPE_NAME );\n\t\t\tEasyMock.expect( workMock.getEntityIdentifier() ).andStubReturn( id );\n\t\t\tEasyMock.expect( entityReferenceFactoryMock.createEntityReference( TYPE_NAME, id ) )\n\t\t\t\t\t.andReturn( entityReference( id ) );\n\t\t}\n\t}",
        "diffSourceCode": "-   57: \n-   58: \tprivate void doTestSuccess(DocumentCommitStrategy commitStrategy, DocumentRefreshStrategy refreshStrategy) {\n-   59: \t\tCompletableFuture<IndexIndexingPlanExecutionReport> workSetFuture = new CompletableFuture<>();\n+   57: \tpublic void success_commitForce_refreshForce() {\n+   58: \t\tdoTestSuccess( DocumentCommitStrategy.FORCE, DocumentRefreshStrategy.FORCE );\n+   59: \t}\n    60: \n-  179: \tprivate void expectWorkGetInfo(int ... ids) {\n-  180: \t\tfor ( int id : ids ) {\n-  181: \t\t\tLuceneSingleDocumentWriteWork<?> workMock = workMocks.get( id );\n-  182: \t\t\tEasyMock.expect( workMock.getInfo() ).andStubReturn( workInfo( id ) );\n-  183: \t\t\tEasyMock.expect( workMock.getDocumentId() ).andStubReturn( String.valueOf( id ) );\n-  184: \t\t}\n-  185: \t}\n-  186: \n-  187: \tprivate List<LuceneSingleDocumentWriteWork<?>> createWorkMocks(int count) {\n-  188: \t\tList<LuceneSingleDocumentWriteWork<?>> result = new ArrayList<>();\n-  189: \t\tfor ( int i = 0; i < count; i++ ) {\n-  190: \t\t\tresult.add( createWorkMock() );\n-  191: \t\t}\n-  192: \t\treturn result;\n-  193: \t}\n-  194: \n-  195: \tprivate <T> LuceneSingleDocumentWriteWork<T> createWorkMock() {\n+  179: \t\t\t\t\t\t\t\t// All documents from the current workset, even ones from successful works\n+  180: \t\t\t\t\t\t\t\tentityReference( 0 ), entityReference( 1 ), entityReference( 2 )\n+  181: \t\t\t\t\t\t);\n+  182: \t\t\t} );\n+  183: \t\t} );\n+  184: \t}\n+  185: \n+  186: \tprivate void expectWorkGetInfo(int ... ids) {\n+  187: \t\tfor ( int id : ids ) {\n+  188: \t\t\tLuceneSingleDocumentWriteWork<?> workMock = workMocks.get( id );\n+  189: \t\t\tEasyMock.expect( workMock.getInfo() ).andStubReturn( workInfo( id ) );\n+  190: \t\t\tEasyMock.expect( workMock.getEntityTypeName() ).andStubReturn( TYPE_NAME );\n+  191: \t\t\tEasyMock.expect( workMock.getEntityIdentifier() ).andStubReturn( id );\n+  192: \t\t\tEasyMock.expect( entityReferenceFactoryMock.createEntityReference( TYPE_NAME, id ) )\n+  193: \t\t\t\t\t.andReturn( entityReference( id ) );\n+  194: \t\t}\n+  195: \t}\n",
        "uniqueId": "64d26592b1ac48943690f7690355d44821591f36_179_185__186_195_57_60",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate processBatch(works List<W>) : void extracted from private processBatch() : void in class org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor",
        "diffLocations": [
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutor.java",
                "startLine": 180,
                "endLine": 247,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutor.java",
                "startLine": 196,
                "endLine": 240,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutor.java",
                "startLine": 242,
                "endLine": 264,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "/**\n\t * Takes a batch of worksets from the queue and processes them.\n\t */\n\tprivate void processBatch() {\n\t\ttry {\n\t\t\tCompletableFuture<?> batchFuture;\n\t\t\tprocessor.beginBatch();\n\t\t\tworkBuffer.clear();\n\n\t\t\tworkQueue.drainTo( workBuffer, maxTasksPerBatch );\n\n\t\t\tfor ( W workset : workBuffer ) {\n\t\t\t\ttry {\n\t\t\t\t\tworkset.submitTo( processor );\n\t\t\t\t}\n\t\t\t\tcatch (Throwable e) {\n\t\t\t\t\tworkset.markAsFailed( e );\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Nothing more to do, end the batch and terminate\n\t\t\tbatchFuture = processor.endBatch();\n\n\t\t\t/*\n\t\t\t * Wait for works to complete before trying to handle the next batch.\n\t\t\t * Note: timeout is expected to be handled by the processor\n\t\t\t * (the Elasticsearch client adds per-request timeouts, in particular),\n\t\t\t * so this \"join\" will not last forever\n\t\t\t */\n\t\t\tFutures.unwrappedExceptionJoin( batchFuture );\n\t\t}\n\t\tcatch (Throwable e) {\n\t\t\t// This will only happen if there is a bug in the processor\n\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n\t\t\tcontextBuilder.throwable( e );\n\t\t\tcontextBuilder.failingOperation( \"Work processing in executor '\" + name + \"'\" );\n\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t}\n\t\tfinally {\n\t\t\t// We're done executing this batch.\n\t\t\tif ( workQueue.isEmpty() ) {\n\t\t\t\t// We're done executing the whole queue: handle getCompletion().\n\t\t\t\tCompletableFuture<?> justFinishedQueueFuture = this.completionFuture;\n\t\t\t\tcompletionFuture = null;\n\t\t\t\tjustFinishedQueueFuture.complete( null );\n\t\t\t}\n\t\t\t// Allow this thread (or others) to run processing again.\n\t\t\tprocessingStatus.set( ProcessingStatus.IDLE );\n\t\t\tif ( !workQueue.isEmpty() ) {\n\t\t\t\t/*\n\t\t\t\t * Either the work queue wasn't empty and the \"if\" block above wasn't executed,\n\t\t\t\t * or the \"if\" block above was executed but someone submitted new work between\n\t\t\t\t * the call to workQueue.isEmpty() and the call to processingStatus.set( ... ).\n\t\t\t\t * In either case, we need to re-schedule processing, because no one else will.\n\t\t\t\t */\n\t\t\t\ttry {\n\t\t\t\t\tensureProcessingRunning();\n\t\t\t\t}\n\t\t\t\tcatch (Throwable e) {\n\t\t\t\t\t// This will only happen if there is a bug in this class, but we don't want to fail silently\n\t\t\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n\t\t\t\t\tcontextBuilder.throwable( e );\n\t\t\t\t\tcontextBuilder.failingOperation( \"Scheduling the next batch in executor '\" + name + \"'\" );\n\t\t\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}",
        "filePathBefore": "engine/src/main/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutor.java",
        "isPureRefactoring": true,
        "commitId": "92b50d26be8e3881d6c73cdc16269244834eb180",
        "packageNameBefore": "org.hibernate.search.engine.backend.orchestration.spi",
        "classNameBefore": "org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor",
        "methodNameBefore": "org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#processBatch",
        "invokedMethod": "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchParallelWorkProcessor#beginBatch\n methodBody: public void beginBatch() {\naggregator.reset();\nsequenceFutures.clear();\n}\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkSet#markAsFailed\n methodBody: void markAsFailed(Throwable t);\nmethodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchSerialWorkProcessor#endBatch\n methodBody: public CompletableFuture<Void> endBatch() {\naggregator.finalizeBulkWork();\nreturn future;\n}\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#beginBatch\n methodBody: public void beginBatch() {\n}\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#ensureProcessingRunning\n methodBody: private void ensureProcessingRunning() {\nif(!processingStatus.compareAndSet(ProcessingStatus.IDLE,ProcessingStatus.RUNNING)){return;\n}tryif(completionFuture == null){completionFuture=new CompletableFuture<>();\n}executorService.submit(this::processBatch);\ncatch(Throwable e)tryCompletableFuture<?> future=completionFuture;\ncompletionFuture=null;\nprocessingStatus.set(ProcessingStatus.IDLE);\nfuture.completeExceptionally(e);\ncatch(Throwable e2)e.addSuppressed(e2);\nthrow e;\n}\nmethodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchParallelWorkProcessor#endBatch\n methodBody: public CompletableFuture<Void> endBatch() {\nCompletableFuture<Void> future=CompletableFuture.allOf(sequenceFutures.toArray(new CompletableFuture<?>[0]));\nsequenceFutures.clear();\naggregator.startSequences();\nreturn future;\n}\nmethodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#endBatch\n methodBody: public CompletableFuture<?> endBatch() {\nif(!previousWorkSetsUncommittedWorks.isEmpty()){tryindexAccessor.commit();\ncatch(RuntimeException e)cleanUpAfterFailure(e,\"Commit after a batch of index works\");\nfinallypreviousWorkSetsUncommittedWorks.clear();\n}return CompletableFuture.completedFuture(null);\n}\nmethodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchSerialWorkProcessor#beginBatch\n methodBody: public void beginBatch() {\naggregator.reset();\n}\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkProcessor#endBatch\n methodBody: CompletableFuture<?> endBatch();\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkSet#submitTo\n methodBody: void submitTo(P processor);\nmethodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkProcessor#beginBatch\n methodBody: void beginBatch();",
        "classSignatureBefore": "public final class BatchingExecutor<W extends BatchingExecutor.WorkSet<? super P>, P extends BatchingExecutor.WorkProcessor> ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#processBatch"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor"
        ],
        "classSignatureBeforeSet": [
            "public final class BatchingExecutor<W extends BatchingExecutor.WorkSet<? super P>, P extends BatchingExecutor.WorkProcessor> "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.backend.orchestration.spi;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport org.hibernate.search.engine.logging.impl.Log;\nimport org.hibernate.search.engine.reporting.FailureContext;\nimport org.hibernate.search.engine.reporting.FailureHandler;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.impl.Closer;\nimport org.hibernate.search.engine.environment.thread.spi.ThreadPoolProvider;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * An executor of works that accepts works from multiple threads, puts them in a queue,\n * and processes them in batches in a single background thread.\n * <p>\n * Useful when works can be merged together for optimization purposes (bulking in Elasticsearch),\n * or when they should never be executed in parallel (writes to a Lucene index).\n */\npublic final class BatchingExecutor<W extends BatchingExecutor.WorkSet<? super P>, P extends BatchingExecutor.WorkProcessor> {\n\n\tprivate final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate final String name;\n\n\tprivate final P processor;\n\tprivate final FailureHandler failureHandler;\n\tprivate final int maxTasksPerBatch;\n\n\tprivate final BlockingQueue<W> workQueue;\n\tprivate final List<W> workBuffer;\n\tprivate final AtomicReference<ProcessingStatus> processingStatus;\n\n\tprivate ExecutorService executorService;\n\tprivate volatile CompletableFuture<?> completionFuture;\n\n\t/**\n\t * @param name The name of the executor thread (and of this executor when reporting errors)\n\t * @param processor A task processor. May not be thread-safe.\n\t * @param maxTasksPerBatch The maximum number of tasks to process in a single batch.\n\t * Higher values mean more opportunity for the processor to optimize execution, but higher heap consumption.\n\t * @param fair if {@code true} tasks are always submitted to the\n\t * processor in FIFO order, if {@code false} tasks submitted\n\t * when the internal queue is full may be submitted out of order.\n\t * @param failureHandler A failure handler to report failures of the background thread.\n\t */\n\tpublic BatchingExecutor(String name,\n\t\t\tP processor, int maxTasksPerBatch, boolean fair,\n\t\t\tFailureHandler failureHandler) {\n\t\tthis.name = name;\n\t\tthis.processor = processor;\n\t\tthis.failureHandler = failureHandler;\n\t\tthis.maxTasksPerBatch = maxTasksPerBatch;\n\t\tworkQueue = new ArrayBlockingQueue<>( maxTasksPerBatch, fair );\n\t\tworkBuffer = new ArrayList<>( maxTasksPerBatch );\n\t\tprocessingStatus = new AtomicReference<>( ProcessingStatus.IDLE );\n\t}\n\n\t/**\n\t * Start the executor, allowing works to be submitted\n\t * through {@link #submit(WorkSet)}.\n\t *\n\t * @param threadPoolProvider A provider of thread pools.\n\t */\n\tpublic synchronized void start(ThreadPoolProvider threadPoolProvider) {\n\t\tlog.startingExecutor( name );\n\t\texecutorService = threadPoolProvider.newFixedThreadPool( 1, name );\n\t}\n\n\t/**\n\t * Stop the executor, no longer allowing works to be submitted\n\t * through {@link #submit(WorkSet)}.\n\t * <p>\n\t * This will attempt to forcibly terminate currently executing works,\n\t * and will remove pending works from the queue.\n\t */\n\tpublic synchronized void stop() {\n\t\tlog.stoppingExecutor( name );\n\t\ttry ( Closer<RuntimeException> closer = new Closer<>() ) {\n\t\t\tcloser.push( ExecutorService::shutdownNow, executorService );\n\t\t\texecutorService = null;\n\t\t\tworkQueue.clear();\n\t\t\t// It's possible that processing was successfully scheduled in the executor service but had no chance to run,\n\t\t\t// so we need to release waiting threads:\n\t\t\tif ( completionFuture != null ) {\n\t\t\t\tcompletionFuture.cancel( false );\n\t\t\t\tcompletionFuture = null;\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Submit a set of works for execution.\n\t * <p>\n\t * Must not be called when the executor is stopped.\n\t * @param workset A set of works to execute.\n\t * @throws InterruptedException If the current thread is interrupted while enqueuing the workset.\n\t */\n\tpublic void submit(W workset) throws InterruptedException {\n\t\tif ( executorService == null ) {\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"Attempt to submit a workset to executor '\" + name + \"', which is stopped\"\n\t\t\t\t\t+ \" There is probably a bug in Hibernate Search, please report it.\"\n\t\t\t);\n\t\t}\n\t\tworkQueue.put( workset );\n\t\tensureProcessingRunning();\n\t}\n\n\t/**\n\t * @return A future that completes when all works submitted to the executor so far are completely executed.\n\t * Works submitted to the executor after entering this method may delay the wait.\n\t */\n\tpublic CompletableFuture<?> getCompletion() {\n\t\tCompletableFuture<?> future = completionFuture;\n\t\tif ( future == null ) {\n\t\t\t// No processing in progress or scheduled.\n\t\t\treturn CompletableFuture.completedFuture( null );\n\t\t}\n\t\telse {\n\t\t\t// Processing in progress or scheduled; the future will be completed when the queue becomes empty.\n\t\t\treturn future;\n\t\t}\n\t}\n\n\tprivate void ensureProcessingRunning() {\n\t\tif ( !processingStatus.compareAndSet( ProcessingStatus.IDLE, ProcessingStatus.RUNNING ) ) {\n\t\t\t// Already running\n\t\t\treturn;\n\t\t}\n\n\t\t/*\n\t\t * Our thread successfully switched the status:\n\t\t * processing wasn't in progress, and we're now responsible for scheduling it.\n\t\t */\n\t\ttry {\n\t\t\tif ( completionFuture == null ) {\n\t\t\t\t/*\n\t\t\t\t * The executor was previously idle:\n\t\t\t\t * we need to create a new future for the completion of the queue.\n\t\t\t\t * This is not executed when re-scheduling processing between two batches.\n\t\t\t\t */\n\t\t\t\tcompletionFuture = new CompletableFuture<>();\n\t\t\t}\n\t\t\texecutorService.submit( this::processBatch );\n\t\t}\n\t\tcatch (Throwable e) {\n\t\t\t/*\n\t\t\t * Make sure a failure to submit the processing task\n\t\t\t * to the executor service\n\t\t\t * doesn't leave other threads waiting indefinitely.\n\t\t\t */\n\t\t\ttry {\n\t\t\t\tCompletableFuture<?> future = completionFuture;\n\t\t\t\tcompletionFuture = null;\n\t\t\t\tprocessingStatus.set( ProcessingStatus.IDLE );\n\t\t\t\tfuture.completeExceptionally( e );\n\t\t\t}\n\t\t\tcatch (Throwable e2) {\n\t\t\t\te.addSuppressed( e2 );\n\t\t\t}\n\t\t\tthrow e;\n\t\t}\n\t}\n\n\t/**\n\t * Takes a batch of worksets from the queue and processes them.\n\t */\n\tprivate void processBatch() {\n\t\ttry {\n\t\t\tCompletableFuture<?> batchFuture;\n\t\t\tprocessor.beginBatch();\n\t\t\tworkBuffer.clear();\n\n\t\t\tworkQueue.drainTo( workBuffer, maxTasksPerBatch );\n\n\t\t\tfor ( W workset : workBuffer ) {\n\t\t\t\ttry {\n\t\t\t\t\tworkset.submitTo( processor );\n\t\t\t\t}\n\t\t\t\tcatch (Throwable e) {\n\t\t\t\t\tworkset.markAsFailed( e );\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Nothing more to do, end the batch and terminate\n\t\t\tbatchFuture = processor.endBatch();\n\n\t\t\t/*\n\t\t\t * Wait for works to complete before trying to handle the next batch.\n\t\t\t * Note: timeout is expected to be handled by the processor\n\t\t\t * (the Elasticsearch client adds per-request timeouts, in particular),\n\t\t\t * so this \"join\" will not last forever\n\t\t\t */\n\t\t\tFutures.unwrappedExceptionJoin( batchFuture );\n\t\t}\n\t\tcatch (Throwable e) {\n\t\t\t// This will only happen if there is a bug in the processor\n\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n\t\t\tcontextBuilder.throwable( e );\n\t\t\tcontextBuilder.failingOperation( \"Work processing in executor '\" + name + \"'\" );\n\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t}\n\t\tfinally {\n\t\t\t// We're done executing this batch.\n\t\t\tif ( workQueue.isEmpty() ) {\n\t\t\t\t// We're done executing the whole queue: handle getCompletion().\n\t\t\t\tCompletableFuture<?> justFinishedQueueFuture = this.completionFuture;\n\t\t\t\tcompletionFuture = null;\n\t\t\t\tjustFinishedQueueFuture.complete( null );\n\t\t\t}\n\t\t\t// Allow this thread (or others) to run processing again.\n\t\t\tprocessingStatus.set( ProcessingStatus.IDLE );\n\t\t\tif ( !workQueue.isEmpty() ) {\n\t\t\t\t/*\n\t\t\t\t * Either the work queue wasn't empty and the \"if\" block above wasn't executed,\n\t\t\t\t * or the \"if\" block above was executed but someone submitted new work between\n\t\t\t\t * the call to workQueue.isEmpty() and the call to processingStatus.set( ... ).\n\t\t\t\t * In either case, we need to re-schedule processing, because no one else will.\n\t\t\t\t */\n\t\t\t\ttry {\n\t\t\t\t\tensureProcessingRunning();\n\t\t\t\t}\n\t\t\t\tcatch (Throwable e) {\n\t\t\t\t\t// This will only happen if there is a bug in this class, but we don't want to fail silently\n\t\t\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n\t\t\t\t\tcontextBuilder.throwable( e );\n\t\t\t\t\tcontextBuilder.failingOperation( \"Scheduling the next batch in executor '\" + name + \"'\" );\n\t\t\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic interface WorkProcessor {\n\n\t\tvoid beginBatch();\n\n\t\t/**\n\t\t * Ensure all works submitted since the last call to {@link #beginBatch()} will actually be executed,\n\t\t * along with any finishing task (commit, ...).\n\t\t *\n\t\t * @return A future completing when all works submitted since the last call to {@link #beginBatch()}\n\t\t * have completed.\n\t\t */\n\t\tCompletableFuture<?> endBatch();\n\n\t}\n\n\tpublic interface WorkSet<P extends WorkProcessor> {\n\n\t\tvoid submitTo(P processor);\n\n\t\tvoid markAsFailed(Throwable t);\n\n\t}\n\n\tpublic enum ProcessingStatus {\n\n\t\tIDLE,\n\t\tRUNNING\n\n\t}\n\n}\n",
        "filePathAfter": "engine/src/main/java/org/hibernate/search/engine/backend/orchestration/spi/BatchingExecutor.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.backend.orchestration.spi;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.BlockingQueue;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.ScheduledFuture;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport org.hibernate.search.engine.logging.impl.Log;\nimport org.hibernate.search.engine.reporting.FailureContext;\nimport org.hibernate.search.engine.reporting.FailureHandler;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.impl.Closer;\nimport org.hibernate.search.engine.environment.thread.spi.ThreadPoolProvider;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * An executor of works that accepts works from multiple threads, puts them in a queue,\n * and processes them in batches in a single background thread.\n * <p>\n * Useful when works can be merged together for optimization purposes (bulking in Elasticsearch),\n * or when they should never be executed in parallel (writes to a Lucene index).\n */\npublic final class BatchingExecutor<W extends BatchingExecutor.WorkSet<? super P>, P extends BatchingExecutor.WorkProcessor> {\n\n\tprivate final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate final String name;\n\n\tprivate final P processor;\n\tprivate final FailureHandler failureHandler;\n\tprivate final int maxTasksPerBatch;\n\n\tprivate final BlockingQueue<W> workQueue;\n\tprivate final List<W> workBuffer;\n\tprivate final AtomicReference<ProcessingStatus> processingStatus;\n\n\tprivate ExecutorService executorService;\n\tprivate ScheduledExecutorService scheduledExecutorService;\n\tprivate volatile CompletableFuture<?> completionFuture;\n\tprivate volatile ScheduledFuture<?> scheduledNextProcessing;\n\n\t/**\n\t * @param name The name of the executor thread (and of this executor when reporting errors)\n\t * @param processor A task processor. May not be thread-safe.\n\t * @param maxTasksPerBatch The maximum number of tasks to process in a single batch.\n\t * Higher values mean more opportunity for the processor to optimize execution, but higher heap consumption.\n\t * @param fair if {@code true} tasks are always submitted to the\n\t * processor in FIFO order, if {@code false} tasks submitted\n\t * when the internal queue is full may be submitted out of order.\n\t * @param failureHandler A failure handler to report failures of the background thread.\n\t */\n\tpublic BatchingExecutor(String name,\n\t\t\tP processor, int maxTasksPerBatch, boolean fair,\n\t\t\tFailureHandler failureHandler) {\n\t\tthis.name = name;\n\t\tthis.processor = processor;\n\t\tthis.failureHandler = failureHandler;\n\t\tthis.maxTasksPerBatch = maxTasksPerBatch;\n\t\tworkQueue = new ArrayBlockingQueue<>( maxTasksPerBatch, fair );\n\t\tworkBuffer = new ArrayList<>( maxTasksPerBatch );\n\t\tprocessingStatus = new AtomicReference<>( ProcessingStatus.IDLE );\n\t}\n\n\t/**\n\t * Start the executor, allowing works to be submitted\n\t * through {@link #submit(WorkSet)}.\n\t *\n\t * @param threadPoolProvider A provider of thread pools.\n\t */\n\tpublic synchronized void start(ThreadPoolProvider threadPoolProvider) {\n\t\tlog.startingExecutor( name );\n\t\texecutorService = threadPoolProvider.newFixedThreadPool( 1, name );\n\t\tscheduledExecutorService = threadPoolProvider.getSharedScheduledThreadPool();\n\t}\n\n\t/**\n\t * Stop the executor, no longer allowing works to be submitted\n\t * through {@link #submit(WorkSet)}.\n\t * <p>\n\t * This will attempt to forcibly terminate currently executing works,\n\t * and will remove pending works from the queue.\n\t */\n\tpublic synchronized void stop() {\n\t\tlog.stoppingExecutor( name );\n\t\ttry ( Closer<RuntimeException> closer = new Closer<>() ) {\n\t\t\t// scheduledExecutorService is not ours to close: it's shared\n\t\t\tcloser.push( ExecutorService::shutdownNow, executorService );\n\t\t\texecutorService = null;\n\t\t\tworkQueue.clear();\n\t\t\t// It's possible that processing was successfully scheduled in the executor service but had no chance to run,\n\t\t\t// so we need to release waiting threads:\n\t\t\tif ( completionFuture != null ) {\n\t\t\t\tcompletionFuture.cancel( false );\n\t\t\t\tcompletionFuture = null;\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * Submit a set of works for execution.\n\t * <p>\n\t * Must not be called when the executor is stopped.\n\t * @param workset A set of works to execute.\n\t * @throws InterruptedException If the current thread is interrupted while enqueuing the workset.\n\t */\n\tpublic void submit(W workset) throws InterruptedException {\n\t\tif ( executorService == null ) {\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"Attempt to submit a workset to executor '\" + name + \"', which is stopped\"\n\t\t\t\t\t+ \" There is probably a bug in Hibernate Search, please report it.\"\n\t\t\t);\n\t\t}\n\t\tworkQueue.put( workset );\n\t\tensureProcessingRunning();\n\t}\n\n\t/**\n\t * @return A future that completes when all works submitted to the executor so far are completely executed.\n\t * Works submitted to the executor after entering this method may delay the wait.\n\t */\n\tpublic CompletableFuture<?> getCompletion() {\n\t\tCompletableFuture<?> future = completionFuture;\n\t\tif ( future == null ) {\n\t\t\t// No processing in progress or scheduled.\n\t\t\treturn CompletableFuture.completedFuture( null );\n\t\t}\n\t\telse {\n\t\t\t// Processing in progress or scheduled; the future will be completed when the queue becomes empty.\n\t\t\treturn future;\n\t\t}\n\t}\n\n\tprivate void ensureProcessingRunning() {\n\t\tif ( !processingStatus.compareAndSet( ProcessingStatus.IDLE, ProcessingStatus.RUNNING ) ) {\n\t\t\t// Already running\n\t\t\treturn;\n\t\t}\n\n\t\t/*\n\t\t * Our thread successfully switched the status:\n\t\t * processing wasn't in progress, and we're now responsible for scheduling it.\n\t\t */\n\t\ttry {\n\t\t\tif ( scheduledNextProcessing != null ) {\n\t\t\t\t/*\n\t\t\t\t * We scheduled processing at a later time.\n\t\t\t\t * Since we're going to execute processing right now,\n\t\t\t\t * we can cancel this scheduling.\n\t\t\t\t */\n\t\t\t\tscheduledNextProcessing.cancel( false );\n\t\t\t\tscheduledNextProcessing = null;\n\t\t\t}\n\t\t\tif ( completionFuture == null ) {\n\t\t\t\t/*\n\t\t\t\t * The executor was previously idle:\n\t\t\t\t * we need to create a new future for the completion of the queue.\n\t\t\t\t * This is not executed when re-scheduling processing between two batches.\n\t\t\t\t */\n\t\t\t\tcompletionFuture = new CompletableFuture<>();\n\t\t\t}\n\t\t\texecutorService.submit( this::process );\n\t\t}\n\t\tcatch (Throwable e) {\n\t\t\t/*\n\t\t\t * Make sure a failure to submit the processing task\n\t\t\t * to the executor service\n\t\t\t * doesn't leave other threads waiting indefinitely.\n\t\t\t */\n\t\t\ttry {\n\t\t\t\tCompletableFuture<?> future = completionFuture;\n\t\t\t\tcompletionFuture = null;\n\t\t\t\tprocessingStatus.set( ProcessingStatus.IDLE );\n\t\t\t\tfuture.completeExceptionally( e );\n\t\t\t}\n\t\t\tcatch (Throwable e2) {\n\t\t\t\te.addSuppressed( e2 );\n\t\t\t}\n\t\t\tthrow e;\n\t\t}\n\t}\n\n\t/**\n\t * Takes a batch of worksets from the queue and processes them.\n\t */\n\tprivate void process() {\n\t\ttry {\n\t\t\tworkBuffer.clear();\n\t\t\tworkQueue.drainTo( workBuffer, maxTasksPerBatch );\n\n\t\t\tif ( !workBuffer.isEmpty() ) {\n\t\t\t\tprocessBatch( workBuffer );\n\t\t\t}\n\t\t}\n\t\tcatch (Throwable e) {\n\t\t\t// This will only happen if there is a bug in the processor\n\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n\t\t\tcontextBuilder.throwable( e );\n\t\t\tcontextBuilder.failingOperation( \"Work processing in executor '\" + name + \"'\" );\n\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t}\n\t\tfinally {\n\t\t\t// We're done executing this batch.\n\t\t\ttry {\n\t\t\t\tif ( workQueue.isEmpty() ) {\n\t\t\t\t\t// We managed to process the whole queue.\n\t\t\t\t\t// Inform the processor and callers.\n\t\t\t\t\thandleCompletion();\n\t\t\t\t}\n\t\t\t\t// Allow this thread (or others) to run processing again.\n\t\t\t\tprocessingStatus.set( ProcessingStatus.IDLE );\n\t\t\t\t// Call workQueue.isEmpty() again, since its content may have changed since the last call a few lines above.\n\t\t\t\tif ( !workQueue.isEmpty() ) {\n\t\t\t\t\t// There are still worksets in the queue.\n\t\t\t\t\t// Make sure they will be processed.\n\t\t\t\t\tensureProcessingRunning();\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (Throwable e) {\n\t\t\t\t// This will only happen if there is a bug in this class, but we don't want to fail silently\n\t\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n\t\t\t\tcontextBuilder.throwable( e );\n\t\t\t\tcontextBuilder.failingOperation( \"Handling post-execution in executor '\" + name + \"'\" );\n\t\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate void processBatch(List<W> works) {\n\t\tprocessor.beginBatch();\n\n\t\tfor ( W workset : works ) {\n\t\t\ttry {\n\t\t\t\tworkset.submitTo( processor );\n\t\t\t}\n\t\t\tcatch (Throwable e) {\n\t\t\t\tworkset.markAsFailed( e );\n\t\t\t}\n\t\t}\n\n\t\t// Nothing more to do, end the batch and terminate\n\t\tCompletableFuture<?> batchFuture = processor.endBatch();\n\n\t\t/*\n\t\t * Wait for works to complete before trying to handle the next batch.\n\t\t * Note: timeout is expected to be handled by the processor\n\t\t * (the Elasticsearch client adds per-request timeouts, in particular),\n\t\t * so this \"join\" will not last forever\n\t\t */\n\t\tFutures.unwrappedExceptionJoin( batchFuture );\n\t}\n\n\tprivate void handleCompletion() {\n\t\t// First, tell the processor that we're done processing.\n\t\tlong delay = 0;\n\t\ttry {\n\t\t\tdelay = processor.completeOrDelay();\n\t\t}\n\t\tcatch (Throwable e) {\n\t\t\t// This will only happen if there is a bug in this class, but we don't want to fail silently\n\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n\t\t\tcontextBuilder.throwable( e );\n\t\t\tcontextBuilder.failingOperation( \"Calling processor.complete() in executor '\" + name + \"'\" );\n\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t}\n\n\t\tif ( delay <= 0 ) {\n\t\t\t// The processor acknowledged that all outstanding operations (commit, ...) have completed.\n\t\t\t// Tell callers of getCompletion()\n\t\t\tCompletableFuture<?> justFinishedQueueFuture = this.completionFuture;\n\t\t\tcompletionFuture = null;\n\t\t\tjustFinishedQueueFuture.complete( null );\n\t\t}\n\t\telse {\n\t\t\t// The processor requested that we wait because some outstanding operations (commit, ...)\n\t\t\t// need to be performed later.\n\t\t\tscheduledNextProcessing = scheduledExecutorService.schedule(\n\t\t\t\t\tthis::ensureProcessingRunning, delay, TimeUnit.MILLISECONDS\n\t\t\t);\n\t\t}\n\t}\n\n\tpublic interface WorkProcessor {\n\n\t\t/**\n\t\t * Initializes internal state before works are {@link WorkSet#submit(WorkSet) submitted}.\n\t\t */\n\t\tvoid beginBatch();\n\n\t\t/**\n\t\t * Ensures all works {@link WorkSet#submit(WorkSet) submitted} since the last call to {@link #beginBatch()}\n\t\t * will actually be executed, along with any finishing task (commit, ...).\n\t\t *\n\t\t * @return A future completing when the executor is allowed to start another batch.\n\t\t */\n\t\tCompletableFuture<?> endBatch();\n\n\t\t/**\n\t\t * Executes any outstanding operation if possible, or return an estimation of when they can be executed.\n\t\t * <p>\n\t\t * Called when the executor considers the work queue complete\n\t\t * and does not plan on submitting another batch due to work starvation.\n\t\t *\n\t\t * @return {@code 0} if there is no outstanding operation, or a positive number of milliseconds\n\t\t * if there are outstanding operations and {@link #completeOrDelay()}\n\t\t * must be called again that many milliseconds later.\n\t\t */\n\t\tlong completeOrDelay();\n\n\t}\n\n\tpublic interface WorkSet<P extends WorkProcessor> {\n\n\t\tvoid submitTo(P processor);\n\n\t\tvoid markAsFailed(Throwable t);\n\n\t}\n\n\tpublic enum ProcessingStatus {\n\n\t\tIDLE,\n\t\tRUNNING\n\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "private void processBatch(List<W> works) {\n\t\tprocessor.beginBatch();\n\n\t\tfor ( W workset : works ) {\n\t\t\ttry {\n\t\t\t\tworkset.submitTo( processor );\n\t\t\t}\n\t\t\tcatch (Throwable e) {\n\t\t\t\tworkset.markAsFailed( e );\n\t\t\t}\n\t\t}\n\n\t\t// Nothing more to do, end the batch and terminate\n\t\tCompletableFuture<?> batchFuture = processor.endBatch();\n\n\t\t/*\n\t\t * Wait for works to complete before trying to handle the next batch.\n\t\t * Note: timeout is expected to be handled by the processor\n\t\t * (the Elasticsearch client adds per-request timeouts, in particular),\n\t\t * so this \"join\" will not last forever\n\t\t */\n\t\tFutures.unwrappedExceptionJoin( batchFuture );\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchParallelWorkProcessor#beginBatch\n methodBody: public void beginBatch() {\naggregator.reset();\nsequenceFutures.clear();\n}",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkSet#markAsFailed\n methodBody: void markAsFailed(Throwable t);",
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchSerialWorkProcessor#endBatch\n methodBody: public CompletableFuture<Void> endBatch() {\naggregator.finalizeBulkWork();\nreturn future;\n}",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#beginBatch\n methodBody: public void beginBatch() {\n}",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor#ensureProcessingRunning\n methodBody: private void ensureProcessingRunning() {\nif(!processingStatus.compareAndSet(ProcessingStatus.IDLE,ProcessingStatus.RUNNING)){return;\n}tryif(completionFuture == null){completionFuture=new CompletableFuture<>();\n}executorService.submit(this::processBatch);\ncatch(Throwable e)tryCompletableFuture<?> future=completionFuture;\ncompletionFuture=null;\nprocessingStatus.set(ProcessingStatus.IDLE);\nfuture.completeExceptionally(e);\ncatch(Throwable e2)e.addSuppressed(e2);\nthrow e;\n}",
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchParallelWorkProcessor#endBatch\n methodBody: public CompletableFuture<Void> endBatch() {\nCompletableFuture<Void> future=CompletableFuture.allOf(sequenceFutures.toArray(new CompletableFuture<?>[0]));\nsequenceFutures.clear();\naggregator.startSequences();\nreturn future;\n}",
            "methodSignature: org.hibernate.search.backend.lucene.orchestration.impl.LuceneWriteWorkProcessor#endBatch\n methodBody: public CompletableFuture<?> endBatch() {\nif(!previousWorkSetsUncommittedWorks.isEmpty()){tryindexAccessor.commit();\ncatch(RuntimeException e)cleanUpAfterFailure(e,\"Commit after a batch of index works\");\nfinallypreviousWorkSetsUncommittedWorks.clear();\n}return CompletableFuture.completedFuture(null);\n}",
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchSerialWorkProcessor#beginBatch\n methodBody: public void beginBatch() {\naggregator.reset();\n}",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkProcessor#endBatch\n methodBody: CompletableFuture<?> endBatch();",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkSet#submitTo\n methodBody: void submitTo(P processor);",
            "methodSignature: org.hibernate.search.engine.backend.orchestration.spi.BatchingExecutor.WorkProcessor#beginBatch\n methodBody: void beginBatch();"
        ],
        "sourceCodeAfterRefactoring": "/**\n\t * Takes a batch of worksets from the queue and processes them.\n\t */\n\tprivate void process() {\n\t\ttry {\n\t\t\tworkBuffer.clear();\n\t\t\tworkQueue.drainTo( workBuffer, maxTasksPerBatch );\n\n\t\t\tif ( !workBuffer.isEmpty() ) {\n\t\t\t\tprocessBatch( workBuffer );\n\t\t\t}\n\t\t}\n\t\tcatch (Throwable e) {\n\t\t\t// This will only happen if there is a bug in the processor\n\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n\t\t\tcontextBuilder.throwable( e );\n\t\t\tcontextBuilder.failingOperation( \"Work processing in executor '\" + name + \"'\" );\n\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t}\n\t\tfinally {\n\t\t\t// We're done executing this batch.\n\t\t\ttry {\n\t\t\t\tif ( workQueue.isEmpty() ) {\n\t\t\t\t\t// We managed to process the whole queue.\n\t\t\t\t\t// Inform the processor and callers.\n\t\t\t\t\thandleCompletion();\n\t\t\t\t}\n\t\t\t\t// Allow this thread (or others) to run processing again.\n\t\t\t\tprocessingStatus.set( ProcessingStatus.IDLE );\n\t\t\t\t// Call workQueue.isEmpty() again, since its content may have changed since the last call a few lines above.\n\t\t\t\tif ( !workQueue.isEmpty() ) {\n\t\t\t\t\t// There are still worksets in the queue.\n\t\t\t\t\t// Make sure they will be processed.\n\t\t\t\t\tensureProcessingRunning();\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (Throwable e) {\n\t\t\t\t// This will only happen if there is a bug in this class, but we don't want to fail silently\n\t\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n\t\t\t\tcontextBuilder.throwable( e );\n\t\t\t\tcontextBuilder.failingOperation( \"Handling post-execution in executor '\" + name + \"'\" );\n\t\t\t\tfailureHandler.handle( contextBuilder.build() );\n\t\t\t}\n\t\t}\n\t}\nprivate void processBatch(List<W> works) {\n\t\tprocessor.beginBatch();\n\n\t\tfor ( W workset : works ) {\n\t\t\ttry {\n\t\t\t\tworkset.submitTo( processor );\n\t\t\t}\n\t\t\tcatch (Throwable e) {\n\t\t\t\tworkset.markAsFailed( e );\n\t\t\t}\n\t\t}\n\n\t\t// Nothing more to do, end the batch and terminate\n\t\tCompletableFuture<?> batchFuture = processor.endBatch();\n\n\t\t/*\n\t\t * Wait for works to complete before trying to handle the next batch.\n\t\t * Note: timeout is expected to be handled by the processor\n\t\t * (the Elasticsearch client adds per-request timeouts, in particular),\n\t\t * so this \"join\" will not last forever\n\t\t */\n\t\tFutures.unwrappedExceptionJoin( batchFuture );\n\t}",
        "diffSourceCode": "-  180: \t/**\n-  181: \t * Takes a batch of worksets from the queue and processes them.\n-  182: \t */\n-  183: \tprivate void processBatch() {\n-  184: \t\ttry {\n-  185: \t\t\tCompletableFuture<?> batchFuture;\n-  186: \t\t\tprocessor.beginBatch();\n-  187: \t\t\tworkBuffer.clear();\n-  188: \n-  189: \t\t\tworkQueue.drainTo( workBuffer, maxTasksPerBatch );\n-  190: \n-  191: \t\t\tfor ( W workset : workBuffer ) {\n-  192: \t\t\t\ttry {\n-  193: \t\t\t\t\tworkset.submitTo( processor );\n-  194: \t\t\t\t}\n-  195: \t\t\t\tcatch (Throwable e) {\n-  196: \t\t\t\t\tworkset.markAsFailed( e );\n-  197: \t\t\t\t}\n-  198: \t\t\t}\n-  199: \n-  200: \t\t\t// Nothing more to do, end the batch and terminate\n-  201: \t\t\tbatchFuture = processor.endBatch();\n-  202: \n-  203: \t\t\t/*\n-  204: \t\t\t * Wait for works to complete before trying to handle the next batch.\n-  205: \t\t\t * Note: timeout is expected to be handled by the processor\n-  206: \t\t\t * (the Elasticsearch client adds per-request timeouts, in particular),\n-  207: \t\t\t * so this \"join\" will not last forever\n-  208: \t\t\t */\n-  209: \t\t\tFutures.unwrappedExceptionJoin( batchFuture );\n-  210: \t\t}\n-  211: \t\tcatch (Throwable e) {\n-  212: \t\t\t// This will only happen if there is a bug in the processor\n-  213: \t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n-  214: \t\t\tcontextBuilder.throwable( e );\n-  215: \t\t\tcontextBuilder.failingOperation( \"Work processing in executor '\" + name + \"'\" );\n-  216: \t\t\tfailureHandler.handle( contextBuilder.build() );\n-  217: \t\t}\n-  218: \t\tfinally {\n-  219: \t\t\t// We're done executing this batch.\n-  220: \t\t\tif ( workQueue.isEmpty() ) {\n-  221: \t\t\t\t// We're done executing the whole queue: handle getCompletion().\n-  222: \t\t\t\tCompletableFuture<?> justFinishedQueueFuture = this.completionFuture;\n-  223: \t\t\t\tcompletionFuture = null;\n-  224: \t\t\t\tjustFinishedQueueFuture.complete( null );\n-  225: \t\t\t}\n-  226: \t\t\t// Allow this thread (or others) to run processing again.\n-  227: \t\t\tprocessingStatus.set( ProcessingStatus.IDLE );\n-  228: \t\t\tif ( !workQueue.isEmpty() ) {\n-  229: \t\t\t\t/*\n-  230: \t\t\t\t * Either the work queue wasn't empty and the \"if\" block above wasn't executed,\n-  231: \t\t\t\t * or the \"if\" block above was executed but someone submitted new work between\n-  232: \t\t\t\t * the call to workQueue.isEmpty() and the call to processingStatus.set( ... ).\n-  233: \t\t\t\t * In either case, we need to re-schedule processing, because no one else will.\n-  234: \t\t\t\t */\n-  235: \t\t\t\ttry {\n-  236: \t\t\t\t\tensureProcessingRunning();\n-  237: \t\t\t\t}\n-  238: \t\t\t\tcatch (Throwable e) {\n-  239: \t\t\t\t\t// This will only happen if there is a bug in this class, but we don't want to fail silently\n-  240: \t\t\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n-  241: \t\t\t\t\tcontextBuilder.throwable( e );\n-  242: \t\t\t\t\tcontextBuilder.failingOperation( \"Scheduling the next batch in executor '\" + name + \"'\" );\n-  243: \t\t\t\t\tfailureHandler.handle( contextBuilder.build() );\n-  244: \t\t\t\t}\n-  245: \t\t\t}\n-  246: \t\t}\n-  247: \t}\n-  248: \n-  249: \tpublic interface WorkProcessor {\n-  250: \n-  251: \t\tvoid beginBatch();\n-  252: \n-  253: \t\t/**\n-  254: \t\t * Ensure all works submitted since the last call to {@link #beginBatch()} will actually be executed,\n-  255: \t\t * along with any finishing task (commit, ...).\n-  256: \t\t *\n-  257: \t\t * @return A future completing when all works submitted since the last call to {@link #beginBatch()}\n-  258: \t\t * have completed.\n-  259: \t\t */\n-  260: \t\tCompletableFuture<?> endBatch();\n-  261: \n-  262: \t}\n-  263: \n-  264: \tpublic interface WorkSet<P extends WorkProcessor> {\n+  180: \t\t\t * to the executor service\n+  181: \t\t\t * doesn't leave other threads waiting indefinitely.\n+  182: \t\t\t */\n+  183: \t\t\ttry {\n+  184: \t\t\t\tCompletableFuture<?> future = completionFuture;\n+  185: \t\t\t\tcompletionFuture = null;\n+  186: \t\t\t\tprocessingStatus.set( ProcessingStatus.IDLE );\n+  187: \t\t\t\tfuture.completeExceptionally( e );\n+  188: \t\t\t}\n+  189: \t\t\tcatch (Throwable e2) {\n+  190: \t\t\t\te.addSuppressed( e2 );\n+  191: \t\t\t}\n+  192: \t\t\tthrow e;\n+  193: \t\t}\n+  194: \t}\n+  195: \n+  196: \t/**\n+  197: \t * Takes a batch of worksets from the queue and processes them.\n+  198: \t */\n+  199: \tprivate void process() {\n+  200: \t\ttry {\n+  201: \t\t\tworkBuffer.clear();\n+  202: \t\t\tworkQueue.drainTo( workBuffer, maxTasksPerBatch );\n+  203: \n+  204: \t\t\tif ( !workBuffer.isEmpty() ) {\n+  205: \t\t\t\tprocessBatch( workBuffer );\n+  206: \t\t\t}\n+  207: \t\t}\n+  208: \t\tcatch (Throwable e) {\n+  209: \t\t\t// This will only happen if there is a bug in the processor\n+  210: \t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n+  211: \t\t\tcontextBuilder.throwable( e );\n+  212: \t\t\tcontextBuilder.failingOperation( \"Work processing in executor '\" + name + \"'\" );\n+  213: \t\t\tfailureHandler.handle( contextBuilder.build() );\n+  214: \t\t}\n+  215: \t\tfinally {\n+  216: \t\t\t// We're done executing this batch.\n+  217: \t\t\ttry {\n+  218: \t\t\t\tif ( workQueue.isEmpty() ) {\n+  219: \t\t\t\t\t// We managed to process the whole queue.\n+  220: \t\t\t\t\t// Inform the processor and callers.\n+  221: \t\t\t\t\thandleCompletion();\n+  222: \t\t\t\t}\n+  223: \t\t\t\t// Allow this thread (or others) to run processing again.\n+  224: \t\t\t\tprocessingStatus.set( ProcessingStatus.IDLE );\n+  225: \t\t\t\t// Call workQueue.isEmpty() again, since its content may have changed since the last call a few lines above.\n+  226: \t\t\t\tif ( !workQueue.isEmpty() ) {\n+  227: \t\t\t\t\t// There are still worksets in the queue.\n+  228: \t\t\t\t\t// Make sure they will be processed.\n+  229: \t\t\t\t\tensureProcessingRunning();\n+  230: \t\t\t\t}\n+  231: \t\t\t}\n+  232: \t\t\tcatch (Throwable e) {\n+  233: \t\t\t\t// This will only happen if there is a bug in this class, but we don't want to fail silently\n+  234: \t\t\t\tFailureContext.Builder contextBuilder = FailureContext.builder();\n+  235: \t\t\t\tcontextBuilder.throwable( e );\n+  236: \t\t\t\tcontextBuilder.failingOperation( \"Handling post-execution in executor '\" + name + \"'\" );\n+  237: \t\t\t\tfailureHandler.handle( contextBuilder.build() );\n+  238: \t\t\t}\n+  239: \t\t}\n+  240: \t}\n+  241: \n+  242: \tprivate void processBatch(List<W> works) {\n+  243: \t\tprocessor.beginBatch();\n+  244: \n+  245: \t\tfor ( W workset : works ) {\n+  246: \t\t\ttry {\n+  247: \t\t\t\tworkset.submitTo( processor );\n+  248: \t\t\t}\n+  249: \t\t\tcatch (Throwable e) {\n+  250: \t\t\t\tworkset.markAsFailed( e );\n+  251: \t\t\t}\n+  252: \t\t}\n+  253: \n+  254: \t\t// Nothing more to do, end the batch and terminate\n+  255: \t\tCompletableFuture<?> batchFuture = processor.endBatch();\n+  256: \n+  257: \t\t/*\n+  258: \t\t * Wait for works to complete before trying to handle the next batch.\n+  259: \t\t * Note: timeout is expected to be handled by the processor\n+  260: \t\t * (the Elasticsearch client adds per-request timeouts, in particular),\n+  261: \t\t * so this \"join\" will not last forever\n+  262: \t\t */\n+  263: \t\tFutures.unwrappedExceptionJoin( batchFuture );\n+  264: \t}\n",
        "uniqueId": "92b50d26be8e3881d6c73cdc16269244834eb180_180_247_242_264_196_240",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 24,
                "covered": 92
            },
            "BRANCH": {
                "missed": 0,
                "covered": 6
            },
            "LINE": {
                "missed": 5,
                "covered": 25
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 4
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tpublic awaitIndexingAssertions(atMost Duration, assertions ThrowingRunnable) : void extracted from public awaitIndexingAssertions(assertions ThrowingRunnable) : void in class org.hibernate.search.util.impl.integrationtest.common.extension.BackendIndexingWorkExpectations",
        "diffLocations": [
            {
                "filePath": "util/internal/integrationtest/common/src/main/java/org/hibernate/search/util/impl/integrationtest/common/extension/BackendIndexingWorkExpectations.java",
                "startLine": 50,
                "endLine": 69,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/integrationtest/common/src/main/java/org/hibernate/search/util/impl/integrationtest/common/extension/BackendIndexingWorkExpectations.java",
                "startLine": 50,
                "endLine": 52,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "util/internal/integrationtest/common/src/main/java/org/hibernate/search/util/impl/integrationtest/common/extension/BackendIndexingWorkExpectations.java",
                "startLine": 54,
                "endLine": 73,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public void awaitIndexingAssertions(ThrowingRunnable assertions) {\n\t\tif ( sync ) {\n\t\t\ttry {\n\t\t\t\tassertions.run();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tthrow Throwables.toRuntimeException( t );\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tawait( \"Waiting for indexing assertions\" )\n\t\t\t\t\t.pollDelay( Duration.ZERO )\n\t\t\t\t\t// Most of the time the assertions\n\t\t\t\t\t// are only about in-memory state (i.e. the CallQueues in BackendMock),\n\t\t\t\t\t// so it's fine to poll aggressively every 5ms.\n\t\t\t\t\t.pollInterval( Duration.ofMillis( 5 ) )\n\t\t\t\t\t.atMost( Duration.ofSeconds( 30 ) )\n\t\t\t\t\t.untilAsserted( assertions );\n\t\t}\n\t}",
        "filePathBefore": "util/internal/integrationtest/common/src/main/java/org/hibernate/search/util/impl/integrationtest/common/extension/BackendIndexingWorkExpectations.java",
        "isPureRefactoring": true,
        "commitId": "fee1c5f90c639ec7fe30699873788b892b84e4c7",
        "packageNameBefore": "org.hibernate.search.util.impl.integrationtest.common.extension",
        "classNameBefore": "org.hibernate.search.util.impl.integrationtest.common.extension.BackendIndexingWorkExpectations",
        "methodNameBefore": "org.hibernate.search.util.impl.integrationtest.common.extension.BackendIndexingWorkExpectations#awaitIndexingAssertions",
        "classSignatureBefore": "public final class BackendIndexingWorkExpectations ",
        "methodNameBeforeSet": [
            "org.hibernate.search.util.impl.integrationtest.common.extension.BackendIndexingWorkExpectations#awaitIndexingAssertions"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.util.impl.integrationtest.common.extension.BackendIndexingWorkExpectations"
        ],
        "classSignatureBeforeSet": [
            "public final class BackendIndexingWorkExpectations "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics",
                "description": "All the mappings are matched! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.integrationtest.common.extension;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.awaitility.Awaitility.await;\n\nimport java.time.Duration;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.regex.Pattern;\n\nimport org.hibernate.search.util.common.impl.Throwables;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubDocumentWork;\n\nimport org.awaitility.core.ThrowingRunnable;\n\npublic final class BackendIndexingWorkExpectations {\n\n\tpublic static BackendIndexingWorkExpectations sync() {\n\t\treturn new BackendIndexingWorkExpectations( true, null, StubDocumentWork.Type.ADD );\n\t}\n\n\tpublic static BackendIndexingWorkExpectations async(String threadNamePattern) {\n\t\treturn async( threadNamePattern, StubDocumentWork.Type.ADD );\n\t}\n\n\tpublic static BackendIndexingWorkExpectations async(String threadNamePattern, StubDocumentWork.Type addWorkType) {\n\t\treturn new BackendIndexingWorkExpectations( false, Pattern.compile( threadNamePattern ), addWorkType );\n\t}\n\n\tprivate final boolean sync;\n\tprivate final Pattern expectedThreadNamePattern;\n\tfinal StubDocumentWork.Type addWorkType;\n\n\tprivate BackendIndexingWorkExpectations(boolean sync, Pattern expectedThreadNamePattern,\n\t\t\tStubDocumentWork.Type addWorkType) {\n\t\tthis.sync = sync;\n\t\tthis.expectedThreadNamePattern = expectedThreadNamePattern;\n\t\tthis.addWorkType = addWorkType;\n\t}\n\n\tpublic boolean allowDuplicateIndexing() {\n\t\treturn !sync;\n\t}\n\n\tpublic void awaitIndexingAssertions(ThrowingRunnable assertions) {\n\t\tif ( sync ) {\n\t\t\ttry {\n\t\t\t\tassertions.run();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tthrow Throwables.toRuntimeException( t );\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tawait( \"Waiting for indexing assertions\" )\n\t\t\t\t\t.pollDelay( Duration.ZERO )\n\t\t\t\t\t// Most of the time the assertions\n\t\t\t\t\t// are only about in-memory state (i.e. the CallQueues in BackendMock),\n\t\t\t\t\t// so it's fine to poll aggressively every 5ms.\n\t\t\t\t\t.pollInterval( Duration.ofMillis( 5 ) )\n\t\t\t\t\t.atMost( Duration.ofSeconds( 30 ) )\n\t\t\t\t\t.untilAsserted( assertions );\n\t\t}\n\t}\n\n\tpublic void awaitBackgroundIndexingCompletion(CompletableFuture<?> completion) {\n\t\tif ( sync ) {\n\t\t\treturn;\n\t\t}\n\t\telse {\n\t\t\tawait( \"Waiting for background process completion\" )\n\t\t\t\t\t.pollDelay( Duration.ZERO )\n\t\t\t\t\t// We're only waiting for in-memory state to change,\n\t\t\t\t\t// so it's fine to poll aggressively every 5ms.\n\t\t\t\t\t.pollInterval( Duration.ofMillis( 5 ) )\n\t\t\t\t\t.atMost( Duration.ofSeconds( 30 ) )\n\t\t\t\t\t.until( completion::isDone );\n\t\t}\n\t}\n\n\tvoid checkCurrentThread(Object work) {\n\t\tif ( expectedThreadNamePattern == null ) {\n\t\t\treturn;\n\t\t}\n\t\tassertThat( Thread.currentThread().getName() )\n\t\t\t\t.as( \"Name of current thread when executing work \" + work )\n\t\t\t\t.matches( expectedThreadNamePattern );\n\t}\n}\n",
        "filePathAfter": "util/internal/integrationtest/common/src/main/java/org/hibernate/search/util/impl/integrationtest/common/extension/BackendIndexingWorkExpectations.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.util.impl.integrationtest.common.extension;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.awaitility.Awaitility.await;\n\nimport java.time.Duration;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.regex.Pattern;\n\nimport org.hibernate.search.util.common.impl.Throwables;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubDocumentWork;\n\nimport org.awaitility.core.ThrowingRunnable;\n\npublic final class BackendIndexingWorkExpectations {\n\n\tpublic static BackendIndexingWorkExpectations sync() {\n\t\treturn new BackendIndexingWorkExpectations( true, null, StubDocumentWork.Type.ADD );\n\t}\n\n\tpublic static BackendIndexingWorkExpectations async(String threadNamePattern) {\n\t\treturn async( threadNamePattern, StubDocumentWork.Type.ADD );\n\t}\n\n\tpublic static BackendIndexingWorkExpectations async(String threadNamePattern, StubDocumentWork.Type addWorkType) {\n\t\treturn new BackendIndexingWorkExpectations( false, Pattern.compile( threadNamePattern ), addWorkType );\n\t}\n\n\tprivate final boolean sync;\n\tprivate final Pattern expectedThreadNamePattern;\n\tfinal StubDocumentWork.Type addWorkType;\n\n\tprivate BackendIndexingWorkExpectations(boolean sync, Pattern expectedThreadNamePattern,\n\t\t\tStubDocumentWork.Type addWorkType) {\n\t\tthis.sync = sync;\n\t\tthis.expectedThreadNamePattern = expectedThreadNamePattern;\n\t\tthis.addWorkType = addWorkType;\n\t}\n\n\tpublic boolean allowDuplicateIndexing() {\n\t\treturn !sync;\n\t}\n\n\tpublic void awaitIndexingAssertions(ThrowingRunnable assertions) {\n\t\tawaitIndexingAssertions( Duration.ofSeconds( 30 ), assertions );\n\t}\n\n\tpublic void awaitIndexingAssertions(Duration atMost, ThrowingRunnable assertions) {\n\t\tif ( sync ) {\n\t\t\ttry {\n\t\t\t\tassertions.run();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tthrow Throwables.toRuntimeException( t );\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tawait( \"Waiting for indexing assertions\" )\n\t\t\t\t\t.pollDelay( Duration.ZERO )\n\t\t\t\t\t// Most of the time the assertions\n\t\t\t\t\t// are only about in-memory state (i.e. the CallQueues in BackendMock),\n\t\t\t\t\t// so it's fine to poll aggressively every 5ms.\n\t\t\t\t\t.pollInterval( Duration.ofMillis( 5 ) )\n\t\t\t\t\t.atMost( atMost )\n\t\t\t\t\t.untilAsserted( assertions );\n\t\t}\n\t}\n\n\tpublic void awaitBackgroundIndexingCompletion(CompletableFuture<?> completion) {\n\t\tif ( sync ) {\n\t\t\treturn;\n\t\t}\n\t\telse {\n\t\t\tawait( \"Waiting for background process completion\" )\n\t\t\t\t\t.pollDelay( Duration.ZERO )\n\t\t\t\t\t// We're only waiting for in-memory state to change,\n\t\t\t\t\t// so it's fine to poll aggressively every 5ms.\n\t\t\t\t\t.pollInterval( Duration.ofMillis( 5 ) )\n\t\t\t\t\t.atMost( Duration.ofSeconds( 30 ) )\n\t\t\t\t\t.until( completion::isDone );\n\t\t}\n\t}\n\n\tvoid checkCurrentThread(Object work) {\n\t\tif ( expectedThreadNamePattern == null ) {\n\t\t\treturn;\n\t\t}\n\t\tassertThat( Thread.currentThread().getName() )\n\t\t\t\t.as( \"Name of current thread when executing work \" + work )\n\t\t\t\t.matches( expectedThreadNamePattern );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "public void awaitIndexingAssertions(Duration atMost, ThrowingRunnable assertions) {\n\t\tif ( sync ) {\n\t\t\ttry {\n\t\t\t\tassertions.run();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tthrow Throwables.toRuntimeException( t );\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tawait( \"Waiting for indexing assertions\" )\n\t\t\t\t\t.pollDelay( Duration.ZERO )\n\t\t\t\t\t// Most of the time the assertions\n\t\t\t\t\t// are only about in-memory state (i.e. the CallQueues in BackendMock),\n\t\t\t\t\t// so it's fine to poll aggressively every 5ms.\n\t\t\t\t\t.pollInterval( Duration.ofMillis( 5 ) )\n\t\t\t\t\t.atMost( atMost )\n\t\t\t\t\t.untilAsserted( assertions );\n\t\t}\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "public void awaitIndexingAssertions(ThrowingRunnable assertions) {\n\t\tawaitIndexingAssertions( Duration.ofSeconds( 30 ), assertions );\n\t}\npublic void awaitIndexingAssertions(Duration atMost, ThrowingRunnable assertions) {\n\t\tif ( sync ) {\n\t\t\ttry {\n\t\t\t\tassertions.run();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tthrow Throwables.toRuntimeException( t );\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tawait( \"Waiting for indexing assertions\" )\n\t\t\t\t\t.pollDelay( Duration.ZERO )\n\t\t\t\t\t// Most of the time the assertions\n\t\t\t\t\t// are only about in-memory state (i.e. the CallQueues in BackendMock),\n\t\t\t\t\t// so it's fine to poll aggressively every 5ms.\n\t\t\t\t\t.pollInterval( Duration.ofMillis( 5 ) )\n\t\t\t\t\t.atMost( atMost )\n\t\t\t\t\t.untilAsserted( assertions );\n\t\t}\n\t}",
        "diffSourceCode": "    50: \tpublic void awaitIndexingAssertions(ThrowingRunnable assertions) {\n-   51: \t\tif ( sync ) {\n-   52: \t\t\ttry {\n-   53: \t\t\t\tassertions.run();\n-   54: \t\t\t}\n-   55: \t\t\tcatch (Throwable t) {\n-   56: \t\t\t\tthrow Throwables.toRuntimeException( t );\n-   57: \t\t\t}\n-   58: \t\t}\n-   59: \t\telse {\n-   60: \t\t\tawait( \"Waiting for indexing assertions\" )\n-   61: \t\t\t\t\t.pollDelay( Duration.ZERO )\n-   62: \t\t\t\t\t// Most of the time the assertions\n-   63: \t\t\t\t\t// are only about in-memory state (i.e. the CallQueues in BackendMock),\n-   64: \t\t\t\t\t// so it's fine to poll aggressively every 5ms.\n-   65: \t\t\t\t\t.pollInterval( Duration.ofMillis( 5 ) )\n-   66: \t\t\t\t\t.atMost( Duration.ofSeconds( 30 ) )\n-   67: \t\t\t\t\t.untilAsserted( assertions );\n-   68: \t\t}\n-   69: \t}\n-   70: \n-   71: \tpublic void awaitBackgroundIndexingCompletion(CompletableFuture<?> completion) {\n-   72: \t\tif ( sync ) {\n-   73: \t\t\treturn;\n+   51: \t\tawaitIndexingAssertions( Duration.ofSeconds( 30 ), assertions );\n+   52: \t}\n+   53: \n+   54: \tpublic void awaitIndexingAssertions(Duration atMost, ThrowingRunnable assertions) {\n+   55: \t\tif ( sync ) {\n+   56: \t\t\ttry {\n+   57: \t\t\t\tassertions.run();\n+   58: \t\t\t}\n+   59: \t\t\tcatch (Throwable t) {\n+   60: \t\t\t\tthrow Throwables.toRuntimeException( t );\n+   61: \t\t\t}\n+   62: \t\t}\n+   63: \t\telse {\n+   64: \t\t\tawait( \"Waiting for indexing assertions\" )\n+   65: \t\t\t\t\t.pollDelay( Duration.ZERO )\n+   66: \t\t\t\t\t// Most of the time the assertions\n+   67: \t\t\t\t\t// are only about in-memory state (i.e. the CallQueues in BackendMock),\n+   68: \t\t\t\t\t// so it's fine to poll aggressively every 5ms.\n+   69: \t\t\t\t\t.pollInterval( Duration.ofMillis( 5 ) )\n+   70: \t\t\t\t\t.atMost( atMost )\n+   71: \t\t\t\t\t.untilAsserted( assertions );\n+   72: \t\t}\n+   73: \t}\n",
        "uniqueId": "fee1c5f90c639ec7fe30699873788b892b84e4c7_50_69_54_73_50_52",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 4,
                "covered": 19
            },
            "BRANCH": {
                "missed": 0,
                "covered": 2
            },
            "LINE": {
                "missed": 2,
                "covered": 9
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 2
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate initDataSimple(index SimpleMappedIndex<IndexBinding>) : void extracted from private initData() : void in class org.hibernate.search.integrationtest.backend.lucene.vector.LuceneVectorFieldIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/vector/LuceneVectorFieldIT.java",
                "startLine": 128,
                "endLine": 204,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/vector/LuceneVectorFieldIT.java",
                "startLine": 371,
                "endLine": 434,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/vector/LuceneVectorFieldIT.java",
                "startLine": 286,
                "endLine": 299,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void initData() {\n\t\tindex.bulkIndexer()\n\t\t\t\t.add( \"ID:1\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().string, \"keyword1\" );\n\t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_1 );\n\t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_1 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:2\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().string, \"keyword2\" );\n\t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_2 );\n\t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_2 );\n\t\t\t\t} )\n\t\t\t\t.join();\n\n\t\tpredicateIndex.bulkIndexer()\n\t\t\t\t.add( \"ID:1\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 5.2f, 4.4f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 5 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:2\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 5.2f, 3.9f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, false );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 4 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:3\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 4.9f, 3.4f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 9 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:4\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 4.2f, 4.6f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, false );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 6 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:5\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 3.3f, 4.5f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 8 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:6\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 6.4f, 3.4f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 9 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:7\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 4.2f, 6.2f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 5 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:8\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 2.4f, 4.0f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 8 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:9\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 1.4f, 3.2f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, false );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 5 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:10\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 7.0f, 9.9f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 9 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:11\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 3.0f, 2.3f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, false );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 6 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:12\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 5.0f, 1.0f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 3 );\n\t\t\t\t} )\n\t\t\t\t.join();\n\t}",
        "filePathBefore": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/vector/LuceneVectorFieldIT.java",
        "isPureRefactoring": true,
        "commitId": "f7c9efb7912be0d92bb8a29654ac537d5be9683f",
        "packageNameBefore": "org.hibernate.search.integrationtest.backend.lucene.vector",
        "classNameBefore": "org.hibernate.search.integrationtest.backend.lucene.vector.LuceneVectorFieldIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.backend.lucene.vector.LuceneVectorFieldIT#initData",
        "classSignatureBefore": "class LuceneVectorFieldIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.lucene.vector.LuceneVectorFieldIT#initData"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.backend.lucene.vector.LuceneVectorFieldIT"
        ],
        "classSignatureBeforeSet": [
            "class LuceneVectorFieldIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.lucene.vector;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.hibernate.search.integrationtest.backend.lucene.testsupport.util.DocumentAssert.containsDocument;\n\nimport java.util.List;\n\nimport org.hibernate.search.backend.lucene.LuceneExtension;\nimport org.hibernate.search.engine.backend.document.IndexFieldReference;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.types.Projectable;\nimport org.hibernate.search.engine.backend.types.VectorSimilarity;\nimport org.hibernate.search.engine.search.query.SearchQuery;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.extension.SearchSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.stub.SimpleMappedIndex;\n\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\nimport org.junit.jupiter.api.extension.RegisterExtension;\n\nimport org.apache.lucene.document.Document;\n\nclass LuceneVectorFieldIT {\n\n\tprivate static final byte[] BYTE_VECTOR_1 = new byte[] { 1, 2, 3, 4 };\n\tprivate static final byte[] BYTE_VECTOR_2 = new byte[] { 1, 1, 1, 1 };\n\n\tprivate static final float[] FLOAT_VECTOR_1 = new float[] { 1.0f, 2.0f, 3.0f, 4.0f, 1.0f, 2.0f, 3.0f, 4.0f };\n\tprivate static final float[] FLOAT_VECTOR_2 = new float[] { 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f };\n\n\t@RegisterExtension\n\tpublic final SearchSetupHelper setupHelper = SearchSetupHelper.create();\n\n\tprivate final SimpleMappedIndex<IndexBinding> index = SimpleMappedIndex.of( IndexBinding::new )\n\t\t\t.name( \"index\" );\n\tprivate final SimpleMappedIndex<PredicateIndexBinding> predicateIndex = SimpleMappedIndex.of( PredicateIndexBinding::new )\n\t\t\t.name( \"predicateIndex\" );\n\n\t@BeforeEach\n\tvoid setup() {\n\t\tsetupHelper.start()\n\t\t\t\t.withIndexes( index, predicateIndex )\n\t\t\t\t.setup();\n\t\tinitData();\n\t}\n\n\t@Test\n\tvoid simpleVectorSavedAndRetrieved() {\n\t\tSearchQuery<Document> query = index.createScope().query()\n\t\t\t\t.select(\n\t\t\t\t\t\tf -> f.extension( LuceneExtension.get() ).document()\n\t\t\t\t)\n\t\t\t\t.where( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\n\t\tList<Document> result = query.fetchAll().hits();\n\t\tassertThat( result )\n\t\t\t\t.hasSize( 2 )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"string\", \"keyword1\" )\n\t\t\t\t\t\t\t\t.hasVectorField( \"byteVector\", BYTE_VECTOR_1 )\n\t\t\t\t\t\t\t\t.hasVectorField( \"floatVector\", FLOAT_VECTOR_1 )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"string\", \"keyword2\" )\n\t\t\t\t\t\t\t\t.hasVectorField( \"byteVector\", BYTE_VECTOR_2 )\n\t\t\t\t\t\t\t\t.hasVectorField( \"floatVector\", FLOAT_VECTOR_2 )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tvoid simpleVectorSavedAndRetrievedViaProjection() {\n\t\tSearchQuery<Object[]> query = index.createScope().query()\n\t\t\t\t.select(\n\t\t\t\t\t\tf -> f.composite().from(\n\t\t\t\t\t\t\t\tf.field( \"string\" ),\n\t\t\t\t\t\t\t\tf.field( \"byteVector\" ),\n\t\t\t\t\t\t\t\tf.field( \"floatVector\" )\n\t\t\t\t\t\t).asArray()\n\t\t\t\t)\n\t\t\t\t.where( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\n\t\tList<Object[]> result = query.fetchAll().hits();\n\t\tassertThat( result )\n\t\t\t\t.hasSize( 2 )\n\t\t\t\t.containsOnly(\n\t\t\t\t\t\tnew Object[] { \"keyword1\", BYTE_VECTOR_1, FLOAT_VECTOR_1 },\n\t\t\t\t\t\tnew Object[] { \"keyword2\", BYTE_VECTOR_2, FLOAT_VECTOR_2 }\n\t\t\t\t);\n\t}\n\n\t@Test\n\tvoid simpleVectorPredicate() {\n\t\t// took the sample data and query from this example https://opensearch.org/docs/latest/search-plugins/knn/filter-search-knn/#using-a-lucene-k-nn-filter\n\t\t// to see if we'll get the same results... and looks like we do :smile:\n\t\tint k = 3;\n\t\tSearchQuery<float[]> query = predicateIndex.createScope().query()\n\t\t\t\t.select(\n\t\t\t\t\t\tf -> f.field( \"location\", float[].class )\n\t\t\t\t)\n\t\t\t\t.where( f -> f.knn( k )\n\t\t\t\t\t\t.field( \"location\" )\n\t\t\t\t\t\t.matching( 5f, 4f )\n\t\t\t\t\t\t.filter( f.range().field( \"rating\" ).between( 8, 10 ) )\n\t\t\t\t\t\t.filter( f.terms().field( \"parking\" ).matchingAny( true ) )\n\t\t\t\t).toQuery();\n\n\t\tList<float[]> result = query.fetchAll().hits();\n\n\t\tassertThat( result )\n\t\t\t\t.hasSize( k ) // since that is how many neighbors we were asking for in the predicate\n\t\t\t\t.containsExactly(\n\t\t\t\t\t\tnew float[] { 4.9f, 3.4f },\n\t\t\t\t\t\tnew float[] { 6.4f, 3.4f },\n\t\t\t\t\t\tnew float[] { 3.3f, 4.5f }\n\t\t\t\t);\n\t}\n\n\tprivate void initData() {\n\t\tindex.bulkIndexer()\n\t\t\t\t.add( \"ID:1\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().string, \"keyword1\" );\n\t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_1 );\n\t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_1 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:2\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().string, \"keyword2\" );\n\t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_2 );\n\t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_2 );\n\t\t\t\t} )\n\t\t\t\t.join();\n\n\t\tpredicateIndex.bulkIndexer()\n\t\t\t\t.add( \"ID:1\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 5.2f, 4.4f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 5 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:2\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 5.2f, 3.9f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, false );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 4 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:3\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 4.9f, 3.4f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 9 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:4\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 4.2f, 4.6f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, false );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 6 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:5\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 3.3f, 4.5f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 8 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:6\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 6.4f, 3.4f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 9 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:7\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 4.2f, 6.2f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 5 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:8\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 2.4f, 4.0f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 8 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:9\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 1.4f, 3.2f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, false );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 5 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:10\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 7.0f, 9.9f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 9 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:11\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 3.0f, 2.3f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, false );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 6 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:12\", document -> {\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 5.0f, 1.0f } );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n\t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 3 );\n\t\t\t\t} )\n\t\t\t\t.join();\n\t}\n\n\tprivate static class IndexBinding {\n\t\tfinal IndexFieldReference<String> string;\n\t\tfinal IndexFieldReference<byte[]> byteVector;\n\t\tfinal IndexFieldReference<float[]> floatVector;\n\n\t\tIndexBinding(IndexSchemaElement root) {\n\t\t\tstring = root.field( \"string\", f -> f.asString().projectable( Projectable.YES ) ).toReference();\n\t\t\tbyteVector = root\n\t\t\t\t\t.field( \"byteVector\",\n\t\t\t\t\t\t\tf -> f.asByteVector().dimension( 4 ).projectable( Projectable.YES ).maxConnections( 16 )\n\t\t\t\t\t\t\t\t\t.vectorSimilarity( VectorSimilarity.L2 ) )\n\t\t\t\t\t.toReference();\n\t\t\tfloatVector = root\n\t\t\t\t\t.field( \"floatVector\",\n\t\t\t\t\t\t\tf -> f.asFloatVector().dimension( 8 ).projectable( Projectable.YES ).maxConnections( 48 )\n\t\t\t\t\t\t\t\t\t.beamWidth( 256 ).vectorSimilarity( VectorSimilarity.INNER_PRODUCT ) )\n\t\t\t\t\t.toReference();\n\t\t}\n\t}\n\n\tprivate static class PredicateIndexBinding {\n\t\tfinal IndexFieldReference<Boolean> parking;\n\t\tfinal IndexFieldReference<Integer> rating;\n\t\tfinal IndexFieldReference<float[]> location;\n\n\t\tPredicateIndexBinding(IndexSchemaElement root) {\n\t\t\tparking = root.field( \"parking\", f -> f.asBoolean().projectable( Projectable.YES ) ).toReference();\n\t\t\trating = root.field( \"rating\", f -> f.asInteger().projectable( Projectable.YES ) ).toReference();\n\t\t\tlocation = root.field( \"location\", f -> f.asFloatVector().dimension( 2 ).projectable( Projectable.YES )\n\t\t\t\t\t.maxConnections( 16 ).beamWidth( 100 ).vectorSimilarity( VectorSimilarity.L2 ) ).toReference();\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/backend/lucene/src/test/java/org/hibernate/search/integrationtest/backend/lucene/vector/LuceneVectorFieldIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.backend.lucene.vector;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.hibernate.search.integrationtest.backend.lucene.testsupport.util.DocumentAssert.containsDocument;\nimport static org.hibernate.search.util.impl.integrationtest.common.assertion.SearchResultAssert.assertThatQuery;\n\nimport java.util.Arrays;\nimport java.util.List;\n\nimport org.hibernate.search.backend.lucene.LuceneExtension;\nimport org.hibernate.search.engine.backend.document.DocumentElement;\nimport org.hibernate.search.engine.backend.document.IndexFieldReference;\nimport org.hibernate.search.engine.backend.document.IndexObjectFieldReference;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaElement;\nimport org.hibernate.search.engine.backend.document.model.dsl.IndexSchemaObjectField;\nimport org.hibernate.search.engine.backend.types.ObjectStructure;\nimport org.hibernate.search.engine.backend.types.Projectable;\nimport org.hibernate.search.engine.backend.types.VectorSimilarity;\nimport org.hibernate.search.engine.search.query.SearchQuery;\nimport org.hibernate.search.integrationtest.backend.tck.testsupport.util.extension.SearchSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.stub.BulkIndexer;\nimport org.hibernate.search.util.impl.integrationtest.mapper.stub.SimpleMappedIndex;\n\nimport org.junit.jupiter.api.Test;\nimport org.junit.jupiter.api.extension.RegisterExtension;\nimport org.junit.jupiter.params.ParameterizedTest;\nimport org.junit.jupiter.params.provider.EnumSource;\n\nimport org.apache.lucene.document.Document;\n\nclass LuceneVectorFieldIT {\n\n\tprivate static final byte[] BYTE_VECTOR_1 = new byte[] { 1, 2, 3, 4 };\n\tprivate static final byte[] BYTE_VECTOR_2 = new byte[] { 1, 1, 1, 1 };\n\n\tprivate static final float[] FLOAT_VECTOR_1 = new float[] { 1.0f, 2.0f, 3.0f, 4.0f, 1.0f, 2.0f, 3.0f, 4.0f };\n\tprivate static final float[] FLOAT_VECTOR_2 = new float[] { 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f };\n\n\t@RegisterExtension\n\tpublic final SearchSetupHelper setupHelper = SearchSetupHelper.create();\n\n\t@Test\n\tvoid simpleVectorSavedAndRetrieved() {\n\t\tSimpleMappedIndex<IndexBinding> index = SimpleMappedIndex.of( IndexBinding::new ).name( \"index\" );\n\t\tsetupHelper.start().withIndexes( index ).setup();\n\t\tinitDataSimple( index );\n\n\t\tSearchQuery<Document> query = index.createScope().query()\n\t\t\t\t.select(\n\t\t\t\t\t\tf -> f.extension( LuceneExtension.get() ).document()\n\t\t\t\t)\n\t\t\t\t.where( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\n\t\tList<Document> result = query.fetchAll().hits();\n\t\tassertThat( result )\n\t\t\t\t.hasSize( 2 )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"string\", \"keyword1\" )\n\t\t\t\t\t\t\t\t.hasVectorField( \"byteVector\", BYTE_VECTOR_1 )\n\t\t\t\t\t\t\t\t.hasVectorField( \"floatVector\", FLOAT_VECTOR_1 )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) )\n\t\t\t\t.satisfies( containsDocument(\n\t\t\t\t\t\tdoc -> doc.hasField( \"string\", \"keyword2\" )\n\t\t\t\t\t\t\t\t.hasVectorField( \"byteVector\", BYTE_VECTOR_2 )\n\t\t\t\t\t\t\t\t.hasVectorField( \"floatVector\", FLOAT_VECTOR_2 )\n\t\t\t\t\t\t\t\t.andOnlyInternalFields()\n\t\t\t\t) );\n\t}\n\n\t@Test\n\tvoid simpleVectorSavedAndRetrievedViaProjection() {\n\t\tSimpleMappedIndex<IndexBinding> index = SimpleMappedIndex.of( IndexBinding::new ).name( \"index\" );\n\t\tsetupHelper.start().withIndexes( index ).setup();\n\t\tinitDataSimple( index );\n\n\t\tSearchQuery<Object[]> query = index.createScope().query()\n\t\t\t\t.select(\n\t\t\t\t\t\tf -> f.composite().from(\n\t\t\t\t\t\t\t\tf.field( \"string\" ),\n\t\t\t\t\t\t\t\tf.field( \"byteVector\" ),\n\t\t\t\t\t\t\t\tf.field( \"floatVector\" )\n\t\t\t\t\t\t).asArray()\n\t\t\t\t)\n\t\t\t\t.where( f -> f.matchAll() )\n\t\t\t\t.toQuery();\n\n\t\tList<Object[]> result = query.fetchAll().hits();\n\t\tassertThat( result )\n\t\t\t\t.hasSize( 2 )\n\t\t\t\t.containsOnly(\n\t\t\t\t\t\tnew Object[] { \"keyword1\", BYTE_VECTOR_1, FLOAT_VECTOR_1 },\n\t\t\t\t\t\tnew Object[] { \"keyword2\", BYTE_VECTOR_2, FLOAT_VECTOR_2 }\n\t\t\t\t);\n\t}\n\n\t@Test\n\tvoid simpleVectorPredicateNoFilter() {\n\t\tSimpleMappedIndex<IndexBinding> index = SimpleMappedIndex.of( IndexBinding::new ).name( \"index\" );\n\t\tsetupHelper.start().withIndexes( index ).setup();\n\t\tinitDataSimpleNoFilter( index );\n\n\t\tassertThatQuery(\n\t\t\t\tindex.createScope().query()\n\t\t\t\t\t\t.select( f -> f.field( \"byteVector\" ) )\n\t\t\t\t\t\t.where( f -> f.knn( 3 ).field( \"byteVector\" )\n\t\t\t\t\t\t\t\t.matching( bytes( 4, (byte) 5 ) ) )\n\t\t).hasTotalHitCount( 3 )\n\t\t\t\t.hasHitsExactOrder(\n\t\t\t\t\t\tbytes( 4, (byte) 5 ),\n\t\t\t\t\t\tbytes( 4, (byte) 6 ),\n\t\t\t\t\t\tbytes( 4, (byte) 4 )\n\t\t\t\t);\n\n\t\tassertThatQuery(\n\t\t\t\tindex.createScope().query()\n\t\t\t\t\t\t.select( f -> f.field( \"floatVector\" ) )\n\t\t\t\t\t\t.where( f -> f.knn( 3 ).field( \"floatVector\" )\n\t\t\t\t\t\t\t\t.matching( floats( 8, 0.051f ) ) )\n\t\t).hasTotalHitCount( 3 )\n\t\t\t\t.hasHitsExactOrder(\n\t\t\t\t\t\tfloats( 8, 0.05f ),\n\t\t\t\t\t\tfloats( 8, 0.06f ),\n\t\t\t\t\t\tfloats( 8, 0.04f )\n\t\t\t\t);\n\t}\n\n\t@ParameterizedTest\n\t@EnumSource(VectorSimilarity.class)\n\tvoid similarity(VectorSimilarity similarity) {\n\t\tSimpleMappedIndex<SimilarityIndexBinding> index =\n\t\t\t\tSimpleMappedIndex.of( root -> new SimilarityIndexBinding( similarity, root ) ).name( \"index_\" + similarity );\n\t\tsetupHelper.start().withIndexes( index ).setup();\n\t\tinitSimilarityIndexBinding( index );\n\n\t\tassertThatQuery(\n\t\t\t\tindex.createScope().query()\n\t\t\t\t\t\t.select( f -> f.id() )\n\t\t\t\t\t\t.where( f -> f.knn( 1 ).field( \"byteVector\" )\n\t\t\t\t\t\t\t\t.matching( bytes( 4, (byte) 5 ) ) )\n\t\t).hasTotalHitCount( 1 );\n\n\t\tassertThatQuery(\n\t\t\t\tindex.createScope().query()\n\t\t\t\t\t\t.select( f -> f.id() )\n\t\t\t\t\t\t.where( f -> f.knn( 1 ).field( \"floatVector\" )\n\t\t\t\t\t\t\t\t.matching( floats( 8, 0.051f ) ) )\n\t\t).hasTotalHitCount( 1 );\n\t}\n\n\t@Test\n\tvoid simpleVectorPredicateWithFilter() {\n\t\t// took the sample data and query from this example https://opensearch.org/docs/latest/search-plugins/knn/filter-search-knn/#using-a-lucene-k-nn-filter\n\t\t// to see if we'll get the same results... and looks like we do :smile:\n\t\tSimpleMappedIndex<PredicateIndexBinding> index = SimpleMappedIndex.of( PredicateIndexBinding::new )\n\t\t\t\t.name( \"predicateIndex\" );\n\t\tsetupHelper.start().withIndexes( index ).setup();\n\t\tinitDataSimplePredicate( index );\n\n\n\t\tint k = 3;\n\t\tSearchQuery<float[]> query = index.createScope().query()\n\t\t\t\t.select(\n\t\t\t\t\t\tf -> f.field( \"location\", float[].class )\n\t\t\t\t)\n\t\t\t\t.where( f -> f.knn( k )\n\t\t\t\t\t\t.field( \"location\" )\n\t\t\t\t\t\t.matching( 5f, 4f )\n\t\t\t\t\t\t.filter( f.range().field( \"rating\" ).between( 8, 10 ) )\n\t\t\t\t\t\t.filter( f.terms().field( \"parking\" ).matchingAny( true ) )\n\t\t\t\t).toQuery();\n\n\t\tList<float[]> result = query.fetchAll().hits();\n\n\t\tassertThat( result )\n\t\t\t\t.hasSize( k ) // since that is how many neighbors we were asking for in the predicate\n\t\t\t\t.containsExactly(\n\t\t\t\t\t\tnew float[] { 4.9f, 3.4f },\n\t\t\t\t\t\tnew float[] { 6.4f, 3.4f },\n\t\t\t\t\t\tnew float[] { 3.3f, 4.5f }\n\t\t\t\t);\n\t}\n\n\t@Test\n\tvoid knnPredicateInsideOrYieldsMoreResults() {\n\t\tSimpleMappedIndex<PredicateIndexBinding> index = SimpleMappedIndex.of( PredicateIndexBinding::new )\n\t\t\t\t.name( \"predicateIndex\" );\n\t\tsetupHelper.start().withIndexes( index ).setup();\n\t\tinitDataSimplePredicate( index );\n\n\t\tint k = 3;\n\t\tSearchQuery<float[]> query = index.createScope().query()\n\t\t\t\t.select(\n\t\t\t\t\t\tf -> f.field( \"location\", float[].class )\n\t\t\t\t)\n\t\t\t\t.where( f -> f.or(\n\t\t\t\t\t\tf.knn( k )\n\t\t\t\t\t\t\t\t.field( \"location\" )\n\t\t\t\t\t\t\t\t.matching( 5f, 4f ),\n\t\t\t\t\t\tf.terms().field( \"parking\" ).matchingAny( true )\n\t\t\t\t)\n\t\t\t\t).toQuery();\n\n\t\tList<float[]> result = query.fetchAll().hits();\n\n\t\tassertThat( result )\n\t\t\t\t.hasSize( k ) // since that is how many neighbors we were asking for in the predicate\n\t\t\t\t.containsExactly(\n\t\t\t\t\t\tnew float[] { 5.2f, 4.4f },\n\t\t\t\t\t\tnew float[] { 4.9f, 3.4f },\n\t\t\t\t\t\tnew float[] { 7.0f, 9.9f },\n\t\t\t\t\t\tnew float[] { 6.4f, 3.4f },\n\t\t\t\t\t\tnew float[] { 2.4f, 4.0f },\n\t\t\t\t\t\tnew float[] { 3.3f, 4.5f },\n\t\t\t\t\t\tnew float[] { 5.0f, 1.0f },\n\t\t\t\t\t\tnew float[] { 4.2f, 6.2f },\n\t\t\t\t\t\tnew float[] { 5.2f, 3.9f }\n\t\t\t\t);\n\t}\n\n\t@Test\n\tvoid insideOtherPredicate() {\n\t\tSimpleMappedIndex<IndexBinding> index = SimpleMappedIndex.of( IndexBinding::new ).name( \"index\" );\n\t\tsetupHelper.start().withIndexes( index ).setup();\n\t\tinitDataSimple( index );\n\n\t\tSearchQuery<String> query = index.createScope().query()\n\t\t\t\t.select(\n\t\t\t\t\t\tf -> f.field( \"string\", String.class )\n\t\t\t\t)\n\t\t\t\t.where( f -> f.bool()\n\t\t\t\t\t\t.must( f.knn( 5 ).field( \"byteVector\" ).matching( bytes( 4, (byte) 5 ) ) )\n\t\t\t\t\t\t.must( f.match().field( \"string\" ).matching( \"keyword1\" ) )\n\t\t\t\t)\n\t\t\t\t.toQuery();\n\n\t\tList<String> result = query.fetchAll().hits();\n\t\tassertThat( result )\n\t\t\t\t.hasSize( 1 )\n\t\t\t\t.containsOnly( \"keyword1\" );\n\t}\n\n\t@Test\n\tvoid nestedVector() {\n\t\tSimpleMappedIndex<NestedIndexBinding> index = SimpleMappedIndex.of( NestedIndexBinding::new ).name( \"index\" );\n\t\tsetupHelper.start().withIndexes( index ).setup();\n\t\tinitNestedIndex( index );\n\n\t\tassertThat(\n\t\t\t\tindex.createScope().query()\n\t\t\t\t\t\t.select( f -> f.composite()\n\t\t\t\t\t\t\t\t.from(\n\t\t\t\t\t\t\t\t\t\tf.id(),\n\t\t\t\t\t\t\t\t\t\tf.object( \"nested\" )\n\t\t\t\t\t\t\t\t\t\t\t\t.from(\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tf.field( \"nested.byteVector\" ),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tf.field( \"nested.floatVector\" )\n\t\t\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t\t\t\t\t.asList()\n\t\t\t\t\t\t\t\t\t\t\t\t.multi()\n\t\t\t\t\t\t\t\t).asList()\n\t\t\t\t\t\t).where(\n\t\t\t\t\t\t\t\tf -> f.nested( \"nested\" )\n\t\t\t\t\t\t\t\t\t\t.add( f.knn( 1 ).field( \"nested.byteVector\" ).matching( bytes( 2, (byte) -120 ) ) )\n\t\t\t\t\t\t).fetchAllHits()\n\t\t).hasSize( 1 )\n\t\t\t\t.element( 0 )\n\t\t\t\t.satisfies( el -> {\n\t\t\t\t\tassertThat( el ).hasSize( 2 );\n\t\t\t\t\tassertThat( el ).element( 0 ).isEqualTo( \"ID:2\" );\n\t\t\t\t\tassertThat( el ).element( 1 ).satisfies( inner -> {\n\t\t\t\t\t\tList<Object> vectors = (List<Object>) ( (List<Object>) inner ).get( 0 );\n\t\t\t\t\t\tassertThat( vectors ).element( 0 ).isEqualTo( bytes( 2, (byte) -120 ) );\n\t\t\t\t\t\tassertThat( vectors ).element( 1 ).isEqualTo( floats( 2, 12345.0f ) );\n\t\t\t\t\t} );\n\t\t\t\t} );\n\t}\n\n\tprivate void initDataSimple(SimpleMappedIndex<IndexBinding> index) {\n\t\tindex.bulkIndexer()\n\t\t\t\t.add( \"ID:1\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().string, \"keyword1\" );\n\t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_1 );\n\t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_1 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:2\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().string, \"keyword2\" );\n\t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_2 );\n\t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_2 );\n\t\t\t\t} )\n\t\t\t\t.join();\n\t}\n\n\tprivate void initNestedIndex(SimpleMappedIndex<NestedIndexBinding> index) {\n\t\tindex.bulkIndexer()\n\t\t\t\t.add( \"ID:1\", document -> {\n\t\t\t\t\tDocumentElement nested = document.addObject( index.binding().nested );\n\t\t\t\t\tnested.addValue( index.binding().byteVector, bytes( 2, (byte) 1 ) );\n\t\t\t\t\tnested.addValue( index.binding().floatVector, floats( 2, 1.0f ) );\n\n\t\t\t\t\tnested = document.addObject( index.binding().nested );\n\t\t\t\t\tnested.addValue( index.binding().byteVector, bytes( 2, (byte) 10 ) );\n\t\t\t\t\tnested.addValue( index.binding().floatVector, floats( 2, 10.0f ) );\n\n\t\t\t\t\tnested = document.addObject( index.binding().nested );\n\t\t\t\t\tnested.addValue( index.binding().byteVector, bytes( 2, (byte) 100 ) );\n\t\t\t\t\tnested.addValue( index.binding().floatVector, floats( 2, 100.0f ) );\n\n\t\t\t\t\tnested = document.addObject( index.binding().nested );\n\t\t\t\t\tnested.addValue( index.binding().byteVector, bytes( 2, (byte) 127 ) );\n\t\t\t\t\tnested.addValue( index.binding().floatVector, floats( 2, 1000.0f ) );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:2\", document -> {\n\t\t\t\t\tDocumentElement nested = document.addObject( index.binding().nested );\n\t\t\t\t\tnested.addValue( index.binding().byteVector, bytes( 2, (byte) -120 ) );\n\t\t\t\t\tnested.addValue( index.binding().floatVector, floats( 2, 12345.0f ) );\n\t\t\t\t} )\n\t\t\t\t.join();\n\t}\n\n\tprivate void initDataSimpleNoFilter(SimpleMappedIndex<IndexBinding> index) {\n\t\tBulkIndexer bulkIndexer = index.bulkIndexer();\n\n\t\tfor ( int i = 0; i < 10; i++ ) {\n\t\t\tint id = i;\n\t\t\tbulkIndexer\n\t\t\t\t\t.add( \"ID:\" + i, document -> {\n\t\t\t\t\t\tdocument.addValue( index.binding().string, ( \"keyword\" + id ) );\n\t\t\t\t\t\tdocument.addValue( index.binding().byteVector, bytes( 4, (byte) id ) );\n\t\t\t\t\t\tdocument.addValue( index.binding().floatVector, floats( 8, id / 100.0f ) );\n\t\t\t\t\t} );\n\t\t}\n\n\t\tbulkIndexer.join();\n\t}\n\n\tprivate void initSimilarityIndexBinding(SimpleMappedIndex<SimilarityIndexBinding> index) {\n\t\tBulkIndexer bulkIndexer = index.bulkIndexer();\n\n\t\tfor ( int i = 1; i < 11; i++ ) {\n\t\t\tint id = i;\n\t\t\tbulkIndexer\n\t\t\t\t\t.add( \"ID:\" + i, document -> {\n\t\t\t\t\t\tdocument.addValue( index.binding().byteVector, bytes( 4, (byte) id ) );\n\t\t\t\t\t\tdocument.addValue( index.binding().floatVector, floats( 8, id / 100.0f ) );\n\t\t\t\t\t} );\n\t\t}\n\n\t\tbulkIndexer.join();\n\t}\n\n\tprivate byte[] bytes(int size, byte value) {\n\t\tbyte[] bytes = new byte[size];\n\t\tArrays.fill( bytes, value );\n\t\treturn bytes;\n\t}\n\n\tprivate float[] floats(int size, float value) {\n\t\tfloat[] bytes = new float[size];\n\t\tArrays.fill( bytes, value );\n\t\treturn bytes;\n\t}\n\n\tprivate void initDataSimplePredicate(SimpleMappedIndex<PredicateIndexBinding> index) {\n\t\tindex.bulkIndexer()\n\t\t\t\t.add( \"ID:1\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 5.2f, 4.4f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 5 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:2\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 5.2f, 3.9f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, false );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 4 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:3\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 4.9f, 3.4f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 9 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:4\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 4.2f, 4.6f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, false );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 6 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:5\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 3.3f, 4.5f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 8 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:6\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 6.4f, 3.4f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 9 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:7\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 4.2f, 6.2f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 5 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:8\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 2.4f, 4.0f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 8 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:9\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 1.4f, 3.2f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, false );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 5 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:10\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 7.0f, 9.9f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 9 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:11\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 3.0f, 2.3f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, false );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 6 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:12\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 5.0f, 1.0f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 3 );\n\t\t\t\t} )\n\t\t\t\t.join();\n\t}\n\n\tprivate static class SimilarityIndexBinding {\n\t\tfinal IndexFieldReference<byte[]> byteVector;\n\t\tfinal IndexFieldReference<float[]> floatVector;\n\n\t\tSimilarityIndexBinding(VectorSimilarity similarity, IndexSchemaElement root) {\n\t\t\tbyteVector = root\n\t\t\t\t\t.field(\n\t\t\t\t\t\t\t\"byteVector\",\n\t\t\t\t\t\t\tf -> f.asByteVector().dimension( 4 ).vectorSimilarity( similarity )\n\t\t\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tfloatVector = root\n\t\t\t\t\t.field(\n\t\t\t\t\t\t\t\"floatVector\",\n\t\t\t\t\t\t\tf -> f.asFloatVector().dimension( 8 ).vectorSimilarity( similarity )\n\t\t\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t}\n\t}\n\n\tprivate static class NestedIndexBinding {\n\t\tfinal IndexObjectFieldReference nested;\n\t\tfinal IndexFieldReference<byte[]> byteVector;\n\t\tfinal IndexFieldReference<float[]> floatVector;\n\n\t\tNestedIndexBinding(IndexSchemaElement root) {\n\t\t\tIndexSchemaObjectField nestedField = root.objectField( \"nested\", ObjectStructure.NESTED )\n\t\t\t\t\t.multiValued();\n\t\t\tnested = nestedField.toReference();\n\n\t\t\tbyteVector = nestedField.field(\n\t\t\t\t\t\"byteVector\", f -> f.asByteVector().dimension( 2 ).projectable( Projectable.YES ) )\n\t\t\t\t\t.toReference();\n\t\t\tfloatVector = nestedField\n\t\t\t\t\t.field( \"floatVector\", f -> f.asFloatVector().dimension( 2 ).projectable( Projectable.YES ) )\n\t\t\t\t\t.toReference();\n\t\t}\n\t}\n\n\tprivate static class IndexBinding {\n\t\tfinal IndexFieldReference<String> string;\n\t\tfinal IndexFieldReference<byte[]> byteVector;\n\t\tfinal IndexFieldReference<float[]> floatVector;\n\n\t\tIndexBinding(IndexSchemaElement root) {\n\t\t\tstring = root.field( \"string\", f -> f.asString().projectable( Projectable.YES ) ).toReference();\n\t\t\tbyteVector = root\n\t\t\t\t\t.field(\n\t\t\t\t\t\t\t\"byteVector\",\n\t\t\t\t\t\t\tf -> f.asByteVector().dimension( 4 ).projectable( Projectable.YES ).maxConnections( 16 )\n\t\t\t\t\t\t\t\t\t.vectorSimilarity( VectorSimilarity.L2 )\n\t\t\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t\tfloatVector = root\n\t\t\t\t\t.field(\n\t\t\t\t\t\t\t\"floatVector\",\n\t\t\t\t\t\t\tf -> f.asFloatVector().dimension( 8 ).projectable( Projectable.YES ).maxConnections( 48 )\n\t\t\t\t\t\t\t\t\t.beamWidth( 256 ).vectorSimilarity( VectorSimilarity.L2 )\n\t\t\t\t\t)\n\t\t\t\t\t.toReference();\n\t\t}\n\t}\n\n\tprivate static class PredicateIndexBinding {\n\t\tfinal IndexFieldReference<Boolean> parking;\n\t\tfinal IndexFieldReference<Integer> rating;\n\t\tfinal IndexFieldReference<float[]> location;\n\n\t\tPredicateIndexBinding(IndexSchemaElement root) {\n\t\t\tparking = root.field( \"parking\", f -> f.asBoolean().projectable( Projectable.YES ) ).toReference();\n\t\t\trating = root.field( \"rating\", f -> f.asInteger().projectable( Projectable.YES ) ).toReference();\n\t\t\tlocation = root.field( \"location\", f -> f.asFloatVector().dimension( 2 ).projectable( Projectable.YES )\n\t\t\t\t\t.maxConnections( 16 ).beamWidth( 100 ).vectorSimilarity( VectorSimilarity.L2 ) ).toReference();\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private void initDataSimple(SimpleMappedIndex<IndexBinding> index) {\n\t\tindex.bulkIndexer()\n\t\t\t\t.add( \"ID:1\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().string, \"keyword1\" );\n\t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_1 );\n\t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_1 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:2\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().string, \"keyword2\" );\n\t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_2 );\n\t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_2 );\n\t\t\t\t} )\n\t\t\t\t.join();\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "private void initDataSimplePredicate(SimpleMappedIndex<PredicateIndexBinding> index) {\n\t\tindex.bulkIndexer()\n\t\t\t\t.add( \"ID:1\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 5.2f, 4.4f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 5 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:2\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 5.2f, 3.9f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, false );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 4 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:3\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 4.9f, 3.4f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 9 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:4\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 4.2f, 4.6f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, false );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 6 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:5\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 3.3f, 4.5f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 8 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:6\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 6.4f, 3.4f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 9 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:7\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 4.2f, 6.2f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 5 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:8\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 2.4f, 4.0f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 8 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:9\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 1.4f, 3.2f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, false );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 5 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:10\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 7.0f, 9.9f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 9 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:11\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 3.0f, 2.3f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, false );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 6 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:12\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 5.0f, 1.0f } );\n\t\t\t\t\tdocument.addValue( index.binding().parking, true );\n\t\t\t\t\tdocument.addValue( index.binding().rating, 3 );\n\t\t\t\t} )\n\t\t\t\t.join();\n\t}\nprivate void initDataSimple(SimpleMappedIndex<IndexBinding> index) {\n\t\tindex.bulkIndexer()\n\t\t\t\t.add( \"ID:1\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().string, \"keyword1\" );\n\t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_1 );\n\t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_1 );\n\t\t\t\t} )\n\t\t\t\t.add( \"ID:2\", document -> {\n\t\t\t\t\tdocument.addValue( index.binding().string, \"keyword2\" );\n\t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_2 );\n\t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_2 );\n\t\t\t\t} )\n\t\t\t\t.join();\n\t}",
        "diffSourceCode": "-  128: \tprivate void initData() {\n-  129: \t\tindex.bulkIndexer()\n-  130: \t\t\t\t.add( \"ID:1\", document -> {\n-  131: \t\t\t\t\tdocument.addValue( index.binding().string, \"keyword1\" );\n-  132: \t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_1 );\n-  133: \t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_1 );\n-  134: \t\t\t\t} )\n-  135: \t\t\t\t.add( \"ID:2\", document -> {\n-  136: \t\t\t\t\tdocument.addValue( index.binding().string, \"keyword2\" );\n-  137: \t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_2 );\n-  138: \t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_2 );\n-  139: \t\t\t\t} )\n-  140: \t\t\t\t.join();\n-  141: \n-  142: \t\tpredicateIndex.bulkIndexer()\n-  143: \t\t\t\t.add( \"ID:1\", document -> {\n-  144: \t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 5.2f, 4.4f } );\n-  145: \t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n-  146: \t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 5 );\n-  147: \t\t\t\t} )\n-  148: \t\t\t\t.add( \"ID:2\", document -> {\n-  149: \t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 5.2f, 3.9f } );\n-  150: \t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, false );\n-  151: \t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 4 );\n-  152: \t\t\t\t} )\n-  153: \t\t\t\t.add( \"ID:3\", document -> {\n-  154: \t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 4.9f, 3.4f } );\n-  155: \t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n-  156: \t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 9 );\n-  157: \t\t\t\t} )\n-  158: \t\t\t\t.add( \"ID:4\", document -> {\n-  159: \t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 4.2f, 4.6f } );\n-  160: \t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, false );\n-  161: \t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 6 );\n-  162: \t\t\t\t} )\n-  163: \t\t\t\t.add( \"ID:5\", document -> {\n-  164: \t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 3.3f, 4.5f } );\n-  165: \t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n-  166: \t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 8 );\n-  167: \t\t\t\t} )\n-  168: \t\t\t\t.add( \"ID:6\", document -> {\n-  169: \t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 6.4f, 3.4f } );\n-  170: \t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n-  171: \t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 9 );\n-  172: \t\t\t\t} )\n-  173: \t\t\t\t.add( \"ID:7\", document -> {\n-  174: \t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 4.2f, 6.2f } );\n-  175: \t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n-  176: \t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 5 );\n-  177: \t\t\t\t} )\n-  178: \t\t\t\t.add( \"ID:8\", document -> {\n-  179: \t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 2.4f, 4.0f } );\n-  180: \t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n-  181: \t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 8 );\n-  182: \t\t\t\t} )\n-  183: \t\t\t\t.add( \"ID:9\", document -> {\n-  184: \t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 1.4f, 3.2f } );\n-  185: \t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, false );\n-  186: \t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 5 );\n-  187: \t\t\t\t} )\n-  188: \t\t\t\t.add( \"ID:10\", document -> {\n-  189: \t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 7.0f, 9.9f } );\n-  190: \t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n-  191: \t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 9 );\n-  192: \t\t\t\t} )\n-  193: \t\t\t\t.add( \"ID:11\", document -> {\n-  194: \t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 3.0f, 2.3f } );\n-  195: \t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, false );\n-  196: \t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 6 );\n-  197: \t\t\t\t} )\n-  198: \t\t\t\t.add( \"ID:12\", document -> {\n-  199: \t\t\t\t\tdocument.addValue( predicateIndex.binding().location, new float[] { 5.0f, 1.0f } );\n-  200: \t\t\t\t\tdocument.addValue( predicateIndex.binding().parking, true );\n-  201: \t\t\t\t\tdocument.addValue( predicateIndex.binding().rating, 3 );\n-  202: \t\t\t\t} )\n-  203: \t\t\t\t.join();\n-  204: \t}\n+  128: \t\t\t\t.hasHitsExactOrder(\n+  129: \t\t\t\t\t\tfloats( 8, 0.05f ),\n+  130: \t\t\t\t\t\tfloats( 8, 0.06f ),\n+  131: \t\t\t\t\t\tfloats( 8, 0.04f )\n+  132: \t\t\t\t);\n+  133: \t}\n+  134: \n+  135: \t@ParameterizedTest\n+  136: \t@EnumSource(VectorSimilarity.class)\n+  137: \tvoid similarity(VectorSimilarity similarity) {\n+  138: \t\tSimpleMappedIndex<SimilarityIndexBinding> index =\n+  139: \t\t\t\tSimpleMappedIndex.of( root -> new SimilarityIndexBinding( similarity, root ) ).name( \"index_\" + similarity );\n+  140: \t\tsetupHelper.start().withIndexes( index ).setup();\n+  141: \t\tinitSimilarityIndexBinding( index );\n+  142: \n+  143: \t\tassertThatQuery(\n+  144: \t\t\t\tindex.createScope().query()\n+  145: \t\t\t\t\t\t.select( f -> f.id() )\n+  146: \t\t\t\t\t\t.where( f -> f.knn( 1 ).field( \"byteVector\" )\n+  147: \t\t\t\t\t\t\t\t.matching( bytes( 4, (byte) 5 ) ) )\n+  148: \t\t).hasTotalHitCount( 1 );\n+  149: \n+  150: \t\tassertThatQuery(\n+  151: \t\t\t\tindex.createScope().query()\n+  152: \t\t\t\t\t\t.select( f -> f.id() )\n+  153: \t\t\t\t\t\t.where( f -> f.knn( 1 ).field( \"floatVector\" )\n+  154: \t\t\t\t\t\t\t\t.matching( floats( 8, 0.051f ) ) )\n+  155: \t\t).hasTotalHitCount( 1 );\n+  156: \t}\n+  157: \n+  158: \t@Test\n+  159: \tvoid simpleVectorPredicateWithFilter() {\n+  160: \t\t// took the sample data and query from this example https://opensearch.org/docs/latest/search-plugins/knn/filter-search-knn/#using-a-lucene-k-nn-filter\n+  161: \t\t// to see if we'll get the same results... and looks like we do :smile:\n+  162: \t\tSimpleMappedIndex<PredicateIndexBinding> index = SimpleMappedIndex.of( PredicateIndexBinding::new )\n+  163: \t\t\t\t.name( \"predicateIndex\" );\n+  164: \t\tsetupHelper.start().withIndexes( index ).setup();\n+  165: \t\tinitDataSimplePredicate( index );\n+  166: \n+  167: \n+  168: \t\tint k = 3;\n+  169: \t\tSearchQuery<float[]> query = index.createScope().query()\n+  170: \t\t\t\t.select(\n+  171: \t\t\t\t\t\tf -> f.field( \"location\", float[].class )\n+  172: \t\t\t\t)\n+  173: \t\t\t\t.where( f -> f.knn( k )\n+  174: \t\t\t\t\t\t.field( \"location\" )\n+  175: \t\t\t\t\t\t.matching( 5f, 4f )\n+  176: \t\t\t\t\t\t.filter( f.range().field( \"rating\" ).between( 8, 10 ) )\n+  177: \t\t\t\t\t\t.filter( f.terms().field( \"parking\" ).matchingAny( true ) )\n+  178: \t\t\t\t).toQuery();\n+  179: \n+  180: \t\tList<float[]> result = query.fetchAll().hits();\n+  181: \n+  182: \t\tassertThat( result )\n+  183: \t\t\t\t.hasSize( k ) // since that is how many neighbors we were asking for in the predicate\n+  184: \t\t\t\t.containsExactly(\n+  185: \t\t\t\t\t\tnew float[] { 4.9f, 3.4f },\n+  186: \t\t\t\t\t\tnew float[] { 6.4f, 3.4f },\n+  187: \t\t\t\t\t\tnew float[] { 3.3f, 4.5f }\n+  188: \t\t\t\t);\n+  189: \t}\n+  190: \n+  191: \t@Test\n+  192: \tvoid knnPredicateInsideOrYieldsMoreResults() {\n+  193: \t\tSimpleMappedIndex<PredicateIndexBinding> index = SimpleMappedIndex.of( PredicateIndexBinding::new )\n+  194: \t\t\t\t.name( \"predicateIndex\" );\n+  195: \t\tsetupHelper.start().withIndexes( index ).setup();\n+  196: \t\tinitDataSimplePredicate( index );\n+  197: \n+  198: \t\tint k = 3;\n+  199: \t\tSearchQuery<float[]> query = index.createScope().query()\n+  200: \t\t\t\t.select(\n+  201: \t\t\t\t\t\tf -> f.field( \"location\", float[].class )\n+  202: \t\t\t\t)\n+  203: \t\t\t\t.where( f -> f.or(\n+  204: \t\t\t\t\t\tf.knn( k )\n+  286: \tprivate void initDataSimple(SimpleMappedIndex<IndexBinding> index) {\n+  287: \t\tindex.bulkIndexer()\n+  288: \t\t\t\t.add( \"ID:1\", document -> {\n+  289: \t\t\t\t\tdocument.addValue( index.binding().string, \"keyword1\" );\n+  290: \t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_1 );\n+  291: \t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_1 );\n+  292: \t\t\t\t} )\n+  293: \t\t\t\t.add( \"ID:2\", document -> {\n+  294: \t\t\t\t\tdocument.addValue( index.binding().string, \"keyword2\" );\n+  295: \t\t\t\t\tdocument.addValue( index.binding().byteVector, BYTE_VECTOR_2 );\n+  296: \t\t\t\t\tdocument.addValue( index.binding().floatVector, FLOAT_VECTOR_2 );\n+  297: \t\t\t\t} )\n+  298: \t\t\t\t.join();\n+  299: \t}\n+  371: \tprivate void initDataSimplePredicate(SimpleMappedIndex<PredicateIndexBinding> index) {\n+  372: \t\tindex.bulkIndexer()\n+  373: \t\t\t\t.add( \"ID:1\", document -> {\n+  374: \t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 5.2f, 4.4f } );\n+  375: \t\t\t\t\tdocument.addValue( index.binding().parking, true );\n+  376: \t\t\t\t\tdocument.addValue( index.binding().rating, 5 );\n+  377: \t\t\t\t} )\n+  378: \t\t\t\t.add( \"ID:2\", document -> {\n+  379: \t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 5.2f, 3.9f } );\n+  380: \t\t\t\t\tdocument.addValue( index.binding().parking, false );\n+  381: \t\t\t\t\tdocument.addValue( index.binding().rating, 4 );\n+  382: \t\t\t\t} )\n+  383: \t\t\t\t.add( \"ID:3\", document -> {\n+  384: \t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 4.9f, 3.4f } );\n+  385: \t\t\t\t\tdocument.addValue( index.binding().parking, true );\n+  386: \t\t\t\t\tdocument.addValue( index.binding().rating, 9 );\n+  387: \t\t\t\t} )\n+  388: \t\t\t\t.add( \"ID:4\", document -> {\n+  389: \t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 4.2f, 4.6f } );\n+  390: \t\t\t\t\tdocument.addValue( index.binding().parking, false );\n+  391: \t\t\t\t\tdocument.addValue( index.binding().rating, 6 );\n+  392: \t\t\t\t} )\n+  393: \t\t\t\t.add( \"ID:5\", document -> {\n+  394: \t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 3.3f, 4.5f } );\n+  395: \t\t\t\t\tdocument.addValue( index.binding().parking, true );\n+  396: \t\t\t\t\tdocument.addValue( index.binding().rating, 8 );\n+  397: \t\t\t\t} )\n+  398: \t\t\t\t.add( \"ID:6\", document -> {\n+  399: \t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 6.4f, 3.4f } );\n+  400: \t\t\t\t\tdocument.addValue( index.binding().parking, true );\n+  401: \t\t\t\t\tdocument.addValue( index.binding().rating, 9 );\n+  402: \t\t\t\t} )\n+  403: \t\t\t\t.add( \"ID:7\", document -> {\n+  404: \t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 4.2f, 6.2f } );\n+  405: \t\t\t\t\tdocument.addValue( index.binding().parking, true );\n+  406: \t\t\t\t\tdocument.addValue( index.binding().rating, 5 );\n+  407: \t\t\t\t} )\n+  408: \t\t\t\t.add( \"ID:8\", document -> {\n+  409: \t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 2.4f, 4.0f } );\n+  410: \t\t\t\t\tdocument.addValue( index.binding().parking, true );\n+  411: \t\t\t\t\tdocument.addValue( index.binding().rating, 8 );\n+  412: \t\t\t\t} )\n+  413: \t\t\t\t.add( \"ID:9\", document -> {\n+  414: \t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 1.4f, 3.2f } );\n+  415: \t\t\t\t\tdocument.addValue( index.binding().parking, false );\n+  416: \t\t\t\t\tdocument.addValue( index.binding().rating, 5 );\n+  417: \t\t\t\t} )\n+  418: \t\t\t\t.add( \"ID:10\", document -> {\n+  419: \t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 7.0f, 9.9f } );\n+  420: \t\t\t\t\tdocument.addValue( index.binding().parking, true );\n+  421: \t\t\t\t\tdocument.addValue( index.binding().rating, 9 );\n+  422: \t\t\t\t} )\n+  423: \t\t\t\t.add( \"ID:11\", document -> {\n+  424: \t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 3.0f, 2.3f } );\n+  425: \t\t\t\t\tdocument.addValue( index.binding().parking, false );\n+  426: \t\t\t\t\tdocument.addValue( index.binding().rating, 6 );\n+  427: \t\t\t\t} )\n+  428: \t\t\t\t.add( \"ID:12\", document -> {\n+  429: \t\t\t\t\tdocument.addValue( index.binding().location, new float[] { 5.0f, 1.0f } );\n+  430: \t\t\t\t\tdocument.addValue( index.binding().parking, true );\n+  431: \t\t\t\t\tdocument.addValue( index.binding().rating, 3 );\n+  432: \t\t\t\t} )\n+  433: \t\t\t\t.join();\n+  434: \t}\n",
        "uniqueId": "f7c9efb7912be0d92bb8a29654ac537d5be9683f_128_204_286_299_371_434",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Inline Method",
        "description": "Inline Method\tprivate renderFailures() : String inlined to public checkNoFailure() : void in class org.hibernate.search.engine.reporting.spi.RootFailureCollector",
        "diffLocations": [
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/reporting/spi/RootFailureCollector.java",
                "startLine": 50,
                "endLine": 55,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/reporting/spi/RootFailureCollector.java",
                "startLine": 51,
                "endLine": 71,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "engine/src/main/java/org/hibernate/search/engine/reporting/spi/RootFailureCollector.java",
                "startLine": 57,
                "endLine": 74,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private String renderFailures() {\n\t\tToStringStyle style = ToStringStyle.multilineIndentStructure(\n\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportContextFailuresSeparator(),\n\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportContextIndent(),\n\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportFailuresBulletPoint(),\n\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportFailuresNoBulletPoint()\n\t\t);\n\t\tToStringTreeBuilder builder = new ToStringTreeBuilder( style );\n\t\tbuilder.startObject();\n\t\tif ( failureCount.get() > FAILURE_LIMIT ) {\n\t\t\tbuilder.value( log.collectedFailureLimitReached( process, FAILURE_LIMIT, failureCount.get() ) );\n\t\t}\n\t\tif ( delegate != null ) {\n\t\t\tdelegate.appendChildrenFailuresTo( builder );\n\t\t}\n\t\tbuilder.endObject();\n\t\treturn builder.toString();\n\t}",
        "filePathBefore": "engine/src/main/java/org/hibernate/search/engine/reporting/spi/RootFailureCollector.java",
        "isPureRefactoring": true,
        "commitId": "66f97c223ef9df56c196dc78e045c544d6d9f05e",
        "packageNameBefore": "org.hibernate.search.engine.reporting.spi",
        "classNameBefore": "org.hibernate.search.engine.reporting.spi.RootFailureCollector",
        "methodNameBefore": "org.hibernate.search.engine.reporting.spi.RootFailureCollector#renderFailures",
        "invokedMethod": "methodSignature: org.hibernate.search.engine.reporting.spi.RootFailureCollector.NonRootFailureCollector#appendChildrenFailuresTo\n methodBody: final void appendChildrenFailuresTo(ToStringTreeBuilder builder) {\nfor(ContextualFailureCollectorImpl child: children.values()){if(child.hasFailure()){child.appendFailuresTo(builder);\n}}}\nmethodSignature: org.hibernate.search.engine.logging.impl.Log#collectedFailureLimitReached\n methodBody: String collectedFailureLimitReached(String process, int failureLimit, int failureCount);",
        "classSignatureBefore": "public final class RootFailureCollector implements FailureCollector ",
        "methodNameBeforeSet": [
            "org.hibernate.search.engine.reporting.spi.RootFailureCollector#renderFailures"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.engine.reporting.spi.RootFailureCollector"
        ],
        "classSignatureBeforeSet": [
            "public final class RootFailureCollector implements FailureCollector "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Overlapped refactoring - can be identical by undoing the overlapped refactoring\n- Add Parameter-",
                "description": "Return statements added",
                "mappingState": 2
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.reporting.spi;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.Collection;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.StringJoiner;\nimport java.util.concurrent.ConcurrentLinkedDeque;\nimport java.util.concurrent.ConcurrentSkipListMap;\nimport java.util.concurrent.atomic.AtomicInteger;\n\nimport org.hibernate.search.engine.logging.impl.Log;\nimport org.hibernate.search.engine.reporting.impl.EngineEventContextMessages;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.common.data.impl.InsertionOrder;\nimport org.hibernate.search.util.common.impl.ToStringStyle;\nimport org.hibernate.search.util.common.impl.ToStringTreeBuilder;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\nimport org.hibernate.search.util.common.reporting.EventContext;\nimport org.hibernate.search.util.common.reporting.EventContextElement;\nimport org.hibernate.search.util.common.reporting.impl.CommonEventContextMessages;\n\npublic final class RootFailureCollector implements FailureCollector {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\t/**\n\t * This prevents Hibernate Search from trying too hard to collect errors,\n\t * which could be a problem when there is something fundamentally wrong\n\t * that will cause almost every operation to fail.\n\t */\n\t// Exposed for tests\n\tstatic final int FAILURE_LIMIT = 100;\n\n\tprivate final String process;\n\tprivate final NonRootFailureCollector delegate;\n\tprivate final AtomicInteger failureCount = new AtomicInteger();\n\n\tpublic RootFailureCollector(String process) {\n\t\tthis.process = process;\n\t\tthis.delegate = new NonRootFailureCollector( this );\n\t}\n\n\tpublic void checkNoFailure() {\n\t\tif ( failureCount.get() > 0 ) {\n\t\t\tString renderedFailures = renderFailures();\n\t\t\tthrow log.collectedFailures( process, renderedFailures );\n\t\t}\n\t}\n\n\tprivate String renderFailures() {\n\t\tToStringStyle style = ToStringStyle.multilineIndentStructure(\n\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportContextFailuresSeparator(),\n\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportContextIndent(),\n\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportFailuresBulletPoint(),\n\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportFailuresNoBulletPoint()\n\t\t);\n\t\tToStringTreeBuilder builder = new ToStringTreeBuilder( style );\n\t\tbuilder.startObject();\n\t\tif ( failureCount.get() > FAILURE_LIMIT ) {\n\t\t\tbuilder.value( log.collectedFailureLimitReached( process, FAILURE_LIMIT, failureCount.get() ) );\n\t\t}\n\t\tif ( delegate != null ) {\n\t\t\tdelegate.appendChildrenFailuresTo( builder );\n\t\t}\n\t\tbuilder.endObject();\n\t\treturn builder.toString();\n\t}\n\n\t@Override\n\tpublic ContextualFailureCollector withContext(EventContext context) {\n\t\treturn delegate.withContext( context );\n\t}\n\n\t@Override\n\tpublic ContextualFailureCollector withContext(EventContextElement contextElement) {\n\t\treturn delegate.withContext( contextElement );\n\t}\n\n\tprivate boolean shouldAddFailure() {\n\t\treturn failureCount.incrementAndGet() <= FAILURE_LIMIT;\n\t}\n\n\tprivate static class NonRootFailureCollector implements FailureCollector {\n\t\tprotected final RootFailureCollector root;\n\t\tprivate final InsertionOrder<EventContextElement> childrenInsertionOrder = new InsertionOrder<>();\n\t\t// Avoiding blocking implementations because we access this from reactive event loops\n\t\tprivate final Map<InsertionOrder.Key<EventContextElement>, ContextualFailureCollectorImpl> children =\n\t\t\t\tnew ConcurrentSkipListMap<>();\n\n\t\tprivate NonRootFailureCollector(RootFailureCollector root) {\n\t\t\tthis.root = root;\n\t\t}\n\n\t\tprotected NonRootFailureCollector(NonRootFailureCollector parent) {\n\t\t\tthis.root = parent.root;\n\t\t}\n\n\t\t@Override\n\t\tpublic ContextualFailureCollectorImpl withContext(EventContext context) {\n\t\t\tif ( context == null ) {\n\t\t\t\treturn withDefaultContext();\n\t\t\t}\n\t\t\tList<EventContextElement> elements = context.elements();\n\t\t\ttry {\n\t\t\t\tNonRootFailureCollector failureCollector = this;\n\t\t\t\tfor ( EventContextElement contextElement : elements ) {\n\t\t\t\t\tfailureCollector = failureCollector.withContext( contextElement );\n\t\t\t\t}\n\t\t\t\treturn (ContextualFailureCollectorImpl) failureCollector;\n\t\t\t}\n\t\t\t// This should not happen, but we want to be extra-cautious to avoid failures while handling failures\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\t// Just log the problem and degrade gracefully.\n\t\t\t\tlog.exceptionWhileCollectingFailure( e.getMessage(), e );\n\t\t\t\treturn withDefaultContext();\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic ContextualFailureCollectorImpl withContext(EventContextElement contextElement) {\n\t\t\tif ( contextElement == null ) {\n\t\t\t\treturn withDefaultContext();\n\t\t\t}\n\t\t\treturn children.computeIfAbsent(\n\t\t\t\t\tchildrenInsertionOrder.wrapKey( contextElement ),\n\t\t\t\t\tkey -> new ContextualFailureCollectorImpl( this, key.get() )\n\t\t\t);\n\t\t}\n\n\t\tContextualFailureCollectorImpl withDefaultContext() {\n\t\t\treturn withContext( EventContexts.defaultContext() );\n\t\t}\n\n\t\tvoid appendContextTo(StringJoiner joiner) {\n\t\t\t// Nothing to do\n\t\t}\n\n\t\tfinal void appendChildrenFailuresTo(ToStringTreeBuilder builder) {\n\t\t\tfor ( ContextualFailureCollectorImpl child : children.values() ) {\n\t\t\t\t// Some contexts may have been mentioned without any failure being ever reported.\n\t\t\t\t// Only display contexts that had at least one failure reported.\n\t\t\t\tif ( child.hasFailure() ) {\n\t\t\t\t\tchild.appendFailuresTo( builder );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfinal Collection<ContextualFailureCollectorImpl> children() {\n\t\t\treturn children.values();\n\t\t}\n\t}\n\n\tprivate static class ContextualFailureCollectorImpl extends NonRootFailureCollector implements ContextualFailureCollector {\n\t\tprivate final NonRootFailureCollector parent;\n\t\tprivate final EventContextElement context;\n\n\t\t// Avoiding blocking implementations because we access this from reactive event loops\n\t\tprivate final Collection<String> failureMessages = new ConcurrentLinkedDeque<>();\n\n\t\tprivate ContextualFailureCollectorImpl(NonRootFailureCollector parent, EventContextElement context) {\n\t\t\tsuper( parent );\n\t\t\tthis.parent = parent;\n\t\t\tthis.context = context;\n\t\t}\n\n\t\t@Override\n\t\tpublic boolean hasFailure() {\n\t\t\tif ( !failureMessages.isEmpty() ) {\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\tfor ( ContextualFailureCollectorImpl child : children() ) {\n\t\t\t\tif ( child.hasFailure() ) {\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn false;\n\t\t}\n\n\t\t@Override\n\t\tpublic void add(Throwable t) {\n\t\t\tif ( t instanceof SearchException ) {\n\t\t\t\tSearchException e = (SearchException) t;\n\t\t\t\tContextualFailureCollectorImpl failureCollector = this;\n\t\t\t\tEventContext eventContext = e.context();\n\t\t\t\tif ( eventContext != null ) {\n\t\t\t\t\tfailureCollector = failureCollector.withContext( e.context() );\n\t\t\t\t}\n\t\t\t\t// Do not include the context in the failure message, since we will render it as part of the failure report\n\t\t\t\tfailureCollector.doAdd( e, e.messageWithoutContext() );\n\t\t\t}\n\t\t\telse {\n\t\t\t\tdoAdd( t, t.getMessage() );\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic void add(String failureMessage) {\n\t\t\tdoAdd( failureMessage );\n\t\t}\n\n\t\t@Override\n\t\tContextualFailureCollectorImpl withDefaultContext() {\n\t\t\treturn this;\n\t\t}\n\n\t\t@Override\n\t\tvoid appendContextTo(StringJoiner joiner) {\n\t\t\tparent.appendContextTo( joiner );\n\t\t\tjoiner.add( context.render() );\n\t\t}\n\n\t\tvoid appendFailuresTo(ToStringTreeBuilder builder) {\n\t\t\tbuilder.startObject( context.render() );\n\t\t\tif ( !failureMessages.isEmpty() ) {\n\t\t\t\tbuilder.attribute( EngineEventContextMessages.INSTANCE.failureReportFailures(), failureMessages );\n\t\t\t}\n\t\t\tappendChildrenFailuresTo( builder );\n\t\t\tbuilder.endObject();\n\t\t}\n\n\t\tprivate void doAdd(Throwable failure, String failureMessage) {\n\t\t\tStringJoiner contextJoiner = new StringJoiner( CommonEventContextMessages.INSTANCE.contextSeparator() );\n\t\t\tappendContextTo( contextJoiner );\n\t\t\tlog.newCollectedFailure( root.process, contextJoiner.toString(), failure );\n\n\t\t\tdoAdd( failureMessage );\n\t\t}\n\n\t\tprivate void doAdd(String failureMessage) {\n\t\t\tif ( root.shouldAddFailure() ) {\n\t\t\t\tfailureMessages.add( failureMessage );\n\t\t\t}\n\t\t}\n\t}\n\n}\n",
        "filePathAfter": "engine/src/main/java/org/hibernate/search/engine/reporting/spi/RootFailureCollector.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.engine.reporting.spi;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.StringJoiner;\nimport java.util.concurrent.ConcurrentLinkedDeque;\nimport java.util.concurrent.ConcurrentSkipListMap;\nimport java.util.concurrent.atomic.AtomicInteger;\n\nimport org.hibernate.search.engine.logging.impl.Log;\nimport org.hibernate.search.engine.reporting.impl.EngineEventContextMessages;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.common.data.impl.InsertionOrder;\nimport org.hibernate.search.util.common.impl.ToStringStyle;\nimport org.hibernate.search.util.common.impl.ToStringTreeBuilder;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\nimport org.hibernate.search.util.common.reporting.EventContext;\nimport org.hibernate.search.util.common.reporting.EventContextElement;\nimport org.hibernate.search.util.common.reporting.impl.CommonEventContextMessages;\n\npublic final class RootFailureCollector implements FailureCollector {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\t/**\n\t * This prevents Hibernate Search from trying too hard to collect errors,\n\t * which could be a problem when there is something fundamentally wrong\n\t * that will cause almost every operation to fail.\n\t */\n\t// Exposed for tests\n\tstatic final int FAILURE_LIMIT = 100;\n\n\tprivate final String process;\n\tprivate final NonRootFailureCollector delegate;\n\tprivate final AtomicInteger failureCount = new AtomicInteger();\n\n\tpublic RootFailureCollector(String process) {\n\t\tthis.process = process;\n\t\tthis.delegate = new NonRootFailureCollector( this );\n\t}\n\n\tpublic void checkNoFailure() {\n\t\tif ( failureCount.get() > 0 ) {\n\t\t\tList<Throwable> failures = new ArrayList<>();\n\t\t\tToStringStyle style = ToStringStyle.multilineIndentStructure(\n\t\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportContextFailuresSeparator(),\n\t\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportContextIndent(),\n\t\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportFailuresBulletPoint(),\n\t\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportFailuresNoBulletPoint()\n\t\t\t);\n\t\t\tToStringTreeBuilder builder = new ToStringTreeBuilder( style );\n\t\t\tbuilder.startObject();\n\t\t\tif ( failureCount.get() > FAILURE_LIMIT ) {\n\t\t\t\tbuilder.value( log.collectedFailureLimitReached( process, FAILURE_LIMIT, failureCount.get() ) );\n\t\t\t}\n\t\t\tif ( delegate != null ) {\n\t\t\t\tdelegate.appendChildrenFailuresTo( failures, builder );\n\t\t\t}\n\t\t\tbuilder.endObject();\n\t\t\tthrow log.collectedFailures( process, builder.toString(), failures );\n\t\t}\n\t}\n\n\t@Override\n\tpublic ContextualFailureCollector withContext(EventContext context) {\n\t\treturn delegate.withContext( context );\n\t}\n\n\t@Override\n\tpublic ContextualFailureCollector withContext(EventContextElement contextElement) {\n\t\treturn delegate.withContext( contextElement );\n\t}\n\n\tprivate boolean shouldAddFailure() {\n\t\treturn failureCount.incrementAndGet() <= FAILURE_LIMIT;\n\t}\n\n\tprivate static class NonRootFailureCollector implements FailureCollector {\n\t\tprotected final RootFailureCollector root;\n\t\tprivate final InsertionOrder<EventContextElement> childrenInsertionOrder = new InsertionOrder<>();\n\t\t// Avoiding blocking implementations because we access this from reactive event loops\n\t\tprivate final Map<InsertionOrder.Key<EventContextElement>, ContextualFailureCollectorImpl> children =\n\t\t\t\tnew ConcurrentSkipListMap<>();\n\n\t\tprivate NonRootFailureCollector(RootFailureCollector root) {\n\t\t\tthis.root = root;\n\t\t}\n\n\t\tprotected NonRootFailureCollector(NonRootFailureCollector parent) {\n\t\t\tthis.root = parent.root;\n\t\t}\n\n\t\t@Override\n\t\tpublic ContextualFailureCollectorImpl withContext(EventContext context) {\n\t\t\tif ( context == null ) {\n\t\t\t\treturn withDefaultContext();\n\t\t\t}\n\t\t\tList<EventContextElement> elements = context.elements();\n\t\t\ttry {\n\t\t\t\tNonRootFailureCollector failureCollector = this;\n\t\t\t\tfor ( EventContextElement contextElement : elements ) {\n\t\t\t\t\tfailureCollector = failureCollector.withContext( contextElement );\n\t\t\t\t}\n\t\t\t\treturn (ContextualFailureCollectorImpl) failureCollector;\n\t\t\t}\n\t\t\t// This should not happen, but we want to be extra-cautious to avoid failures while handling failures\n\t\t\tcatch (RuntimeException e) {\n\t\t\t\t// Just log the problem and degrade gracefully.\n\t\t\t\tlog.exceptionWhileCollectingFailure( e.getMessage(), e );\n\t\t\t\treturn withDefaultContext();\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic ContextualFailureCollectorImpl withContext(EventContextElement contextElement) {\n\t\t\tif ( contextElement == null ) {\n\t\t\t\treturn withDefaultContext();\n\t\t\t}\n\t\t\treturn children.computeIfAbsent(\n\t\t\t\t\tchildrenInsertionOrder.wrapKey( contextElement ),\n\t\t\t\t\tkey -> new ContextualFailureCollectorImpl( this, key.get() )\n\t\t\t);\n\t\t}\n\n\t\tContextualFailureCollectorImpl withDefaultContext() {\n\t\t\treturn withContext( EventContexts.defaultContext() );\n\t\t}\n\n\t\tvoid appendContextTo(StringJoiner joiner) {\n\t\t\t// Nothing to do\n\t\t}\n\n\t\tfinal void appendChildrenFailuresTo(List<Throwable> failures, ToStringTreeBuilder builder) {\n\t\t\tfor ( ContextualFailureCollectorImpl child : children.values() ) {\n\t\t\t\t// Some contexts may have been mentioned without any failure being ever reported.\n\t\t\t\t// Only display contexts that had at least one failure reported.\n\t\t\t\tif ( child.hasFailure() ) {\n\t\t\t\t\tchild.appendFailuresTo( failures, builder );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfinal Collection<ContextualFailureCollectorImpl> children() {\n\t\t\treturn children.values();\n\t\t}\n\t}\n\n\tprivate static class ContextualFailureCollectorImpl extends NonRootFailureCollector implements ContextualFailureCollector {\n\t\tprivate final NonRootFailureCollector parent;\n\t\tprivate final EventContextElement context;\n\n\t\t// Avoiding blocking implementations because we access this from reactive event loops\n\t\tprivate final Collection<Throwable> failures = new ConcurrentLinkedDeque<>();\n\t\tprivate final Collection<String> failureMessages = new ConcurrentLinkedDeque<>();\n\n\t\tprivate ContextualFailureCollectorImpl(NonRootFailureCollector parent, EventContextElement context) {\n\t\t\tsuper( parent );\n\t\t\tthis.parent = parent;\n\t\t\tthis.context = context;\n\t\t}\n\n\t\t@Override\n\t\tpublic boolean hasFailure() {\n\t\t\tif ( !failureMessages.isEmpty() ) {\n\t\t\t\treturn true;\n\t\t\t}\n\t\t\tfor ( ContextualFailureCollectorImpl child : children() ) {\n\t\t\t\tif ( child.hasFailure() ) {\n\t\t\t\t\treturn true;\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn false;\n\t\t}\n\n\t\t@Override\n\t\tpublic void add(Throwable t) {\n\t\t\tif ( t instanceof SearchException ) {\n\t\t\t\tSearchException e = (SearchException) t;\n\t\t\t\tContextualFailureCollectorImpl failureCollector = this;\n\t\t\t\tEventContext eventContext = e.context();\n\t\t\t\tif ( eventContext != null ) {\n\t\t\t\t\tfailureCollector = failureCollector.withContext( e.context() );\n\t\t\t\t}\n\t\t\t\t// Do not include the context in the failure message, since we will render it as part of the failure report\n\t\t\t\tfailureCollector.doAdd( e, e.messageWithoutContext() );\n\t\t\t}\n\t\t\telse {\n\t\t\t\tdoAdd( t, t.getMessage() );\n\t\t\t}\n\t\t}\n\n\t\t@Override\n\t\tpublic void add(String failureMessage) {\n\t\t\tdoAdd( null, failureMessage );\n\t\t}\n\n\t\t@Override\n\t\tContextualFailureCollectorImpl withDefaultContext() {\n\t\t\treturn this;\n\t\t}\n\n\t\t@Override\n\t\tvoid appendContextTo(StringJoiner joiner) {\n\t\t\tparent.appendContextTo( joiner );\n\t\t\tjoiner.add( context.render() );\n\t\t}\n\n\t\tvoid appendFailuresTo(List<Throwable> failures, ToStringTreeBuilder builder) {\n\t\t\tbuilder.startObject( context.render() );\n\t\t\tfailures.addAll( this.failures );\n\t\t\tif ( !failureMessages.isEmpty() ) {\n\t\t\t\tbuilder.attribute( EngineEventContextMessages.INSTANCE.failureReportFailures(), failureMessages );\n\t\t\t}\n\t\t\tappendChildrenFailuresTo( failures, builder );\n\t\t\tbuilder.endObject();\n\t\t}\n\n\t\tprivate void doAdd(Throwable failure, String failureMessage) {\n\t\t\tStringJoiner contextJoiner = new StringJoiner( CommonEventContextMessages.INSTANCE.contextSeparator() );\n\t\t\tappendContextTo( contextJoiner );\n\t\t\tlog.newCollectedFailure( root.process, contextJoiner.toString(), failure );\n\n\t\t\tif ( root.shouldAddFailure() ) {\n\t\t\t\tfailureMessages.add( failureMessage );\n\t\t\t\tif ( failure != null ) {\n\t\t\t\t\tfailures.add( failure );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.engine.reporting.spi.RootFailureCollector.NonRootFailureCollector#appendChildrenFailuresTo\n methodBody: final void appendChildrenFailuresTo(ToStringTreeBuilder builder) {\nfor(ContextualFailureCollectorImpl child: children.values()){if(child.hasFailure()){child.appendFailuresTo(builder);\n}}}",
            "methodSignature: org.hibernate.search.engine.logging.impl.Log#collectedFailureLimitReached\n methodBody: String collectedFailureLimitReached(String process, int failureLimit, int failureCount);"
        ],
        "sourceCodeAfterRefactoring": "public void checkNoFailure() {\n\t\tif ( failureCount.get() > 0 ) {\n\t\t\tList<Throwable> failures = new ArrayList<>();\n\t\t\tToStringStyle style = ToStringStyle.multilineIndentStructure(\n\t\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportContextFailuresSeparator(),\n\t\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportContextIndent(),\n\t\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportFailuresBulletPoint(),\n\t\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportFailuresNoBulletPoint()\n\t\t\t);\n\t\t\tToStringTreeBuilder builder = new ToStringTreeBuilder( style );\n\t\t\tbuilder.startObject();\n\t\t\tif ( failureCount.get() > FAILURE_LIMIT ) {\n\t\t\t\tbuilder.value( log.collectedFailureLimitReached( process, FAILURE_LIMIT, failureCount.get() ) );\n\t\t\t}\n\t\t\tif ( delegate != null ) {\n\t\t\t\tdelegate.appendChildrenFailuresTo( failures, builder );\n\t\t\t}\n\t\t\tbuilder.endObject();\n\t\t\tthrow log.collectedFailures( process, builder.toString(), failures );\n\t\t}\n\t}",
        "diffSourceCode": "-   50: \tpublic void checkNoFailure() {\n-   51: \t\tif ( failureCount.get() > 0 ) {\n-   52: \t\t\tString renderedFailures = renderFailures();\n-   53: \t\t\tthrow log.collectedFailures( process, renderedFailures );\n-   54: \t\t}\n-   55: \t}\n-   56: \n-   57: \tprivate String renderFailures() {\n-   58: \t\tToStringStyle style = ToStringStyle.multilineIndentStructure(\n-   59: \t\t\t\tEngineEventContextMessages.INSTANCE.failureReportContextFailuresSeparator(),\n-   60: \t\t\t\tEngineEventContextMessages.INSTANCE.failureReportContextIndent(),\n-   61: \t\t\t\tEngineEventContextMessages.INSTANCE.failureReportFailuresBulletPoint(),\n-   62: \t\t\t\tEngineEventContextMessages.INSTANCE.failureReportFailuresNoBulletPoint()\n-   63: \t\t);\n-   64: \t\tToStringTreeBuilder builder = new ToStringTreeBuilder( style );\n-   65: \t\tbuilder.startObject();\n-   66: \t\tif ( failureCount.get() > FAILURE_LIMIT ) {\n-   67: \t\t\tbuilder.value( log.collectedFailureLimitReached( process, FAILURE_LIMIT, failureCount.get() ) );\n-   68: \t\t}\n-   69: \t\tif ( delegate != null ) {\n-   70: \t\t\tdelegate.appendChildrenFailuresTo( builder );\n-   71: \t\t}\n-   72: \t\tbuilder.endObject();\n-   73: \t\treturn builder.toString();\n-   74: \t}\n+   50: \n+   51: \tpublic void checkNoFailure() {\n+   52: \t\tif ( failureCount.get() > 0 ) {\n+   53: \t\t\tList<Throwable> failures = new ArrayList<>();\n+   54: \t\t\tToStringStyle style = ToStringStyle.multilineIndentStructure(\n+   55: \t\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportContextFailuresSeparator(),\n+   56: \t\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportContextIndent(),\n+   57: \t\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportFailuresBulletPoint(),\n+   58: \t\t\t\t\tEngineEventContextMessages.INSTANCE.failureReportFailuresNoBulletPoint()\n+   59: \t\t\t);\n+   60: \t\t\tToStringTreeBuilder builder = new ToStringTreeBuilder( style );\n+   61: \t\t\tbuilder.startObject();\n+   62: \t\t\tif ( failureCount.get() > FAILURE_LIMIT ) {\n+   63: \t\t\t\tbuilder.value( log.collectedFailureLimitReached( process, FAILURE_LIMIT, failureCount.get() ) );\n+   64: \t\t\t}\n+   65: \t\t\tif ( delegate != null ) {\n+   66: \t\t\t\tdelegate.appendChildrenFailuresTo( failures, builder );\n+   67: \t\t\t}\n+   68: \t\t\tbuilder.endObject();\n+   69: \t\t\tthrow log.collectedFailures( process, builder.toString(), failures );\n+   70: \t\t}\n+   71: \t}\n+   72: \n+   73: \t@Override\n+   74: \tpublic ContextualFailureCollector withContext(EventContext context) {\n",
        "uniqueId": "66f97c223ef9df56c196dc78e045c544d6d9f05e_50_55__51_71_57_74",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 47
            },
            "BRANCH": {
                "missed": 1,
                "covered": 3
            },
            "LINE": {
                "missed": 0,
                "covered": 13
            },
            "COMPLEXITY": {
                "missed": 1,
                "covered": 2
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tprotected createValueReadHandle(member Member) : ValueReadHandle<?> extracted from package createValueReadHandle(holderClass Class<?>, member Member, ormPropertyMetadata HibernateOrmBasicClassPropertyMetadata) : ValueReadHandle<?> in class org.hibernate.search.mapper.orm.model.impl.HibernateOrmBootstrapIntrospector & moved to class org.hibernate.search.mapper.pojo.model.hcann.spi.AbstractPojoHCAnnBootstrapIntrospector",
        "diffLocations": [
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmBootstrapIntrospector.java",
                "startLine": 116,
                "endLine": 140,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmBootstrapIntrospector.java",
                "startLine": 122,
                "endLine": 133,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmBootstrapIntrospector.java",
                "startLine": 84,
                "endLine": 96,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "ValueReadHandle<?> createValueReadHandle(Class<?> holderClass, Member member,\n\t\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata)\n\t\t\tthrows IllegalAccessException {\n\t\tif ( member instanceof Method ) {\n\t\t\tMethod method = (Method) member;\n\t\t\tsetAccessible( method );\n\t\t\treturn valueHandleFactory.createForMethod( method );\n\t\t}\n\t\telse if ( member instanceof Field ) {\n\t\t\tField field = (Field) member;\n\t\t\tif ( ormPropertyMetadata != null && !ormPropertyMetadata.isId() ) {\n\t\t\t\tMethod bytecodeEnhancerReaderMethod = getBytecodeEnhancerReaderMethod( holderClass, field );\n\t\t\t\tif ( bytecodeEnhancerReaderMethod != null ) {\n\t\t\t\t\tsetAccessible( bytecodeEnhancerReaderMethod );\n\t\t\t\t\treturn valueHandleFactory.createForMethod( bytecodeEnhancerReaderMethod );\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tsetAccessible( field );\n\t\t\treturn valueHandleFactory.createForField( field );\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected type for a \" + Member.class.getName() + \": \" + member );\n\t\t}\n\t}",
        "filePathBefore": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmBootstrapIntrospector.java",
        "isPureRefactoring": true,
        "commitId": "4809368dec30478582c165d2c01aa254f8bf06ab",
        "packageNameBefore": "org.hibernate.search.mapper.orm.model.impl",
        "classNameBefore": "org.hibernate.search.mapper.orm.model.impl.HibernateOrmBootstrapIntrospector",
        "methodNameBefore": "org.hibernate.search.mapper.orm.model.impl.HibernateOrmBootstrapIntrospector#createValueReadHandle",
        "invokedMethod": "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmBootstrapIntrospector#getBytecodeEnhancerReaderMethod\n methodBody: private static Method getBytecodeEnhancerReaderMethod(Class<?> holderClass, Field field) {\nif(!PersistentAttributeInterceptable.class.isAssignableFrom(holderClass)){return null;\n}tryreturn holderClass.getMethod(EnhancerConstants.PERSISTENT_FIELD_READER_PREFIX + field.getName());\ncatch(NoSuchMethodException e)throw new AssertionFailure(\"Read method for enhanced field \" + field + \" is unexpectedly missing.\",e);\n}\nmethodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmBootstrapIntrospector#setAccessible\n methodBody: private static void setAccessible(AccessibleObject member) {\ntrymember.setAccessible(true);\ncatch(SecurityException se)if(!Modifier.isPublic(((Member)member).getModifiers())){throw se;\n}}\nmethodSignature: org.hibernate.search.mapper.pojo.standalone.model.impl.StandalonePojoBootstrapIntrospector#setAccessible\n methodBody: private static void setAccessible(AccessibleObject member) {\ntrymember.setAccessible(true);\ncatch(SecurityException se)if(!Modifier.isPublic(((Member)member).getModifiers())){throw se;\n}}",
        "classSignatureBefore": "public class HibernateOrmBootstrapIntrospector extends AbstractPojoHCAnnBootstrapIntrospector\n\t\timplements PojoBootstrapIntrospector ",
        "methodNameBeforeSet": [
            "org.hibernate.search.mapper.orm.model.impl.HibernateOrmBootstrapIntrospector#createValueReadHandle"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.mapper.orm.model.impl.HibernateOrmBootstrapIntrospector"
        ],
        "classSignatureBeforeSet": [
            "public class HibernateOrmBootstrapIntrospector extends AbstractPojoHCAnnBootstrapIntrospector\n\t\timplements PojoBootstrapIntrospector "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.orm.model.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.lang.reflect.AccessibleObject;\nimport java.lang.reflect.Constructor;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Modifier;\nimport java.util.HashMap;\nimport java.util.LinkedHashSet;\nimport java.util.Map;\nimport java.util.Set;\n\nimport org.hibernate.AssertionFailure;\nimport org.hibernate.annotations.common.reflection.ReflectionManager;\nimport org.hibernate.bytecode.enhance.spi.EnhancerConstants;\nimport org.hibernate.engine.spi.PersistentAttributeInterceptable;\nimport org.hibernate.search.mapper.orm.logging.impl.Log;\nimport org.hibernate.search.mapper.pojo.model.hcann.spi.AbstractPojoHCAnnBootstrapIntrospector;\nimport org.hibernate.search.mapper.pojo.model.hcann.spi.PojoHCannOrmGenericContextHelper;\nimport org.hibernate.search.mapper.pojo.model.spi.AbstractPojoRawTypeModel;\nimport org.hibernate.search.mapper.pojo.model.spi.GenericContextAwarePojoGenericTypeModel.RawTypeDeclaringContext;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoBootstrapIntrospector;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeIdentifier;\nimport org.hibernate.search.util.common.impl.ReflectionHelper;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\nimport org.hibernate.search.util.common.reflect.spi.ValueCreateHandle;\nimport org.hibernate.search.util.common.reflect.spi.ValueHandleFactory;\nimport org.hibernate.search.util.common.reflect.spi.ValueReadHandle;\n\npublic class HibernateOrmBootstrapIntrospector extends AbstractPojoHCAnnBootstrapIntrospector\n\t\timplements PojoBootstrapIntrospector {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tpublic static HibernateOrmBootstrapIntrospector create(\n\t\t\tHibernateOrmBasicTypeMetadataProvider basicTypeMetadataProvider,\n\t\t\tReflectionManager ormReflectionManager,\n\t\t\tValueHandleFactory valueHandleFactory) {\n\t\treturn new HibernateOrmBootstrapIntrospector(\n\t\t\t\tbasicTypeMetadataProvider, ormReflectionManager, valueHandleFactory\n\t\t);\n\t}\n\n\tprivate final HibernateOrmBasicTypeMetadataProvider basicTypeMetadataProvider;\n\tprivate final PojoHCannOrmGenericContextHelper genericContextHelper;\n\n\t/*\n\t * Note: the main purpose of these caches is not to improve performance,\n\t * but to ensure the unicity of the returned PojoTypeModels.\n\t * so as to ensure the unicity of PojoPropertyModels,\n\t * which lowers the risk of generating duplicate ValueReadHandles.\n\t *\n\t * Also, this cache allows to not care at all about implementing equals and hashcode,\n\t * since type models are presumably instantiated only once per type.\n\t */\n\tprivate final Map<Class<?>, HibernateOrmClassRawTypeModel<?>> classTypeModelCache = new HashMap<>();\n\tprivate final Map<String, HibernateOrmDynamicMapRawTypeModel> dynamicMapTypeModelCache = new HashMap<>();\n\n\tprivate HibernateOrmBootstrapIntrospector(\n\t\t\tHibernateOrmBasicTypeMetadataProvider basicTypeMetadataProvider,\n\t\t\tReflectionManager reflectionManager,\n\t\t\tValueHandleFactory valueHandleFactory) {\n\t\tsuper( reflectionManager, valueHandleFactory );\n\t\tthis.basicTypeMetadataProvider = basicTypeMetadataProvider;\n\t\tthis.genericContextHelper = new PojoHCannOrmGenericContextHelper( this );\n\t}\n\n\t@Override\n\tpublic AbstractPojoRawTypeModel<?, ?> typeModel(String name) {\n\t\tHibernateOrmBasicDynamicMapTypeMetadata dynamicMapTypeOrmMetadata =\n\t\t\t\tbasicTypeMetadataProvider.getBasicDynamicMapTypeMetadata( name );\n\t\tif ( dynamicMapTypeOrmMetadata != null ) {\n\t\t\t// Dynamic-map entity *or component* type\n\t\t\treturn dynamicMapTypeModelCache.computeIfAbsent( name, this::createDynamicMapTypeModel );\n\t\t}\n\n\t\tPojoRawTypeIdentifier<?> typeIdentifier = basicTypeMetadataProvider.getTypeIdentifierResolver()\n\t\t\t\t.resolveByJpaOrHibernateOrmEntityName( name );\n\t\tif ( typeIdentifier != null ) {\n\t\t\t// Class entity type\n\t\t\treturn typeModel( typeIdentifier.javaClass() );\n\t\t}\n\n\t\tSet<String> typeNames = new LinkedHashSet<>( basicTypeMetadataProvider.getKnownDynamicMapTypeNames() );\n\t\ttypeNames.addAll( basicTypeMetadataProvider.getTypeIdentifierResolver().allKnownJpaOrHibernateOrmEntityNames() );\n\t\tthrow log.unknownNamedType( name, typeNames );\n\t}\n\n\t@Override\n\t@SuppressWarnings(\"unchecked\")\n\tpublic <T> HibernateOrmClassRawTypeModel<T> typeModel(Class<T> clazz) {\n\t\tif ( clazz.isPrimitive() ) {\n\t\t\t/*\n\t\t\t * We'll never manipulate the primitive type, as we're using generics everywhere,\n\t\t\t * so let's consider every occurrence of the primitive type as an occurrence of its wrapper type.\n\t\t\t */\n\t\t\tclazz = (Class<T>) ReflectionHelper.getPrimitiveWrapperType( clazz );\n\t\t}\n\t\treturn (HibernateOrmClassRawTypeModel<T>) classTypeModelCache.computeIfAbsent( clazz, this::createClassTypeModel );\n\t}\n\n\t@Override\n\tprotected <T> ValueCreateHandle<T> createValueCreateHandle(Constructor<T> constructor) throws IllegalAccessException {\n\t\tsetAccessible( constructor );\n\t\treturn valueHandleFactory.createForConstructor( constructor );\n\t}\n\n\tValueReadHandle<?> createValueReadHandle(Class<?> holderClass, Member member,\n\t\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata)\n\t\t\tthrows IllegalAccessException {\n\t\tif ( member instanceof Method ) {\n\t\t\tMethod method = (Method) member;\n\t\t\tsetAccessible( method );\n\t\t\treturn valueHandleFactory.createForMethod( method );\n\t\t}\n\t\telse if ( member instanceof Field ) {\n\t\t\tField field = (Field) member;\n\t\t\tif ( ormPropertyMetadata != null && !ormPropertyMetadata.isId() ) {\n\t\t\t\tMethod bytecodeEnhancerReaderMethod = getBytecodeEnhancerReaderMethod( holderClass, field );\n\t\t\t\tif ( bytecodeEnhancerReaderMethod != null ) {\n\t\t\t\t\tsetAccessible( bytecodeEnhancerReaderMethod );\n\t\t\t\t\treturn valueHandleFactory.createForMethod( bytecodeEnhancerReaderMethod );\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tsetAccessible( field );\n\t\t\treturn valueHandleFactory.createForField( field );\n\t\t}\n\t\telse {\n\t\t\tthrow new AssertionFailure( \"Unexpected type for a \" + Member.class.getName() + \": \" + member );\n\t\t}\n\t}\n\n\t@SuppressWarnings(\"rawtypes\")\n\tprivate HibernateOrmDynamicMapRawTypeModel createDynamicMapTypeModel(String name) {\n\t\tHibernateOrmBasicDynamicMapTypeMetadata ormMetadata = basicTypeMetadataProvider.getBasicDynamicMapTypeMetadata( name );\n\t\tPojoRawTypeIdentifier<Map> typeIdentifier =\n\t\t\t\tHibernateOrmRawTypeIdentifierResolver.createDynamicMapTypeIdentifier( name );\n\t\treturn new HibernateOrmDynamicMapRawTypeModel(\n\t\t\t\tthis, typeIdentifier, ormMetadata\n\t\t);\n\t}\n\n\tprivate <T> HibernateOrmClassRawTypeModel<T> createClassTypeModel(Class<T> type) {\n\t\tHibernateOrmBasicClassTypeMetadata ormMetadataOrNull =\n\t\t\t\tbasicTypeMetadataProvider.getBasicClassTypeMetadata( type );\n\t\tPojoRawTypeIdentifier<T> typeIdentifier =\n\t\t\t\tHibernateOrmRawTypeIdentifierResolver.createClassTypeIdentifier( type );\n\t\treturn new HibernateOrmClassRawTypeModel<>(\n\t\t\t\tthis, typeIdentifier, ormMetadataOrNull,\n\t\t\t\tnew RawTypeDeclaringContext<>( genericContextHelper, type )\n\t\t);\n\t}\n\n\tprivate static void setAccessible(AccessibleObject member) {\n\t\ttry {\n\t\t\t// always set accessible to true as it bypass the security model checks\n\t\t\t// at execution time and is faster.\n\t\t\tmember.setAccessible( true );\n\t\t}\n\t\tcatch (SecurityException se) {\n\t\t\tif ( !Modifier.isPublic( ( (Member) member ).getModifiers() ) ) {\n\t\t\t\tthrow se;\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * @param holderClass A class exposing the given field.\n\t * @param field A member field from the Hibernate metamodel or from a XProperty.\n\t * @return A method generated through bytecode enhancement that triggers lazy-loading before returning the member's value,\n\t * or {@code null} if there is no such method.\n\t */\n\tprivate static Method getBytecodeEnhancerReaderMethod(Class<?> holderClass, Field field) {\n\t\tif ( !PersistentAttributeInterceptable.class.isAssignableFrom( holderClass ) ) {\n\t\t\t// The declaring class is not enhanced, the only way to access the field is to read it directly.\n\t\t\treturn null;\n\t\t}\n\n\t\t/*\n\t\t * The class is enhanced.\n\t\t * Use the \"magic\" methods that trigger lazy loading instead of accessing the field directly.\n\t\t */\n\t\ttry {\n\t\t\treturn holderClass.getMethod( EnhancerConstants.PERSISTENT_FIELD_READER_PREFIX + field.getName() );\n\t\t}\n\t\tcatch (NoSuchMethodException e) {\n\t\t\tthrow new AssertionFailure( \"Read method for enhanced field \" + field + \" is unexpectedly missing.\", e );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmBootstrapIntrospector.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.orm.model.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.lang.reflect.AccessibleObject;\nimport java.lang.reflect.Constructor;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.lang.reflect.Modifier;\nimport java.util.HashMap;\nimport java.util.LinkedHashSet;\nimport java.util.Map;\nimport java.util.Set;\n\nimport org.hibernate.AssertionFailure;\nimport org.hibernate.annotations.common.reflection.ReflectionManager;\nimport org.hibernate.bytecode.enhance.spi.EnhancerConstants;\nimport org.hibernate.engine.spi.PersistentAttributeInterceptable;\nimport org.hibernate.search.mapper.orm.logging.impl.Log;\nimport org.hibernate.search.mapper.pojo.model.hcann.spi.AbstractPojoHCAnnBootstrapIntrospector;\nimport org.hibernate.search.mapper.pojo.model.hcann.spi.PojoHCannOrmGenericContextHelper;\nimport org.hibernate.search.mapper.pojo.model.spi.AbstractPojoRawTypeModel;\nimport org.hibernate.search.mapper.pojo.model.spi.GenericContextAwarePojoGenericTypeModel.RawTypeDeclaringContext;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoBootstrapIntrospector;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeIdentifier;\nimport org.hibernate.search.util.common.impl.ReflectionHelper;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\nimport org.hibernate.search.util.common.reflect.spi.ValueCreateHandle;\nimport org.hibernate.search.util.common.reflect.spi.ValueHandleFactory;\nimport org.hibernate.search.util.common.reflect.spi.ValueReadHandle;\n\npublic class HibernateOrmBootstrapIntrospector extends AbstractPojoHCAnnBootstrapIntrospector\n\t\timplements PojoBootstrapIntrospector {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tpublic static HibernateOrmBootstrapIntrospector create(\n\t\t\tHibernateOrmBasicTypeMetadataProvider basicTypeMetadataProvider,\n\t\t\tReflectionManager ormReflectionManager,\n\t\t\tValueHandleFactory valueHandleFactory) {\n\t\treturn new HibernateOrmBootstrapIntrospector(\n\t\t\t\tbasicTypeMetadataProvider, ormReflectionManager, valueHandleFactory\n\t\t);\n\t}\n\n\tprivate final HibernateOrmBasicTypeMetadataProvider basicTypeMetadataProvider;\n\tprivate final PojoHCannOrmGenericContextHelper genericContextHelper;\n\n\t/*\n\t * Note: the main purpose of these caches is not to improve performance,\n\t * but to ensure the unicity of the returned PojoTypeModels.\n\t * so as to ensure the unicity of PojoPropertyModels,\n\t * which lowers the risk of generating duplicate ValueReadHandles.\n\t *\n\t * Also, this cache allows to not care at all about implementing equals and hashcode,\n\t * since type models are presumably instantiated only once per type.\n\t */\n\tprivate final Map<Class<?>, HibernateOrmClassRawTypeModel<?>> classTypeModelCache = new HashMap<>();\n\tprivate final Map<String, HibernateOrmDynamicMapRawTypeModel> dynamicMapTypeModelCache = new HashMap<>();\n\n\tprivate HibernateOrmBootstrapIntrospector(\n\t\t\tHibernateOrmBasicTypeMetadataProvider basicTypeMetadataProvider,\n\t\t\tReflectionManager reflectionManager,\n\t\t\tValueHandleFactory valueHandleFactory) {\n\t\tsuper( reflectionManager, valueHandleFactory );\n\t\tthis.basicTypeMetadataProvider = basicTypeMetadataProvider;\n\t\tthis.genericContextHelper = new PojoHCannOrmGenericContextHelper( this );\n\t}\n\n\t@Override\n\tpublic AbstractPojoRawTypeModel<?, ?> typeModel(String name) {\n\t\tHibernateOrmBasicDynamicMapTypeMetadata dynamicMapTypeOrmMetadata =\n\t\t\t\tbasicTypeMetadataProvider.getBasicDynamicMapTypeMetadata( name );\n\t\tif ( dynamicMapTypeOrmMetadata != null ) {\n\t\t\t// Dynamic-map entity *or component* type\n\t\t\treturn dynamicMapTypeModelCache.computeIfAbsent( name, this::createDynamicMapTypeModel );\n\t\t}\n\n\t\tPojoRawTypeIdentifier<?> typeIdentifier = basicTypeMetadataProvider.getTypeIdentifierResolver()\n\t\t\t\t.resolveByJpaOrHibernateOrmEntityName( name );\n\t\tif ( typeIdentifier != null ) {\n\t\t\t// Class entity type\n\t\t\treturn typeModel( typeIdentifier.javaClass() );\n\t\t}\n\n\t\tSet<String> typeNames = new LinkedHashSet<>( basicTypeMetadataProvider.getKnownDynamicMapTypeNames() );\n\t\ttypeNames.addAll( basicTypeMetadataProvider.getTypeIdentifierResolver().allKnownJpaOrHibernateOrmEntityNames() );\n\t\tthrow log.unknownNamedType( name, typeNames );\n\t}\n\n\t@Override\n\t@SuppressWarnings(\"unchecked\")\n\tpublic <T> HibernateOrmClassRawTypeModel<T> typeModel(Class<T> clazz) {\n\t\tif ( clazz.isPrimitive() ) {\n\t\t\t/*\n\t\t\t * We'll never manipulate the primitive type, as we're using generics everywhere,\n\t\t\t * so let's consider every occurrence of the primitive type as an occurrence of its wrapper type.\n\t\t\t */\n\t\t\tclazz = (Class<T>) ReflectionHelper.getPrimitiveWrapperType( clazz );\n\t\t}\n\t\treturn (HibernateOrmClassRawTypeModel<T>) classTypeModelCache.computeIfAbsent( clazz, this::createClassTypeModel );\n\t}\n\n\t@Override\n\tprotected <T> ValueCreateHandle<T> createValueCreateHandle(Constructor<T> constructor) throws IllegalAccessException {\n\t\tsetAccessible( constructor );\n\t\treturn valueHandleFactory.createForConstructor( constructor );\n\t}\n\n\t@Override\n\tprotected ValueReadHandle<?> createValueReadHandle(Member member) throws IllegalAccessException {\n\t\tsetAccessible( member );\n\t\treturn super.createValueReadHandle( member );\n\t}\n\n\tValueReadHandle<?> createValueReadHandle(Class<?> holderClass, Member member,\n\t\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata)\n\t\t\tthrows IllegalAccessException {\n\t\tif ( member instanceof Field && ormPropertyMetadata != null && !ormPropertyMetadata.isId() ) {\n\t\t\tMethod bytecodeEnhancerReaderMethod = getBytecodeEnhancerReaderMethod( holderClass, (Field) member );\n\t\t\tif ( bytecodeEnhancerReaderMethod != null ) {\n\t\t\t\treturn createValueReadHandle( bytecodeEnhancerReaderMethod );\n\t\t\t}\n\t\t}\n\n\t\treturn createValueReadHandle( member );\n\t}\n\n\t@SuppressWarnings(\"rawtypes\")\n\tprivate HibernateOrmDynamicMapRawTypeModel createDynamicMapTypeModel(String name) {\n\t\tHibernateOrmBasicDynamicMapTypeMetadata ormMetadata = basicTypeMetadataProvider.getBasicDynamicMapTypeMetadata( name );\n\t\tPojoRawTypeIdentifier<Map> typeIdentifier =\n\t\t\t\tHibernateOrmRawTypeIdentifierResolver.createDynamicMapTypeIdentifier( name );\n\t\treturn new HibernateOrmDynamicMapRawTypeModel(\n\t\t\t\tthis, typeIdentifier, ormMetadata\n\t\t);\n\t}\n\n\tprivate <T> HibernateOrmClassRawTypeModel<T> createClassTypeModel(Class<T> type) {\n\t\tHibernateOrmBasicClassTypeMetadata ormMetadataOrNull =\n\t\t\t\tbasicTypeMetadataProvider.getBasicClassTypeMetadata( type );\n\t\tPojoRawTypeIdentifier<T> typeIdentifier =\n\t\t\t\tHibernateOrmRawTypeIdentifierResolver.createClassTypeIdentifier( type );\n\t\treturn new HibernateOrmClassRawTypeModel<>(\n\t\t\t\tthis, typeIdentifier, ormMetadataOrNull,\n\t\t\t\tnew RawTypeDeclaringContext<>( genericContextHelper, type )\n\t\t);\n\t}\n\n\tprivate static void setAccessible(Member member) {\n\t\ttry {\n\t\t\t// always try to set accessible to true regardless of visibility\n\t\t\t// as it's faster even for public fields:\n\t\t\t// it bypasses the security model checks at execution time.\n\t\t\t( (AccessibleObject) member ).setAccessible( true );\n\t\t}\n\t\tcatch (SecurityException se) {\n\t\t\tif ( !Modifier.isPublic( member.getModifiers() ) ) {\n\t\t\t\tthrow se;\n\t\t\t}\n\t\t}\n\t}\n\n\t/**\n\t * @param holderClass A class exposing the given field.\n\t * @param field A member field from the Hibernate metamodel or from a XProperty.\n\t * @return A method generated through bytecode enhancement that triggers lazy-loading before returning the member's value,\n\t * or {@code null} if there is no such method.\n\t */\n\tprivate static Method getBytecodeEnhancerReaderMethod(Class<?> holderClass, Field field) {\n\t\tif ( !PersistentAttributeInterceptable.class.isAssignableFrom( holderClass ) ) {\n\t\t\t// The declaring class is not enhanced, the only way to access the field is to read it directly.\n\t\t\treturn null;\n\t\t}\n\n\t\t/*\n\t\t * The class is enhanced.\n\t\t * Use the \"magic\" methods that trigger lazy loading instead of accessing the field directly.\n\t\t */\n\t\ttry {\n\t\t\treturn holderClass.getMethod( EnhancerConstants.PERSISTENT_FIELD_READER_PREFIX + field.getName() );\n\t\t}\n\t\tcatch (NoSuchMethodException e) {\n\t\t\tthrow new AssertionFailure( \"Read method for enhanced field \" + field + \" is unexpectedly missing.\", e );\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "PojoRawTypeIdentifier<?> typeIdentifier = basicTypeMetadataProvider.getTypeIdentifierResolver()\n\t\t\t\t.resolveByJpaOrHibernateOrmEntityName( name );\n\t\tif ( typeIdentifier != null ) {\n\t\t\t// Class entity type\n\t\t\treturn typeModel( typeIdentifier.javaClass() );\n\t\t}\n\n\t\tSet<String> typeNames = new LinkedHashSet<>( basicTypeMetadataProvider.getKnownDynamicMapTypeNames() );\n\t\ttypeNames.addAll( basicTypeMetadataProvider.getTypeIdentifierResolver().allKnownJpaOrHibernateOrmEntityNames() );\n\t\tthrow log.unknownNamedType( name, typeNames );\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmBootstrapIntrospector#getBytecodeEnhancerReaderMethod\n methodBody: private static Method getBytecodeEnhancerReaderMethod(Class<?> holderClass, Field field) {\nif(!PersistentAttributeInterceptable.class.isAssignableFrom(holderClass)){return null;\n}tryreturn holderClass.getMethod(EnhancerConstants.PERSISTENT_FIELD_READER_PREFIX + field.getName());\ncatch(NoSuchMethodException e)throw new AssertionFailure(\"Read method for enhanced field \" + field + \" is unexpectedly missing.\",e);\n}",
            "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmBootstrapIntrospector#setAccessible\n methodBody: private static void setAccessible(AccessibleObject member) {\ntrymember.setAccessible(true);\ncatch(SecurityException se)if(!Modifier.isPublic(((Member)member).getModifiers())){throw se;\n}}",
            "methodSignature: org.hibernate.search.mapper.pojo.standalone.model.impl.StandalonePojoBootstrapIntrospector#setAccessible\n methodBody: private static void setAccessible(AccessibleObject member) {\ntrymember.setAccessible(true);\ncatch(SecurityException se)if(!Modifier.isPublic(((Member)member).getModifiers())){throw se;\n}}"
        ],
        "sourceCodeAfterRefactoring": "ValueReadHandle<?> createValueReadHandle(Class<?> holderClass, Member member,\n\t\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata)\n\t\t\tthrows IllegalAccessException {\n\t\tif ( member instanceof Field && ormPropertyMetadata != null && !ormPropertyMetadata.isId() ) {\n\t\t\tMethod bytecodeEnhancerReaderMethod = getBytecodeEnhancerReaderMethod( holderClass, (Field) member );\n\t\t\tif ( bytecodeEnhancerReaderMethod != null ) {\n\t\t\t\treturn createValueReadHandle( bytecodeEnhancerReaderMethod );\n\t\t\t}\n\t\t}\n\n\t\treturn createValueReadHandle( member );\n\t}\nPojoRawTypeIdentifier<?> typeIdentifier = basicTypeMetadataProvider.getTypeIdentifierResolver()\n\t\t\t\t.resolveByJpaOrHibernateOrmEntityName( name );\n\t\tif ( typeIdentifier != null ) {\n\t\t\t// Class entity type\n\t\t\treturn typeModel( typeIdentifier.javaClass() );\n\t\t}\n\n\t\tSet<String> typeNames = new LinkedHashSet<>( basicTypeMetadataProvider.getKnownDynamicMapTypeNames() );\n\t\ttypeNames.addAll( basicTypeMetadataProvider.getTypeIdentifierResolver().allKnownJpaOrHibernateOrmEntityNames() );\n\t\tthrow log.unknownNamedType( name, typeNames );\n\t}",
        "diffSourceCode": "    84: \n    85: \t\tPojoRawTypeIdentifier<?> typeIdentifier = basicTypeMetadataProvider.getTypeIdentifierResolver()\n    86: \t\t\t\t.resolveByJpaOrHibernateOrmEntityName( name );\n    87: \t\tif ( typeIdentifier != null ) {\n    88: \t\t\t// Class entity type\n    89: \t\t\treturn typeModel( typeIdentifier.javaClass() );\n    90: \t\t}\n    91: \n    92: \t\tSet<String> typeNames = new LinkedHashSet<>( basicTypeMetadataProvider.getKnownDynamicMapTypeNames() );\n    93: \t\ttypeNames.addAll( basicTypeMetadataProvider.getTypeIdentifierResolver().allKnownJpaOrHibernateOrmEntityNames() );\n    94: \t\tthrow log.unknownNamedType( name, typeNames );\n    95: \t}\n    96: \n-  116: \tValueReadHandle<?> createValueReadHandle(Class<?> holderClass, Member member,\n-  117: \t\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata)\n-  118: \t\t\tthrows IllegalAccessException {\n-  119: \t\tif ( member instanceof Method ) {\n-  120: \t\t\tMethod method = (Method) member;\n-  121: \t\t\tsetAccessible( method );\n-  122: \t\t\treturn valueHandleFactory.createForMethod( method );\n-  123: \t\t}\n-  124: \t\telse if ( member instanceof Field ) {\n-  125: \t\t\tField field = (Field) member;\n-  126: \t\t\tif ( ormPropertyMetadata != null && !ormPropertyMetadata.isId() ) {\n-  127: \t\t\t\tMethod bytecodeEnhancerReaderMethod = getBytecodeEnhancerReaderMethod( holderClass, field );\n-  128: \t\t\t\tif ( bytecodeEnhancerReaderMethod != null ) {\n-  129: \t\t\t\t\tsetAccessible( bytecodeEnhancerReaderMethod );\n-  130: \t\t\t\t\treturn valueHandleFactory.createForMethod( bytecodeEnhancerReaderMethod );\n-  131: \t\t\t\t}\n-  132: \t\t\t}\n-  133: \n-  134: \t\t\tsetAccessible( field );\n-  135: \t\t\treturn valueHandleFactory.createForField( field );\n-  136: \t\t}\n-  137: \t\telse {\n-  138: \t\t\tthrow new AssertionFailure( \"Unexpected type for a \" + Member.class.getName() + \": \" + member );\n-  139: \t\t}\n-  140: \t}\n+  116: \t@Override\n+  117: \tprotected ValueReadHandle<?> createValueReadHandle(Member member) throws IllegalAccessException {\n+  118: \t\tsetAccessible( member );\n+  119: \t\treturn super.createValueReadHandle( member );\n+  120: \t}\n+  121: \n+  122: \tValueReadHandle<?> createValueReadHandle(Class<?> holderClass, Member member,\n+  123: \t\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata)\n+  124: \t\t\tthrows IllegalAccessException {\n+  125: \t\tif ( member instanceof Field && ormPropertyMetadata != null && !ormPropertyMetadata.isId() ) {\n+  126: \t\t\tMethod bytecodeEnhancerReaderMethod = getBytecodeEnhancerReaderMethod( holderClass, (Field) member );\n+  127: \t\t\tif ( bytecodeEnhancerReaderMethod != null ) {\n+  128: \t\t\t\treturn createValueReadHandle( bytecodeEnhancerReaderMethod );\n+  129: \t\t\t}\n+  130: \t\t}\n+  131: \n+  132: \t\treturn createValueReadHandle( member );\n+  133: \t}\n+  134: \n+  135: \t@SuppressWarnings(\"rawtypes\")\n+  136: \tprivate HibernateOrmDynamicMapRawTypeModel createDynamicMapTypeModel(String name) {\n+  137: \t\tHibernateOrmBasicDynamicMapTypeMetadata ormMetadata = basicTypeMetadataProvider.getBasicDynamicMapTypeMetadata( name );\n+  138: \t\tPojoRawTypeIdentifier<Map> typeIdentifier =\n+  139: \t\t\t\tHibernateOrmRawTypeIdentifierResolver.createDynamicMapTypeIdentifier( name );\n+  140: \t\treturn new HibernateOrmDynamicMapRawTypeModel(\n",
        "uniqueId": "4809368dec30478582c165d2c01aa254f8bf06ab_116_140_84_96_122_133",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 17,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 15,
                "covered": 37
            },
            "BRANCH": {
                "missed": 3,
                "covered": 7
            },
            "LINE": {
                "missed": 3,
                "covered": 11
            },
            "COMPLEXITY": {
                "missed": 3,
                "covered": 3
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate createProtocolDialectV5(version ElasticsearchVersion, minor int) : ElasticsearchProtocolDialect extracted from public createProtocolDialect(version ElasticsearchVersion) : ElasticsearchProtocolDialect in class org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory",
        "diffLocations": [
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
                "startLine": 60,
                "endLine": 107,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
                "startLine": 52,
                "endLine": 79,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
                "startLine": 93,
                "endLine": 102,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "public ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n\t\t\t);\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\tif ( minor < 6 ) {\n\t\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t\t}\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( minor != 6 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch56ProtocolDialect();\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\tif ( minor < 3 ) {\n\t\t\t\treturn new Elasticsearch60ProtocolDialect();\n\t\t\t}\n\t\t\tif ( minor < 4 ) {\n\t\t\t\treturn new Elasticsearch63ProtocolDialect();\n\t\t\t}\n\t\t\tif ( minor < 7 ) {\n\t\t\t\treturn new Elasticsearch64ProtocolDialect();\n\t\t\t}\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( minor > 8 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch67ProtocolDialect();\n\t\t}\n\t\telse {\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( major != 7 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch70ProtocolDialect();\n\t\t}\n\t}",
        "filePathBefore": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
        "isPureRefactoring": true,
        "commitId": "b77e41600e4d9e32170dbb10d77e5f676f429d42",
        "packageNameBefore": "org.hibernate.search.backend.elasticsearch.dialect.impl",
        "classNameBefore": "org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory",
        "methodNameBefore": "org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory#createProtocolDialect",
        "classSignatureBefore": "public class ElasticsearchDialectFactory ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory#createProtocolDialect"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.dialect.impl.ElasticsearchDialectFactory"
        ],
        "classSignatureBeforeSet": [
            "public class ElasticsearchDialectFactory "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.dialect.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.OptionalInt;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchVersion;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch56ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch6ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch7ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.ElasticsearchModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch56ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch60ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch63ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch64ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch67ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch70ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.ElasticsearchProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * Allows to create an Elasticsearch dialect by detecting the version of a remote cluster.\n */\npublic class ElasticsearchDialectFactory {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tpublic ElasticsearchModelDialect createModelDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\t\tOptionalInt minorOptional = version.minor();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\tif ( !minorOptional.isPresent() ) {\n\t\t\t\tthrow log.ambiguousElasticsearchVersion( version );\n\t\t\t}\n\t\t\tint minor = minorOptional.getAsInt();\n\t\t\tif ( minor < 6 ) {\n\t\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch56ModelDialect();\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn new Elasticsearch6ModelDialect();\n\t\t}\n\t\telse {\n\t\t\treturn new Elasticsearch7ModelDialect();\n\t\t}\n\t}\n\n\tpublic ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n\t\t\t);\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\tif ( minor < 6 ) {\n\t\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t\t}\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( minor != 6 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch56ProtocolDialect();\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\tif ( minor < 3 ) {\n\t\t\t\treturn new Elasticsearch60ProtocolDialect();\n\t\t\t}\n\t\t\tif ( minor < 4 ) {\n\t\t\t\treturn new Elasticsearch63ProtocolDialect();\n\t\t\t}\n\t\t\tif ( minor < 7 ) {\n\t\t\t\treturn new Elasticsearch64ProtocolDialect();\n\t\t\t}\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( minor > 8 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch67ProtocolDialect();\n\t\t}\n\t\telse {\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( major != 7 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch70ProtocolDialect();\n\t\t}\n\t}\n\n}\n",
        "filePathAfter": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/dialect/impl/ElasticsearchDialectFactory.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.dialect.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.OptionalInt;\n\nimport org.hibernate.search.backend.elasticsearch.ElasticsearchVersion;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch56ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch6ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.Elasticsearch7ModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.model.impl.ElasticsearchModelDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch56ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch60ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch63ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch64ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch67ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.Elasticsearch70ProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.dialect.protocol.impl.ElasticsearchProtocolDialect;\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.util.common.AssertionFailure;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * Allows to create an Elasticsearch dialect by detecting the version of a remote cluster.\n */\npublic class ElasticsearchDialectFactory {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tpublic ElasticsearchModelDialect createModelDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\treturn createModelDialectV5( version );\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn new Elasticsearch6ModelDialect();\n\t\t}\n\t\telse {\n\t\t\treturn new Elasticsearch7ModelDialect();\n\t\t}\n\t}\n\n\tpublic ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n\t\t\t);\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\treturn createProtocolDialectV5( version, minor );\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn createProtocolDialectV6( version, minor );\n\t\t}\n\t\telse {\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( major != 7 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch70ProtocolDialect();\n\t\t}\n\t}\n\n\tprivate ElasticsearchModelDialect createModelDialectV5(ElasticsearchVersion version) {\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\tthrow log.ambiguousElasticsearchVersion( version );\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\t\tif ( minor < 6 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch56ModelDialect();\n\t}\n\n\tprivate ElasticsearchProtocolDialect createProtocolDialectV5(ElasticsearchVersion version, int minor) {\n\t\tif ( minor < 6 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\t// Either the latest supported version, or a newer/unknown one\n\t\tif ( minor != 6 ) {\n\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch56ProtocolDialect();\n\t}\n\n\tprivate ElasticsearchProtocolDialect createProtocolDialectV6(ElasticsearchVersion version, int minor) {\n\t\tif ( minor < 3 ) {\n\t\t\treturn new Elasticsearch60ProtocolDialect();\n\t\t}\n\t\tif ( minor < 4 ) {\n\t\t\treturn new Elasticsearch63ProtocolDialect();\n\t\t}\n\t\tif ( minor < 7 ) {\n\t\t\treturn new Elasticsearch64ProtocolDialect();\n\t\t}\n\t\t// Either the latest supported version, or a newer/unknown one\n\t\tif ( minor > 8 ) {\n\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch67ProtocolDialect();\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "private ElasticsearchProtocolDialect createProtocolDialectV5(ElasticsearchVersion version, int minor) {\n\t\tif ( minor < 6 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\t// Either the latest supported version, or a newer/unknown one\n\t\tif ( minor != 6 ) {\n\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch56ProtocolDialect();\n\t}"
        ],
        "invokedMethodSet": [],
        "sourceCodeAfterRefactoring": "public ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n\t\tint major = version.major();\n\t\tOptionalInt minorOptional = version.minor();\n\t\tif ( !minorOptional.isPresent() ) {\n\t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n\t\t\tthrow new AssertionFailure(\n\t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n\t\t\t);\n\t\t}\n\t\tint minor = minorOptional.getAsInt();\n\n\t\tif ( major < 5 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\telse if ( major == 5 ) {\n\t\t\treturn createProtocolDialectV5( version, minor );\n\t\t}\n\t\telse if ( major == 6 ) {\n\t\t\treturn createProtocolDialectV6( version, minor );\n\t\t}\n\t\telse {\n\t\t\t// Either the latest supported version, or a newer/unknown one\n\t\t\tif ( major != 7 ) {\n\t\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t\t}\n\t\t\treturn new Elasticsearch70ProtocolDialect();\n\t\t}\n\t}\nprivate ElasticsearchProtocolDialect createProtocolDialectV5(ElasticsearchVersion version, int minor) {\n\t\tif ( minor < 6 ) {\n\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n\t\t}\n\t\t// Either the latest supported version, or a newer/unknown one\n\t\tif ( minor != 6 ) {\n\t\t\tlog.unknownElasticsearchVersion( version );\n\t\t}\n\t\treturn new Elasticsearch56ProtocolDialect();\n\t}",
        "diffSourceCode": "-   52: \t\telse if ( major == 6 ) {\n-   53: \t\t\treturn new Elasticsearch6ModelDialect();\n-   54: \t\t}\n-   55: \t\telse {\n-   56: \t\t\treturn new Elasticsearch7ModelDialect();\n-   57: \t\t}\n-   58: \t}\n-   59: \n-   60: \tpublic ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n-   61: \t\tint major = version.major();\n-   62: \t\tOptionalInt minorOptional = version.minor();\n-   63: \t\tif ( !minorOptional.isPresent() ) {\n-   64: \t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n-   65: \t\t\tthrow new AssertionFailure(\n-   66: \t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n-   67: \t\t\t);\n+   52: \tpublic ElasticsearchProtocolDialect createProtocolDialect(ElasticsearchVersion version) {\n+   53: \t\tint major = version.major();\n+   54: \t\tOptionalInt minorOptional = version.minor();\n+   55: \t\tif ( !minorOptional.isPresent() ) {\n+   56: \t\t\t// The version is supposed to be fetched from the cluster itself, so it should be complete\n+   57: \t\t\tthrow new AssertionFailure(\n+   58: \t\t\t\t\t\"The Elasticsearch version is incomplete when creating the protocol dialect.\"\n+   59: \t\t\t);\n+   60: \t\t}\n+   61: \t\tint minor = minorOptional.getAsInt();\n+   62: \n+   63: \t\tif ( major < 5 ) {\n+   64: \t\t\tthrow log.unsupportedElasticsearchVersion( version );\n+   65: \t\t}\n+   66: \t\telse if ( major == 5 ) {\n+   67: \t\t\treturn createProtocolDialectV5( version, minor );\n    68: \t\t}\n-   69: \t\tint minor = minorOptional.getAsInt();\n-   70: \n-   71: \t\tif ( major < 5 ) {\n-   72: \t\t\tthrow log.unsupportedElasticsearchVersion( version );\n-   73: \t\t}\n-   74: \t\telse if ( major == 5 ) {\n-   75: \t\t\tif ( minor < 6 ) {\n-   76: \t\t\t\tthrow log.unsupportedElasticsearchVersion( version );\n-   77: \t\t\t}\n-   78: \t\t\t// Either the latest supported version, or a newer/unknown one\n-   79: \t\t\tif ( minor != 6 ) {\n-   80: \t\t\t\tlog.unknownElasticsearchVersion( version );\n-   81: \t\t\t}\n-   82: \t\t\treturn new Elasticsearch56ProtocolDialect();\n-   83: \t\t}\n-   84: \t\telse if ( major == 6 ) {\n-   85: \t\t\tif ( minor < 3 ) {\n-   86: \t\t\t\treturn new Elasticsearch60ProtocolDialect();\n-   87: \t\t\t}\n-   88: \t\t\tif ( minor < 4 ) {\n-   89: \t\t\t\treturn new Elasticsearch63ProtocolDialect();\n-   90: \t\t\t}\n-   91: \t\t\tif ( minor < 7 ) {\n-   92: \t\t\t\treturn new Elasticsearch64ProtocolDialect();\n-   93: \t\t\t}\n-   94: \t\t\t// Either the latest supported version, or a newer/unknown one\n-   95: \t\t\tif ( minor > 8 ) {\n-   96: \t\t\t\tlog.unknownElasticsearchVersion( version );\n-   97: \t\t\t}\n-   98: \t\t\treturn new Elasticsearch67ProtocolDialect();\n-   99: \t\t}\n-  100: \t\telse {\n-  101: \t\t\t// Either the latest supported version, or a newer/unknown one\n-  102: \t\t\tif ( major != 7 ) {\n-  103: \t\t\t\tlog.unknownElasticsearchVersion( version );\n-  104: \t\t\t}\n-  105: \t\t\treturn new Elasticsearch70ProtocolDialect();\n-  106: \t\t}\n-  107: \t}\n+   69: \t\telse if ( major == 6 ) {\n+   70: \t\t\treturn createProtocolDialectV6( version, minor );\n+   71: \t\t}\n+   72: \t\telse {\n+   73: \t\t\t// Either the latest supported version, or a newer/unknown one\n+   74: \t\t\tif ( major != 7 ) {\n+   75: \t\t\t\tlog.unknownElasticsearchVersion( version );\n+   76: \t\t\t}\n+   77: \t\t\treturn new Elasticsearch70ProtocolDialect();\n+   78: \t\t}\n+   79: \t}\n+   80: \n+   81: \tprivate ElasticsearchModelDialect createModelDialectV5(ElasticsearchVersion version) {\n+   82: \t\tOptionalInt minorOptional = version.minor();\n+   83: \t\tif ( !minorOptional.isPresent() ) {\n+   84: \t\t\tthrow log.ambiguousElasticsearchVersion( version );\n+   85: \t\t}\n+   86: \t\tint minor = minorOptional.getAsInt();\n+   87: \t\tif ( minor < 6 ) {\n+   88: \t\t\tthrow log.unsupportedElasticsearchVersion( version );\n+   89: \t\t}\n+   90: \t\treturn new Elasticsearch56ModelDialect();\n+   91: \t}\n+   92: \n+   93: \tprivate ElasticsearchProtocolDialect createProtocolDialectV5(ElasticsearchVersion version, int minor) {\n+   94: \t\tif ( minor < 6 ) {\n+   95: \t\t\tthrow log.unsupportedElasticsearchVersion( version );\n+   96: \t\t}\n+   97: \t\t// Either the latest supported version, or a newer/unknown one\n+   98: \t\tif ( minor != 6 ) {\n+   99: \t\t\tlog.unknownElasticsearchVersion( version );\n+  100: \t\t}\n+  101: \t\treturn new Elasticsearch56ProtocolDialect();\n+  102: \t}\n+  103: \n+  104: \tprivate ElasticsearchProtocolDialect createProtocolDialectV6(ElasticsearchVersion version, int minor) {\n+  105: \t\tif ( minor < 3 ) {\n+  106: \t\t\treturn new Elasticsearch60ProtocolDialect();\n+  107: \t\t}\n",
        "uniqueId": "b77e41600e4d9e32170dbb10d77e5f676f429d42_60_107_93_102_52_79",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 9,
                "covered": 41
            },
            "BRANCH": {
                "missed": 2,
                "covered": 8
            },
            "LINE": {
                "missed": 2,
                "covered": 12
            },
            "COMPLEXITY": {
                "missed": 2,
                "covered": 4
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate countBooksByCategory(entityManager EntityManager, categoryNameTerms String) : long extracted from public noReindexing() : void in class org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.shallow.correct.ReindexOnUpdateShallowIT",
        "diffLocations": [
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/mapper/orm/reindexing/reindexonupdate/ReindexOnUpdateIT.java",
                "startLine": 60,
                "endLine": 102,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/mapper/orm/reindexing/reindexonupdate/shallow/correct/ReindexOnUpdateShallowIT.java",
                "startLine": 60,
                "endLine": 116,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "documentation/src/test/java/org/hibernate/search/documentation/mapper/orm/reindexing/reindexonupdate/shallow/correct/ReindexOnUpdateShallowIT.java",
                "startLine": 118,
                "endLine": 122,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void noReindexing() {\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tBookCategory category = new BookCategory();\n\t\t\tcategory.setId( 1 );\n\t\t\tcategory.setName( \"Science-fiction\" );\n\t\t\tentityManager.persist( category );\n\n\t\t\tfor ( int i = 0 ; i < 100 ; ++i ) {\n\t\t\t\tBook book = new Book();\n\t\t\t\tbook.setId( i );\n\t\t\t\tbook.setTitle( \"Book \" + i );\n\t\t\t\tbook.setCategory( category );\n\t\t\t\tentityManager.persist( book );\n\t\t\t}\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tSearchSession searchSession = Search.session( entityManager );\n\n\t\t\tlong hitCount = searchSession.search( Book.class )\n\t\t\t\t\t.where( f -> f.match().field( \"category.name\" ).matching( \"science\" ) )\n\t\t\t\t\t.fetchTotalHitCount();\n\t\t\tassertThat( hitCount ).isEqualTo( 100L );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tBookCategory category = entityManager.getReference( BookCategory.class, 1 );\n\t\t\tcategory.setName( \"Anticipation\" );\n\t\t\tentityManager.persist( category );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tSearchSession searchSession = Search.session( entityManager );\n\n\t\t\tlong hitCount = searchSession.search( Book.class )\n\t\t\t\t\t.where( f -> f.match().field( \"category.name\" ).matching( \"science\" ) )\n\t\t\t\t\t.fetchTotalHitCount();\n\t\t\t// The books haven't been reindexed, as expected.\n\t\t\tassertThat( hitCount ).isEqualTo( 100L );\n\t\t} );\n\n\t}",
        "filePathBefore": "documentation/src/test/java/org/hibernate/search/documentation/mapper/orm/reindexing/reindexonupdate/ReindexOnUpdateIT.java",
        "isPureRefactoring": true,
        "commitId": "bb500fd954c9c039e32b9549bc1b18bcd28b86ab",
        "packageNameBefore": "org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate",
        "classNameBefore": "org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.ReindexOnUpdateIT",
        "methodNameBefore": "org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.ReindexOnUpdateIT#noReindexing",
        "invokedMethod": "methodSignature: org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.BookCategory#setName\n methodBody: public void setName(String name) {\nthis.name=name;\n}\nmethodSignature: org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.Book#setId\n methodBody: public void setId(Integer id) {\nthis.id=id;\n}\nmethodSignature: org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.Book#setCategory\n methodBody: public void setCategory(BookCategory category) {\nthis.category=category;\n}\nmethodSignature: org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.Book#setTitle\n methodBody: public void setTitle(String title) {\nthis.title=title;\n}\nmethodSignature: org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.BookCategory#setId\n methodBody: public void setId(Integer id) {\nthis.id=id;\n}",
        "classSignatureBefore": "public class ReindexOnUpdateIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.ReindexOnUpdateIT#noReindexing"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.ReindexOnUpdateIT"
        ],
        "classSignatureBeforeSet": [
            "public class ReindexOnUpdateIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport java.util.List;\nimport javax.persistence.EntityManagerFactory;\n\nimport org.hibernate.search.documentation.testsupport.BackendConfigurations;\nimport org.hibernate.search.documentation.testsupport.DocumentationSetupHelper;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.session.SearchSession;\nimport org.hibernate.search.mapper.pojo.automaticindexing.ReindexOnUpdate;\nimport org.hibernate.search.mapper.pojo.mapping.definition.programmatic.TypeMappingStep;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils;\n\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.junit.runners.Parameterized;\n\n@RunWith(Parameterized.class)\npublic class ReindexOnUpdateIT {\n\n\t@Parameterized.Parameters(name = \"{0}\")\n\tpublic static List<?> params() {\n\t\treturn DocumentationSetupHelper.testParamsForBothAnnotationsAndProgrammatic(\n\t\t\t\tBackendConfigurations.simple(),\n\t\t\t\tmapping -> {\n\t\t\t\t\t//tag::programmatic[]\n\t\t\t\t\tTypeMappingStep bookMapping = mapping.type( Book.class );\n\t\t\t\t\tbookMapping.indexed();\n\t\t\t\t\tbookMapping.property( \"category\" )\n\t\t\t\t\t\t\t.indexedEmbedded()\n\t\t\t\t\t\t\t.indexingDependency().reindexOnUpdate( ReindexOnUpdate.NO );\n\t\t\t\t\tTypeMappingStep bookCategoryMapping = mapping.type( BookCategory.class );\n\t\t\t\t\tbookCategoryMapping.property( \"name\" )\n\t\t\t\t\t\t\t.fullTextField().analyzer( \"english\" );\n\t\t\t\t\t//end::programmatic[]\n\t\t\t\t} );\n\t}\n\n\t@Parameterized.Parameter\n\t@Rule\n\tpublic DocumentationSetupHelper setupHelper;\n\n\tprivate EntityManagerFactory entityManagerFactory;\n\n\t@Before\n\tpublic void setup() {\n\t\tentityManagerFactory = setupHelper.start().setup( Book.class, BookCategory.class );\n\t}\n\n\t@Test\n\tpublic void noReindexing() {\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tBookCategory category = new BookCategory();\n\t\t\tcategory.setId( 1 );\n\t\t\tcategory.setName( \"Science-fiction\" );\n\t\t\tentityManager.persist( category );\n\n\t\t\tfor ( int i = 0 ; i < 100 ; ++i ) {\n\t\t\t\tBook book = new Book();\n\t\t\t\tbook.setId( i );\n\t\t\t\tbook.setTitle( \"Book \" + i );\n\t\t\t\tbook.setCategory( category );\n\t\t\t\tentityManager.persist( book );\n\t\t\t}\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tSearchSession searchSession = Search.session( entityManager );\n\n\t\t\tlong hitCount = searchSession.search( Book.class )\n\t\t\t\t\t.where( f -> f.match().field( \"category.name\" ).matching( \"science\" ) )\n\t\t\t\t\t.fetchTotalHitCount();\n\t\t\tassertThat( hitCount ).isEqualTo( 100L );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tBookCategory category = entityManager.getReference( BookCategory.class, 1 );\n\t\t\tcategory.setName( \"Anticipation\" );\n\t\t\tentityManager.persist( category );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tSearchSession searchSession = Search.session( entityManager );\n\n\t\t\tlong hitCount = searchSession.search( Book.class )\n\t\t\t\t\t.where( f -> f.match().field( \"category.name\" ).matching( \"science\" ) )\n\t\t\t\t\t.fetchTotalHitCount();\n\t\t\t// The books haven't been reindexed, as expected.\n\t\t\tassertThat( hitCount ).isEqualTo( 100L );\n\t\t} );\n\n\t}\n\n}\n",
        "filePathAfter": "documentation/src/test/java/org/hibernate/search/documentation/mapper/orm/reindexing/reindexonupdate/shallow/correct/ReindexOnUpdateShallowIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.shallow.correct;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\nimport java.util.List;\nimport javax.persistence.EntityManager;\nimport javax.persistence.EntityManagerFactory;\n\nimport org.hibernate.search.documentation.testsupport.BackendConfigurations;\nimport org.hibernate.search.documentation.testsupport.DocumentationSetupHelper;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.pojo.automaticindexing.ReindexOnUpdate;\nimport org.hibernate.search.mapper.pojo.mapping.definition.programmatic.TypeMappingStep;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils;\n\nimport org.junit.Before;\nimport org.junit.Rule;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.junit.runners.Parameterized;\n\n@RunWith(Parameterized.class)\npublic class ReindexOnUpdateShallowIT {\n\n\t@Parameterized.Parameters(name = \"{0}\")\n\tpublic static List<?> params() {\n\t\treturn DocumentationSetupHelper.testParamsForBothAnnotationsAndProgrammatic(\n\t\t\t\tBackendConfigurations.simple(),\n\t\t\t\tmapping -> {\n\t\t\t\t\t//tag::programmatic[]\n\t\t\t\t\tTypeMappingStep bookMapping = mapping.type( Book.class );\n\t\t\t\t\tbookMapping.indexed();\n\t\t\t\t\tbookMapping.property( \"category\" )\n\t\t\t\t\t\t\t.indexedEmbedded()\n\t\t\t\t\t\t\t.indexingDependency().reindexOnUpdate( ReindexOnUpdate.SHALLOW );\n\t\t\t\t\tTypeMappingStep bookCategoryMapping = mapping.type( BookCategory.class );\n\t\t\t\t\tbookCategoryMapping.property( \"name\" )\n\t\t\t\t\t\t\t.fullTextField().analyzer( \"english\" );\n\t\t\t\t\t//end::programmatic[]\n\t\t\t\t} );\n\t}\n\n\t@Parameterized.Parameter\n\t@Rule\n\tpublic DocumentationSetupHelper setupHelper;\n\n\tprivate EntityManagerFactory entityManagerFactory;\n\n\t@Before\n\tpublic void setup() {\n\t\tentityManagerFactory = setupHelper.start().setup( Book.class, BookCategory.class );\n\t}\n\n\t@Test\n\tpublic void reindexOnUpdateShallow() {\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tBookCategory category = new BookCategory();\n\t\t\tcategory.setId( 1 );\n\t\t\tcategory.setName( \"Science-fiction\" );\n\t\t\tentityManager.persist( category );\n\n\t\t\tfor ( int i = 0 ; i < 100 ; ++i ) {\n\t\t\t\tBook book = new Book();\n\t\t\t\tbook.setId( i );\n\t\t\t\tbook.setTitle( \"Book \" + i );\n\t\t\t\tbook.setCategory( category );\n\t\t\t\tentityManager.persist( book );\n\t\t\t}\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tassertThat( countBooksByCategory( entityManager, \"science\" ) )\n\t\t\t\t\t.isEqualTo( 100L );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tBookCategory category = entityManager.getReference( BookCategory.class, 1 );\n\t\t\tcategory.setName( \"Anticipation\" );\n\t\t\tentityManager.persist( category );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\t// The books weren't reindexed, as expected.\n\t\t\tassertThat( countBooksByCategory( entityManager, \"science\" ) )\n\t\t\t\t\t.isEqualTo( 100L );\n\t\t\tassertThat( countBooksByCategory( entityManager, \"anticipation\" ) )\n\t\t\t\t\t.isEqualTo( 0L );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tassertThat( countBooksByCategory( entityManager, \"crime\" ) )\n\t\t\t\t\t.isEqualTo( 0L );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tBookCategory category = new BookCategory();\n\t\t\tcategory.setId( 2 );\n\t\t\tcategory.setName( \"Crime fiction\" );\n\t\t\tentityManager.persist( category );\n\n\t\t\tBook book = entityManager.getReference( Book.class, 5 );\n\t\t\tbook.setCategory( category );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\t// The book was reindexed, as expected.\n\t\t\tassertThat( countBooksByCategory( entityManager, \"crime\" ) )\n\t\t\t\t\t.isEqualTo( 1L );\n\t\t} );\n\t}\n\n\tprivate long countBooksByCategory(EntityManager entityManager, String categoryNameTerms) {\n\t\treturn Search.session( entityManager ).search( Book.class )\n\t\t\t\t.where( f -> f.match().field( \"category.name\" ).matching( categoryNameTerms ) )\n\t\t\t\t.fetchTotalHitCount();\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "private long countBooksByCategory(EntityManager entityManager, String categoryNameTerms) {\n\t\treturn Search.session( entityManager ).search( Book.class )\n\t\t\t\t.where( f -> f.match().field( \"category.name\" ).matching( categoryNameTerms ) )\n\t\t\t\t.fetchTotalHitCount();\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.BookCategory#setName\n methodBody: public void setName(String name) {\nthis.name=name;\n}",
            "methodSignature: org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.Book#setId\n methodBody: public void setId(Integer id) {\nthis.id=id;\n}",
            "methodSignature: org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.Book#setCategory\n methodBody: public void setCategory(BookCategory category) {\nthis.category=category;\n}",
            "methodSignature: org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.Book#setTitle\n methodBody: public void setTitle(String title) {\nthis.title=title;\n}",
            "methodSignature: org.hibernate.search.documentation.mapper.orm.reindexing.reindexonupdate.BookCategory#setId\n methodBody: public void setId(Integer id) {\nthis.id=id;\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void reindexOnUpdateShallow() {\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tBookCategory category = new BookCategory();\n\t\t\tcategory.setId( 1 );\n\t\t\tcategory.setName( \"Science-fiction\" );\n\t\t\tentityManager.persist( category );\n\n\t\t\tfor ( int i = 0 ; i < 100 ; ++i ) {\n\t\t\t\tBook book = new Book();\n\t\t\t\tbook.setId( i );\n\t\t\t\tbook.setTitle( \"Book \" + i );\n\t\t\t\tbook.setCategory( category );\n\t\t\t\tentityManager.persist( book );\n\t\t\t}\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tassertThat( countBooksByCategory( entityManager, \"science\" ) )\n\t\t\t\t\t.isEqualTo( 100L );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tBookCategory category = entityManager.getReference( BookCategory.class, 1 );\n\t\t\tcategory.setName( \"Anticipation\" );\n\t\t\tentityManager.persist( category );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\t// The books weren't reindexed, as expected.\n\t\t\tassertThat( countBooksByCategory( entityManager, \"science\" ) )\n\t\t\t\t\t.isEqualTo( 100L );\n\t\t\tassertThat( countBooksByCategory( entityManager, \"anticipation\" ) )\n\t\t\t\t\t.isEqualTo( 0L );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tassertThat( countBooksByCategory( entityManager, \"crime\" ) )\n\t\t\t\t\t.isEqualTo( 0L );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\tBookCategory category = new BookCategory();\n\t\t\tcategory.setId( 2 );\n\t\t\tcategory.setName( \"Crime fiction\" );\n\t\t\tentityManager.persist( category );\n\n\t\t\tBook book = entityManager.getReference( Book.class, 5 );\n\t\t\tbook.setCategory( category );\n\t\t} );\n\n\t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n\t\t\t// The book was reindexed, as expected.\n\t\t\tassertThat( countBooksByCategory( entityManager, \"crime\" ) )\n\t\t\t\t\t.isEqualTo( 1L );\n\t\t} );\n\t}\nprivate long countBooksByCategory(EntityManager entityManager, String categoryNameTerms) {\n\t\treturn Search.session( entityManager ).search( Book.class )\n\t\t\t\t.where( f -> f.match().field( \"category.name\" ).matching( categoryNameTerms ) )\n\t\t\t\t.fetchTotalHitCount();\n\t}",
        "diffSourceCode": "    60: \t@Test\n-   61: \tpublic void noReindexing() {\n+   61: \tpublic void reindexOnUpdateShallow() {\n    62: \t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n    63: \t\t\tBookCategory category = new BookCategory();\n    64: \t\t\tcategory.setId( 1 );\n    65: \t\t\tcategory.setName( \"Science-fiction\" );\n    66: \t\t\tentityManager.persist( category );\n    67: \n    68: \t\t\tfor ( int i = 0 ; i < 100 ; ++i ) {\n    69: \t\t\t\tBook book = new Book();\n    70: \t\t\t\tbook.setId( i );\n    71: \t\t\t\tbook.setTitle( \"Book \" + i );\n    72: \t\t\t\tbook.setCategory( category );\n    73: \t\t\t\tentityManager.persist( book );\n    74: \t\t\t}\n    75: \t\t} );\n    76: \n    77: \t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n-   78: \t\t\tSearchSession searchSession = Search.session( entityManager );\n-   79: \n-   80: \t\t\tlong hitCount = searchSession.search( Book.class )\n-   81: \t\t\t\t\t.where( f -> f.match().field( \"category.name\" ).matching( \"science\" ) )\n-   82: \t\t\t\t\t.fetchTotalHitCount();\n-   83: \t\t\tassertThat( hitCount ).isEqualTo( 100L );\n-   84: \t\t} );\n-   85: \n-   86: \t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n-   87: \t\t\tBookCategory category = entityManager.getReference( BookCategory.class, 1 );\n-   88: \t\t\tcategory.setName( \"Anticipation\" );\n-   89: \t\t\tentityManager.persist( category );\n-   90: \t\t} );\n-   91: \n-   92: \t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n-   93: \t\t\tSearchSession searchSession = Search.session( entityManager );\n-   94: \n-   95: \t\t\tlong hitCount = searchSession.search( Book.class )\n-   96: \t\t\t\t\t.where( f -> f.match().field( \"category.name\" ).matching( \"science\" ) )\n-   97: \t\t\t\t\t.fetchTotalHitCount();\n-   98: \t\t\t// The books haven't been reindexed, as expected.\n-   99: \t\t\tassertThat( hitCount ).isEqualTo( 100L );\n-  100: \t\t} );\n-  101: \n-  102: \t}\n-  103: \n-  104: }\n+   78: \t\t\tassertThat( countBooksByCategory( entityManager, \"science\" ) )\n+   79: \t\t\t\t\t.isEqualTo( 100L );\n+   80: \t\t} );\n+   81: \n+   82: \t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n+   83: \t\t\tBookCategory category = entityManager.getReference( BookCategory.class, 1 );\n+   84: \t\t\tcategory.setName( \"Anticipation\" );\n+   85: \t\t\tentityManager.persist( category );\n+   86: \t\t} );\n+   87: \n+   88: \t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n+   89: \t\t\t// The books weren't reindexed, as expected.\n+   90: \t\t\tassertThat( countBooksByCategory( entityManager, \"science\" ) )\n+   91: \t\t\t\t\t.isEqualTo( 100L );\n+   92: \t\t\tassertThat( countBooksByCategory( entityManager, \"anticipation\" ) )\n+   93: \t\t\t\t\t.isEqualTo( 0L );\n+   94: \t\t} );\n+   95: \n+   96: \t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n+   97: \t\t\tassertThat( countBooksByCategory( entityManager, \"crime\" ) )\n+   98: \t\t\t\t\t.isEqualTo( 0L );\n+   99: \t\t} );\n+  100: \n+  101: \t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n+  102: \t\t\tBookCategory category = new BookCategory();\n+  103: \t\t\tcategory.setId( 2 );\n+  104: \t\t\tcategory.setName( \"Crime fiction\" );\n+  105: \t\t\tentityManager.persist( category );\n+  106: \n+  107: \t\t\tBook book = entityManager.getReference( Book.class, 5 );\n+  108: \t\t\tbook.setCategory( category );\n+  109: \t\t} );\n+  110: \n+  111: \t\tOrmUtils.withinJPATransaction( entityManagerFactory, entityManager -> {\n+  112: \t\t\t// The book was reindexed, as expected.\n+  113: \t\t\tassertThat( countBooksByCategory( entityManager, \"crime\" ) )\n+  114: \t\t\t\t\t.isEqualTo( 1L );\n+  115: \t\t} );\n+  116: \t}\n+  118: \tprivate long countBooksByCategory(EntityManager entityManager, String categoryNameTerms) {\n+  119: \t\treturn Search.session( entityManager ).search( Book.class )\n+  120: \t\t\t\t.where( f -> f.match().field( \"category.name\" ).matching( categoryNameTerms ) )\n+  121: \t\t\t\t.fetchTotalHitCount();\n+  122: \t}\n",
        "uniqueId": "bb500fd954c9c039e32b9549bc1b18bcd28b86ab_60_102_118_122_60_116",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tprotected assertEntityNonIdGetterFailureHandling(entityName String, entityReferenceAsString String, exceptionMessage String, failingOperationAsString String) : void extracted from public getTitle() : void in class org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT & moved to class org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomBackgroundFailureHandlerIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
                "startLine": 143,
                "endLine": 182,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
                "startLine": 143,
                "endLine": 182,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
                "startLine": 80,
                "endLine": 86,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Test\n\tpublic void getTitle() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getTitle failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}",
        "filePathBefore": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
        "isPureRefactoring": true,
        "commitId": "f307576b2d1a89c3814067a2b6b62df1d8cd45fb",
        "packageNameBefore": "org.hibernate.search.integrationtest.mapper.orm.massindexing",
        "classNameBefore": "org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#getTitle",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomMassIndexingFailureHandlerIT#assertEntityGetterFailureHandling\n methodBody: protected void assertEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString) {\nverify(failureHandler);\nMassIndexingEntityFailureContext context=entityFailureContextCapture.getValue();\nassertThat(context.throwable()).isInstanceOf(SearchException.class).hasMessageContaining(\"Exception while invoking\").extracting(Throwable::getCause,InstanceOfAssertFactories.THROWABLE).isInstanceOf(SimulatedFailure.class).hasMessageContaining(exceptionMessage);\nassertThat(context.failingOperation()).asString().isEqualTo(failingOperationAsString);\nassertThat(context.entityReferences()).hasSize(1).element(0).asString().isEqualTo(entityReferenceAsString);\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomBackgroundFailureHandlerIT#assertEntityGetterFailureHandling\n methodBody: protected void assertEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString) {\nassertThat(staticCounters.get(StubFailureHandler.CREATE)).isEqualTo(1);\nassertThat(staticCounters.get(StubFailureHandler.HANDLE_GENERIC_CONTEXT)).isEqualTo(0);\nassertThat(staticCounters.get(StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT)).isEqualTo(1);\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureDefaultBackgroundFailureHandlerIT#assertEntityGetterFailureHandling\n methodBody: protected void assertEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString) {\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomBackgroundFailureHandlerIT#expectEntityGetterFailureHandling\n methodBody: protected void expectEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString) {\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomMassIndexingFailureHandlerIT#expectEntityGetterFailureHandling\n methodBody: protected void expectEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString) {\nreset(failureHandler);\nfailureHandler.handle(capture(entityFailureContextCapture));\nreplay(failureHandler);\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexScaleWork\n methodBody: private Runnable expectIndexScaleWork(StubIndexScaleWork.Type type, ExecutionExpectation executionExpectation) {\nreturn () -> {\nswitch (executionExpectation) {\ncase SUCCEED:    backendMock.expectIndexScaleWorks(Book.NAME).indexScaleWork(type);\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(type.name() + \" failure\"));\nbackendMock.expectIndexScaleWorks(Book.NAME).indexScaleWork(type,failingFuture);\nbreak;\ncase SKIP:break;\n}\n}\n;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#setup\n methodBody: private SessionFactory setup() {\nassertBeforeSetup();\nbackendMock.expectAnySchema(Book.NAME);\nSessionFactory sessionFactory=ormSetupHelper.start().withPropertyRadical(HibernateOrmMapperSettings.Radicals.AUTOMATIC_INDEXING_STRATEGY,AutomaticIndexingStrategyName.NONE).withPropertyRadical(EngineSettings.Radicals.BACKGROUND_FAILURE_HANDLER,getBackgroundFailureHandlerReference()).withPropertyRadical(EngineSpiSettings.Radicals.THREAD_PROVIDER,threadSpy.getThreadProvider()).setup(Book.class);\nbackendMock.verifyExpectationsMet();\nOrmUtils.withinTransaction(sessionFactory,session -> {\n  session.persist(new Book(1,TITLE_1,AUTHOR_1));\n  session.persist(new Book(2,TITLE_2,AUTHOR_2));\n  session.persist(new Book(3,TITLE_3,AUTHOR_3));\n}\n);\nassertAfterSetup();\nreturn sessionFactory;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithFailure\n methodBody: private void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\nBook.failOnBook2GetId.set(ExecutionExpectation.FAIL.equals(book2GetIdExpectation));\nBook.failOnBook2GetTitle.set(ExecutionExpectation.FAIL.equals(book2GetTitleExpectation));\nAssertionError assertionError=null;\ntryMassIndexingFailureHandler massIndexingFailureHandler=getMassIndexingFailureHandler();\nif(massIndexingFailureHandler != null){massIndexer.failureHandler(massIndexingFailureHandler);\n}for(Runnable expectationSetter: expectationSetters){expectationSetter.run();\n}Runnable runnable=() -> {\n  try {\n    massIndexer.startAndWait();\n  }\n catch (  InterruptedException e) {\n    fail(\"Unexpected InterruptedException: \" + e.getMessage());\n  }\n}\n;\nif(thrownExpectation == null){runnable.run();\n}{Assertions.assertThatThrownBy(runnable::run).asInstanceOf(InstanceOfAssertFactories.type(Throwable.class)).satisfies(thrownExpectation);\n}backendMock.verifyExpectationsMet();\ncatch(AssertionError e)assertionError=e;\nthrow e;\nfinallyBook.failOnBook2GetId.set(false);\nBook.failOnBook2GetTitle.set(false);\nif(assertionError == null){switch(threadExpectation)case CREATED_AND_TERMINATED:Awaitility.await().untilAsserted(() -> assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isNotEmpty().allSatisfy(t -> assertThat(t).extracting(Thread::getState).isEqualTo(Thread.State.TERMINATED)));\nbreak;\ncase NOT_CREATED:assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isEmpty();\nbreak;\n}}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#assertEntityGetterFailureHandling\n methodBody: protected abstract void assertEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexingWorks\n methodBody: private Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\nreturn () -> {\nswitch (workTwoExecutionExpectation) {\ncase SUCCEED:    backendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"2\",b -> b.field(\"title\",TITLE_2).field(\"author\",AUTHOR_2)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(\"Indexing failure\"));\nbackendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\nbackendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"2\",b -> b.field(\"title\",TITLE_2).field(\"author\",AUTHOR_2)).processedThenExecuted(failingFuture);\nbreak;\ncase SKIP:backendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\nbreak;\n}\n}\n;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectEntityGetterFailureHandling\n methodBody: protected abstract void expectEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureDefaultBackgroundFailureHandlerIT#expectEntityGetterFailureHandling\n methodBody: protected void expectEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString) {\nlogged.expectEvent(Level.ERROR,ExceptionMatcherBuilder.isException(SearchException.class).withMessage(\"Exception while invoking\").causedBy(SimulatedFailure.class).withMessage(exceptionMessage).build(),failingOperationAsString,\"Entities that could not be indexed correctly:\",entityReferenceAsString).once();\n}",
        "classSignatureBefore": "public abstract class AbstractMassIndexingFailureIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#getTitle"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT"
        ],
        "classSignatureBeforeSet": [
            "public abstract class AbstractMassIndexingFailureIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Tolerable changes in the body\n",
                "description": "All replacements have been justified - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.massindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Fail.fail;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.function.Consumer;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.cfg.EngineSettings;\nimport org.hibernate.search.engine.cfg.spi.EngineSpiSettings;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.automaticindexing.AutomaticIndexingStrategyName;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexer;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexingFailureHandler;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.FailureReportUtils;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.ThreadSpy;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubIndexScaleWork;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubSchemaManagementWork;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils;\nimport org.assertj.core.api.Assertions;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.assertj.core.api.InstanceOfAssertFactories;\nimport org.awaitility.Awaitility;\n\npublic abstract class AbstractMassIndexingFailureIT {\n\n\tpublic static final String TITLE_1 = \"Oliver Twist\";\n\tpublic static final String AUTHOR_1 = \"Charles Dickens\";\n\tpublic static final String TITLE_2 = \"Ulysses\";\n\tpublic static final String AUTHOR_2 = \"James Joyce\";\n\tpublic static final String TITLE_3 = \"Frankenstein\";\n\tpublic static final String AUTHOR_3 = \"Mary Shelley\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock();\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock );\n\n\t@Rule\n\tpublic ThreadSpy threadSpy = new ThreadSpy();\n\n\t@Test\n\tpublic void indexing() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"Indexing failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getId() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getId failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getTitle() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getTitle failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void dropAndCreateSchema_exception() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"DROP_AND_CREATE failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SearchException.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer().dropAndCreateSchemaOnStart( true ),\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t\t\t.typeContext( Book.class.getName() )\n\t\t\t\t\t\t\t\t.failure( exceptionMessage )\n\t\t\t\t\t\t\t\t.build() ),\n\t\t\t\texpectSchemaManagementWorkException( StubSchemaManagementWork.Type.DROP_AND_CREATE )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SearchException.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void purge() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"PURGE failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsBefore() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsAfter() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer()\n\t\t\t\t\t\t.mergeSegmentsOnFinish( true ),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void flush() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"FLUSH failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void refresh() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"REFRESH failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void indexingAndFlush() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString failingEntityIndexingExceptionMessage = \"Indexing failure\";\n\t\tString failingEntityIndexingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\t\tString failingMassIndexerOperationExceptionMessage = \"FLUSH failure\";\n\t\tString failingMassIndexerOperationAsString = \"MassIndexer operation\";\n\n\t\texpectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( failingMassIndexerOperationExceptionMessage )\n\t\t\t\t\t\t// Indexing failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\t\tfailingEntityIndexingExceptionMessage\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void indexingAndRefresh() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString failingEntityIndexingExceptionMessage = \"Indexing failure\";\n\t\tString failingEntityIndexingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\t\tString failingMassIndexerOperationExceptionMessage = \"REFRESH failure\";\n\t\tString failingMassIndexerOperationAsString = \"MassIndexer operation\";\n\n\t\texpectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( failingMassIndexerOperationExceptionMessage )\n\t\t\t\t\t\t// Indexing failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\t\tfailingEntityIndexingExceptionMessage\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\t}\n\n\tprotected abstract String getBackgroundFailureHandlerReference();\n\n\tprotected abstract MassIndexingFailureHandler getMassIndexingFailureHandler();\n\n\tprotected void assertBeforeSetup() {\n\t}\n\n\tprotected void assertAfterSetup() {\n\t}\n\n\tprotected abstract void expectEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectMassIndexerOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString);\n\n\tprotected abstract void assertMassIndexerOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString);\n\n\tprotected abstract void expectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprotected abstract void assertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprivate void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmassIndexer,\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tBook.failOnBook2GetId.set( ExecutionExpectation.FAIL.equals( book2GetIdExpectation ) );\n\t\tBook.failOnBook2GetTitle.set( ExecutionExpectation.FAIL.equals( book2GetTitleExpectation ) );\n\t\tAssertionError assertionError = null;\n\t\ttry {\n\t\t\tMassIndexingFailureHandler massIndexingFailureHandler = getMassIndexingFailureHandler();\n\t\t\tif ( massIndexingFailureHandler != null ) {\n\t\t\t\tmassIndexer.failureHandler( massIndexingFailureHandler );\n\t\t\t}\n\n\t\t\tfor ( Runnable expectationSetter : expectationSetters ) {\n\t\t\t\texpectationSetter.run();\n\t\t\t}\n\n\t\t\t// TODO HSEARCH-3728 simplify this when even indexing exceptions are propagated\n\t\t\tRunnable runnable = () -> {\n\t\t\t\ttry {\n\t\t\t\t\tmassIndexer.startAndWait();\n\t\t\t\t}\n\t\t\t\tcatch (InterruptedException e) {\n\t\t\t\t\tfail( \"Unexpected InterruptedException: \" + e.getMessage() );\n\t\t\t\t}\n\t\t\t};\n\t\t\tif ( thrownExpectation == null ) {\n\t\t\t\trunnable.run();\n\t\t\t}\n\t\t\telse {\n\t\t\t\tAssertions.assertThatThrownBy( runnable::run )\n\t\t\t\t\t\t.asInstanceOf( InstanceOfAssertFactories.type( Throwable.class ) )\n\t\t\t\t\t\t.satisfies( thrownExpectation );\n\t\t\t}\n\t\t\tbackendMock.verifyExpectationsMet();\n\t\t}\n\t\tcatch (AssertionError e) {\n\t\t\tassertionError = e;\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tBook.failOnBook2GetId.set( false );\n\t\t\tBook.failOnBook2GetTitle.set( false );\n\n\t\t\tif ( assertionError == null ) {\n\t\t\t\tswitch ( threadExpectation ) {\n\t\t\t\t\tcase CREATED_AND_TERMINATED:\n\t\t\t\t\t\tAwaitility.await().untilAsserted(\n\t\t\t\t\t\t\t\t() -> assertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t\t\t.isNotEmpty()\n\t\t\t\t\t\t\t\t\t\t.allSatisfy( t -> assertThat( t )\n\t\t\t\t\t\t\t\t\t\t\t\t.extracting( Thread::getState )\n\t\t\t\t\t\t\t\t\t\t\t\t.isEqualTo( Thread.State.TERMINATED )\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase NOT_CREATED:\n\t\t\t\t\t\tassertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t.isEmpty();\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate Runnable expectSchemaManagementWorkException(StubSchemaManagementWork.Type type) {\n\t\treturn () -> {\n\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\tbackendMock.expectSchemaManagementWorks( Book.NAME )\n\t\t\t\t\t.work( type, failingFuture );\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexScaleWork(StubIndexScaleWork.Type type, ExecutionExpectation executionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( executionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectIndexScaleWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScaleWork( type );\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\t\t\tbackendMock.expectIndexScaleWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScaleWork( type, failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( workTwoExecutionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( \"Indexing failure\" ) );\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted( failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate SessionFactory setup() {\n\t\tassertBeforeSetup();\n\n\t\tbackendMock.expectAnySchema( Book.NAME );\n\n\t\tSessionFactory sessionFactory = ormSetupHelper.start()\n\t\t\t\t.withPropertyRadical( HibernateOrmMapperSettings.Radicals.AUTOMATIC_INDEXING_STRATEGY, AutomaticIndexingStrategyName.NONE )\n\t\t\t\t.withPropertyRadical( EngineSettings.Radicals.BACKGROUND_FAILURE_HANDLER, getBackgroundFailureHandlerReference() )\n\t\t\t\t.withPropertyRadical( EngineSpiSettings.Radicals.THREAD_PROVIDER, threadSpy.getThreadProvider() )\n\t\t\t\t.setup( Book.class );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tOrmUtils.withinTransaction( sessionFactory, session -> {\n\t\t\tsession.persist( new Book( 1, TITLE_1, AUTHOR_1 ) );\n\t\t\tsession.persist( new Book( 2, TITLE_2, AUTHOR_2 ) );\n\t\t\tsession.persist( new Book( 3, TITLE_3, AUTHOR_3 ) );\n\t\t} );\n\n\t\tassertAfterSetup();\n\n\t\treturn sessionFactory;\n\t}\n\n\tprivate enum ExecutionExpectation {\n\t\tSUCCEED,\n\t\tFAIL,\n\t\tSKIP;\n\t}\n\n\tprivate enum ThreadExpectation {\n\t\tCREATED_AND_TERMINATED,\n\t\tNOT_CREATED;\n\t}\n\n\t@Entity(name = Book.NAME)\n\t@Indexed(index = Book.NAME)\n\tpublic static class Book {\n\n\t\tpublic static final String NAME = \"Book\";\n\n\t\tprivate static final AtomicBoolean failOnBook2GetId = new AtomicBoolean( false );\n\t\tprivate static final AtomicBoolean failOnBook2GetTitle = new AtomicBoolean( false );\n\n\t\tprivate Integer id;\n\n\t\tprivate String title;\n\n\t\tprivate String author;\n\n\t\tpublic Book() {\n\t\t}\n\n\t\tpublic Book(Integer id, String title, String author) {\n\t\t\tthis.id = id;\n\t\t\tthis.title = title;\n\t\t\tthis.author = author;\n\t\t}\n\n\t\t@Id // This must be on the getter, so that Hibernate Search uses getters instead of direct field access\n\t\tpublic Integer getId() {\n\t\t\tif ( id == 2 && failOnBook2GetId.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getId failure\" );\n\t\t\t}\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getTitle() {\n\t\t\tif ( id == 2 && failOnBook2GetTitle.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getTitle failure\" );\n\t\t\t}\n\t\t\treturn title;\n\t\t}\n\n\t\tpublic void setTitle(String title) {\n\t\t\tthis.title = title;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getAuthor() {\n\t\t\treturn author;\n\t\t}\n\n\t\tpublic void setAuthor(String author) {\n\t\t\tthis.author = author;\n\t\t}\n\t}\n\n\tprotected static class SimulatedFailure extends RuntimeException {\n\t\tSimulatedFailure(String message) {\n\t\t\tsuper( message );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.massindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Fail.fail;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.function.Consumer;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.cfg.EngineSettings;\nimport org.hibernate.search.engine.cfg.spi.EngineSpiSettings;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.automaticindexing.AutomaticIndexingStrategyName;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexer;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexingFailureHandler;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.FailureReportUtils;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.ThreadSpy;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubIndexScaleWork;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubSchemaManagementWork;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils;\nimport org.assertj.core.api.Assertions;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.assertj.core.api.InstanceOfAssertFactories;\nimport org.awaitility.Awaitility;\n\npublic abstract class AbstractMassIndexingFailureIT {\n\n\tpublic static final String TITLE_1 = \"Oliver Twist\";\n\tpublic static final String AUTHOR_1 = \"Charles Dickens\";\n\tpublic static final String TITLE_2 = \"Ulysses\";\n\tpublic static final String AUTHOR_2 = \"James Joyce\";\n\tpublic static final String TITLE_3 = \"Frankenstein\";\n\tpublic static final String AUTHOR_3 = \"Mary Shelley\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock();\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock );\n\n\t@Rule\n\tpublic ThreadSpy threadSpy = new ThreadSpy();\n\n\t@Test\n\tpublic void indexing() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"Indexing failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getId() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getId failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getTitle() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getTitle failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityNonIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityNonIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void dropAndCreateSchema_exception() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"DROP_AND_CREATE failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SearchException.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer().dropAndCreateSchemaOnStart( true ),\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageMatching( FailureReportUtils.buildFailureReportPattern()\n\t\t\t\t\t\t\t\t.typeContext( Book.class.getName() )\n\t\t\t\t\t\t\t\t.failure( exceptionMessage )\n\t\t\t\t\t\t\t\t.build() ),\n\t\t\t\texpectSchemaManagementWorkException( StubSchemaManagementWork.Type.DROP_AND_CREATE )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SearchException.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void purge() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"PURGE failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsBefore() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsAfter() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer()\n\t\t\t\t\t\t.mergeSegmentsOnFinish( true ),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void flush() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"FLUSH failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void refresh() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"REFRESH failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( SimulatedFailure.class, exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void indexingAndFlush() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString failingEntityIndexingExceptionMessage = \"Indexing failure\";\n\t\tString failingEntityIndexingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\t\tString failingMassIndexerOperationExceptionMessage = \"FLUSH failure\";\n\t\tString failingMassIndexerOperationAsString = \"MassIndexer operation\";\n\n\t\texpectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( failingMassIndexerOperationExceptionMessage )\n\t\t\t\t\t\t// Indexing failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\t\tfailingEntityIndexingExceptionMessage\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void indexingAndRefresh() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString failingEntityIndexingExceptionMessage = \"Indexing failure\";\n\t\tString failingEntityIndexingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\t\tString failingMassIndexerOperationExceptionMessage = \"REFRESH failure\";\n\t\tString failingMassIndexerOperationAsString = \"MassIndexer operation\";\n\n\t\texpectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( failingMassIndexerOperationExceptionMessage )\n\t\t\t\t\t\t// Indexing failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\t\tfailingEntityIndexingExceptionMessage\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\t}\n\n\tprotected abstract String getBackgroundFailureHandlerReference();\n\n\tprotected abstract MassIndexingFailureHandler getMassIndexingFailureHandler();\n\n\tprotected void assertBeforeSetup() {\n\t}\n\n\tprotected void assertAfterSetup() {\n\t}\n\n\tprotected abstract void expectEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityIdGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityIdGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityNonIdGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityNonIdGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectMassIndexerOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString);\n\n\tprotected abstract void assertMassIndexerOperationFailureHandling(\n\t\t\tClass<? extends Throwable> exceptionType, String exceptionMessage,\n\t\t\tString failingOperationAsString);\n\n\tprotected abstract void expectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprotected abstract void assertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprivate void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tmassIndexer,\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tBook.failOnBook2GetId.set( ExecutionExpectation.FAIL.equals( book2GetIdExpectation ) );\n\t\tBook.failOnBook2GetTitle.set( ExecutionExpectation.FAIL.equals( book2GetTitleExpectation ) );\n\t\tAssertionError assertionError = null;\n\t\ttry {\n\t\t\tMassIndexingFailureHandler massIndexingFailureHandler = getMassIndexingFailureHandler();\n\t\t\tif ( massIndexingFailureHandler != null ) {\n\t\t\t\tmassIndexer.failureHandler( massIndexingFailureHandler );\n\t\t\t}\n\n\t\t\tfor ( Runnable expectationSetter : expectationSetters ) {\n\t\t\t\texpectationSetter.run();\n\t\t\t}\n\n\t\t\t// TODO HSEARCH-3728 simplify this when even indexing exceptions are propagated\n\t\t\tRunnable runnable = () -> {\n\t\t\t\ttry {\n\t\t\t\t\tmassIndexer.startAndWait();\n\t\t\t\t}\n\t\t\t\tcatch (InterruptedException e) {\n\t\t\t\t\tfail( \"Unexpected InterruptedException: \" + e.getMessage() );\n\t\t\t\t}\n\t\t\t};\n\t\t\tif ( thrownExpectation == null ) {\n\t\t\t\trunnable.run();\n\t\t\t}\n\t\t\telse {\n\t\t\t\tAssertions.assertThatThrownBy( runnable::run )\n\t\t\t\t\t\t.asInstanceOf( InstanceOfAssertFactories.type( Throwable.class ) )\n\t\t\t\t\t\t.satisfies( thrownExpectation );\n\t\t\t}\n\t\t\tbackendMock.verifyExpectationsMet();\n\t\t}\n\t\tcatch (AssertionError e) {\n\t\t\tassertionError = e;\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tBook.failOnBook2GetId.set( false );\n\t\t\tBook.failOnBook2GetTitle.set( false );\n\n\t\t\tif ( assertionError == null ) {\n\t\t\t\tswitch ( threadExpectation ) {\n\t\t\t\t\tcase CREATED_AND_TERMINATED:\n\t\t\t\t\t\tAwaitility.await().untilAsserted(\n\t\t\t\t\t\t\t\t() -> assertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t\t\t.isNotEmpty()\n\t\t\t\t\t\t\t\t\t\t.allSatisfy( t -> assertThat( t )\n\t\t\t\t\t\t\t\t\t\t\t\t.extracting( Thread::getState )\n\t\t\t\t\t\t\t\t\t\t\t\t.isEqualTo( Thread.State.TERMINATED )\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase NOT_CREATED:\n\t\t\t\t\t\tassertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t.isEmpty();\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate Runnable expectSchemaManagementWorkException(StubSchemaManagementWork.Type type) {\n\t\treturn () -> {\n\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\tbackendMock.expectSchemaManagementWorks( Book.NAME )\n\t\t\t\t\t.work( type, failingFuture );\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexScaleWork(StubIndexScaleWork.Type type, ExecutionExpectation executionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( executionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectIndexScaleWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScaleWork( type );\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\t\t\tbackendMock.expectIndexScaleWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScaleWork( type, failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( workTwoExecutionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( \"Indexing failure\" ) );\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted( failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate SessionFactory setup() {\n\t\tassertBeforeSetup();\n\n\t\tbackendMock.expectAnySchema( Book.NAME );\n\n\t\tSessionFactory sessionFactory = ormSetupHelper.start()\n\t\t\t\t.withPropertyRadical( HibernateOrmMapperSettings.Radicals.AUTOMATIC_INDEXING_STRATEGY, AutomaticIndexingStrategyName.NONE )\n\t\t\t\t.withPropertyRadical( EngineSettings.Radicals.BACKGROUND_FAILURE_HANDLER, getBackgroundFailureHandlerReference() )\n\t\t\t\t.withPropertyRadical( EngineSpiSettings.Radicals.THREAD_PROVIDER, threadSpy.getThreadProvider() )\n\t\t\t\t.setup( Book.class );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tOrmUtils.withinTransaction( sessionFactory, session -> {\n\t\t\tsession.persist( new Book( 1, TITLE_1, AUTHOR_1 ) );\n\t\t\tsession.persist( new Book( 2, TITLE_2, AUTHOR_2 ) );\n\t\t\tsession.persist( new Book( 3, TITLE_3, AUTHOR_3 ) );\n\t\t} );\n\n\t\tassertAfterSetup();\n\n\t\treturn sessionFactory;\n\t}\n\n\tprivate enum ExecutionExpectation {\n\t\tSUCCEED,\n\t\tFAIL,\n\t\tSKIP;\n\t}\n\n\tprivate enum ThreadExpectation {\n\t\tCREATED_AND_TERMINATED,\n\t\tNOT_CREATED;\n\t}\n\n\t@Entity(name = Book.NAME)\n\t@Indexed(index = Book.NAME)\n\tpublic static class Book {\n\n\t\tpublic static final String NAME = \"Book\";\n\n\t\tprivate static final AtomicBoolean failOnBook2GetId = new AtomicBoolean( false );\n\t\tprivate static final AtomicBoolean failOnBook2GetTitle = new AtomicBoolean( false );\n\n\t\tprivate Integer id;\n\n\t\tprivate String title;\n\n\t\tprivate String author;\n\n\t\tpublic Book() {\n\t\t}\n\n\t\tpublic Book(Integer id, String title, String author) {\n\t\t\tthis.id = id;\n\t\t\tthis.title = title;\n\t\t\tthis.author = author;\n\t\t}\n\n\t\t@Id // This must be on the getter, so that Hibernate Search uses getters instead of direct field access\n\t\tpublic Integer getId() {\n\t\t\tif ( id == 2 && failOnBook2GetId.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getId failure\" );\n\t\t\t}\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getTitle() {\n\t\t\tif ( id == 2 && failOnBook2GetTitle.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getTitle failure\" );\n\t\t\t}\n\t\t\treturn title;\n\t\t}\n\n\t\tpublic void setTitle(String title) {\n\t\t\tthis.title = title;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getAuthor() {\n\t\t\treturn author;\n\t\t}\n\n\t\tpublic void setAuthor(String author) {\n\t\t\tthis.author = author;\n\t\t}\n\t}\n\n\tprotected static class SimulatedFailure extends RuntimeException {\n\t\tSimulatedFailure(String message) {\n\t\t\tsuper( message );\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "ThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\texceptionMessage"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomMassIndexingFailureHandlerIT#assertEntityGetterFailureHandling\n methodBody: protected void assertEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString) {\nverify(failureHandler);\nMassIndexingEntityFailureContext context=entityFailureContextCapture.getValue();\nassertThat(context.throwable()).isInstanceOf(SearchException.class).hasMessageContaining(\"Exception while invoking\").extracting(Throwable::getCause,InstanceOfAssertFactories.THROWABLE).isInstanceOf(SimulatedFailure.class).hasMessageContaining(exceptionMessage);\nassertThat(context.failingOperation()).asString().isEqualTo(failingOperationAsString);\nassertThat(context.entityReferences()).hasSize(1).element(0).asString().isEqualTo(entityReferenceAsString);\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomBackgroundFailureHandlerIT#assertEntityGetterFailureHandling\n methodBody: protected void assertEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString) {\nassertThat(staticCounters.get(StubFailureHandler.CREATE)).isEqualTo(1);\nassertThat(staticCounters.get(StubFailureHandler.HANDLE_GENERIC_CONTEXT)).isEqualTo(0);\nassertThat(staticCounters.get(StubFailureHandler.HANDLE_ENTITY_INDEXING_CONTEXT)).isEqualTo(1);\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureDefaultBackgroundFailureHandlerIT#assertEntityGetterFailureHandling\n methodBody: protected void assertEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString) {\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomBackgroundFailureHandlerIT#expectEntityGetterFailureHandling\n methodBody: protected void expectEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString) {\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureCustomMassIndexingFailureHandlerIT#expectEntityGetterFailureHandling\n methodBody: protected void expectEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString) {\nreset(failureHandler);\nfailureHandler.handle(capture(entityFailureContextCapture));\nreplay(failureHandler);\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexScaleWork\n methodBody: private Runnable expectIndexScaleWork(StubIndexScaleWork.Type type, ExecutionExpectation executionExpectation) {\nreturn () -> {\nswitch (executionExpectation) {\ncase SUCCEED:    backendMock.expectIndexScaleWorks(Book.NAME).indexScaleWork(type);\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(type.name() + \" failure\"));\nbackendMock.expectIndexScaleWorks(Book.NAME).indexScaleWork(type,failingFuture);\nbreak;\ncase SKIP:break;\n}\n}\n;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#setup\n methodBody: private SessionFactory setup() {\nassertBeforeSetup();\nbackendMock.expectAnySchema(Book.NAME);\nSessionFactory sessionFactory=ormSetupHelper.start().withPropertyRadical(HibernateOrmMapperSettings.Radicals.AUTOMATIC_INDEXING_STRATEGY,AutomaticIndexingStrategyName.NONE).withPropertyRadical(EngineSettings.Radicals.BACKGROUND_FAILURE_HANDLER,getBackgroundFailureHandlerReference()).withPropertyRadical(EngineSpiSettings.Radicals.THREAD_PROVIDER,threadSpy.getThreadProvider()).setup(Book.class);\nbackendMock.verifyExpectationsMet();\nOrmUtils.withinTransaction(sessionFactory,session -> {\n  session.persist(new Book(1,TITLE_1,AUTHOR_1));\n  session.persist(new Book(2,TITLE_2,AUTHOR_2));\n  session.persist(new Book(3,TITLE_3,AUTHOR_3));\n}\n);\nassertAfterSetup();\nreturn sessionFactory;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithFailure\n methodBody: private void doMassIndexingWithFailure(MassIndexer massIndexer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\nBook.failOnBook2GetId.set(ExecutionExpectation.FAIL.equals(book2GetIdExpectation));\nBook.failOnBook2GetTitle.set(ExecutionExpectation.FAIL.equals(book2GetTitleExpectation));\nAssertionError assertionError=null;\ntryMassIndexingFailureHandler massIndexingFailureHandler=getMassIndexingFailureHandler();\nif(massIndexingFailureHandler != null){massIndexer.failureHandler(massIndexingFailureHandler);\n}for(Runnable expectationSetter: expectationSetters){expectationSetter.run();\n}Runnable runnable=() -> {\n  try {\n    massIndexer.startAndWait();\n  }\n catch (  InterruptedException e) {\n    fail(\"Unexpected InterruptedException: \" + e.getMessage());\n  }\n}\n;\nif(thrownExpectation == null){runnable.run();\n}{Assertions.assertThatThrownBy(runnable::run).asInstanceOf(InstanceOfAssertFactories.type(Throwable.class)).satisfies(thrownExpectation);\n}backendMock.verifyExpectationsMet();\ncatch(AssertionError e)assertionError=e;\nthrow e;\nfinallyBook.failOnBook2GetId.set(false);\nBook.failOnBook2GetTitle.set(false);\nif(assertionError == null){switch(threadExpectation)case CREATED_AND_TERMINATED:Awaitility.await().untilAsserted(() -> assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isNotEmpty().allSatisfy(t -> assertThat(t).extracting(Thread::getState).isEqualTo(Thread.State.TERMINATED)));\nbreak;\ncase NOT_CREATED:assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isEmpty();\nbreak;\n}}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#assertEntityGetterFailureHandling\n methodBody: protected abstract void assertEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexingWorks\n methodBody: private Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\nreturn () -> {\nswitch (workTwoExecutionExpectation) {\ncase SUCCEED:    backendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"2\",b -> b.field(\"title\",TITLE_2).field(\"author\",AUTHOR_2)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(\"Indexing failure\"));\nbackendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\nbackendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"2\",b -> b.field(\"title\",TITLE_2).field(\"author\",AUTHOR_2)).processedThenExecuted(failingFuture);\nbreak;\ncase SKIP:backendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\nbreak;\n}\n}\n;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectEntityGetterFailureHandling\n methodBody: protected abstract void expectEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.MassIndexingFailureDefaultBackgroundFailureHandlerIT#expectEntityGetterFailureHandling\n methodBody: protected void expectEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString) {\nlogged.expectEvent(Level.ERROR,ExceptionMatcherBuilder.isException(SearchException.class).withMessage(\"Exception while invoking\").causedBy(SimulatedFailure.class).withMessage(exceptionMessage).build(),failingOperationAsString,\"Entities that could not be indexed correctly:\",entityReferenceAsString).once();\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void getTitle() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getTitle failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityNonIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityNonIdGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\nThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\texceptionMessage",
        "diffSourceCode": "    80: \t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n    81: \t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n    82: \t\t\t\t\t\t.hasMessageContainingAll(\n    83: \t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n    84: \t\t\t\t\t\t\t\t\"See the logs for details.\",\n    85: \t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n    86: \t\t\t\t\t\t\t\texceptionMessage\n   143: \t@Test\n   144: \tpublic void getTitle() {\n   145: \t\tSessionFactory sessionFactory = setup();\n   146: \n   147: \t\tString entityName = Book.NAME;\n   148: \t\tString entityReferenceAsString = Book.NAME + \"#2\";\n   149: \t\tString exceptionMessage = \"getTitle failure\";\n   150: \t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n   151: \n-  152: \t\texpectEntityGetterFailureHandling(\n+  152: \t\texpectEntityNonIdGetterFailureHandling(\n   153: \t\t\t\tentityName, entityReferenceAsString,\n   154: \t\t\t\texceptionMessage, failingOperationAsString\n   155: \t\t);\n   156: \n   157: \t\tdoMassIndexingWithFailure(\n   158: \t\t\t\tSearch.mapping( sessionFactory ).scope( Object.class ).massIndexer(),\n   159: \t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n   160: \t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n   161: \t\t\t\t\t\t.hasMessageContainingAll(\n   162: \t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n   163: \t\t\t\t\t\t\t\t\"See the logs for details.\",\n   164: \t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n   165: \t\t\t\t\t\t\t\t\"Exception while invoking\"\n   166: \t\t\t\t\t\t)\n   167: \t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n   168: \t\t\t\t\t\t.isInstanceOf( SearchException.class )\n   169: \t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n   170: \t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n   171: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n   172: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n   173: \t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n   174: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.FLUSH, ExecutionExpectation.SUCCEED ),\n   175: \t\t\t\texpectIndexScaleWork( StubIndexScaleWork.Type.REFRESH, ExecutionExpectation.SUCCEED )\n   176: \t\t);\n   177: \n-  178: \t\tassertEntityGetterFailureHandling(\n+  178: \t\tassertEntityNonIdGetterFailureHandling(\n   179: \t\t\t\tentityName, entityReferenceAsString,\n   180: \t\t\t\texceptionMessage, failingOperationAsString\n   181: \t\t);\n   182: \t}\n",
        "uniqueId": "f307576b2d1a89c3814067a2b6b62df1d8cd45fb_143_182_80_86_143_182",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11
    },
    {
        "type": "Inline Method",
        "description": "Inline Method\tprivate getPropertyMemberFromThisType(propertyName String, propertyMetadataFromHibernateOrmMetamodel HibernateOrmBasicClassPropertyMetadata) : Member inlined to private findPropertyMember(propertyName String, ormPropertyMetadata HibernateOrmBasicClassPropertyMetadata) : Member in class org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel",
        "diffLocations": [
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
                "startLine": 152,
                "endLine": 163,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
                "startLine": 158,
                "endLine": 193,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
                "startLine": 195,
                "endLine": 213,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private Member getPropertyMemberFromThisType(String propertyName,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n\t\tXProperty fieldAccessXProperty = getDeclaredFieldAccessXPropertiesByName().get( propertyName );\n\t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n\t\t\t// Hibernate ORM has metadata for this property (the property is persisted).\n\t\t\t// Use ORM metadata to find the corresponding member (field/method).\n\t\t\treturn getPropertyMemberUsingHibernateOrmMetadataFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty, propertyMetadataFromHibernateOrmMetamodel\n\t\t\t);\n\t\t}\n\t\telse {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n\t\t\t// Use reflection to find the corresponding member (field/method).\n\t\t\treturn getPropertyMemberUsingReflectionFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty\n\t\t\t);\n\t\t}\n\t}",
        "filePathBefore": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
        "isPureRefactoring": true,
        "commitId": "e07810b971a03041a62151cae0317714db553c6b",
        "packageNameBefore": "org.hibernate.search.mapper.orm.model.impl",
        "classNameBefore": "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel",
        "methodNameBefore": "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getPropertyMemberFromThisType",
        "invokedMethod": "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getDeclaredMethodAccessXPropertiesByName\n methodBody: private Map<String, XProperty> getDeclaredMethodAccessXPropertiesByName() {\nif(declaredMethodAccessXPropertiesByName == null){declaredMethodAccessXPropertiesByName=introspector.declaredMethodAccessXPropertiesByName(xClass);\n}return declaredMethodAccessXPropertiesByName;\n}\nmethodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getPropertyMemberUsingReflectionFromThisType\n methodBody: private Member getPropertyMemberUsingReflectionFromThisType(\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty) {\nif(methodAccessXProperty != null){return PojoCommonsAnnotationsHelper.extractUnderlyingMember(methodAccessXProperty);\n}if(fieldAccessXProperty != null){return PojoCommonsAnnotationsHelper.extractUnderlyingMember(fieldAccessXProperty);\n}{return null;\n}}\nmethodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getDeclaredFieldAccessXPropertiesByName\n methodBody: private Map<String, XProperty> getDeclaredFieldAccessXPropertiesByName() {\nif(declaredFieldAccessXPropertiesByName == null){declaredFieldAccessXPropertiesByName=introspector.declaredFieldAccessXPropertiesByName(xClass);\n}return declaredFieldAccessXPropertiesByName;\n}\nmethodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getPropertyMemberUsingHibernateOrmMetadataFromThisType\n methodBody: private Member getPropertyMemberUsingHibernateOrmMetadataFromThisType(XProperty methodAccessXProperty,\n\t\t\tXProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\nMember memberFromHibernateOrmMetamodel=propertyMetadataFromHibernateOrmMetamodel.getMember();\nif(memberFromHibernateOrmMetamodel instanceof Method){return methodAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.extractUnderlyingMember(methodAccessXProperty);\n}if(memberFromHibernateOrmMetamodel instanceof Field){return fieldAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.extractUnderlyingMember(fieldAccessXProperty);\n}{return null;\n}}",
        "classSignatureBefore": "public class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> ",
        "methodNameBeforeSet": [
            "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getPropertyMemberFromThisType"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel"
        ],
        "classSignatureBeforeSet": [
            "public class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "",
                "description": "Return statements added",
                "mappingState": 2
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.orm.model.impl;\n\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport org.hibernate.annotations.common.reflection.XProperty;\nimport org.hibernate.search.engine.mapper.model.spi.MappableTypeModel;\nimport org.hibernate.search.mapper.pojo.model.hcann.spi.PojoCommonsAnnotationsHelper;\nimport org.hibernate.search.mapper.pojo.model.spi.GenericContextAwarePojoGenericTypeModel.RawTypeDeclaringContext;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeIdentifier;\n\npublic class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> {\n\n\tprivate final HibernateOrmBasicClassTypeMetadata ormTypeMetadata;\n\tprivate final RawTypeDeclaringContext<T> rawTypeDeclaringContext;\n\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> ascendingSuperTypesCache;\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> descendingSuperTypesCache;\n\n\tprivate final Map<String, HibernateOrmClassPropertyModel<?>> propertyModelCache = new HashMap<>();\n\n\tprivate Map<String, XProperty> declaredFieldAccessXPropertiesByName;\n\tprivate Map<String, XProperty> declaredMethodAccessXPropertiesByName;\n\n\tHibernateOrmClassRawTypeModel(HibernateOrmBootstrapIntrospector introspector,\n\t\t\tPojoRawTypeIdentifier<T> typeIdentifier,\n\t\t\tHibernateOrmBasicClassTypeMetadata ormTypeMetadata, RawTypeDeclaringContext<T> rawTypeDeclaringContext) {\n\t\tsuper( introspector, typeIdentifier );\n\t\tthis.ormTypeMetadata = ormTypeMetadata;\n\t\tthis.rawTypeDeclaringContext = rawTypeDeclaringContext;\n\t}\n\n\t@Override\n\tpublic boolean isAbstract() {\n\t\treturn xClass.isAbstract();\n\t}\n\n\t@Override\n\tpublic boolean isSubTypeOf(MappableTypeModel superTypeCandidate) {\n\t\treturn superTypeCandidate instanceof HibernateOrmClassRawTypeModel\n\t\t\t\t&& ( (HibernateOrmClassRawTypeModel<?>) superTypeCandidate ).xClass.isAssignableFrom( xClass );\n\t}\n\n\t@Override\n\t@SuppressWarnings(\"unchecked\") // xClass represents T, so its supertypes represent ? super T\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> ascendingSuperTypes() {\n\t\tif ( ascendingSuperTypesCache == null ) {\n\t\t\tascendingSuperTypesCache =\n\t\t\t\t\t( (Stream<HibernateOrmClassRawTypeModel<? super T>>) introspector.getAscendingSuperTypes( xClass ) )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn ascendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\t@SuppressWarnings(\"unchecked\") // xClass represents T, so its supertypes represent ? super T\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> descendingSuperTypes() {\n\t\tif ( descendingSuperTypesCache == null ) {\n\t\t\tdescendingSuperTypesCache =\n\t\t\t\t\t( (Stream<HibernateOrmClassRawTypeModel<? super T>>) introspector.getDescendingSuperTypes( xClass ) )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn descendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\tpublic Stream<Annotation> annotations() {\n\t\treturn introspector.annotations( xClass );\n\t}\n\n\t@Override\n\tStream<String> getDeclaredPropertyNames() {\n\t\treturn Stream.concat(\n\t\t\t\tgetDeclaredFieldAccessXPropertiesByName().keySet().stream(),\n\t\t\t\tgetDeclaredMethodAccessXPropertiesByName().keySet().stream()\n\t\t)\n\t\t\t\t.distinct();\n\t}\n\n\t@Override\n\tHibernateOrmClassPropertyModel<?> getPropertyOrNull(String propertyName) {\n\t\treturn propertyModelCache.computeIfAbsent( propertyName, this::createPropertyModel );\n\t}\n\n\tRawTypeDeclaringContext<T> getRawTypeDeclaringContext() {\n\t\treturn rawTypeDeclaringContext;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredFieldAccessXPropertiesByName() {\n\t\tif ( declaredFieldAccessXPropertiesByName == null ) {\n\t\t\tdeclaredFieldAccessXPropertiesByName =\n\t\t\t\t\tintrospector.declaredFieldAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredFieldAccessXPropertiesByName;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredMethodAccessXPropertiesByName() {\n\t\tif ( declaredMethodAccessXPropertiesByName == null ) {\n\t\t\tdeclaredMethodAccessXPropertiesByName =\n\t\t\t\t\tintrospector.declaredMethodAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredMethodAccessXPropertiesByName;\n\t}\n\n\tprivate HibernateOrmClassPropertyModel<?> createPropertyModel(String propertyName) {\n\t\tList<XProperty> declaredXProperties = new ArrayList<>( 2 );\n\t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( methodAccessXProperty );\n\t\t}\n\t\tXProperty fieldAccessXProperty = getDeclaredFieldAccessXPropertiesByName().get( propertyName );\n\t\tif ( fieldAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( fieldAccessXProperty );\n\t\t}\n\n\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata = findOrmPropertyMetadata( propertyName );\n\t\tMember member = findPropertyMember( propertyName, ormPropertyMetadata );\n\n\t\tif ( member == null ) {\n\t\t\treturn null;\n\t\t}\n\n\t\treturn new HibernateOrmClassPropertyModel<>(\n\t\t\t\tintrospector, this, propertyName,\n\t\t\t\tdeclaredXProperties, ormPropertyMetadata, member\n\t\t);\n\t}\n\n\tprivate HibernateOrmBasicClassPropertyMetadata findOrmPropertyMetadata(String propertyName) {\n\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadata = getOrmPropertyMetadataFromThisType( propertyName );\n\t\tif ( propertyMetadata == null ) {\n\t\t\tpropertyMetadata = getOrmPropertyMetadataFromParentTypes( propertyName );\n\t\t}\n\t\treturn propertyMetadata;\n\t}\n\n\tprivate Member findPropertyMember(String propertyName,\n\t\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata) {\n\t\tMember result = getPropertyMemberFromThisType( propertyName, ormPropertyMetadata );\n\n\t\tif ( result == null ) {\n\t\t\t// There is no member for this property on the current type.\n\t\t\t// Try to find one in the closest supertype.\n\t\t\tresult = getPropertyMemberFromParentTypes( propertyName, ormPropertyMetadata );\n\t\t}\n\n\t\treturn result;\n\t}\n\n\tprivate HibernateOrmBasicClassPropertyMetadata getOrmPropertyMetadataFromParentTypes(String propertyName) {\n\t\t// TODO HSEARCH-3056 remove lambdas if possible\n\t\treturn ascendingSuperTypes()\n\t\t\t\t.skip( 1 ) // Ignore self\n\t\t\t\t.map( type -> type.getOrmPropertyMetadataFromThisType( propertyName ) )\n\t\t\t\t.filter( Objects::nonNull )\n\t\t\t\t.findFirst()\n\t\t\t\t.orElse( null );\n\t}\n\n\tprivate HibernateOrmBasicClassPropertyMetadata getOrmPropertyMetadataFromThisType(String propertyName) {\n\t\tif ( ormTypeMetadata != null ) {\n\t\t\treturn ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n\t\t}\n\t\telse {\n\t\t\treturn null;\n\t\t}\n\t}\n\n\tprivate Member getPropertyMemberFromParentTypes(String propertyName,\n\t\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata) {\n\t\t// TODO HSEARCH-3056 remove lambdas if possible\n\t\treturn ascendingSuperTypes()\n\t\t\t\t.skip( 1 ) // Ignore self\n\t\t\t\t.map( type -> type.getPropertyMemberFromThisType( propertyName, ormPropertyMetadata ) )\n\t\t\t\t.filter( Objects::nonNull )\n\t\t\t\t.findFirst()\n\t\t\t\t.orElse( null );\n\t}\n\n\tprivate Member getPropertyMemberFromThisType(String propertyName,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n\t\tXProperty fieldAccessXProperty = getDeclaredFieldAccessXPropertiesByName().get( propertyName );\n\t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n\t\t\t// Hibernate ORM has metadata for this property (the property is persisted).\n\t\t\t// Use ORM metadata to find the corresponding member (field/method).\n\t\t\treturn getPropertyMemberUsingHibernateOrmMetadataFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty, propertyMetadataFromHibernateOrmMetamodel\n\t\t\t);\n\t\t}\n\t\telse {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n\t\t\t// Use reflection to find the corresponding member (field/method).\n\t\t\treturn getPropertyMemberUsingReflectionFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty\n\t\t\t);\n\t\t}\n\t}\n\n\tprivate Member getPropertyMemberUsingHibernateOrmMetadataFromThisType(XProperty methodAccessXProperty,\n\t\t\tXProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n\t\t/*\n\t\t * Hibernate ORM has metadata for this property,\n\t\t * which means this property is persisted.\n\t\t *\n\t\t * Hibernate ORM might return us the member as declared in a supertype,\n\t\t * in which case the type of that member will not be up-to-date.\n\t\t * Thus we try to get the overridden member declared in the current type,\n\t\t * and failing that we look for the member in supertypes.\n\t\t *\n\t\t * We still try to comply with JPA's configured access type,\n\t\t * which explains the two if/else branches below.\n\t\t */\n\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n\t\t\treturn methodAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.extractUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n\t\t\treturn fieldAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.extractUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\treturn null;\n\t\t}\n\t}\n\n\t/*\n\t * Hibernate ORM doesn't have any metadata for this property,\n\t * which means this property is transient.\n\t * We don't need to worry about JPA's access type.\n\t */\n\tprivate Member getPropertyMemberUsingReflectionFromThisType(\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty) {\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\t// Method access is available. Get values from the getter.\n\t\t\treturn PojoCommonsAnnotationsHelper.extractUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( fieldAccessXProperty != null ) {\n\t\t\t// Method access is not available, but field access is. Get values directly from the field.\n\t\t\treturn PojoCommonsAnnotationsHelper.extractUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\t// Neither method access nor field access is available.\n\t\t\t// The property is not declared in this type.\n\t\t\treturn null;\n\t\t}\n\t}\n}\n",
        "filePathAfter": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.orm.model.impl;\n\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.function.Function;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport org.hibernate.annotations.common.reflection.XProperty;\nimport org.hibernate.search.engine.mapper.model.spi.MappableTypeModel;\nimport org.hibernate.search.mapper.pojo.model.hcann.spi.PojoCommonsAnnotationsHelper;\nimport org.hibernate.search.mapper.pojo.model.spi.GenericContextAwarePojoGenericTypeModel.RawTypeDeclaringContext;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeIdentifier;\n\npublic class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> {\n\n\tprivate final HibernateOrmBasicClassTypeMetadata ormTypeMetadata;\n\tprivate final RawTypeDeclaringContext<T> rawTypeDeclaringContext;\n\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> ascendingSuperTypesCache;\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> descendingSuperTypesCache;\n\n\tprivate final Map<String, HibernateOrmClassPropertyModel<?>> propertyModelCache = new HashMap<>();\n\n\tprivate Map<String, XProperty> declaredFieldAccessXPropertiesByName;\n\tprivate Map<String, XProperty> declaredMethodAccessXPropertiesByName;\n\n\tHibernateOrmClassRawTypeModel(HibernateOrmBootstrapIntrospector introspector,\n\t\t\tPojoRawTypeIdentifier<T> typeIdentifier,\n\t\t\tHibernateOrmBasicClassTypeMetadata ormTypeMetadata, RawTypeDeclaringContext<T> rawTypeDeclaringContext) {\n\t\tsuper( introspector, typeIdentifier );\n\t\tthis.ormTypeMetadata = ormTypeMetadata;\n\t\tthis.rawTypeDeclaringContext = rawTypeDeclaringContext;\n\t}\n\n\t@Override\n\tpublic boolean isAbstract() {\n\t\treturn xClass.isAbstract();\n\t}\n\n\t@Override\n\tpublic boolean isSubTypeOf(MappableTypeModel superTypeCandidate) {\n\t\treturn superTypeCandidate instanceof HibernateOrmClassRawTypeModel\n\t\t\t\t&& ( (HibernateOrmClassRawTypeModel<?>) superTypeCandidate ).xClass.isAssignableFrom( xClass );\n\t}\n\n\t@Override\n\t@SuppressWarnings(\"unchecked\") // xClass represents T, so its supertypes represent ? super T\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> ascendingSuperTypes() {\n\t\tif ( ascendingSuperTypesCache == null ) {\n\t\t\tascendingSuperTypesCache =\n\t\t\t\t\t( (Stream<HibernateOrmClassRawTypeModel<? super T>>) introspector.getAscendingSuperTypes( xClass ) )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn ascendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\t@SuppressWarnings(\"unchecked\") // xClass represents T, so its supertypes represent ? super T\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> descendingSuperTypes() {\n\t\tif ( descendingSuperTypesCache == null ) {\n\t\t\tdescendingSuperTypesCache =\n\t\t\t\t\t( (Stream<HibernateOrmClassRawTypeModel<? super T>>) introspector.getDescendingSuperTypes( xClass ) )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn descendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\tpublic Stream<Annotation> annotations() {\n\t\treturn introspector.annotations( xClass );\n\t}\n\n\t@Override\n\tStream<String> getDeclaredPropertyNames() {\n\t\treturn Stream.concat(\n\t\t\t\tgetDeclaredFieldAccessXPropertiesByName().keySet().stream(),\n\t\t\t\tgetDeclaredMethodAccessXPropertiesByName().keySet().stream()\n\t\t)\n\t\t\t\t.distinct();\n\t}\n\n\t@Override\n\tHibernateOrmClassPropertyModel<?> getPropertyOrNull(String propertyName) {\n\t\treturn propertyModelCache.computeIfAbsent( propertyName, this::createPropertyModel );\n\t}\n\n\tRawTypeDeclaringContext<T> getRawTypeDeclaringContext() {\n\t\treturn rawTypeDeclaringContext;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredFieldAccessXPropertiesByName() {\n\t\tif ( declaredFieldAccessXPropertiesByName == null ) {\n\t\t\tdeclaredFieldAccessXPropertiesByName =\n\t\t\t\t\tintrospector.declaredFieldAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredFieldAccessXPropertiesByName;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredMethodAccessXPropertiesByName() {\n\t\tif ( declaredMethodAccessXPropertiesByName == null ) {\n\t\t\tdeclaredMethodAccessXPropertiesByName =\n\t\t\t\t\tintrospector.declaredMethodAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredMethodAccessXPropertiesByName;\n\t}\n\n\tprivate HibernateOrmClassPropertyModel<?> createPropertyModel(String propertyName) {\n\t\tList<XProperty> declaredXProperties = new ArrayList<>( 2 );\n\t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( methodAccessXProperty );\n\t\t}\n\t\tXProperty fieldAccessXProperty = getDeclaredFieldAccessXPropertiesByName().get( propertyName );\n\t\tif ( fieldAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( fieldAccessXProperty );\n\t\t}\n\n\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata = findOrmPropertyMetadata( propertyName );\n\t\tMember member = findPropertyMember( propertyName, ormPropertyMetadata );\n\n\t\tif ( member == null ) {\n\t\t\treturn null;\n\t\t}\n\n\t\treturn new HibernateOrmClassPropertyModel<>(\n\t\t\t\tintrospector, this, propertyName,\n\t\t\t\tdeclaredXProperties, ormPropertyMetadata, member\n\t\t);\n\t}\n\n\tprivate HibernateOrmBasicClassPropertyMetadata findOrmPropertyMetadata(String propertyName) {\n\t\treturn findInSelfOrParents( t -> t.ormPropertyMetadataFromThisType( propertyName ) );\n\t}\n\n\tprivate HibernateOrmBasicClassPropertyMetadata ormPropertyMetadataFromThisType(String propertyName) {\n\t\tif ( ormTypeMetadata != null ) {\n\t\t\treturn ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n\t\t}\n\t\telse {\n\t\t\treturn null;\n\t\t}\n\t}\n\n\tprivate Member findPropertyMember(String propertyName, HibernateOrmBasicClassPropertyMetadata ormPropertyMetadata) {\n\t\tif ( ormPropertyMetadata != null ) {\n\t\t\t/*\n\t\t\t * Hibernate ORM has metadata for this property,\n\t\t\t * which means this property is persisted.\n\t\t\t *\n\t\t\t * Hibernate ORM might return us the member as declared in a supertype,\n\t\t\t * in which case the type of that member will not be up-to-date.\n\t\t\t * Thus we try to get the overridden member declared in the current type,\n\t\t\t * and failing that we look for the member in supertypes.\n\t\t\t *\n\t\t\t * We still try to comply with JPA's configured access type,\n\t\t\t * which explains the two if/else branches below.\n\t\t\t */\n\t\t\tMember memberFromHibernateOrmMetamodel = ormPropertyMetadata.getMember();\n\t\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n\t\t\t\treturn findInSelfOrParents( t -> t.declaredPropertyGetter( propertyName ) );\n\t\t\t}\n\t\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n\t\t\t\treturn findInSelfOrParents( t -> t.declaredPropertyField( propertyName ) );\n\t\t\t}\n\t\t\telse {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n\t\t\t// Try using the getter first (if declared)...\n\t\t\tMember getter = findInSelfOrParents( t -> t.declaredPropertyGetter( propertyName ) );\n\t\t\tif ( getter != null ) {\n\t\t\t\treturn getter;\n\t\t\t}\n\t\t\t// ... and fall back to the field (or null if not found)\n\t\t\treturn findInSelfOrParents( t -> t.declaredPropertyField( propertyName ) );\n\t\t}\n\t}\n\n\tprivate <T2> T2 findInSelfOrParents(Function<HibernateOrmClassRawTypeModel<?>, T2> getter) {\n\t\treturn ascendingSuperTypes()\n\t\t\t\t.skip( 1 ) // Ignore self\n\t\t\t\t.map( getter )\n\t\t\t\t.filter( Objects::nonNull )\n\t\t\t\t.findFirst()\n\t\t\t\t.orElse( null );\n\t}\n\n\tprivate Member declaredPropertyGetter(String propertyName) {\n\t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\t// Method access is available. Get values from the getter.\n\t\t\treturn PojoCommonsAnnotationsHelper.extractUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\treturn null;\n\t}\n\n\tprivate Member declaredPropertyField(String propertyName) {\n\t\tXProperty fieldAccessXProperty = getDeclaredFieldAccessXPropertiesByName().get( propertyName );\n\t\tif ( fieldAccessXProperty != null ) {\n\t\t\t// Method access is available. Get values from the getter.\n\t\t\treturn PojoCommonsAnnotationsHelper.extractUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\treturn null;\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getDeclaredMethodAccessXPropertiesByName\n methodBody: private Map<String, XProperty> getDeclaredMethodAccessXPropertiesByName() {\nif(declaredMethodAccessXPropertiesByName == null){declaredMethodAccessXPropertiesByName=introspector.declaredMethodAccessXPropertiesByName(xClass);\n}return declaredMethodAccessXPropertiesByName;\n}",
            "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getPropertyMemberUsingReflectionFromThisType\n methodBody: private Member getPropertyMemberUsingReflectionFromThisType(\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty) {\nif(methodAccessXProperty != null){return PojoCommonsAnnotationsHelper.extractUnderlyingMember(methodAccessXProperty);\n}if(fieldAccessXProperty != null){return PojoCommonsAnnotationsHelper.extractUnderlyingMember(fieldAccessXProperty);\n}{return null;\n}}",
            "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getDeclaredFieldAccessXPropertiesByName\n methodBody: private Map<String, XProperty> getDeclaredFieldAccessXPropertiesByName() {\nif(declaredFieldAccessXPropertiesByName == null){declaredFieldAccessXPropertiesByName=introspector.declaredFieldAccessXPropertiesByName(xClass);\n}return declaredFieldAccessXPropertiesByName;\n}",
            "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getPropertyMemberUsingHibernateOrmMetadataFromThisType\n methodBody: private Member getPropertyMemberUsingHibernateOrmMetadataFromThisType(XProperty methodAccessXProperty,\n\t\t\tXProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\nMember memberFromHibernateOrmMetamodel=propertyMetadataFromHibernateOrmMetamodel.getMember();\nif(memberFromHibernateOrmMetamodel instanceof Method){return methodAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.extractUnderlyingMember(methodAccessXProperty);\n}if(memberFromHibernateOrmMetamodel instanceof Field){return fieldAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.extractUnderlyingMember(fieldAccessXProperty);\n}{return null;\n}}"
        ],
        "sourceCodeAfterRefactoring": "private Member findPropertyMember(String propertyName, HibernateOrmBasicClassPropertyMetadata ormPropertyMetadata) {\n\t\tif ( ormPropertyMetadata != null ) {\n\t\t\t/*\n\t\t\t * Hibernate ORM has metadata for this property,\n\t\t\t * which means this property is persisted.\n\t\t\t *\n\t\t\t * Hibernate ORM might return us the member as declared in a supertype,\n\t\t\t * in which case the type of that member will not be up-to-date.\n\t\t\t * Thus we try to get the overridden member declared in the current type,\n\t\t\t * and failing that we look for the member in supertypes.\n\t\t\t *\n\t\t\t * We still try to comply with JPA's configured access type,\n\t\t\t * which explains the two if/else branches below.\n\t\t\t */\n\t\t\tMember memberFromHibernateOrmMetamodel = ormPropertyMetadata.getMember();\n\t\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n\t\t\t\treturn findInSelfOrParents( t -> t.declaredPropertyGetter( propertyName ) );\n\t\t\t}\n\t\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n\t\t\t\treturn findInSelfOrParents( t -> t.declaredPropertyField( propertyName ) );\n\t\t\t}\n\t\t\telse {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n\t\t\t// Try using the getter first (if declared)...\n\t\t\tMember getter = findInSelfOrParents( t -> t.declaredPropertyGetter( propertyName ) );\n\t\t\tif ( getter != null ) {\n\t\t\t\treturn getter;\n\t\t\t}\n\t\t\t// ... and fall back to the field (or null if not found)\n\t\t\treturn findInSelfOrParents( t -> t.declaredPropertyField( propertyName ) );\n\t\t}\n\t}",
        "diffSourceCode": "-  152: \tprivate Member findPropertyMember(String propertyName,\n-  153: \t\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata) {\n-  154: \t\tMember result = getPropertyMemberFromThisType( propertyName, ormPropertyMetadata );\n-  155: \n-  156: \t\tif ( result == null ) {\n-  157: \t\t\t// There is no member for this property on the current type.\n-  158: \t\t\t// Try to find one in the closest supertype.\n-  159: \t\t\tresult = getPropertyMemberFromParentTypes( propertyName, ormPropertyMetadata );\n-  160: \t\t}\n-  161: \n-  162: \t\treturn result;\n-  163: \t}\n-  164: \n-  165: \tprivate HibernateOrmBasicClassPropertyMetadata getOrmPropertyMetadataFromParentTypes(String propertyName) {\n-  166: \t\t// TODO HSEARCH-3056 remove lambdas if possible\n-  167: \t\treturn ascendingSuperTypes()\n-  168: \t\t\t\t.skip( 1 ) // Ignore self\n-  169: \t\t\t\t.map( type -> type.getOrmPropertyMetadataFromThisType( propertyName ) )\n-  170: \t\t\t\t.filter( Objects::nonNull )\n-  171: \t\t\t\t.findFirst()\n-  172: \t\t\t\t.orElse( null );\n-  173: \t}\n-  174: \n-  175: \tprivate HibernateOrmBasicClassPropertyMetadata getOrmPropertyMetadataFromThisType(String propertyName) {\n-  176: \t\tif ( ormTypeMetadata != null ) {\n-  177: \t\t\treturn ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n-  178: \t\t}\n-  179: \t\telse {\n-  180: \t\t\treturn null;\n-  181: \t\t}\n-  182: \t}\n-  183: \n-  184: \tprivate Member getPropertyMemberFromParentTypes(String propertyName,\n-  185: \t\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata) {\n-  186: \t\t// TODO HSEARCH-3056 remove lambdas if possible\n-  187: \t\treturn ascendingSuperTypes()\n-  188: \t\t\t\t.skip( 1 ) // Ignore self\n-  189: \t\t\t\t.map( type -> type.getPropertyMemberFromThisType( propertyName, ormPropertyMetadata ) )\n-  190: \t\t\t\t.filter( Objects::nonNull )\n-  191: \t\t\t\t.findFirst()\n-  192: \t\t\t\t.orElse( null );\n+  152: \t\t}\n+  153: \t\telse {\n+  154: \t\t\treturn null;\n+  155: \t\t}\n+  156: \t}\n+  157: \n+  158: \tprivate Member findPropertyMember(String propertyName, HibernateOrmBasicClassPropertyMetadata ormPropertyMetadata) {\n+  159: \t\tif ( ormPropertyMetadata != null ) {\n+  160: \t\t\t/*\n+  161: \t\t\t * Hibernate ORM has metadata for this property,\n+  162: \t\t\t * which means this property is persisted.\n+  163: \t\t\t *\n+  164: \t\t\t * Hibernate ORM might return us the member as declared in a supertype,\n+  165: \t\t\t * in which case the type of that member will not be up-to-date.\n+  166: \t\t\t * Thus we try to get the overridden member declared in the current type,\n+  167: \t\t\t * and failing that we look for the member in supertypes.\n+  168: \t\t\t *\n+  169: \t\t\t * We still try to comply with JPA's configured access type,\n+  170: \t\t\t * which explains the two if/else branches below.\n+  171: \t\t\t */\n+  172: \t\t\tMember memberFromHibernateOrmMetamodel = ormPropertyMetadata.getMember();\n+  173: \t\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n+  174: \t\t\t\treturn findInSelfOrParents( t -> t.declaredPropertyGetter( propertyName ) );\n+  175: \t\t\t}\n+  176: \t\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n+  177: \t\t\t\treturn findInSelfOrParents( t -> t.declaredPropertyField( propertyName ) );\n+  178: \t\t\t}\n+  179: \t\t\telse {\n+  180: \t\t\t\treturn null;\n+  181: \t\t\t}\n+  182: \t\t}\n+  183: \t\telse {\n+  184: \t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n+  185: \t\t\t// Try using the getter first (if declared)...\n+  186: \t\t\tMember getter = findInSelfOrParents( t -> t.declaredPropertyGetter( propertyName ) );\n+  187: \t\t\tif ( getter != null ) {\n+  188: \t\t\t\treturn getter;\n+  189: \t\t\t}\n+  190: \t\t\t// ... and fall back to the field (or null if not found)\n+  191: \t\t\treturn findInSelfOrParents( t -> t.declaredPropertyField( propertyName ) );\n+  192: \t\t}\n   193: \t}\n-  195: \tprivate Member getPropertyMemberFromThisType(String propertyName,\n-  196: \t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n-  197: \t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n-  198: \t\tXProperty fieldAccessXProperty = getDeclaredFieldAccessXPropertiesByName().get( propertyName );\n-  199: \t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n-  200: \t\t\t// Hibernate ORM has metadata for this property (the property is persisted).\n-  201: \t\t\t// Use ORM metadata to find the corresponding member (field/method).\n-  202: \t\t\treturn getPropertyMemberUsingHibernateOrmMetadataFromThisType(\n-  203: \t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty, propertyMetadataFromHibernateOrmMetamodel\n-  204: \t\t\t);\n-  205: \t\t}\n-  206: \t\telse {\n-  207: \t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n-  208: \t\t\t// Use reflection to find the corresponding member (field/method).\n-  209: \t\t\treturn getPropertyMemberUsingReflectionFromThisType(\n-  210: \t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty\n-  211: \t\t\t);\n-  212: \t\t}\n-  213: \t}\n+  195: \tprivate <T2> T2 findInSelfOrParents(Function<HibernateOrmClassRawTypeModel<?>, T2> getter) {\n+  196: \t\treturn ascendingSuperTypes()\n+  197: \t\t\t\t.skip( 1 ) // Ignore self\n+  198: \t\t\t\t.map( getter )\n+  199: \t\t\t\t.filter( Objects::nonNull )\n+  200: \t\t\t\t.findFirst()\n+  201: \t\t\t\t.orElse( null );\n+  202: \t}\n+  203: \n+  204: \tprivate Member declaredPropertyGetter(String propertyName) {\n+  205: \t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n+  206: \t\tif ( methodAccessXProperty != null ) {\n+  207: \t\t\t// Method access is available. Get values from the getter.\n+  208: \t\t\treturn PojoCommonsAnnotationsHelper.extractUnderlyingMember( methodAccessXProperty );\n+  209: \t\t}\n+  210: \t\treturn null;\n+  211: \t}\n+  212: \n+  213: \tprivate Member declaredPropertyField(String propertyName) {\n",
        "uniqueId": "e07810b971a03041a62151cae0317714db553c6b_152_163__158_193_195_213",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 5,
                "covered": 20
            },
            "BRANCH": {
                "missed": 1,
                "covered": 1
            },
            "LINE": {
                "missed": 1,
                "covered": 4
            },
            "COMPLEXITY": {
                "missed": 1,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprotected getHits(targetIndexes List<String>, query SearchQuery<T>, hitDocumentReferences List<DocumentReference>) : List<T> extracted from protected testLoading(sessionSetup Consumer<Session>, targetClasses List<? extends Class<? extends T>>, targetIndexes List<String>, loadingOptionsContributor Consumer<SearchLoadingOptionsStep>, hitDocumentReferencesContributor Consumer<DocumentReferenceCollector>, expectedLoadedEntitiesContributor Consumer<EntityCollector<T>>, assertionsContributor BiConsumer<OrmSoftAssertions,List<T>>) : void in class org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/search/loading/AbstractSearchQueryEntityLoadingIT.java",
                "startLine": 58,
                "endLine": 126,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/search/loading/AbstractSearchQueryEntityLoadingIT.java",
                "startLine": 58,
                "endLine": 117,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/search/loading/AbstractSearchQueryEntityLoadingIT.java",
                "startLine": 119,
                "endLine": 130,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "protected final <T> void testLoading(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tList<? extends Class<? extends T>> targetClasses,\n\t\t\tList<String> targetIndexes,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tConsumer<DocumentReferenceCollector> hitDocumentReferencesContributor,\n\t\t\tConsumer<EntityCollector<T>> expectedLoadedEntitiesContributor,\n\t\t\tBiConsumer<OrmSoftAssertions, List<T>> assertionsContributor) {\n\t\tOrmSoftAssertions.withinSession( sessionFactory(), (session, softAssertions) -> {\n\t\t\tsessionSetup.accept( session );\n\n\t\t\tsoftAssertions.resetListenerData();\n\n\t\t\tSearchSession searchSession = Search.session( session );\n\n\t\t\tSearchQuery<T> query = searchSession.search( targetClasses )\n\t\t\t\t\t.where( f -> f.matchAll() )\n\t\t\t\t\t.loading( loadingOptionsContributor )\n\t\t\t\t\t.toQuery();\n\n\t\t\tDocumentReferenceCollector documentReferenceCollector = new DocumentReferenceCollector();\n\t\t\thitDocumentReferencesContributor.accept( documentReferenceCollector );\n\t\t\tList<DocumentReference> hitDocumentReferences = documentReferenceCollector.collected;\n\n\t\t\tbackendMock.expectSearchObjects(\n\t\t\t\t\ttargetIndexes,\n\t\t\t\t\tb -> { },\n\t\t\t\t\tStubSearchWorkBehavior.of(\n\t\t\t\t\t\t\thitDocumentReferences.size(),\n\t\t\t\t\t\t\thitDocumentReferences\n\t\t\t\t\t)\n\t\t\t);\n\n\t\t\tList<T> loadedEntities = query.fetchAllHits();\n\n\t\t\tsoftAssertions.assertThat( loadedEntities )\n\t\t\t\t\t.as(\n\t\t\t\t\t\t\t\"Loaded entities when targeting types \" + targetClasses\n\t\t\t\t\t\t\t\t\t+ \" and when the backend returns document references \" + hitDocumentReferences\n\t\t\t\t\t)\n\t\t\t\t\t.allSatisfy( loadedEntity -> {\n\t\t\t\t\t\t// Loading should fully initialize entities\n\t\t\t\t\t\tassertThat( Hibernate.isInitialized( loadedEntity ) ).isTrue();\n\t\t\t\t\t} );\n\n\t\t\tassertionsContributor.accept( softAssertions, loadedEntities );\n\n\t\t\t// Be sure to do this after having executed the query and checked loading,\n\t\t\t// because it may trigger additional loading.\n\t\t\tEntityCollector<T> entityCollector = new EntityCollector<>( session );\n\t\t\texpectedLoadedEntitiesContributor.accept( entityCollector );\n\t\t\tList<T> expectedLoadedEntities = entityCollector.collected;\n\n\t\t\t// Both the expected and actual list may contain proxies: unproxy everything so that equals() works correctly\n\t\t\tList<T> unproxyfiedExpectedLoadedEntities = unproxyAll( expectedLoadedEntities );\n\t\t\tList<T> unproxyfiedLoadedEntities = unproxyAll( loadedEntities );\n\n\t\t\tsoftAssertions.assertThat( unproxyfiedLoadedEntities )\n\t\t\t\t\t.as(\n\t\t\t\t\t\t\t\"Loaded, then unproxified entities when targeting types \" + targetClasses\n\t\t\t\t\t\t\t\t\t+ \" and when the backend returns document references \" + hitDocumentReferences\n\t\t\t\t\t)\n\t\t\t\t\t.allSatisfy(\n\t\t\t\t\t\t\telement -> assertThat( element )\n\t\t\t\t\t\t\t\t\t.isInstanceOfAny( targetClasses.toArray( new Class<?>[0] ) )\n\t\t\t\t\t)\n\t\t\t\t\t.containsExactlyElementsOf( unproxyfiedExpectedLoadedEntities );\n\t\t} );\n\t}",
        "filePathBefore": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/search/loading/AbstractSearchQueryEntityLoadingIT.java",
        "isPureRefactoring": true,
        "commitId": "a491e38e1782d640c73a5cc042eae0ffdc50ae87",
        "packageNameBefore": "org.hibernate.search.integrationtest.mapper.orm.search.loading",
        "classNameBefore": "org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingIT#testLoading",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingIT#sessionFactory\n methodBody: protected abstract SessionFactory sessionFactory();\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingIT#unproxyAll\n methodBody: private <T> List<T> unproxyAll(List<T> entityList) {\nreturn entityList.stream().map(entity -> (T)Hibernate.unproxy(entity)).collect(Collectors.toList());\n}",
        "classSignatureBefore": "public abstract class AbstractSearchQueryEntityLoadingIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingIT#testLoading"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingIT"
        ],
        "classSignatureBeforeSet": [
            "public abstract class AbstractSearchQueryEntityLoadingIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.search.loading;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.backend.StubBackendUtils.reference;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.function.BiConsumer;\nimport java.util.function.Consumer;\nimport java.util.stream.Collectors;\n\nimport org.hibernate.Hibernate;\nimport org.hibernate.Session;\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.common.DocumentReference;\nimport org.hibernate.search.engine.search.query.SearchQuery;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.search.loading.dsl.SearchLoadingOptionsStep;\nimport org.hibernate.search.mapper.orm.session.SearchSession;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.StubSearchWorkBehavior;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSoftAssertions;\n\nimport org.junit.Rule;\n\npublic abstract class AbstractSearchQueryEntityLoadingIT {\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock();\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock );\n\n\tprotected abstract SessionFactory sessionFactory();\n\n\tprotected final <T> void testLoading(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tList<? extends Class<? extends T>> targetClasses,\n\t\t\tList<String> targetIndexes,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tConsumer<DocumentReferenceCollector> hitDocumentReferencesContributor,\n\t\t\tConsumer<EntityCollector<T>> expectedLoadedEntitiesContributor,\n\t\t\tConsumer<OrmSoftAssertions> assertionsContributor) {\n\t\ttestLoading(\n\t\t\t\tsessionSetup, targetClasses, targetIndexes,\n\t\t\t\tloadingOptionsContributor, hitDocumentReferencesContributor, expectedLoadedEntitiesContributor,\n\t\t\t\t(assertions, ignored) -> assertionsContributor.accept( assertions )\n\t\t);\n\t}\n\n\tprotected final <T> void testLoading(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tList<? extends Class<? extends T>> targetClasses,\n\t\t\tList<String> targetIndexes,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tConsumer<DocumentReferenceCollector> hitDocumentReferencesContributor,\n\t\t\tConsumer<EntityCollector<T>> expectedLoadedEntitiesContributor,\n\t\t\tBiConsumer<OrmSoftAssertions, List<T>> assertionsContributor) {\n\t\tOrmSoftAssertions.withinSession( sessionFactory(), (session, softAssertions) -> {\n\t\t\tsessionSetup.accept( session );\n\n\t\t\tsoftAssertions.resetListenerData();\n\n\t\t\tSearchSession searchSession = Search.session( session );\n\n\t\t\tSearchQuery<T> query = searchSession.search( targetClasses )\n\t\t\t\t\t.where( f -> f.matchAll() )\n\t\t\t\t\t.loading( loadingOptionsContributor )\n\t\t\t\t\t.toQuery();\n\n\t\t\tDocumentReferenceCollector documentReferenceCollector = new DocumentReferenceCollector();\n\t\t\thitDocumentReferencesContributor.accept( documentReferenceCollector );\n\t\t\tList<DocumentReference> hitDocumentReferences = documentReferenceCollector.collected;\n\n\t\t\tbackendMock.expectSearchObjects(\n\t\t\t\t\ttargetIndexes,\n\t\t\t\t\tb -> { },\n\t\t\t\t\tStubSearchWorkBehavior.of(\n\t\t\t\t\t\t\thitDocumentReferences.size(),\n\t\t\t\t\t\t\thitDocumentReferences\n\t\t\t\t\t)\n\t\t\t);\n\n\t\t\tList<T> loadedEntities = query.fetchAllHits();\n\n\t\t\tsoftAssertions.assertThat( loadedEntities )\n\t\t\t\t\t.as(\n\t\t\t\t\t\t\t\"Loaded entities when targeting types \" + targetClasses\n\t\t\t\t\t\t\t\t\t+ \" and when the backend returns document references \" + hitDocumentReferences\n\t\t\t\t\t)\n\t\t\t\t\t.allSatisfy( loadedEntity -> {\n\t\t\t\t\t\t// Loading should fully initialize entities\n\t\t\t\t\t\tassertThat( Hibernate.isInitialized( loadedEntity ) ).isTrue();\n\t\t\t\t\t} );\n\n\t\t\tassertionsContributor.accept( softAssertions, loadedEntities );\n\n\t\t\t// Be sure to do this after having executed the query and checked loading,\n\t\t\t// because it may trigger additional loading.\n\t\t\tEntityCollector<T> entityCollector = new EntityCollector<>( session );\n\t\t\texpectedLoadedEntitiesContributor.accept( entityCollector );\n\t\t\tList<T> expectedLoadedEntities = entityCollector.collected;\n\n\t\t\t// Both the expected and actual list may contain proxies: unproxy everything so that equals() works correctly\n\t\t\tList<T> unproxyfiedExpectedLoadedEntities = unproxyAll( expectedLoadedEntities );\n\t\t\tList<T> unproxyfiedLoadedEntities = unproxyAll( loadedEntities );\n\n\t\t\tsoftAssertions.assertThat( unproxyfiedLoadedEntities )\n\t\t\t\t\t.as(\n\t\t\t\t\t\t\t\"Loaded, then unproxified entities when targeting types \" + targetClasses\n\t\t\t\t\t\t\t\t\t+ \" and when the backend returns document references \" + hitDocumentReferences\n\t\t\t\t\t)\n\t\t\t\t\t.allSatisfy(\n\t\t\t\t\t\t\telement -> assertThat( element )\n\t\t\t\t\t\t\t\t\t.isInstanceOfAny( targetClasses.toArray( new Class<?>[0] ) )\n\t\t\t\t\t)\n\t\t\t\t\t.containsExactlyElementsOf( unproxyfiedExpectedLoadedEntities );\n\t\t} );\n\t}\n\n\t// This cast is fine as long as T is not a proxy interface\n\t@SuppressWarnings(\"unchecked\")\n\tprivate <T> List<T> unproxyAll(List<T> entityList) {\n\t\treturn entityList.stream()\n\t\t\t\t.map( entity -> (T) Hibernate.unproxy( entity ) )\n\t\t\t\t.collect( Collectors.toList() );\n\t}\n\n\tprotected static class DocumentReferenceCollector {\n\t\tprivate final List<DocumentReference> collected = new ArrayList<>();\n\n\t\tpublic DocumentReferenceCollector doc(String indexName, String documentId) {\n\t\t\tcollected.add( reference( indexName, documentId ) );\n\t\t\treturn this;\n\t\t}\n\t}\n\n\tprotected static class EntityCollector<T> {\n\t\tprivate final Session session;\n\t\tprivate final List<T> collected = new ArrayList<>();\n\n\t\tprivate EntityCollector(Session session) {\n\t\t\tthis.session = session;\n\t\t}\n\n\t\tpublic EntityCollector<T> entity(Class<? extends T> entityType, Object entityId) {\n\t\t\tcollected.add( session.getReference( entityType, entityId ) );\n\t\t\treturn this;\n\t\t}\n\t}\n\n}\n",
        "filePathAfter": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/search/loading/AbstractSearchQueryEntityLoadingIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.search.loading;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.hibernate.search.util.impl.integrationtest.common.stub.backend.StubBackendUtils.reference;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.function.BiConsumer;\nimport java.util.function.Consumer;\nimport java.util.stream.Collectors;\n\nimport org.hibernate.Hibernate;\nimport org.hibernate.Session;\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.common.DocumentReference;\nimport org.hibernate.search.engine.search.query.SearchQuery;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.search.loading.dsl.SearchLoadingOptionsStep;\nimport org.hibernate.search.mapper.orm.session.SearchSession;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.StubSearchWorkBehavior;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSoftAssertions;\n\nimport org.junit.Rule;\n\npublic abstract class AbstractSearchQueryEntityLoadingIT {\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock();\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock );\n\n\tprotected abstract SessionFactory sessionFactory();\n\n\tprotected final <T> void testLoading(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tList<? extends Class<? extends T>> targetClasses,\n\t\t\tList<String> targetIndexes,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tConsumer<DocumentReferenceCollector> hitDocumentReferencesContributor,\n\t\t\tConsumer<EntityCollector<T>> expectedLoadedEntitiesContributor,\n\t\t\tConsumer<OrmSoftAssertions> assertionsContributor) {\n\t\ttestLoading(\n\t\t\t\tsessionSetup, targetClasses, targetIndexes,\n\t\t\t\tloadingOptionsContributor, hitDocumentReferencesContributor, expectedLoadedEntitiesContributor,\n\t\t\t\t(assertions, ignored) -> assertionsContributor.accept( assertions )\n\t\t);\n\t}\n\n\tprotected final <T> void testLoading(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tList<? extends Class<? extends T>> targetClasses,\n\t\t\tList<String> targetIndexes,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tConsumer<DocumentReferenceCollector> hitDocumentReferencesContributor,\n\t\t\tConsumer<EntityCollector<T>> expectedLoadedEntitiesContributor,\n\t\t\tBiConsumer<OrmSoftAssertions, List<T>> assertionsContributor) {\n\t\tOrmSoftAssertions.withinSession( sessionFactory(), (session, softAssertions) -> {\n\t\t\tsessionSetup.accept( session );\n\n\t\t\tsoftAssertions.resetListenerData();\n\n\t\t\tSearchSession searchSession = Search.session( session );\n\n\t\t\tSearchQuery<T> query = searchSession.search( targetClasses )\n\t\t\t\t\t.where( f -> f.matchAll() )\n\t\t\t\t\t.loading( loadingOptionsContributor )\n\t\t\t\t\t.toQuery();\n\n\t\t\tDocumentReferenceCollector documentReferenceCollector = new DocumentReferenceCollector();\n\t\t\thitDocumentReferencesContributor.accept( documentReferenceCollector );\n\t\t\tList<DocumentReference> hitDocumentReferences = documentReferenceCollector.collected;\n\n\t\t\tList<T> loadedEntities = getHits( targetIndexes, query, hitDocumentReferences );\n\n\t\t\tsoftAssertions.assertThat( loadedEntities )\n\t\t\t\t\t.as(\n\t\t\t\t\t\t\t\"Loaded entities when targeting types \" + targetClasses\n\t\t\t\t\t\t\t\t\t+ \" and when the backend returns document references \" + hitDocumentReferences\n\t\t\t\t\t)\n\t\t\t\t\t.allSatisfy( loadedEntity -> {\n\t\t\t\t\t\t// Loading should fully initialize entities\n\t\t\t\t\t\tassertThat( Hibernate.isInitialized( loadedEntity ) ).isTrue();\n\t\t\t\t\t} );\n\n\t\t\tassertionsContributor.accept( softAssertions, loadedEntities );\n\n\t\t\t// Be sure to do this after having executed the query and checked loading,\n\t\t\t// because it may trigger additional loading.\n\t\t\tEntityCollector<T> entityCollector = new EntityCollector<>( session );\n\t\t\texpectedLoadedEntitiesContributor.accept( entityCollector );\n\t\t\tList<T> expectedLoadedEntities = entityCollector.collected;\n\n\t\t\t// Both the expected and actual list may contain proxies: unproxy everything so that equals() works correctly\n\t\t\tList<T> unproxyfiedExpectedLoadedEntities = unproxyAll( expectedLoadedEntities );\n\t\t\tList<T> unproxyfiedLoadedEntities = unproxyAll( loadedEntities );\n\n\t\t\tsoftAssertions.assertThat( unproxyfiedLoadedEntities )\n\t\t\t\t\t.as(\n\t\t\t\t\t\t\t\"Loaded, then unproxified entities when targeting types \" + targetClasses\n\t\t\t\t\t\t\t\t\t+ \" and when the backend returns document references \" + hitDocumentReferences\n\t\t\t\t\t)\n\t\t\t\t\t.allSatisfy(\n\t\t\t\t\t\t\telement -> assertThat( element )\n\t\t\t\t\t\t\t\t\t.isInstanceOfAny( targetClasses.toArray( new Class<?>[0] ) )\n\t\t\t\t\t)\n\t\t\t\t\t.containsExactlyElementsOf( unproxyfiedExpectedLoadedEntities );\n\t\t} );\n\t}\n\n\tprotected <T> List<T> getHits(List<String> targetIndexes, SearchQuery<T> query, List<DocumentReference> hitDocumentReferences) {\n\t\tbackendMock.expectSearchObjects(\n\t\t\t\ttargetIndexes,\n\t\t\t\tb -> { },\n\t\t\t\tStubSearchWorkBehavior.of(\n\t\t\t\t\t\thitDocumentReferences.size(),\n\t\t\t\t\t\thitDocumentReferences\n\t\t\t\t)\n\t\t);\n\n\t\treturn query.fetchAllHits();\n\t}\n\n\t// This cast is fine as long as T is not a proxy interface\n\t@SuppressWarnings(\"unchecked\")\n\tprivate <T> List<T> unproxyAll(List<T> entityList) {\n\t\treturn entityList.stream()\n\t\t\t\t.map( entity -> (T) Hibernate.unproxy( entity ) )\n\t\t\t\t.collect( Collectors.toList() );\n\t}\n\n\tprotected static class DocumentReferenceCollector {\n\t\tprivate final List<DocumentReference> collected = new ArrayList<>();\n\n\t\tpublic DocumentReferenceCollector doc(String indexName, String documentId) {\n\t\t\tcollected.add( reference( indexName, documentId ) );\n\t\t\treturn this;\n\t\t}\n\t}\n\n\tprotected static class EntityCollector<T> {\n\t\tprivate final Session session;\n\t\tprivate final List<T> collected = new ArrayList<>();\n\n\t\tprivate EntityCollector(Session session) {\n\t\t\tthis.session = session;\n\t\t}\n\n\t\tpublic EntityCollector<T> entity(Class<? extends T> entityType, Object entityId) {\n\t\t\tcollected.add( session.getReference( entityType, entityId ) );\n\t\t\treturn this;\n\t\t}\n\t}\n\n}\n",
        "diffSourceCodeSet": [
            "protected <T> List<T> getHits(List<String> targetIndexes, SearchQuery<T> query, List<DocumentReference> hitDocumentReferences) {\n\t\tbackendMock.expectSearchObjects(\n\t\t\t\ttargetIndexes,\n\t\t\t\tb -> { },\n\t\t\t\tStubSearchWorkBehavior.of(\n\t\t\t\t\t\thitDocumentReferences.size(),\n\t\t\t\t\t\thitDocumentReferences\n\t\t\t\t)\n\t\t);\n\n\t\treturn query.fetchAllHits();\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingIT#sessionFactory\n methodBody: protected abstract SessionFactory sessionFactory();",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.search.loading.AbstractSearchQueryEntityLoadingIT#unproxyAll\n methodBody: private <T> List<T> unproxyAll(List<T> entityList) {\nreturn entityList.stream().map(entity -> (T)Hibernate.unproxy(entity)).collect(Collectors.toList());\n}"
        ],
        "sourceCodeAfterRefactoring": "protected final <T> void testLoading(\n\t\t\tConsumer<Session> sessionSetup,\n\t\t\tList<? extends Class<? extends T>> targetClasses,\n\t\t\tList<String> targetIndexes,\n\t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n\t\t\tConsumer<DocumentReferenceCollector> hitDocumentReferencesContributor,\n\t\t\tConsumer<EntityCollector<T>> expectedLoadedEntitiesContributor,\n\t\t\tBiConsumer<OrmSoftAssertions, List<T>> assertionsContributor) {\n\t\tOrmSoftAssertions.withinSession( sessionFactory(), (session, softAssertions) -> {\n\t\t\tsessionSetup.accept( session );\n\n\t\t\tsoftAssertions.resetListenerData();\n\n\t\t\tSearchSession searchSession = Search.session( session );\n\n\t\t\tSearchQuery<T> query = searchSession.search( targetClasses )\n\t\t\t\t\t.where( f -> f.matchAll() )\n\t\t\t\t\t.loading( loadingOptionsContributor )\n\t\t\t\t\t.toQuery();\n\n\t\t\tDocumentReferenceCollector documentReferenceCollector = new DocumentReferenceCollector();\n\t\t\thitDocumentReferencesContributor.accept( documentReferenceCollector );\n\t\t\tList<DocumentReference> hitDocumentReferences = documentReferenceCollector.collected;\n\n\t\t\tList<T> loadedEntities = getHits( targetIndexes, query, hitDocumentReferences );\n\n\t\t\tsoftAssertions.assertThat( loadedEntities )\n\t\t\t\t\t.as(\n\t\t\t\t\t\t\t\"Loaded entities when targeting types \" + targetClasses\n\t\t\t\t\t\t\t\t\t+ \" and when the backend returns document references \" + hitDocumentReferences\n\t\t\t\t\t)\n\t\t\t\t\t.allSatisfy( loadedEntity -> {\n\t\t\t\t\t\t// Loading should fully initialize entities\n\t\t\t\t\t\tassertThat( Hibernate.isInitialized( loadedEntity ) ).isTrue();\n\t\t\t\t\t} );\n\n\t\t\tassertionsContributor.accept( softAssertions, loadedEntities );\n\n\t\t\t// Be sure to do this after having executed the query and checked loading,\n\t\t\t// because it may trigger additional loading.\n\t\t\tEntityCollector<T> entityCollector = new EntityCollector<>( session );\n\t\t\texpectedLoadedEntitiesContributor.accept( entityCollector );\n\t\t\tList<T> expectedLoadedEntities = entityCollector.collected;\n\n\t\t\t// Both the expected and actual list may contain proxies: unproxy everything so that equals() works correctly\n\t\t\tList<T> unproxyfiedExpectedLoadedEntities = unproxyAll( expectedLoadedEntities );\n\t\t\tList<T> unproxyfiedLoadedEntities = unproxyAll( loadedEntities );\n\n\t\t\tsoftAssertions.assertThat( unproxyfiedLoadedEntities )\n\t\t\t\t\t.as(\n\t\t\t\t\t\t\t\"Loaded, then unproxified entities when targeting types \" + targetClasses\n\t\t\t\t\t\t\t\t\t+ \" and when the backend returns document references \" + hitDocumentReferences\n\t\t\t\t\t)\n\t\t\t\t\t.allSatisfy(\n\t\t\t\t\t\t\telement -> assertThat( element )\n\t\t\t\t\t\t\t\t\t.isInstanceOfAny( targetClasses.toArray( new Class<?>[0] ) )\n\t\t\t\t\t)\n\t\t\t\t\t.containsExactlyElementsOf( unproxyfiedExpectedLoadedEntities );\n\t\t} );\n\t}\nprotected <T> List<T> getHits(List<String> targetIndexes, SearchQuery<T> query, List<DocumentReference> hitDocumentReferences) {\n\t\tbackendMock.expectSearchObjects(\n\t\t\t\ttargetIndexes,\n\t\t\t\tb -> { },\n\t\t\t\tStubSearchWorkBehavior.of(\n\t\t\t\t\t\thitDocumentReferences.size(),\n\t\t\t\t\t\thitDocumentReferences\n\t\t\t\t)\n\t\t);\n\n\t\treturn query.fetchAllHits();\n\t}",
        "diffSourceCode": "    58: \tprotected final <T> void testLoading(\n    59: \t\t\tConsumer<Session> sessionSetup,\n    60: \t\t\tList<? extends Class<? extends T>> targetClasses,\n    61: \t\t\tList<String> targetIndexes,\n    62: \t\t\tConsumer<SearchLoadingOptionsStep> loadingOptionsContributor,\n    63: \t\t\tConsumer<DocumentReferenceCollector> hitDocumentReferencesContributor,\n    64: \t\t\tConsumer<EntityCollector<T>> expectedLoadedEntitiesContributor,\n    65: \t\t\tBiConsumer<OrmSoftAssertions, List<T>> assertionsContributor) {\n    66: \t\tOrmSoftAssertions.withinSession( sessionFactory(), (session, softAssertions) -> {\n    67: \t\t\tsessionSetup.accept( session );\n    68: \n    69: \t\t\tsoftAssertions.resetListenerData();\n    70: \n    71: \t\t\tSearchSession searchSession = Search.session( session );\n    72: \n    73: \t\t\tSearchQuery<T> query = searchSession.search( targetClasses )\n    74: \t\t\t\t\t.where( f -> f.matchAll() )\n    75: \t\t\t\t\t.loading( loadingOptionsContributor )\n    76: \t\t\t\t\t.toQuery();\n    77: \n    78: \t\t\tDocumentReferenceCollector documentReferenceCollector = new DocumentReferenceCollector();\n    79: \t\t\thitDocumentReferencesContributor.accept( documentReferenceCollector );\n    80: \t\t\tList<DocumentReference> hitDocumentReferences = documentReferenceCollector.collected;\n    81: \n-   82: \t\t\tbackendMock.expectSearchObjects(\n-   83: \t\t\t\t\ttargetIndexes,\n-   84: \t\t\t\t\tb -> { },\n-   85: \t\t\t\t\tStubSearchWorkBehavior.of(\n-   86: \t\t\t\t\t\t\thitDocumentReferences.size(),\n-   87: \t\t\t\t\t\t\thitDocumentReferences\n+   82: \t\t\tList<T> loadedEntities = getHits( targetIndexes, query, hitDocumentReferences );\n+   83: \n+   84: \t\t\tsoftAssertions.assertThat( loadedEntities )\n+   85: \t\t\t\t\t.as(\n+   86: \t\t\t\t\t\t\t\"Loaded entities when targeting types \" + targetClasses\n+   87: \t\t\t\t\t\t\t\t\t+ \" and when the backend returns document references \" + hitDocumentReferences\n    88: \t\t\t\t\t)\n-   89: \t\t\t);\n-   90: \n-   91: \t\t\tList<T> loadedEntities = query.fetchAllHits();\n-   92: \n-   93: \t\t\tsoftAssertions.assertThat( loadedEntities )\n-   94: \t\t\t\t\t.as(\n-   95: \t\t\t\t\t\t\t\"Loaded entities when targeting types \" + targetClasses\n-   96: \t\t\t\t\t\t\t\t\t+ \" and when the backend returns document references \" + hitDocumentReferences\n-   97: \t\t\t\t\t)\n-   98: \t\t\t\t\t.allSatisfy( loadedEntity -> {\n-   99: \t\t\t\t\t\t// Loading should fully initialize entities\n-  100: \t\t\t\t\t\tassertThat( Hibernate.isInitialized( loadedEntity ) ).isTrue();\n-  101: \t\t\t\t\t} );\n-  102: \n-  103: \t\t\tassertionsContributor.accept( softAssertions, loadedEntities );\n-  104: \n-  105: \t\t\t// Be sure to do this after having executed the query and checked loading,\n-  106: \t\t\t// because it may trigger additional loading.\n-  107: \t\t\tEntityCollector<T> entityCollector = new EntityCollector<>( session );\n-  108: \t\t\texpectedLoadedEntitiesContributor.accept( entityCollector );\n-  109: \t\t\tList<T> expectedLoadedEntities = entityCollector.collected;\n-  110: \n-  111: \t\t\t// Both the expected and actual list may contain proxies: unproxy everything so that equals() works correctly\n-  112: \t\t\tList<T> unproxyfiedExpectedLoadedEntities = unproxyAll( expectedLoadedEntities );\n-  113: \t\t\tList<T> unproxyfiedLoadedEntities = unproxyAll( loadedEntities );\n-  114: \n-  115: \t\t\tsoftAssertions.assertThat( unproxyfiedLoadedEntities )\n-  116: \t\t\t\t\t.as(\n-  117: \t\t\t\t\t\t\t\"Loaded, then unproxified entities when targeting types \" + targetClasses\n-  118: \t\t\t\t\t\t\t\t\t+ \" and when the backend returns document references \" + hitDocumentReferences\n-  119: \t\t\t\t\t)\n-  120: \t\t\t\t\t.allSatisfy(\n-  121: \t\t\t\t\t\t\telement -> assertThat( element )\n-  122: \t\t\t\t\t\t\t\t\t.isInstanceOfAny( targetClasses.toArray( new Class<?>[0] ) )\n-  123: \t\t\t\t\t)\n-  124: \t\t\t\t\t.containsExactlyElementsOf( unproxyfiedExpectedLoadedEntities );\n-  125: \t\t} );\n-  126: \t}\n-  127: \n-  128: \t// This cast is fine as long as T is not a proxy interface\n-  129: \t@SuppressWarnings(\"unchecked\")\n-  130: \tprivate <T> List<T> unproxyAll(List<T> entityList) {\n+   89: \t\t\t\t\t.allSatisfy( loadedEntity -> {\n+   90: \t\t\t\t\t\t// Loading should fully initialize entities\n+   91: \t\t\t\t\t\tassertThat( Hibernate.isInitialized( loadedEntity ) ).isTrue();\n+   92: \t\t\t\t\t} );\n+   93: \n+   94: \t\t\tassertionsContributor.accept( softAssertions, loadedEntities );\n+   95: \n+   96: \t\t\t// Be sure to do this after having executed the query and checked loading,\n+   97: \t\t\t// because it may trigger additional loading.\n+   98: \t\t\tEntityCollector<T> entityCollector = new EntityCollector<>( session );\n+   99: \t\t\texpectedLoadedEntitiesContributor.accept( entityCollector );\n+  100: \t\t\tList<T> expectedLoadedEntities = entityCollector.collected;\n+  101: \n+  102: \t\t\t// Both the expected and actual list may contain proxies: unproxy everything so that equals() works correctly\n+  103: \t\t\tList<T> unproxyfiedExpectedLoadedEntities = unproxyAll( expectedLoadedEntities );\n+  104: \t\t\tList<T> unproxyfiedLoadedEntities = unproxyAll( loadedEntities );\n+  105: \n+  106: \t\t\tsoftAssertions.assertThat( unproxyfiedLoadedEntities )\n+  107: \t\t\t\t\t.as(\n+  108: \t\t\t\t\t\t\t\"Loaded, then unproxified entities when targeting types \" + targetClasses\n+  109: \t\t\t\t\t\t\t\t\t+ \" and when the backend returns document references \" + hitDocumentReferences\n+  110: \t\t\t\t\t)\n+  111: \t\t\t\t\t.allSatisfy(\n+  112: \t\t\t\t\t\t\telement -> assertThat( element )\n+  113: \t\t\t\t\t\t\t\t\t.isInstanceOfAny( targetClasses.toArray( new Class<?>[0] ) )\n+  114: \t\t\t\t\t)\n+  115: \t\t\t\t\t.containsExactlyElementsOf( unproxyfiedExpectedLoadedEntities );\n+  116: \t\t} );\n+  117: \t}\n+  118: \n+  119: \tprotected <T> List<T> getHits(List<String> targetIndexes, SearchQuery<T> query, List<DocumentReference> hitDocumentReferences) {\n+  120: \t\tbackendMock.expectSearchObjects(\n+  121: \t\t\t\ttargetIndexes,\n+  122: \t\t\t\tb -> { },\n+  123: \t\t\t\tStubSearchWorkBehavior.of(\n+  124: \t\t\t\t\t\thitDocumentReferences.size(),\n+  125: \t\t\t\t\t\thitDocumentReferences\n+  126: \t\t\t\t)\n+  127: \t\t);\n+  128: \n+  129: \t\treturn query.fetchAllHits();\n+  130: \t}\n",
        "uniqueId": "a491e38e1782d640c73a5cc042eae0ffdc50ae87_58_126_119_130_58_117",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 11
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate getPropertyMemberUsingReflectionFromThisType(methodAccessXProperty XProperty, fieldAccessXProperty XProperty) : Member extracted from private findPropertyMember(propertyName String, methodAccessXProperty XProperty, fieldAccessXProperty XProperty, propertyMetadataFromHibernateOrmMetamodel HibernateOrmBasicClassPropertyMetadata) : Member in class org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel",
        "diffLocations": [
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
                "startLine": 151,
                "endLine": 205,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
                "startLine": 151,
                "endLine": 177,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
                "startLine": 215,
                "endLine": 230,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private Member findPropertyMember(String propertyName,\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n\t\t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n\t\t\t/*\n\t\t\t * Hibernate ORM has metadata for this property,\n\t\t\t * which means this property is persisted.\n\t\t\t *\n\t\t\t * Hibernate ORM might return us the member as declared in a supertype,\n\t\t\t * in which case the type of that member will not be up-to-date.\n\t\t\t * Thus we try to get the overridden member declared in the current type,\n\t\t\t * and failing that we look for the member in supertypes.\n\t\t\t *\n\t\t\t * We still try to comply with JPA's configured access type,\n\t\t\t * which explains the two if/else branches below.\n\t\t\t */\n\t\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n\t\t\t\treturn methodAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t\t}\n\t\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n\t\t\t\treturn fieldAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t\t}\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * We don't have a declared XProperty for this member in the current type.\n\t\t\t\t * Try to find the member used to access the same property in the closest supertype.\n\t\t\t\t */\n\t\t\t\treturn getPropertyMemberFromParentTypes( propertyName );\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t/*\n\t\t\t * Hibernate ORM doesn't have any metadata for this property,\n\t\t\t * which means this property is transient.\n\t\t\t * We don't need to worry about JPA's access type.\n\t\t\t */\n\t\t\tif ( methodAccessXProperty != null ) {\n\t\t\t\t// We managed to find a declared, method-access XProperty on the current type. Use it.\n\t\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t\t}\n\t\t\telse if ( fieldAccessXProperty != null ) {\n\t\t\t\t// We managed to find a declared, field-access XProperty on the current type. Use it.\n\t\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t\t}\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * We did not manage to find a declared XProperty on the current type.\n\t\t\t\t * Either the property is declared in a supertype, or it does not exist.\n\t\t\t\t * Try to find the member used to access the same property in the closest supertype.\n\t\t\t\t */\n\t\t\t\treturn getPropertyMemberFromParentTypes( propertyName );\n\t\t\t}\n\t\t}\n\t}",
        "filePathBefore": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
        "isPureRefactoring": true,
        "commitId": "a4d2e5b39bb53fefd2c31be404a6682af6c3a5bb",
        "packageNameBefore": "org.hibernate.search.mapper.orm.model.impl",
        "classNameBefore": "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel",
        "methodNameBefore": "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#findPropertyMember",
        "invokedMethod": "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getPropertyMemberFromParentTypes\n methodBody: private Member getPropertyMemberFromParentTypes(String propertyName) {\nreturn getAscendingSuperTypes().skip(1).map(type -> type.getPropertyOrNull(propertyName)).filter(Objects::nonNull).findFirst().map(HibernateOrmClassPropertyModel::getMember).orElse(null);\n}",
        "classSignatureBefore": "public class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> ",
        "methodNameBeforeSet": [
            "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#findPropertyMember"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel"
        ],
        "classSignatureBeforeSet": [
            "public class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics Severe changes",
                "description": "Return expression has been added within the Extract Method mechanics - with non-mapped leaves",
                "mappingState": 2
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.orm.model.impl;\n\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport org.hibernate.annotations.common.reflection.XProperty;\nimport org.hibernate.search.engine.mapper.model.spi.MappableTypeModel;\nimport org.hibernate.search.mapper.pojo.model.hcann.spi.PojoCommonsAnnotationsHelper;\nimport org.hibernate.search.mapper.pojo.model.spi.GenericContextAwarePojoGenericTypeModel.RawTypeDeclaringContext;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeIdentifier;\n\npublic class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> {\n\n\tprivate final HibernateOrmBasicClassTypeMetadata ormTypeMetadata;\n\tprivate final RawTypeDeclaringContext<T> rawTypeDeclaringContext;\n\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> ascendingSuperTypesCache;\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> descendingSuperTypesCache;\n\n\tprivate final Map<String, HibernateOrmClassPropertyModel<?>> propertyModelCache = new HashMap<>();\n\n\tprivate Map<String, XProperty> declaredFieldAccessXPropertiesByName;\n\tprivate Map<String, XProperty> declaredMethodAccessXPropertiesByName;\n\n\tHibernateOrmClassRawTypeModel(HibernateOrmBootstrapIntrospector introspector,\n\t\t\tPojoRawTypeIdentifier<T> typeIdentifier,\n\t\t\tHibernateOrmBasicClassTypeMetadata ormTypeMetadata, RawTypeDeclaringContext<T> rawTypeDeclaringContext) {\n\t\tsuper( introspector, typeIdentifier );\n\t\tthis.ormTypeMetadata = ormTypeMetadata;\n\t\tthis.rawTypeDeclaringContext = rawTypeDeclaringContext;\n\t}\n\n\t@Override\n\tpublic boolean isAbstract() {\n\t\treturn xClass.isAbstract();\n\t}\n\n\t@Override\n\tpublic boolean isSubTypeOf(MappableTypeModel superTypeCandidate) {\n\t\treturn superTypeCandidate instanceof HibernateOrmClassRawTypeModel\n\t\t\t\t&& ( (HibernateOrmClassRawTypeModel<?>) superTypeCandidate ).xClass.isAssignableFrom( xClass );\n\t}\n\n\t@Override\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> getAscendingSuperTypes() {\n\t\tif ( ascendingSuperTypesCache == null ) {\n\t\t\tascendingSuperTypesCache = introspector.getAscendingSuperTypes( xClass )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn ascendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> getDescendingSuperTypes() {\n\t\tif ( descendingSuperTypesCache == null ) {\n\t\t\tdescendingSuperTypesCache = introspector.getDescendingSuperTypes( xClass )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn descendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\tpublic Stream<Annotation> getAnnotations() {\n\t\treturn introspector.getAnnotations( xClass );\n\t}\n\n\t@Override\n\tStream<String> getDeclaredPropertyNames() {\n\t\treturn Stream.concat(\n\t\t\t\tgetDeclaredFieldAccessXPropertiesByName().keySet().stream(),\n\t\t\t\tgetDeclaredMethodAccessXPropertiesByName().keySet().stream()\n\t\t)\n\t\t\t\t.distinct();\n\t}\n\n\t@Override\n\tHibernateOrmClassPropertyModel<?> getPropertyOrNull(String propertyName) {\n\t\treturn propertyModelCache.computeIfAbsent( propertyName, this::createPropertyModel );\n\t}\n\n\tRawTypeDeclaringContext<T> getRawTypeDeclaringContext() {\n\t\treturn rawTypeDeclaringContext;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredFieldAccessXPropertiesByName() {\n\t\tif ( declaredFieldAccessXPropertiesByName == null ) {\n\t\t\tdeclaredFieldAccessXPropertiesByName =\n\t\t\t\t\tintrospector.getDeclaredFieldAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredFieldAccessXPropertiesByName;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredMethodAccessXPropertiesByName() {\n\t\tif ( declaredMethodAccessXPropertiesByName == null ) {\n\t\t\tdeclaredMethodAccessXPropertiesByName =\n\t\t\t\t\tintrospector.getDeclaredMethodAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredMethodAccessXPropertiesByName;\n\t}\n\n\tprivate HibernateOrmClassPropertyModel<?> createPropertyModel(String propertyName) {\n\t\tList<XProperty> declaredXProperties = new ArrayList<>( 2 );\n\t\t// Add the method first on purpose: the first XProperty may be used as a default to create the value accessor handle\n\t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( methodAccessXProperty );\n\t\t}\n\t\tXProperty fieldAccessXProperty = getDeclaredFieldAccessXPropertiesByName().get( propertyName );\n\t\tif ( fieldAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( fieldAccessXProperty );\n\t\t}\n\n\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata;\n\t\tif ( ormTypeMetadata == null ) {\n\t\t\t// There isn't any Hibernate ORM metadata for this type\n\t\t\tormPropertyMetadata = null;\n\t\t}\n\t\telse {\n\t\t\tormPropertyMetadata = ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n\t\t}\n\n\t\tMember member = findPropertyMember(\n\t\t\t\tpropertyName, methodAccessXProperty, fieldAccessXProperty, ormPropertyMetadata\n\t\t);\n\n\t\tif ( member == null ) {\n\t\t\treturn null;\n\t\t}\n\n\t\treturn new HibernateOrmClassPropertyModel<>(\n\t\t\t\tintrospector, this, propertyName,\n\t\t\t\tdeclaredXProperties, ormPropertyMetadata, member\n\t\t);\n\t}\n\n\tprivate Member findPropertyMember(String propertyName,\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n\t\t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n\t\t\t/*\n\t\t\t * Hibernate ORM has metadata for this property,\n\t\t\t * which means this property is persisted.\n\t\t\t *\n\t\t\t * Hibernate ORM might return us the member as declared in a supertype,\n\t\t\t * in which case the type of that member will not be up-to-date.\n\t\t\t * Thus we try to get the overridden member declared in the current type,\n\t\t\t * and failing that we look for the member in supertypes.\n\t\t\t *\n\t\t\t * We still try to comply with JPA's configured access type,\n\t\t\t * which explains the two if/else branches below.\n\t\t\t */\n\t\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n\t\t\t\treturn methodAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t\t}\n\t\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n\t\t\t\treturn fieldAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t\t}\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * We don't have a declared XProperty for this member in the current type.\n\t\t\t\t * Try to find the member used to access the same property in the closest supertype.\n\t\t\t\t */\n\t\t\t\treturn getPropertyMemberFromParentTypes( propertyName );\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t/*\n\t\t\t * Hibernate ORM doesn't have any metadata for this property,\n\t\t\t * which means this property is transient.\n\t\t\t * We don't need to worry about JPA's access type.\n\t\t\t */\n\t\t\tif ( methodAccessXProperty != null ) {\n\t\t\t\t// We managed to find a declared, method-access XProperty on the current type. Use it.\n\t\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t\t}\n\t\t\telse if ( fieldAccessXProperty != null ) {\n\t\t\t\t// We managed to find a declared, field-access XProperty on the current type. Use it.\n\t\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t\t}\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * We did not manage to find a declared XProperty on the current type.\n\t\t\t\t * Either the property is declared in a supertype, or it does not exist.\n\t\t\t\t * Try to find the member used to access the same property in the closest supertype.\n\t\t\t\t */\n\t\t\t\treturn getPropertyMemberFromParentTypes( propertyName );\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate Member getPropertyMemberFromParentTypes(String propertyName) {\n\t\t// TODO HSEARCH-3056 remove lambdas if possible\n\t\treturn getAscendingSuperTypes()\n\t\t\t\t.skip( 1 ) // Ignore self\n\t\t\t\t.map( type -> type.getPropertyOrNull( propertyName ) )\n\t\t\t\t.filter( Objects::nonNull )\n\t\t\t\t.findFirst()\n\t\t\t\t.map( HibernateOrmClassPropertyModel::getMember )\n\t\t\t\t.orElse( null );\n\t}\n}\n",
        "filePathAfter": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.orm.model.impl;\n\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport org.hibernate.annotations.common.reflection.XProperty;\nimport org.hibernate.search.engine.mapper.model.spi.MappableTypeModel;\nimport org.hibernate.search.mapper.pojo.model.hcann.spi.PojoCommonsAnnotationsHelper;\nimport org.hibernate.search.mapper.pojo.model.spi.GenericContextAwarePojoGenericTypeModel.RawTypeDeclaringContext;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeIdentifier;\n\npublic class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> {\n\n\tprivate final HibernateOrmBasicClassTypeMetadata ormTypeMetadata;\n\tprivate final RawTypeDeclaringContext<T> rawTypeDeclaringContext;\n\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> ascendingSuperTypesCache;\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> descendingSuperTypesCache;\n\n\tprivate final Map<String, HibernateOrmClassPropertyModel<?>> propertyModelCache = new HashMap<>();\n\n\tprivate Map<String, XProperty> declaredFieldAccessXPropertiesByName;\n\tprivate Map<String, XProperty> declaredMethodAccessXPropertiesByName;\n\n\tHibernateOrmClassRawTypeModel(HibernateOrmBootstrapIntrospector introspector,\n\t\t\tPojoRawTypeIdentifier<T> typeIdentifier,\n\t\t\tHibernateOrmBasicClassTypeMetadata ormTypeMetadata, RawTypeDeclaringContext<T> rawTypeDeclaringContext) {\n\t\tsuper( introspector, typeIdentifier );\n\t\tthis.ormTypeMetadata = ormTypeMetadata;\n\t\tthis.rawTypeDeclaringContext = rawTypeDeclaringContext;\n\t}\n\n\t@Override\n\tpublic boolean isAbstract() {\n\t\treturn xClass.isAbstract();\n\t}\n\n\t@Override\n\tpublic boolean isSubTypeOf(MappableTypeModel superTypeCandidate) {\n\t\treturn superTypeCandidate instanceof HibernateOrmClassRawTypeModel\n\t\t\t\t&& ( (HibernateOrmClassRawTypeModel<?>) superTypeCandidate ).xClass.isAssignableFrom( xClass );\n\t}\n\n\t@Override\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> getAscendingSuperTypes() {\n\t\tif ( ascendingSuperTypesCache == null ) {\n\t\t\tascendingSuperTypesCache = introspector.getAscendingSuperTypes( xClass )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn ascendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> getDescendingSuperTypes() {\n\t\tif ( descendingSuperTypesCache == null ) {\n\t\t\tdescendingSuperTypesCache = introspector.getDescendingSuperTypes( xClass )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn descendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\tpublic Stream<Annotation> getAnnotations() {\n\t\treturn introspector.getAnnotations( xClass );\n\t}\n\n\t@Override\n\tStream<String> getDeclaredPropertyNames() {\n\t\treturn Stream.concat(\n\t\t\t\tgetDeclaredFieldAccessXPropertiesByName().keySet().stream(),\n\t\t\t\tgetDeclaredMethodAccessXPropertiesByName().keySet().stream()\n\t\t)\n\t\t\t\t.distinct();\n\t}\n\n\t@Override\n\tHibernateOrmClassPropertyModel<?> getPropertyOrNull(String propertyName) {\n\t\treturn propertyModelCache.computeIfAbsent( propertyName, this::createPropertyModel );\n\t}\n\n\tRawTypeDeclaringContext<T> getRawTypeDeclaringContext() {\n\t\treturn rawTypeDeclaringContext;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredFieldAccessXPropertiesByName() {\n\t\tif ( declaredFieldAccessXPropertiesByName == null ) {\n\t\t\tdeclaredFieldAccessXPropertiesByName =\n\t\t\t\t\tintrospector.getDeclaredFieldAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredFieldAccessXPropertiesByName;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredMethodAccessXPropertiesByName() {\n\t\tif ( declaredMethodAccessXPropertiesByName == null ) {\n\t\t\tdeclaredMethodAccessXPropertiesByName =\n\t\t\t\t\tintrospector.getDeclaredMethodAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredMethodAccessXPropertiesByName;\n\t}\n\n\tprivate HibernateOrmClassPropertyModel<?> createPropertyModel(String propertyName) {\n\t\tList<XProperty> declaredXProperties = new ArrayList<>( 2 );\n\t\t// Add the method first on purpose: the first XProperty may be used as a default to create the value accessor handle\n\t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( methodAccessXProperty );\n\t\t}\n\t\tXProperty fieldAccessXProperty = getDeclaredFieldAccessXPropertiesByName().get( propertyName );\n\t\tif ( fieldAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( fieldAccessXProperty );\n\t\t}\n\n\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata;\n\t\tif ( ormTypeMetadata == null ) {\n\t\t\t// There isn't any Hibernate ORM metadata for this type\n\t\t\tormPropertyMetadata = null;\n\t\t}\n\t\telse {\n\t\t\tormPropertyMetadata = ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n\t\t}\n\n\t\tMember member = findPropertyMember(\n\t\t\t\tpropertyName, methodAccessXProperty, fieldAccessXProperty, ormPropertyMetadata\n\t\t);\n\n\t\tif ( member == null ) {\n\t\t\treturn null;\n\t\t}\n\n\t\treturn new HibernateOrmClassPropertyModel<>(\n\t\t\t\tintrospector, this, propertyName,\n\t\t\t\tdeclaredXProperties, ormPropertyMetadata, member\n\t\t);\n\t}\n\n\tprivate Member findPropertyMember(String propertyName,\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tMember result;\n\t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is persisted).\n\t\t\t// Use ORM metadata to find the corresponding member (field/method).\n\t\t\tresult = getPropertyMemberUsingHibernateOrmMetadataFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty, propertyMetadataFromHibernateOrmMetamodel\n\t\t\t);\n\t\t}\n\t\telse {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n\t\t\t// Use reflection to find the corresponding member (field/method).\n\t\t\tresult = getPropertyMemberUsingReflectionFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty\n\t\t\t);\n\t\t}\n\n\t\tif ( result == null ) {\n\t\t\t// There is no member for this property on the current type.\n\t\t\t// Try to find one in the closest supertype.\n\t\t\tresult = getPropertyMemberFromParentTypes( propertyName );\n\t\t}\n\n\t\treturn result;\n\t}\n\n\tprivate Member getPropertyMemberUsingHibernateOrmMetadataFromThisType(XProperty methodAccessXProperty,\n\t\t\tXProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n\t\t/*\n\t\t * Hibernate ORM has metadata for this property,\n\t\t * which means this property is persisted.\n\t\t *\n\t\t * Hibernate ORM might return us the member as declared in a supertype,\n\t\t * in which case the type of that member will not be up-to-date.\n\t\t * Thus we try to get the overridden member declared in the current type,\n\t\t * and failing that we look for the member in supertypes.\n\t\t *\n\t\t * We still try to comply with JPA's configured access type,\n\t\t * which explains the two if/else branches below.\n\t\t */\n\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n\t\t\treturn methodAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n\t\t\treturn fieldAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\treturn null;\n\t\t}\n\t}\n\n\t/*\n\t * Hibernate ORM doesn't have any metadata for this property,\n\t * which means this property is transient.\n\t * We don't need to worry about JPA's access type.\n\t */\n\tprivate Member getPropertyMemberUsingReflectionFromThisType(\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty) {\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\t// Method access is available. Get values from the getter.\n\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( fieldAccessXProperty != null ) {\n\t\t\t// Method access is not available, but field access is. Get values directly from the field.\n\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\t// Neither method access nor field access is available.\n\t\t\t// The property is not declared in this type.\n\t\t\treturn null;\n\t\t}\n\t}\n\n\tprivate Member getPropertyMemberFromParentTypes(String propertyName) {\n\t\t// TODO HSEARCH-3056 remove lambdas if possible\n\t\treturn getAscendingSuperTypes()\n\t\t\t\t.skip( 1 ) // Ignore self\n\t\t\t\t.map( type -> type.getPropertyOrNull( propertyName ) )\n\t\t\t\t.filter( Objects::nonNull )\n\t\t\t\t.findFirst()\n\t\t\t\t.map( HibernateOrmClassPropertyModel::getMember )\n\t\t\t\t.orElse( null );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private Member getPropertyMemberUsingReflectionFromThisType(\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty) {\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\t// Method access is available. Get values from the getter.\n\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( fieldAccessXProperty != null ) {\n\t\t\t// Method access is not available, but field access is. Get values directly from the field.\n\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\t// Neither method access nor field access is available.\n\t\t\t// The property is not declared in this type.\n\t\t\treturn null;\n\t\t}\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getPropertyMemberFromParentTypes\n methodBody: private Member getPropertyMemberFromParentTypes(String propertyName) {\nreturn getAscendingSuperTypes().skip(1).map(type -> type.getPropertyOrNull(propertyName)).filter(Objects::nonNull).findFirst().map(HibernateOrmClassPropertyModel::getMember).orElse(null);\n}"
        ],
        "sourceCodeAfterRefactoring": "private Member findPropertyMember(String propertyName,\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tMember result;\n\t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is persisted).\n\t\t\t// Use ORM metadata to find the corresponding member (field/method).\n\t\t\tresult = getPropertyMemberUsingHibernateOrmMetadataFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty, propertyMetadataFromHibernateOrmMetamodel\n\t\t\t);\n\t\t}\n\t\telse {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n\t\t\t// Use reflection to find the corresponding member (field/method).\n\t\t\tresult = getPropertyMemberUsingReflectionFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty\n\t\t\t);\n\t\t}\n\n\t\tif ( result == null ) {\n\t\t\t// There is no member for this property on the current type.\n\t\t\t// Try to find one in the closest supertype.\n\t\t\tresult = getPropertyMemberFromParentTypes( propertyName );\n\t\t}\n\n\t\treturn result;\n\t}\nprivate Member getPropertyMemberUsingReflectionFromThisType(\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty) {\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\t// Method access is available. Get values from the getter.\n\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( fieldAccessXProperty != null ) {\n\t\t\t// Method access is not available, but field access is. Get values directly from the field.\n\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\t// Neither method access nor field access is available.\n\t\t\t// The property is not declared in this type.\n\t\t\treturn null;\n\t\t}\n\t}",
        "diffSourceCode": "   151: \tprivate Member findPropertyMember(String propertyName,\n   152: \t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty,\n   153: \t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n-  154: \t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n-  155: \t\t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n-  156: \t\t\t/*\n-  157: \t\t\t * Hibernate ORM has metadata for this property,\n-  158: \t\t\t * which means this property is persisted.\n-  159: \t\t\t *\n-  160: \t\t\t * Hibernate ORM might return us the member as declared in a supertype,\n-  161: \t\t\t * in which case the type of that member will not be up-to-date.\n-  162: \t\t\t * Thus we try to get the overridden member declared in the current type,\n-  163: \t\t\t * and failing that we look for the member in supertypes.\n-  164: \t\t\t *\n-  165: \t\t\t * We still try to comply with JPA's configured access type,\n-  166: \t\t\t * which explains the two if/else branches below.\n-  167: \t\t\t */\n-  168: \t\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n-  169: \t\t\t\treturn methodAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n-  170: \t\t\t}\n-  171: \t\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n-  172: \t\t\t\treturn fieldAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n-  173: \t\t\t}\n-  174: \t\t\telse {\n-  175: \t\t\t\t/*\n-  176: \t\t\t\t * We don't have a declared XProperty for this member in the current type.\n-  177: \t\t\t\t * Try to find the member used to access the same property in the closest supertype.\n-  178: \t\t\t\t */\n-  179: \t\t\t\treturn getPropertyMemberFromParentTypes( propertyName );\n-  180: \t\t\t}\n-  181: \t\t}\n-  182: \t\telse {\n-  183: \t\t\t/*\n-  184: \t\t\t * Hibernate ORM doesn't have any metadata for this property,\n-  185: \t\t\t * which means this property is transient.\n-  186: \t\t\t * We don't need to worry about JPA's access type.\n-  187: \t\t\t */\n-  188: \t\t\tif ( methodAccessXProperty != null ) {\n-  189: \t\t\t\t// We managed to find a declared, method-access XProperty on the current type. Use it.\n-  190: \t\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n-  191: \t\t\t}\n-  192: \t\t\telse if ( fieldAccessXProperty != null ) {\n-  193: \t\t\t\t// We managed to find a declared, field-access XProperty on the current type. Use it.\n-  194: \t\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n-  195: \t\t\t}\n-  196: \t\t\telse {\n-  197: \t\t\t\t/*\n-  198: \t\t\t\t * We did not manage to find a declared XProperty on the current type.\n-  199: \t\t\t\t * Either the property is declared in a supertype, or it does not exist.\n-  200: \t\t\t\t * Try to find the member used to access the same property in the closest supertype.\n-  201: \t\t\t\t */\n-  202: \t\t\t\treturn getPropertyMemberFromParentTypes( propertyName );\n-  203: \t\t\t}\n+  154: \t\tMember result;\n+  155: \t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n+  156: \t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is persisted).\n+  157: \t\t\t// Use ORM metadata to find the corresponding member (field/method).\n+  158: \t\t\tresult = getPropertyMemberUsingHibernateOrmMetadataFromThisType(\n+  159: \t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty, propertyMetadataFromHibernateOrmMetamodel\n+  160: \t\t\t);\n+  161: \t\t}\n+  162: \t\telse {\n+  163: \t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n+  164: \t\t\t// Use reflection to find the corresponding member (field/method).\n+  165: \t\t\tresult = getPropertyMemberUsingReflectionFromThisType(\n+  166: \t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty\n+  167: \t\t\t);\n+  168: \t\t}\n+  169: \n+  170: \t\tif ( result == null ) {\n+  171: \t\t\t// There is no member for this property on the current type.\n+  172: \t\t\t// Try to find one in the closest supertype.\n+  173: \t\t\tresult = getPropertyMemberFromParentTypes( propertyName );\n+  174: \t\t}\n+  175: \n+  176: \t\treturn result;\n+  177: \t}\n+  178: \n+  179: \tprivate Member getPropertyMemberUsingHibernateOrmMetadataFromThisType(XProperty methodAccessXProperty,\n+  180: \t\t\tXProperty fieldAccessXProperty,\n+  181: \t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n+  182: \t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n+  183: \t\t/*\n+  184: \t\t * Hibernate ORM has metadata for this property,\n+  185: \t\t * which means this property is persisted.\n+  186: \t\t *\n+  187: \t\t * Hibernate ORM might return us the member as declared in a supertype,\n+  188: \t\t * in which case the type of that member will not be up-to-date.\n+  189: \t\t * Thus we try to get the overridden member declared in the current type,\n+  190: \t\t * and failing that we look for the member in supertypes.\n+  191: \t\t *\n+  192: \t\t * We still try to comply with JPA's configured access type,\n+  193: \t\t * which explains the two if/else branches below.\n+  194: \t\t */\n+  195: \t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n+  196: \t\t\treturn methodAccessXProperty == null\n+  197: \t\t\t\t\t? memberFromHibernateOrmMetamodel\n+  198: \t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n+  199: \t\t}\n+  200: \t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n+  201: \t\t\treturn fieldAccessXProperty == null\n+  202: \t\t\t\t\t? memberFromHibernateOrmMetamodel\n+  203: \t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n   204: \t\t}\n-  205: \t}\n-  215: \t\t\t\t.orElse( null );\n-  216: \t}\n-  217: }\n+  205: \t\telse {\n+  215: \tprivate Member getPropertyMemberUsingReflectionFromThisType(\n+  216: \t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty) {\n+  217: \t\tif ( methodAccessXProperty != null ) {\n+  218: \t\t\t// Method access is available. Get values from the getter.\n+  219: \t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n+  220: \t\t}\n+  221: \t\telse if ( fieldAccessXProperty != null ) {\n+  222: \t\t\t// Method access is not available, but field access is. Get values directly from the field.\n+  223: \t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n+  224: \t\t}\n+  225: \t\telse {\n+  226: \t\t\t// Neither method access nor field access is available.\n+  227: \t\t\t// The property is not declared in this type.\n+  228: \t\t\treturn null;\n+  229: \t\t}\n+  230: \t}\n",
        "uniqueId": "a4d2e5b39bb53fefd2c31be404a6682af6c3a5bb_151_205_215_230_151_177",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 27,
                "covered": 28
            },
            "BRANCH": {
                "missed": 7,
                "covered": 5
            },
            "LINE": {
                "missed": 6,
                "covered": 7
            },
            "COMPLEXITY": {
                "missed": 5,
                "covered": 2
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract Method",
        "description": "Extract Method\tprivate getPropertyMemberUsingHibernateOrmMetadataFromThisType(methodAccessXProperty XProperty, fieldAccessXProperty XProperty, propertyMetadataFromHibernateOrmMetamodel HibernateOrmBasicClassPropertyMetadata) : Member extracted from private findPropertyMember(propertyName String, methodAccessXProperty XProperty, fieldAccessXProperty XProperty, propertyMetadataFromHibernateOrmMetamodel HibernateOrmBasicClassPropertyMetadata) : Member in class org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel",
        "diffLocations": [
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
                "startLine": 151,
                "endLine": 205,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
                "startLine": 151,
                "endLine": 177,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
                "startLine": 179,
                "endLine": 208,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private Member findPropertyMember(String propertyName,\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n\t\t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n\t\t\t/*\n\t\t\t * Hibernate ORM has metadata for this property,\n\t\t\t * which means this property is persisted.\n\t\t\t *\n\t\t\t * Hibernate ORM might return us the member as declared in a supertype,\n\t\t\t * in which case the type of that member will not be up-to-date.\n\t\t\t * Thus we try to get the overridden member declared in the current type,\n\t\t\t * and failing that we look for the member in supertypes.\n\t\t\t *\n\t\t\t * We still try to comply with JPA's configured access type,\n\t\t\t * which explains the two if/else branches below.\n\t\t\t */\n\t\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n\t\t\t\treturn methodAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t\t}\n\t\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n\t\t\t\treturn fieldAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t\t}\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * We don't have a declared XProperty for this member in the current type.\n\t\t\t\t * Try to find the member used to access the same property in the closest supertype.\n\t\t\t\t */\n\t\t\t\treturn getPropertyMemberFromParentTypes( propertyName );\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t/*\n\t\t\t * Hibernate ORM doesn't have any metadata for this property,\n\t\t\t * which means this property is transient.\n\t\t\t * We don't need to worry about JPA's access type.\n\t\t\t */\n\t\t\tif ( methodAccessXProperty != null ) {\n\t\t\t\t// We managed to find a declared, method-access XProperty on the current type. Use it.\n\t\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t\t}\n\t\t\telse if ( fieldAccessXProperty != null ) {\n\t\t\t\t// We managed to find a declared, field-access XProperty on the current type. Use it.\n\t\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t\t}\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * We did not manage to find a declared XProperty on the current type.\n\t\t\t\t * Either the property is declared in a supertype, or it does not exist.\n\t\t\t\t * Try to find the member used to access the same property in the closest supertype.\n\t\t\t\t */\n\t\t\t\treturn getPropertyMemberFromParentTypes( propertyName );\n\t\t\t}\n\t\t}\n\t}",
        "filePathBefore": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
        "isPureRefactoring": true,
        "commitId": "a4d2e5b39bb53fefd2c31be404a6682af6c3a5bb",
        "packageNameBefore": "org.hibernate.search.mapper.orm.model.impl",
        "classNameBefore": "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel",
        "methodNameBefore": "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#findPropertyMember",
        "invokedMethod": "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getPropertyMemberFromParentTypes\n methodBody: private Member getPropertyMemberFromParentTypes(String propertyName) {\nreturn getAscendingSuperTypes().skip(1).map(type -> type.getPropertyOrNull(propertyName)).filter(Objects::nonNull).findFirst().map(HibernateOrmClassPropertyModel::getMember).orElse(null);\n}",
        "classSignatureBefore": "public class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> ",
        "methodNameBeforeSet": [
            "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#findPropertyMember"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel"
        ],
        "classSignatureBeforeSet": [
            "public class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics Severe changes",
                "description": "Return expression has been added within the Extract Method mechanics - with non-mapped leaves",
                "mappingState": 2
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.orm.model.impl;\n\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport org.hibernate.annotations.common.reflection.XProperty;\nimport org.hibernate.search.engine.mapper.model.spi.MappableTypeModel;\nimport org.hibernate.search.mapper.pojo.model.hcann.spi.PojoCommonsAnnotationsHelper;\nimport org.hibernate.search.mapper.pojo.model.spi.GenericContextAwarePojoGenericTypeModel.RawTypeDeclaringContext;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeIdentifier;\n\npublic class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> {\n\n\tprivate final HibernateOrmBasicClassTypeMetadata ormTypeMetadata;\n\tprivate final RawTypeDeclaringContext<T> rawTypeDeclaringContext;\n\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> ascendingSuperTypesCache;\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> descendingSuperTypesCache;\n\n\tprivate final Map<String, HibernateOrmClassPropertyModel<?>> propertyModelCache = new HashMap<>();\n\n\tprivate Map<String, XProperty> declaredFieldAccessXPropertiesByName;\n\tprivate Map<String, XProperty> declaredMethodAccessXPropertiesByName;\n\n\tHibernateOrmClassRawTypeModel(HibernateOrmBootstrapIntrospector introspector,\n\t\t\tPojoRawTypeIdentifier<T> typeIdentifier,\n\t\t\tHibernateOrmBasicClassTypeMetadata ormTypeMetadata, RawTypeDeclaringContext<T> rawTypeDeclaringContext) {\n\t\tsuper( introspector, typeIdentifier );\n\t\tthis.ormTypeMetadata = ormTypeMetadata;\n\t\tthis.rawTypeDeclaringContext = rawTypeDeclaringContext;\n\t}\n\n\t@Override\n\tpublic boolean isAbstract() {\n\t\treturn xClass.isAbstract();\n\t}\n\n\t@Override\n\tpublic boolean isSubTypeOf(MappableTypeModel superTypeCandidate) {\n\t\treturn superTypeCandidate instanceof HibernateOrmClassRawTypeModel\n\t\t\t\t&& ( (HibernateOrmClassRawTypeModel<?>) superTypeCandidate ).xClass.isAssignableFrom( xClass );\n\t}\n\n\t@Override\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> getAscendingSuperTypes() {\n\t\tif ( ascendingSuperTypesCache == null ) {\n\t\t\tascendingSuperTypesCache = introspector.getAscendingSuperTypes( xClass )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn ascendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> getDescendingSuperTypes() {\n\t\tif ( descendingSuperTypesCache == null ) {\n\t\t\tdescendingSuperTypesCache = introspector.getDescendingSuperTypes( xClass )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn descendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\tpublic Stream<Annotation> getAnnotations() {\n\t\treturn introspector.getAnnotations( xClass );\n\t}\n\n\t@Override\n\tStream<String> getDeclaredPropertyNames() {\n\t\treturn Stream.concat(\n\t\t\t\tgetDeclaredFieldAccessXPropertiesByName().keySet().stream(),\n\t\t\t\tgetDeclaredMethodAccessXPropertiesByName().keySet().stream()\n\t\t)\n\t\t\t\t.distinct();\n\t}\n\n\t@Override\n\tHibernateOrmClassPropertyModel<?> getPropertyOrNull(String propertyName) {\n\t\treturn propertyModelCache.computeIfAbsent( propertyName, this::createPropertyModel );\n\t}\n\n\tRawTypeDeclaringContext<T> getRawTypeDeclaringContext() {\n\t\treturn rawTypeDeclaringContext;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredFieldAccessXPropertiesByName() {\n\t\tif ( declaredFieldAccessXPropertiesByName == null ) {\n\t\t\tdeclaredFieldAccessXPropertiesByName =\n\t\t\t\t\tintrospector.getDeclaredFieldAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredFieldAccessXPropertiesByName;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredMethodAccessXPropertiesByName() {\n\t\tif ( declaredMethodAccessXPropertiesByName == null ) {\n\t\t\tdeclaredMethodAccessXPropertiesByName =\n\t\t\t\t\tintrospector.getDeclaredMethodAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredMethodAccessXPropertiesByName;\n\t}\n\n\tprivate HibernateOrmClassPropertyModel<?> createPropertyModel(String propertyName) {\n\t\tList<XProperty> declaredXProperties = new ArrayList<>( 2 );\n\t\t// Add the method first on purpose: the first XProperty may be used as a default to create the value accessor handle\n\t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( methodAccessXProperty );\n\t\t}\n\t\tXProperty fieldAccessXProperty = getDeclaredFieldAccessXPropertiesByName().get( propertyName );\n\t\tif ( fieldAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( fieldAccessXProperty );\n\t\t}\n\n\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata;\n\t\tif ( ormTypeMetadata == null ) {\n\t\t\t// There isn't any Hibernate ORM metadata for this type\n\t\t\tormPropertyMetadata = null;\n\t\t}\n\t\telse {\n\t\t\tormPropertyMetadata = ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n\t\t}\n\n\t\tMember member = findPropertyMember(\n\t\t\t\tpropertyName, methodAccessXProperty, fieldAccessXProperty, ormPropertyMetadata\n\t\t);\n\n\t\tif ( member == null ) {\n\t\t\treturn null;\n\t\t}\n\n\t\treturn new HibernateOrmClassPropertyModel<>(\n\t\t\t\tintrospector, this, propertyName,\n\t\t\t\tdeclaredXProperties, ormPropertyMetadata, member\n\t\t);\n\t}\n\n\tprivate Member findPropertyMember(String propertyName,\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n\t\t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n\t\t\t/*\n\t\t\t * Hibernate ORM has metadata for this property,\n\t\t\t * which means this property is persisted.\n\t\t\t *\n\t\t\t * Hibernate ORM might return us the member as declared in a supertype,\n\t\t\t * in which case the type of that member will not be up-to-date.\n\t\t\t * Thus we try to get the overridden member declared in the current type,\n\t\t\t * and failing that we look for the member in supertypes.\n\t\t\t *\n\t\t\t * We still try to comply with JPA's configured access type,\n\t\t\t * which explains the two if/else branches below.\n\t\t\t */\n\t\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n\t\t\t\treturn methodAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t\t}\n\t\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n\t\t\t\treturn fieldAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t\t}\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * We don't have a declared XProperty for this member in the current type.\n\t\t\t\t * Try to find the member used to access the same property in the closest supertype.\n\t\t\t\t */\n\t\t\t\treturn getPropertyMemberFromParentTypes( propertyName );\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t/*\n\t\t\t * Hibernate ORM doesn't have any metadata for this property,\n\t\t\t * which means this property is transient.\n\t\t\t * We don't need to worry about JPA's access type.\n\t\t\t */\n\t\t\tif ( methodAccessXProperty != null ) {\n\t\t\t\t// We managed to find a declared, method-access XProperty on the current type. Use it.\n\t\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t\t}\n\t\t\telse if ( fieldAccessXProperty != null ) {\n\t\t\t\t// We managed to find a declared, field-access XProperty on the current type. Use it.\n\t\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t\t}\n\t\t\telse {\n\t\t\t\t/*\n\t\t\t\t * We did not manage to find a declared XProperty on the current type.\n\t\t\t\t * Either the property is declared in a supertype, or it does not exist.\n\t\t\t\t * Try to find the member used to access the same property in the closest supertype.\n\t\t\t\t */\n\t\t\t\treturn getPropertyMemberFromParentTypes( propertyName );\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate Member getPropertyMemberFromParentTypes(String propertyName) {\n\t\t// TODO HSEARCH-3056 remove lambdas if possible\n\t\treturn getAscendingSuperTypes()\n\t\t\t\t.skip( 1 ) // Ignore self\n\t\t\t\t.map( type -> type.getPropertyOrNull( propertyName ) )\n\t\t\t\t.filter( Objects::nonNull )\n\t\t\t\t.findFirst()\n\t\t\t\t.map( HibernateOrmClassPropertyModel::getMember )\n\t\t\t\t.orElse( null );\n\t}\n}\n",
        "filePathAfter": "mapper/orm/src/main/java/org/hibernate/search/mapper/orm/model/impl/HibernateOrmClassRawTypeModel.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.mapper.orm.model.impl;\n\nimport java.lang.annotation.Annotation;\nimport java.lang.reflect.Field;\nimport java.lang.reflect.Member;\nimport java.lang.reflect.Method;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\n\nimport org.hibernate.annotations.common.reflection.XProperty;\nimport org.hibernate.search.engine.mapper.model.spi.MappableTypeModel;\nimport org.hibernate.search.mapper.pojo.model.hcann.spi.PojoCommonsAnnotationsHelper;\nimport org.hibernate.search.mapper.pojo.model.spi.GenericContextAwarePojoGenericTypeModel.RawTypeDeclaringContext;\nimport org.hibernate.search.mapper.pojo.model.spi.PojoRawTypeIdentifier;\n\npublic class HibernateOrmClassRawTypeModel<T> extends AbstractHibernateOrmRawTypeModel<T> {\n\n\tprivate final HibernateOrmBasicClassTypeMetadata ormTypeMetadata;\n\tprivate final RawTypeDeclaringContext<T> rawTypeDeclaringContext;\n\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> ascendingSuperTypesCache;\n\tprivate List<HibernateOrmClassRawTypeModel<? super T>> descendingSuperTypesCache;\n\n\tprivate final Map<String, HibernateOrmClassPropertyModel<?>> propertyModelCache = new HashMap<>();\n\n\tprivate Map<String, XProperty> declaredFieldAccessXPropertiesByName;\n\tprivate Map<String, XProperty> declaredMethodAccessXPropertiesByName;\n\n\tHibernateOrmClassRawTypeModel(HibernateOrmBootstrapIntrospector introspector,\n\t\t\tPojoRawTypeIdentifier<T> typeIdentifier,\n\t\t\tHibernateOrmBasicClassTypeMetadata ormTypeMetadata, RawTypeDeclaringContext<T> rawTypeDeclaringContext) {\n\t\tsuper( introspector, typeIdentifier );\n\t\tthis.ormTypeMetadata = ormTypeMetadata;\n\t\tthis.rawTypeDeclaringContext = rawTypeDeclaringContext;\n\t}\n\n\t@Override\n\tpublic boolean isAbstract() {\n\t\treturn xClass.isAbstract();\n\t}\n\n\t@Override\n\tpublic boolean isSubTypeOf(MappableTypeModel superTypeCandidate) {\n\t\treturn superTypeCandidate instanceof HibernateOrmClassRawTypeModel\n\t\t\t\t&& ( (HibernateOrmClassRawTypeModel<?>) superTypeCandidate ).xClass.isAssignableFrom( xClass );\n\t}\n\n\t@Override\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> getAscendingSuperTypes() {\n\t\tif ( ascendingSuperTypesCache == null ) {\n\t\t\tascendingSuperTypesCache = introspector.getAscendingSuperTypes( xClass )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn ascendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\tpublic Stream<HibernateOrmClassRawTypeModel<? super T>> getDescendingSuperTypes() {\n\t\tif ( descendingSuperTypesCache == null ) {\n\t\t\tdescendingSuperTypesCache = introspector.getDescendingSuperTypes( xClass )\n\t\t\t\t\t.collect( Collectors.toList() );\n\t\t}\n\t\treturn descendingSuperTypesCache.stream();\n\t}\n\n\t@Override\n\tpublic Stream<Annotation> getAnnotations() {\n\t\treturn introspector.getAnnotations( xClass );\n\t}\n\n\t@Override\n\tStream<String> getDeclaredPropertyNames() {\n\t\treturn Stream.concat(\n\t\t\t\tgetDeclaredFieldAccessXPropertiesByName().keySet().stream(),\n\t\t\t\tgetDeclaredMethodAccessXPropertiesByName().keySet().stream()\n\t\t)\n\t\t\t\t.distinct();\n\t}\n\n\t@Override\n\tHibernateOrmClassPropertyModel<?> getPropertyOrNull(String propertyName) {\n\t\treturn propertyModelCache.computeIfAbsent( propertyName, this::createPropertyModel );\n\t}\n\n\tRawTypeDeclaringContext<T> getRawTypeDeclaringContext() {\n\t\treturn rawTypeDeclaringContext;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredFieldAccessXPropertiesByName() {\n\t\tif ( declaredFieldAccessXPropertiesByName == null ) {\n\t\t\tdeclaredFieldAccessXPropertiesByName =\n\t\t\t\t\tintrospector.getDeclaredFieldAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredFieldAccessXPropertiesByName;\n\t}\n\n\tprivate Map<String, XProperty> getDeclaredMethodAccessXPropertiesByName() {\n\t\tif ( declaredMethodAccessXPropertiesByName == null ) {\n\t\t\tdeclaredMethodAccessXPropertiesByName =\n\t\t\t\t\tintrospector.getDeclaredMethodAccessXPropertiesByName( xClass );\n\t\t}\n\t\treturn declaredMethodAccessXPropertiesByName;\n\t}\n\n\tprivate HibernateOrmClassPropertyModel<?> createPropertyModel(String propertyName) {\n\t\tList<XProperty> declaredXProperties = new ArrayList<>( 2 );\n\t\t// Add the method first on purpose: the first XProperty may be used as a default to create the value accessor handle\n\t\tXProperty methodAccessXProperty = getDeclaredMethodAccessXPropertiesByName().get( propertyName );\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( methodAccessXProperty );\n\t\t}\n\t\tXProperty fieldAccessXProperty = getDeclaredFieldAccessXPropertiesByName().get( propertyName );\n\t\tif ( fieldAccessXProperty != null ) {\n\t\t\tdeclaredXProperties.add( fieldAccessXProperty );\n\t\t}\n\n\t\tHibernateOrmBasicClassPropertyMetadata ormPropertyMetadata;\n\t\tif ( ormTypeMetadata == null ) {\n\t\t\t// There isn't any Hibernate ORM metadata for this type\n\t\t\tormPropertyMetadata = null;\n\t\t}\n\t\telse {\n\t\t\tormPropertyMetadata = ormTypeMetadata.getClassPropertyMetadataOrNull( propertyName );\n\t\t}\n\n\t\tMember member = findPropertyMember(\n\t\t\t\tpropertyName, methodAccessXProperty, fieldAccessXProperty, ormPropertyMetadata\n\t\t);\n\n\t\tif ( member == null ) {\n\t\t\treturn null;\n\t\t}\n\n\t\treturn new HibernateOrmClassPropertyModel<>(\n\t\t\t\tintrospector, this, propertyName,\n\t\t\t\tdeclaredXProperties, ormPropertyMetadata, member\n\t\t);\n\t}\n\n\tprivate Member findPropertyMember(String propertyName,\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tMember result;\n\t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is persisted).\n\t\t\t// Use ORM metadata to find the corresponding member (field/method).\n\t\t\tresult = getPropertyMemberUsingHibernateOrmMetadataFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty, propertyMetadataFromHibernateOrmMetamodel\n\t\t\t);\n\t\t}\n\t\telse {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n\t\t\t// Use reflection to find the corresponding member (field/method).\n\t\t\tresult = getPropertyMemberUsingReflectionFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty\n\t\t\t);\n\t\t}\n\n\t\tif ( result == null ) {\n\t\t\t// There is no member for this property on the current type.\n\t\t\t// Try to find one in the closest supertype.\n\t\t\tresult = getPropertyMemberFromParentTypes( propertyName );\n\t\t}\n\n\t\treturn result;\n\t}\n\n\tprivate Member getPropertyMemberUsingHibernateOrmMetadataFromThisType(XProperty methodAccessXProperty,\n\t\t\tXProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n\t\t/*\n\t\t * Hibernate ORM has metadata for this property,\n\t\t * which means this property is persisted.\n\t\t *\n\t\t * Hibernate ORM might return us the member as declared in a supertype,\n\t\t * in which case the type of that member will not be up-to-date.\n\t\t * Thus we try to get the overridden member declared in the current type,\n\t\t * and failing that we look for the member in supertypes.\n\t\t *\n\t\t * We still try to comply with JPA's configured access type,\n\t\t * which explains the two if/else branches below.\n\t\t */\n\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n\t\t\treturn methodAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n\t\t\treturn fieldAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\treturn null;\n\t\t}\n\t}\n\n\t/*\n\t * Hibernate ORM doesn't have any metadata for this property,\n\t * which means this property is transient.\n\t * We don't need to worry about JPA's access type.\n\t */\n\tprivate Member getPropertyMemberUsingReflectionFromThisType(\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty) {\n\t\tif ( methodAccessXProperty != null ) {\n\t\t\t// Method access is available. Get values from the getter.\n\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( fieldAccessXProperty != null ) {\n\t\t\t// Method access is not available, but field access is. Get values directly from the field.\n\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\t// Neither method access nor field access is available.\n\t\t\t// The property is not declared in this type.\n\t\t\treturn null;\n\t\t}\n\t}\n\n\tprivate Member getPropertyMemberFromParentTypes(String propertyName) {\n\t\t// TODO HSEARCH-3056 remove lambdas if possible\n\t\treturn getAscendingSuperTypes()\n\t\t\t\t.skip( 1 ) // Ignore self\n\t\t\t\t.map( type -> type.getPropertyOrNull( propertyName ) )\n\t\t\t\t.filter( Objects::nonNull )\n\t\t\t\t.findFirst()\n\t\t\t\t.map( HibernateOrmClassPropertyModel::getMember )\n\t\t\t\t.orElse( null );\n\t}\n}\n",
        "diffSourceCodeSet": [
            "private Member getPropertyMemberUsingHibernateOrmMetadataFromThisType(XProperty methodAccessXProperty,\n\t\t\tXProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n\t\t/*\n\t\t * Hibernate ORM has metadata for this property,\n\t\t * which means this property is persisted.\n\t\t *\n\t\t * Hibernate ORM might return us the member as declared in a supertype,\n\t\t * in which case the type of that member will not be up-to-date.\n\t\t * Thus we try to get the overridden member declared in the current type,\n\t\t * and failing that we look for the member in supertypes.\n\t\t *\n\t\t * We still try to comply with JPA's configured access type,\n\t\t * which explains the two if/else branches below.\n\t\t */\n\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n\t\t\treturn methodAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n\t\t\treturn fieldAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\treturn null;\n\t\t}\n\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.mapper.orm.model.impl.HibernateOrmClassRawTypeModel#getPropertyMemberFromParentTypes\n methodBody: private Member getPropertyMemberFromParentTypes(String propertyName) {\nreturn getAscendingSuperTypes().skip(1).map(type -> type.getPropertyOrNull(propertyName)).filter(Objects::nonNull).findFirst().map(HibernateOrmClassPropertyModel::getMember).orElse(null);\n}"
        ],
        "sourceCodeAfterRefactoring": "private Member findPropertyMember(String propertyName,\n\t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tMember result;\n\t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is persisted).\n\t\t\t// Use ORM metadata to find the corresponding member (field/method).\n\t\t\tresult = getPropertyMemberUsingHibernateOrmMetadataFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty, propertyMetadataFromHibernateOrmMetamodel\n\t\t\t);\n\t\t}\n\t\telse {\n\t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n\t\t\t// Use reflection to find the corresponding member (field/method).\n\t\t\tresult = getPropertyMemberUsingReflectionFromThisType(\n\t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty\n\t\t\t);\n\t\t}\n\n\t\tif ( result == null ) {\n\t\t\t// There is no member for this property on the current type.\n\t\t\t// Try to find one in the closest supertype.\n\t\t\tresult = getPropertyMemberFromParentTypes( propertyName );\n\t\t}\n\n\t\treturn result;\n\t}\nprivate Member getPropertyMemberUsingHibernateOrmMetadataFromThisType(XProperty methodAccessXProperty,\n\t\t\tXProperty fieldAccessXProperty,\n\t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n\t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n\t\t/*\n\t\t * Hibernate ORM has metadata for this property,\n\t\t * which means this property is persisted.\n\t\t *\n\t\t * Hibernate ORM might return us the member as declared in a supertype,\n\t\t * in which case the type of that member will not be up-to-date.\n\t\t * Thus we try to get the overridden member declared in the current type,\n\t\t * and failing that we look for the member in supertypes.\n\t\t *\n\t\t * We still try to comply with JPA's configured access type,\n\t\t * which explains the two if/else branches below.\n\t\t */\n\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n\t\t\treturn methodAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n\t\t}\n\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n\t\t\treturn fieldAccessXProperty == null\n\t\t\t\t\t? memberFromHibernateOrmMetamodel\n\t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n\t\t}\n\t\telse {\n\t\t\treturn null;\n\t\t}\n\t}",
        "diffSourceCode": "   151: \tprivate Member findPropertyMember(String propertyName,\n   152: \t\t\tXProperty methodAccessXProperty, XProperty fieldAccessXProperty,\n   153: \t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n-  154: \t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n-  155: \t\t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n-  156: \t\t\t/*\n-  157: \t\t\t * Hibernate ORM has metadata for this property,\n-  158: \t\t\t * which means this property is persisted.\n-  159: \t\t\t *\n-  160: \t\t\t * Hibernate ORM might return us the member as declared in a supertype,\n-  161: \t\t\t * in which case the type of that member will not be up-to-date.\n-  162: \t\t\t * Thus we try to get the overridden member declared in the current type,\n-  163: \t\t\t * and failing that we look for the member in supertypes.\n-  164: \t\t\t *\n-  165: \t\t\t * We still try to comply with JPA's configured access type,\n-  166: \t\t\t * which explains the two if/else branches below.\n-  167: \t\t\t */\n-  168: \t\t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n-  169: \t\t\t\treturn methodAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n-  170: \t\t\t}\n-  171: \t\t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n-  172: \t\t\t\treturn fieldAccessXProperty == null ? memberFromHibernateOrmMetamodel : PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n-  173: \t\t\t}\n-  174: \t\t\telse {\n-  175: \t\t\t\t/*\n-  176: \t\t\t\t * We don't have a declared XProperty for this member in the current type.\n-  177: \t\t\t\t * Try to find the member used to access the same property in the closest supertype.\n-  178: \t\t\t\t */\n-  179: \t\t\t\treturn getPropertyMemberFromParentTypes( propertyName );\n-  180: \t\t\t}\n-  181: \t\t}\n-  182: \t\telse {\n-  183: \t\t\t/*\n-  184: \t\t\t * Hibernate ORM doesn't have any metadata for this property,\n-  185: \t\t\t * which means this property is transient.\n-  186: \t\t\t * We don't need to worry about JPA's access type.\n-  187: \t\t\t */\n-  188: \t\t\tif ( methodAccessXProperty != null ) {\n-  189: \t\t\t\t// We managed to find a declared, method-access XProperty on the current type. Use it.\n-  190: \t\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n-  191: \t\t\t}\n-  192: \t\t\telse if ( fieldAccessXProperty != null ) {\n-  193: \t\t\t\t// We managed to find a declared, field-access XProperty on the current type. Use it.\n-  194: \t\t\t\treturn PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n-  195: \t\t\t}\n-  196: \t\t\telse {\n-  197: \t\t\t\t/*\n-  198: \t\t\t\t * We did not manage to find a declared XProperty on the current type.\n-  199: \t\t\t\t * Either the property is declared in a supertype, or it does not exist.\n-  200: \t\t\t\t * Try to find the member used to access the same property in the closest supertype.\n-  201: \t\t\t\t */\n-  202: \t\t\t\treturn getPropertyMemberFromParentTypes( propertyName );\n-  203: \t\t\t}\n+  154: \t\tMember result;\n+  155: \t\tif ( propertyMetadataFromHibernateOrmMetamodel != null ) {\n+  156: \t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is persisted).\n+  157: \t\t\t// Use ORM metadata to find the corresponding member (field/method).\n+  158: \t\t\tresult = getPropertyMemberUsingHibernateOrmMetadataFromThisType(\n+  159: \t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty, propertyMetadataFromHibernateOrmMetamodel\n+  160: \t\t\t);\n+  161: \t\t}\n+  162: \t\telse {\n+  163: \t\t\t// Hibernate ORM doesn't have any metadata for this property (the property is transient).\n+  164: \t\t\t// Use reflection to find the corresponding member (field/method).\n+  165: \t\t\tresult = getPropertyMemberUsingReflectionFromThisType(\n+  166: \t\t\t\t\tmethodAccessXProperty, fieldAccessXProperty\n+  167: \t\t\t);\n+  168: \t\t}\n+  169: \n+  170: \t\tif ( result == null ) {\n+  171: \t\t\t// There is no member for this property on the current type.\n+  172: \t\t\t// Try to find one in the closest supertype.\n+  173: \t\t\tresult = getPropertyMemberFromParentTypes( propertyName );\n+  174: \t\t}\n+  175: \n+  176: \t\treturn result;\n+  177: \t}\n+  178: \n+  179: \tprivate Member getPropertyMemberUsingHibernateOrmMetadataFromThisType(XProperty methodAccessXProperty,\n+  180: \t\t\tXProperty fieldAccessXProperty,\n+  181: \t\t\tHibernateOrmBasicClassPropertyMetadata propertyMetadataFromHibernateOrmMetamodel) {\n+  182: \t\tMember memberFromHibernateOrmMetamodel = propertyMetadataFromHibernateOrmMetamodel.getMember();\n+  183: \t\t/*\n+  184: \t\t * Hibernate ORM has metadata for this property,\n+  185: \t\t * which means this property is persisted.\n+  186: \t\t *\n+  187: \t\t * Hibernate ORM might return us the member as declared in a supertype,\n+  188: \t\t * in which case the type of that member will not be up-to-date.\n+  189: \t\t * Thus we try to get the overridden member declared in the current type,\n+  190: \t\t * and failing that we look for the member in supertypes.\n+  191: \t\t *\n+  192: \t\t * We still try to comply with JPA's configured access type,\n+  193: \t\t * which explains the two if/else branches below.\n+  194: \t\t */\n+  195: \t\tif ( memberFromHibernateOrmMetamodel instanceof Method ) {\n+  196: \t\t\treturn methodAccessXProperty == null\n+  197: \t\t\t\t\t? memberFromHibernateOrmMetamodel\n+  198: \t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( methodAccessXProperty );\n+  199: \t\t}\n+  200: \t\telse if ( memberFromHibernateOrmMetamodel instanceof Field ) {\n+  201: \t\t\treturn fieldAccessXProperty == null\n+  202: \t\t\t\t\t? memberFromHibernateOrmMetamodel\n+  203: \t\t\t\t\t: PojoCommonsAnnotationsHelper.getUnderlyingMember( fieldAccessXProperty );\n   204: \t\t}\n-  205: \t}\n-  206: \n-  207: \tprivate Member getPropertyMemberFromParentTypes(String propertyName) {\n-  208: \t\t// TODO HSEARCH-3056 remove lambdas if possible\n+  205: \t\telse {\n+  206: \t\t\treturn null;\n+  207: \t\t}\n+  208: \t}\n",
        "uniqueId": "a4d2e5b39bb53fefd2c31be404a6682af6c3a5bb_151_205_179_208_151_177",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 27,
                "covered": 28
            },
            "BRANCH": {
                "missed": 7,
                "covered": 5
            },
            "LINE": {
                "missed": 6,
                "covered": 7
            },
            "COMPLEXITY": {
                "missed": 5,
                "covered": 2
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tpackage onSequenceComplete() : CompletionStage<Void> extracted from public build() : CompletableFuture<Void> in class org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder & moved to class org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder.SequenceContext",
        "diffLocations": [
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
                "startLine": 134,
                "endLine": 148,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
                "startLine": 122,
                "endLine": 129,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
                "startLine": 201,
                "endLine": 204,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "@Override\n\tpublic CompletableFuture<Void> build() {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = currentlyBuildingSequenceContext;\n\n\t\treturn Futures.whenCompleteExecute(\n\t\t\t\tcurrentlyBuildingSequenceTail,\n\t\t\t\t() -> sequenceContext.executionContext.executePendingRefreshes()\n\t\t\t\t\t\t.whenComplete( Futures.copyHandler( sequenceContext.refreshFuture ) )\n\t\t)\n\t\t\t\t.exceptionally( Futures.handler( t -> {\n\t\t\t\t\tsequenceContext.notifySequenceFailed( t );\n\t\t\t\t\treturn null;\n\t\t\t\t} ) );\n\t}",
        "filePathBefore": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
        "isPureRefactoring": true,
        "commitId": "3ba4b03373611f27e258496ad8376ea8dc123642",
        "packageNameBefore": "org.hibernate.search.backend.elasticsearch.orchestration.impl",
        "classNameBefore": "org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder",
        "methodNameBefore": "org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#build",
        "invokedMethod": "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder.SequenceContext#notifySequenceFailed\n methodBody: void notifySequenceFailed(Throwable throwable) {\nif(!(throwable instanceof PreviousWorkException)){throw Throwables.toRuntimeException(throwable);\n}}",
        "classSignatureBefore": "class ElasticsearchDefaultWorkSequenceBuilder implements ElasticsearchWorkSequenceBuilder ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#build"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder"
        ],
        "classSignatureBeforeSet": [
            "class ElasticsearchDefaultWorkSequenceBuilder implements ElasticsearchWorkSequenceBuilder "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.orchestration.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.function.Supplier;\n\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.backend.elasticsearch.work.impl.BulkableElasticsearchWork;\nimport org.hibernate.search.backend.elasticsearch.work.impl.ElasticsearchWork;\nimport org.hibernate.search.backend.elasticsearch.work.result.impl.BulkResult;\nimport org.hibernate.search.backend.elasticsearch.work.result.impl.BulkResultItemExtractor;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.common.impl.Throwables;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * A simple implementation of {@link ElasticsearchWorkSequenceBuilder}.\n * <p>\n * Works will be executed inside a sequence-scoped context (a {@link ElasticsearchRefreshableWorkExecutionContext}),\n * ultimately leading to a {@link ElasticsearchRefreshableWorkExecutionContext#executePendingRefreshes()}.\n */\nclass ElasticsearchDefaultWorkSequenceBuilder implements ElasticsearchWorkSequenceBuilder {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate final Supplier<ElasticsearchRefreshableWorkExecutionContext> contextSupplier;\n\tprivate final BulkResultExtractionStepImpl bulkResultExtractionStep = new BulkResultExtractionStepImpl();\n\n\tprivate CompletableFuture<?> currentlyBuildingSequenceTail;\n\tprivate SequenceContext currentlyBuildingSequenceContext;\n\n\tElasticsearchDefaultWorkSequenceBuilder(Supplier<ElasticsearchRefreshableWorkExecutionContext> contextSupplier) {\n\t\tthis.contextSupplier = contextSupplier;\n\t}\n\n\t@Override\n\tpublic void init(CompletableFuture<?> previous) {\n\t\t// We only use the previous stage to delay the execution of the sequence, but we ignore its result\n\t\tthis.currentlyBuildingSequenceTail = previous.handle( (ignoredResult, ignoredThrowable) -> null );\n\t\tthis.currentlyBuildingSequenceContext = new SequenceContext(\n\t\t\t\tcontextSupplier.get()\n\t\t);\n\t}\n\n\t/**\n\t * Add a step to execute a new work.\n\t * <p>\n\t * A failure in the previous work will lead to the new work being marked as skipped,\n\t * and a failure during the new work will lead to the new work being marked\n\t * as failed.\n\t *\n\t * @param work The work to be executed\n\t */\n\t@Override\n\tpublic <T> CompletableFuture<T> addNonBulkExecution(ElasticsearchWork<T> work) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\t/*\n\t\t * Use a different future for the caller than the one used in the sequence,\n\t\t * because we manipulate internal exceptions in the sequence\n\t\t * that should not be exposed to the caller.\n \t\t */\n\t\tCompletableFuture<T> workFutureForCaller = new CompletableFuture<>();\n\n\t\t// If the previous work failed, then skip the new work and notify the caller and failure handler as necessary.\n\t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n\t\t\t\t.whenComplete( Futures.handler( (ignoredResult, throwable) -> {\n\t\t\t\t\tif ( throwable != null ) {\n\t\t\t\t\t\tsequenceContext.notifyWorkSkipped( work, throwable, workFutureForCaller );\n\t\t\t\t\t}\n\t\t\t\t} ) )\n\t\t\t\t// If the previous work completed normally, then execute the new work\n\t\t\t\t.thenCompose( Futures.safeComposer(\n\t\t\t\t\t\tignoredPreviousResult -> {\n\t\t\t\t\t\t\tCompletableFuture<T> workExecutionFuture = work.execute( sequenceContext.executionContext );\n\t\t\t\t\t\t\treturn addPostExecutionHandlers( work, workExecutionFuture, workFutureForCaller, sequenceContext );\n\t\t\t\t\t\t}\n\t\t\t\t) );\n\n\t\t/*\n\t\t * Make sure that the sequence will only advance to the next work\n\t\t * after both the work and *all* the handlers are executed,\n\t\t * because otherwise failureHandler.handle() could be called before all failed/skipped works are reported.\n\t\t */\n\t\tcurrentlyBuildingSequenceTail = handledWorkExecutionFuture;\n\n\t\treturn workFutureForCaller;\n\t}\n\n\t/**\n\t * Add a step to execute a bulk work.\n\t * <p>\n\t * The bulk work won't be marked as skipped or failed, regardless of errors.\n\t * Only the bulked works will be marked (as skipped) if a previous work or the bulk work fails.\n\t *\n\t * @param workFuture The work to be executed\n\t */\n\t@Override\n\tpublic CompletableFuture<BulkResult> addBulkExecution(CompletableFuture<? extends ElasticsearchWork<BulkResult>> workFuture) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext currentSequenceAttributes = this.currentlyBuildingSequenceContext;\n\n\t\tCompletableFuture<BulkResult> bulkWorkResultFuture =\n\t\t\t\t// When the previous work completes successfully *and* the bulk work is available...\n\t\t\t\tcurrentlyBuildingSequenceTail.thenCombine( workFuture, (ignored, work) -> work )\n\t\t\t\t// ... execute the bulk work\n\t\t\t\t.thenCompose( work -> work.execute( currentSequenceAttributes.executionContext ) );\n\t\t// Do not propagate the exception as is: we expect the exception to be handled by each bulked work separately.\n\t\t// ... but still propagate *something*, in case a *previous* work failed.\n\t\tcurrentlyBuildingSequenceTail = bulkWorkResultFuture.exceptionally( Futures.handler( throwable -> {\n\t\t\tthrow new PreviousWorkException( throwable );\n\t\t} ) );\n\t\treturn bulkWorkResultFuture;\n\t}\n\n\t@Override\n\tpublic BulkResultExtractionStep addBulkResultExtraction(CompletableFuture<BulkResult> bulkResultFuture) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext currentSequenceAttributes = this.currentlyBuildingSequenceContext;\n\n\t\tCompletableFuture<BulkResultItemExtractor> extractorFuture =\n\t\t\t\tbulkResultFuture.thenApply( bulkResult -> bulkResult.withContext( currentSequenceAttributes.executionContext ) );\n\t\tbulkResultExtractionStep.init( extractorFuture );\n\t\treturn bulkResultExtractionStep;\n\t}\n\n\t@Override\n\tpublic CompletableFuture<Void> build() {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = currentlyBuildingSequenceContext;\n\n\t\treturn Futures.whenCompleteExecute(\n\t\t\t\tcurrentlyBuildingSequenceTail,\n\t\t\t\t() -> sequenceContext.executionContext.executePendingRefreshes()\n\t\t\t\t\t\t.whenComplete( Futures.copyHandler( sequenceContext.refreshFuture ) )\n\t\t)\n\t\t\t\t.exceptionally( Futures.handler( t -> {\n\t\t\t\t\tsequenceContext.notifySequenceFailed( t );\n\t\t\t\t\treturn null;\n\t\t\t\t} ) );\n\t}\n\n\t<T> CompletableFuture<T> addPostExecutionHandlers(ElasticsearchWork<T> work,\n\t\t\tCompletableFuture<T> workExecutionFuture, CompletableFuture<T> workFutureForCaller,\n\t\t\tSequenceContext sequenceContext) {\n\t\t/*\n\t\t * In case of success, wait for the refresh and propagate the result to the client.\n\t\t * We ABSOLUTELY DO NOT WANT the resulting future to be included in the sequence,\n\t\t * because it would create a deadlock:\n\t\t * future A will only complete when the refresh future (B) is executed,\n\t\t * which will only happen when the sequence ends,\n\t\t * which will only happen after A completes...\n\t\t */\n\t\tworkExecutionFuture.thenCombine( sequenceContext.refreshFuture, (workResult, refreshResult) -> workResult )\n\t\t\t\t.whenComplete( Futures.copyHandler( workFutureForCaller ) );\n\t\t/*\n\t\t * In case of error, propagate the exception immediately to both the failure handler and the client.\n\t\t *\n\t\t * Also, make sure to re-throw an exception\n\t\t * so that execution of following works in the sequence will be skipped.\n\t\t *\n\t\t * Make sure to return the resulting stage, and not executedWorkStage,\n\t\t * so that exception handling happens before the end of the sequence,\n\t\t * meaning notifyWorkFailed() is guaranteed to be called before notifySequenceFailed().\n\t\t */\n\t\treturn workExecutionFuture.exceptionally( Futures.handler( throwable -> {\n\t\t\tsequenceContext.notifyWorkFailed( work, throwable, workFutureForCaller );\n\t\t\tthrow new PreviousWorkException( throwable );\n\t\t} ) );\n\t}\n\n\tprivate final class BulkResultExtractionStepImpl implements BulkResultExtractionStep {\n\n\t\tprivate CompletableFuture<BulkResultItemExtractor> extractorFuture;\n\n\t\tvoid init(CompletableFuture<BulkResultItemExtractor> extractorFuture) {\n\t\t\tthis.extractorFuture = extractorFuture;\n\t\t}\n\n\t\t@Override\n\t\tpublic <T> CompletableFuture<T> add(BulkableElasticsearchWork<T> bulkedWork, int index) {\n\t\t\t// Use local variables to make sure the lambdas won't be affected by a reset()\n\t\t\tfinal SequenceContext sequenceContext = ElasticsearchDefaultWorkSequenceBuilder.this.currentlyBuildingSequenceContext;\n\n\t\t\t/*\n\t\t\t * Use a different future for the caller than the one used in the sequence,\n\t\t\t * because we manipulate internal exceptions in the sequence\n\t\t\t * that should not be exposed to the caller.\n\t\t\t */\n\t\t\tCompletableFuture<T> workFutureForCaller = new CompletableFuture<>();\n\n\t\t\t// If the bulk work fails, make sure to notify the caller and failure handler as necessary.\n\t\t\tCompletableFuture<T> handledWorkExecutionFuture = extractorFuture\n\t\t\t\t\t.whenComplete( Futures.handler( (result, throwable) -> {\n\t\t\t\t\t\tif ( throwable == null ) {\n\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse if ( throwable instanceof PreviousWorkException ) {\n\t\t\t\t\t\t\t// The bulk work itself was skipped; mark the bulked work as skipped too\n\t\t\t\t\t\t\tsequenceContext.notifyWorkSkipped( bulkedWork, throwable, workFutureForCaller );\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\t// The bulk work failed; mark the bulked work as failed too\n\t\t\t\t\t\t\tsequenceContext.notifyWorkFailedBecauseBulkFailed( bulkedWork, throwable, workFutureForCaller );\n\t\t\t\t\t\t}\n\t\t\t\t\t} ) )\n\t\t\t\t\t// If the bulk work succeeds, then extract the bulked work result and notify as necessary\n\t\t\t\t\t.thenCompose( extractor -> {\n\t\t\t\t\t\t// Use Futures.create to catch any exception thrown by extractor.extract\n\t\t\t\t\t\tCompletableFuture<T> workExecutionFuture = Futures.create(\n\t\t\t\t\t\t\t\t() -> extractor.extract( bulkedWork, index )\n\t\t\t\t\t\t);\n\t\t\t\t\t\treturn addPostExecutionHandlers( bulkedWork, workExecutionFuture, workFutureForCaller, sequenceContext );\n\t\t\t\t\t} );\n\n\t\t\t/*\n\t\t\t * Make sure that the sequence will only advance to the next work\n\t\t\t * after both the work and *all* the handlers are executed,\n\t\t\t * because otherwise failureHandler.handle(...) could be called before all failed/skipped works are reported.\n\t\t\t */\n\t\t\tcurrentlyBuildingSequenceTail = CompletableFuture.allOf(\n\t\t\t\t\tcurrentlyBuildingSequenceTail,\n\t\t\t\t\thandledWorkExecutionFuture\n\t\t\t);\n\n\t\t\treturn workFutureForCaller;\n\t\t}\n\n\t}\n\n\tprivate static final class PreviousWorkException extends RuntimeException {\n\n\t\tpublic PreviousWorkException(Throwable cause) {\n\t\t\tsuper( cause );\n\t\t}\n\n\t}\n\n\t/**\n\t * Regroups all objects that may be shared among multiple steps in the same sequence.\n\t * <p>\n\t * This was introduced to make references to data from a previous sequence less likely;\n\t * see\n\t * org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilderTest#intertwinedSequenceExecution()\n\t * for an example of what can go wrong if we don't take care to avoid that.\n\t */\n\tprivate static final class SequenceContext {\n\t\tprivate final ElasticsearchRefreshableWorkExecutionContext executionContext;\n\t\tprivate final CompletableFuture<Void> refreshFuture;\n\n\t\tSequenceContext(ElasticsearchRefreshableWorkExecutionContext executionContext) {\n\t\t\tthis.executionContext = executionContext;\n\t\t\tthis.refreshFuture = new CompletableFuture<>();\n\t\t}\n\n\t\t<R> void notifyWorkSkipped(ElasticsearchWork<R> work, Throwable throwable,\n\t\t\t\tCompletableFuture<R> workFutureForCaller) {\n\t\t\tThrowable skippingCause = throwable instanceof PreviousWorkException ? throwable.getCause() : throwable;\n\t\t\tworkFutureForCaller.completeExceptionally(\n\t\t\t\t\tlog.elasticsearchSkippedBecauseOfPreviousWork( skippingCause )\n\t\t\t);\n\t\t}\n\n\t\t<R> void notifyWorkFailedBecauseBulkFailed(BulkableElasticsearchWork<R> work, Throwable throwable,\n\t\t\t\tCompletableFuture<R> workFutureForCaller) {\n\t\t\tnotifyWorkFailed(\n\t\t\t\t\twork,\n\t\t\t\t\tlog.elasticsearchFailedBecauseOfBulkFailure( throwable ),\n\t\t\t\t\tworkFutureForCaller\n\t\t\t);\n\t\t}\n\n\t\t<R> void notifyWorkFailed(ElasticsearchWork<R> work, Throwable throwable,\n\t\t\t\tCompletableFuture<R> workFutureForCaller) {\n\t\t\tworkFutureForCaller.completeExceptionally( throwable );\n\t\t}\n\n\t\tvoid notifySequenceFailed(Throwable throwable) {\n\t\t\tif ( !(throwable instanceof PreviousWorkException) ) {\n\t\t\t\tthrow Throwables.toRuntimeException( throwable );\n\t\t\t}\n\t\t}\n\t}\n}\n",
        "filePathAfter": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.orchestration.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\nimport java.util.function.Supplier;\n\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.backend.elasticsearch.work.impl.BulkableElasticsearchWork;\nimport org.hibernate.search.backend.elasticsearch.work.impl.ElasticsearchWork;\nimport org.hibernate.search.backend.elasticsearch.work.result.impl.BulkResult;\nimport org.hibernate.search.backend.elasticsearch.work.result.impl.BulkResultItemExtractor;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.common.impl.Throwables;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * A simple implementation of {@link ElasticsearchWorkSequenceBuilder}.\n * <p>\n * Works will be executed inside a sequence-scoped context (a {@link ElasticsearchRefreshableWorkExecutionContext}),\n * ultimately leading to a {@link ElasticsearchRefreshableWorkExecutionContext#executePendingRefreshes()}.\n */\nclass ElasticsearchDefaultWorkSequenceBuilder implements ElasticsearchWorkSequenceBuilder {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate final Supplier<ElasticsearchRefreshableWorkExecutionContext> contextSupplier;\n\tprivate final BulkResultExtractionStepImpl bulkResultExtractionStep = new BulkResultExtractionStepImpl();\n\n\tprivate CompletableFuture<?> currentlyBuildingSequenceTail;\n\tprivate SequenceContext currentlyBuildingSequenceContext;\n\n\tElasticsearchDefaultWorkSequenceBuilder(Supplier<ElasticsearchRefreshableWorkExecutionContext> contextSupplier) {\n\t\tthis.contextSupplier = contextSupplier;\n\t}\n\n\t@Override\n\tpublic void init(CompletableFuture<?> previous) {\n\t\t// We only use the previous stage to delay the execution of the sequence, but we ignore its result\n\t\tthis.currentlyBuildingSequenceTail = previous.handle( (ignoredResult, ignoredThrowable) -> null );\n\t\tthis.currentlyBuildingSequenceContext = new SequenceContext(\n\t\t\t\tcontextSupplier.get()\n\t\t);\n\t}\n\n\t/**\n\t * Add a step to execute a new work.\n\t * <p>\n\t * A failure in the previous work will lead to the new work being marked as skipped,\n\t * and a failure during the new work will lead to the new work being marked\n\t * as failed.\n\t *\n\t * @param work The work to be executed\n\t */\n\t@Override\n\tpublic <T> CompletableFuture<T> addNonBulkExecution(ElasticsearchWork<T> work) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tNonBulkedWorkExecutionState<T> workExecutionState =\n\t\t\t\tnew NonBulkedWorkExecutionState<>( sequenceContext, work );\n\n\t\t// If the previous work failed, then skip the new work and notify the caller and failure handler as necessary.\n\t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n\t\t\t\t.whenComplete( Futures.handler( workExecutionState::onPreviousWorkComplete ) )\n\t\t\t\t// If the previous work completed normally, then execute the new work\n\t\t\t\t.thenCompose( Futures.safeComposer( workExecutionState::onPreviousWorkSuccess ) );\n\n\t\t/*\n\t\t * Make sure that the sequence will only advance to the next work\n\t\t * after both the work and *all* the handlers are executed,\n\t\t * because otherwise failureHandler.handle() could be called before all failed/skipped works are reported.\n\t\t */\n\t\tcurrentlyBuildingSequenceTail = handledWorkExecutionFuture;\n\n\t\treturn workExecutionState.workFutureForCaller;\n\t}\n\n\t/**\n\t * Add a step to execute a bulk work.\n\t * <p>\n\t * The bulk work won't be marked as skipped or failed, regardless of errors.\n\t * Only the bulked works will be marked (as skipped) if a previous work or the bulk work fails.\n\t *\n\t * @param workFuture The work to be executed\n\t */\n\t@Override\n\tpublic CompletableFuture<BulkResult> addBulkExecution(CompletableFuture<? extends ElasticsearchWork<BulkResult>> workFuture) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext currentSequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tCompletableFuture<BulkResult> bulkWorkResultFuture =\n\t\t\t\t// When the previous work completes successfully *and* the bulk work is available...\n\t\t\t\tcurrentlyBuildingSequenceTail.thenCombine( workFuture, (ignored, work) -> work )\n\t\t\t\t// ... execute the bulk work\n\t\t\t\t.thenCompose( currentSequenceContext::execute );\n\t\t// Do not propagate the exception as is: we expect the exception to be handled by each bulked work separately.\n\t\t// ... but still propagate *something*, in case a *previous* work failed.\n\t\tcurrentlyBuildingSequenceTail = bulkWorkResultFuture.exceptionally( Futures.handler( throwable -> {\n\t\t\tthrow new PreviousWorkException( throwable );\n\t\t} ) );\n\t\treturn bulkWorkResultFuture;\n\t}\n\n\t@Override\n\tpublic BulkResultExtractionStep addBulkResultExtraction(CompletableFuture<BulkResult> bulkResultFuture) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext currentSequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tCompletableFuture<BulkResultItemExtractor> extractorFuture =\n\t\t\t\tbulkResultFuture.thenApply( currentSequenceContext::addContext );\n\t\tbulkResultExtractionStep.init( extractorFuture );\n\t\treturn bulkResultExtractionStep;\n\t}\n\n\t@Override\n\tpublic CompletableFuture<Void> build() {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = currentlyBuildingSequenceContext;\n\n\t\treturn Futures.whenCompleteExecute( currentlyBuildingSequenceTail, sequenceContext::onSequenceComplete )\n\t\t\t\t.exceptionally( Futures.handler( sequenceContext::onSequenceFailed ) );\n\t}\n\n\tprivate final class BulkResultExtractionStepImpl implements BulkResultExtractionStep {\n\n\t\tprivate CompletableFuture<BulkResultItemExtractor> extractorFuture;\n\n\t\tvoid init(CompletableFuture<BulkResultItemExtractor> extractorFuture) {\n\t\t\tthis.extractorFuture = extractorFuture;\n\t\t}\n\n\t\t@Override\n\t\tpublic <T> CompletableFuture<T> add(BulkableElasticsearchWork<T> bulkedWork, int index) {\n\t\t\t// Use local variables to make sure the lambdas won't be affected by a reset()\n\t\t\tfinal SequenceContext sequenceContext = ElasticsearchDefaultWorkSequenceBuilder.this.currentlyBuildingSequenceContext;\n\n\t\t\tBulkedWorkExecutionState<T> workExecutionState =\n\t\t\t\t\tnew BulkedWorkExecutionState<>( sequenceContext, bulkedWork, index );\n\n\t\t\t// If the bulk work fails, make sure to notify the caller and failure handler as necessary.\n\t\t\tCompletableFuture<T> handledWorkExecutionFuture = extractorFuture\n\t\t\t\t\t.whenComplete( Futures.handler( workExecutionState::onBulkWorkComplete ) )\n\t\t\t\t\t// If the bulk work succeeds, then extract the bulked work result and notify as necessary\n\t\t\t\t\t.thenCompose( workExecutionState::onBulkWorkSuccess );\n\n\t\t\t/*\n\t\t\t * Make sure that the sequence will only advance to the next work\n\t\t\t * after both the work and *all* the handlers are executed,\n\t\t\t * because otherwise failureHandler.handle(...) could be called before all failed/skipped works are reported.\n\t\t\t */\n\t\t\tcurrentlyBuildingSequenceTail = CompletableFuture.allOf(\n\t\t\t\t\tcurrentlyBuildingSequenceTail,\n\t\t\t\t\thandledWorkExecutionFuture\n\t\t\t);\n\n\t\t\treturn workExecutionState.workFutureForCaller;\n\t\t}\n\n\t}\n\n\tprivate static final class PreviousWorkException extends RuntimeException {\n\n\t\tpublic PreviousWorkException(Throwable cause) {\n\t\t\tsuper( cause );\n\t\t}\n\n\t}\n\n\t/**\n\t * Regroups all objects that may be shared among multiple steps in the same sequence.\n\t * <p>\n\t * This was introduced to make references to data from a previous sequence less likely;\n\t * see\n\t * org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilderTest#intertwinedSequenceExecution()\n\t * for an example of what can go wrong if we don't take care to avoid that.\n\t */\n\tprivate static final class SequenceContext {\n\t\tprivate final ElasticsearchRefreshableWorkExecutionContext executionContext;\n\t\tprivate final CompletableFuture<Void> refreshFuture;\n\n\t\tSequenceContext(ElasticsearchRefreshableWorkExecutionContext executionContext) {\n\t\t\tthis.executionContext = executionContext;\n\t\t\tthis.refreshFuture = new CompletableFuture<>();\n\t\t}\n\n\t\t<T> CompletionStage<T> execute(ElasticsearchWork<T> work) {\n\t\t\treturn work.execute( executionContext );\n\t\t}\n\n\t\tpublic BulkResultItemExtractor addContext(BulkResult bulkResult) {\n\t\t\treturn bulkResult.withContext( executionContext );\n\t\t}\n\n\t\tCompletionStage<Void> onSequenceComplete() {\n\t\t\treturn executionContext.executePendingRefreshes()\n\t\t\t\t\t.whenComplete( Futures.copyHandler( refreshFuture ) );\n\t\t}\n\n\t\t<T> T onSequenceFailed(Throwable throwable) {\n\t\t\tif ( !(throwable instanceof PreviousWorkException) ) {\n\t\t\t\tthrow Throwables.toRuntimeException( throwable );\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n\n\tprivate abstract static class AbstractWorkExecutionState<T> {\n\n\t\tprotected final SequenceContext sequenceContext;\n\n\t\tprotected final ElasticsearchWork<T> work;\n\n\t\t/*\n\t\t * Use a different future for the caller than the one used in the sequence,\n\t\t * because we manipulate internal exceptions in the sequence\n\t\t * that should not be exposed to the caller.\n\t\t */\n\t\tfinal CompletableFuture<T> workFutureForCaller = new CompletableFuture<>();\n\n\t\tprivate AbstractWorkExecutionState(SequenceContext sequenceContext, ElasticsearchWork<T> work) {\n\t\t\tthis.sequenceContext = sequenceContext;\n\t\t\tthis.work = work;\n\t\t}\n\n\t\tprotected CompletableFuture<T> addPostExecutionHandlers(CompletableFuture<T> workExecutionFuture) {\n\t\t\t/*\n\t\t\t * In case of success, wait for the refresh and propagate the result to the client.\n\t\t\t * We ABSOLUTELY DO NOT WANT the resulting future to be included in the sequence,\n\t\t\t * because it would create a deadlock:\n\t\t\t * future A will only complete when the refresh future (B) is executed,\n\t\t\t * which will only happen when the sequence ends,\n\t\t\t * which will only happen after A completes...\n\t\t\t */\n\t\t\tworkExecutionFuture.thenCombine( sequenceContext.refreshFuture, (workResult, refreshResult) -> workResult )\n\t\t\t\t\t.whenComplete( Futures.copyHandler( workFutureForCaller ) );\n\t\t\t/*\n\t\t\t * In case of error, propagate the exception immediately to both the failure handler and the client.\n\t\t\t *\n\t\t\t * Also, make sure to re-throw an exception\n\t\t\t * so that execution of following works in the sequence will be skipped.\n\t\t\t *\n\t\t\t * Make sure to return the resulting stage, and not executedWorkStage,\n\t\t\t * so that exception handling happens before the end of the sequence,\n\t\t\t * meaning notifyWorkFailed() is guaranteed to be called before notifySequenceFailed().\n\t\t\t */\n\t\t\treturn workExecutionFuture.exceptionally( Futures.handler( this::fail ) );\n\t\t}\n\n\t\tprotected void skip(Throwable throwable) {\n\t\t\tThrowable skippingCause = throwable instanceof PreviousWorkException ? throwable.getCause() : throwable;\n\t\t\tworkFutureForCaller.completeExceptionally(\n\t\t\t\t\tlog.elasticsearchSkippedBecauseOfPreviousWork( skippingCause )\n\t\t\t);\n\t\t}\n\n\t\tprotected T fail(Throwable throwable) {\n\t\t\tworkFutureForCaller.completeExceptionally( throwable );\n\t\t\tthrow new PreviousWorkException( throwable );\n\t\t}\n\t}\n\n\tprivate static final class NonBulkedWorkExecutionState<R> extends AbstractWorkExecutionState<R> {\n\n\t\tprivate NonBulkedWorkExecutionState(SequenceContext sequenceContext, ElasticsearchWork<R> work) {\n\t\t\tsuper( sequenceContext, work );\n\t\t}\n\n\t\tvoid onPreviousWorkComplete(Object ignored, Throwable throwable) {\n\t\t\tif ( throwable != null ) {\n\t\t\t\tskip( throwable );\n\t\t\t}\n\t\t}\n\n\t\tCompletableFuture<R> onPreviousWorkSuccess(Object ignored) {\n\t\t\tCompletableFuture<R> workExecutionFuture = work.execute( sequenceContext.executionContext );\n\t\t\treturn addPostExecutionHandlers( workExecutionFuture );\n\t\t}\n\t}\n\n\tprivate static final class BulkedWorkExecutionState<R> extends AbstractWorkExecutionState<R> {\n\n\t\tprivate final BulkableElasticsearchWork<R> bulkedWork;\n\n\t\tprivate final int index;\n\n\t\tprivate BulkResultItemExtractor extractor;\n\n\t\tprivate BulkedWorkExecutionState(SequenceContext sequenceContext,\n\t\t\t\tBulkableElasticsearchWork<R> bulkedWork, int index) {\n\t\t\tsuper( sequenceContext, bulkedWork );\n\t\t\tthis.bulkedWork = bulkedWork;\n\t\t\tthis.index = index;\n\t\t}\n\n\t\tvoid onBulkWorkComplete(BulkResultItemExtractor ignored, Throwable throwable) {\n\t\t\tif ( throwable == null ) {\n\t\t\t\t// No failure: nothing to handle.\n\t\t\t\treturn;\n\t\t\t}\n\t\t\telse if ( throwable instanceof PreviousWorkException ) {\n\t\t\t\t// The bulk work itself was skipped; mark the bulked work as skipped too\n\t\t\t\tskip( throwable );\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// The bulk work failed; mark the bulked work as failed too\n\t\t\t\tfailBecauseBulkFailed( throwable );\n\t\t\t}\n\t\t}\n\n\t\tCompletableFuture<R> onBulkWorkSuccess(BulkResultItemExtractor extractor) {\n\t\t\tthis.extractor = extractor;\n\t\t\t// Use Futures.create to catch any exception thrown by extractor.extract\n\t\t\tCompletableFuture<R> workExecutionFuture = Futures.create( this::extract );\n\t\t\treturn addPostExecutionHandlers( workExecutionFuture );\n\t\t}\n\n\t\tprivate CompletableFuture<R> extract() {\n\t\t\treturn extractor.extract( bulkedWork, index );\n\t\t}\n\n\t\tprivate void failBecauseBulkFailed(Throwable throwable) {\n\t\t\tfail( log.elasticsearchFailedBecauseOfBulkFailure( throwable ) );\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "CompletionStage<Void> onSequenceComplete() {\n\t\t\treturn executionContext.executePendingRefreshes()\n\t\t\t\t\t.whenComplete( Futures.copyHandler( refreshFuture ) );\n\t\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder.SequenceContext#notifySequenceFailed\n methodBody: void notifySequenceFailed(Throwable throwable) {\nif(!(throwable instanceof PreviousWorkException)){throw Throwables.toRuntimeException(throwable);\n}}"
        ],
        "sourceCodeAfterRefactoring": "@Override\n\tpublic CompletableFuture<Void> build() {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = currentlyBuildingSequenceContext;\n\n\t\treturn Futures.whenCompleteExecute( currentlyBuildingSequenceTail, sequenceContext::onSequenceComplete )\n\t\t\t\t.exceptionally( Futures.handler( sequenceContext::onSequenceFailed ) );\n\t}\nCompletionStage<Void> onSequenceComplete() {\n\t\t\treturn executionContext.executePendingRefreshes()\n\t\t\t\t\t.whenComplete( Futures.copyHandler( refreshFuture ) );\n\t\t}",
        "diffSourceCode": "-  122: \n-  123: \t@Override\n-  124: \tpublic BulkResultExtractionStep addBulkResultExtraction(CompletableFuture<BulkResult> bulkResultFuture) {\n-  125: \t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n-  126: \t\tfinal SequenceContext currentSequenceAttributes = this.currentlyBuildingSequenceContext;\n-  127: \n-  128: \t\tCompletableFuture<BulkResultItemExtractor> extractorFuture =\n-  129: \t\t\t\tbulkResultFuture.thenApply( bulkResult -> bulkResult.withContext( currentSequenceAttributes.executionContext ) );\n-  134: \t@Override\n-  135: \tpublic CompletableFuture<Void> build() {\n-  136: \t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n-  137: \t\tfinal SequenceContext sequenceContext = currentlyBuildingSequenceContext;\n+  122: \t@Override\n+  123: \tpublic CompletableFuture<Void> build() {\n+  124: \t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n+  125: \t\tfinal SequenceContext sequenceContext = currentlyBuildingSequenceContext;\n+  126: \n+  127: \t\treturn Futures.whenCompleteExecute( currentlyBuildingSequenceTail, sequenceContext::onSequenceComplete )\n+  128: \t\t\t\t.exceptionally( Futures.handler( sequenceContext::onSequenceFailed ) );\n+  129: \t}\n+  134: \n+  135: \t\tvoid init(CompletableFuture<BulkResultItemExtractor> extractorFuture) {\n+  136: \t\t\tthis.extractorFuture = extractorFuture;\n+  137: \t\t}\n   138: \n-  139: \t\treturn Futures.whenCompleteExecute(\n-  140: \t\t\t\tcurrentlyBuildingSequenceTail,\n-  141: \t\t\t\t() -> sequenceContext.executionContext.executePendingRefreshes()\n-  142: \t\t\t\t\t\t.whenComplete( Futures.copyHandler( sequenceContext.refreshFuture ) )\n-  143: \t\t)\n-  144: \t\t\t\t.exceptionally( Futures.handler( t -> {\n-  145: \t\t\t\t\tsequenceContext.notifySequenceFailed( t );\n-  146: \t\t\t\t\treturn null;\n-  147: \t\t\t\t} ) );\n-  148: \t}\n-  201: \t\t\t\t\t.whenComplete( Futures.handler( (result, throwable) -> {\n-  202: \t\t\t\t\t\tif ( throwable == null ) {\n-  203: \t\t\t\t\t\t\treturn;\n-  204: \t\t\t\t\t\t}\n+  139: \t\t@Override\n+  140: \t\tpublic <T> CompletableFuture<T> add(BulkableElasticsearchWork<T> bulkedWork, int index) {\n+  141: \t\t\t// Use local variables to make sure the lambdas won't be affected by a reset()\n+  142: \t\t\tfinal SequenceContext sequenceContext = ElasticsearchDefaultWorkSequenceBuilder.this.currentlyBuildingSequenceContext;\n+  143: \n+  144: \t\t\tBulkedWorkExecutionState<T> workExecutionState =\n+  145: \t\t\t\t\tnew BulkedWorkExecutionState<>( sequenceContext, bulkedWork, index );\n+  146: \n+  147: \t\t\t// If the bulk work fails, make sure to notify the caller and failure handler as necessary.\n+  148: \t\t\tCompletableFuture<T> handledWorkExecutionFuture = extractorFuture\n+  201: \t\tCompletionStage<Void> onSequenceComplete() {\n+  202: \t\t\treturn executionContext.executePendingRefreshes()\n+  203: \t\t\t\t\t.whenComplete( Futures.copyHandler( refreshFuture ) );\n+  204: \t\t}\n",
        "uniqueId": "3ba4b03373611f27e258496ad8376ea8dc123642_134_148_201_204_122_129",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 13
            },
            "LINE": {
                "missed": 0,
                "covered": 3
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    },
    {
        "type": "Extract And Move Method",
        "description": "Extract And Move Method\tpackage onPreviousWorkSuccess(ignored Object) : CompletableFuture<R> extracted from public addNonBulkExecution(work ElasticsearchWork<T>) : CompletableFuture<T> in class org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder & moved to class org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder.NonBulkedWorkExecutionState",
        "diffLocations": [
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
                "startLine": 51,
                "endLine": 95,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
                "startLine": 52,
                "endLine": 83,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
                "startLine": 281,
                "endLine": 284,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "/**\n\t * Add a step to execute a new work.\n\t * <p>\n\t * A failure in the previous work will lead to the new work being marked as skipped,\n\t * and a failure during the new work will lead to the new work being marked\n\t * as failed.\n\t *\n\t * @param work The work to be executed\n\t */\n\t@Override\n\tpublic <T> CompletableFuture<T> addNonBulkExecution(ElasticsearchWork<T> work) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\t/*\n\t\t * Use a different future for the caller than the one used in the sequence,\n\t\t * because we manipulate internal exceptions in the sequence\n\t\t * that should not be exposed to the caller.\n \t\t */\n\t\tCompletableFuture<T> workFutureForCaller = new CompletableFuture<>();\n\n\t\t// If the previous work failed, then skip the new work and notify the caller and failure handler as necessary.\n\t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n\t\t\t\t.whenComplete( Futures.handler( (ignoredResult, throwable) -> {\n\t\t\t\t\tif ( throwable != null ) {\n\t\t\t\t\t\tsequenceContext.notifyWorkSkipped( work, throwable, workFutureForCaller );\n\t\t\t\t\t}\n\t\t\t\t} ) )\n\t\t\t\t// If the previous work completed normally, then execute the new work\n\t\t\t\t.thenCompose( Futures.safeComposer(\n\t\t\t\t\t\tignoredPreviousResult -> {\n\t\t\t\t\t\t\tCompletableFuture<T> workExecutionFuture = work.execute( sequenceContext.executionContext );\n\t\t\t\t\t\t\treturn addPostExecutionHandlers( work, workExecutionFuture, workFutureForCaller, sequenceContext );\n\t\t\t\t\t\t}\n\t\t\t\t) );\n\n\t\t/*\n\t\t * Make sure that the sequence will only advance to the next work\n\t\t * after both the work and *all* the handlers are executed,\n\t\t * because otherwise failureHandler.handle() could be called before all failed/skipped works are reported.\n\t\t */\n\t\tcurrentlyBuildingSequenceTail = handledWorkExecutionFuture;\n\n\t\treturn workFutureForCaller;\n\t}",
        "filePathBefore": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
        "isPureRefactoring": true,
        "commitId": "3ba4b03373611f27e258496ad8376ea8dc123642",
        "packageNameBefore": "org.hibernate.search.backend.elasticsearch.orchestration.impl",
        "classNameBefore": "org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder",
        "methodNameBefore": "org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#addNonBulkExecution",
        "invokedMethod": "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder.SequenceContext#notifyWorkSkipped\n methodBody: R> void notifyWorkSkipped(ElasticsearchWork<R> work, Throwable throwable,\n\t\t\t\tCompletableFuture<R> workFutureForCaller) {\nThrowable skippingCause=throwable instanceof PreviousWorkException ? throwable.getCause() : throwable;\nworkFutureForCaller.completeExceptionally(log.elasticsearchSkippedBecauseOfPreviousWork(skippingCause));\n}\nmethodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#addPostExecutionHandlers\n methodBody: T> CompletableFuture<T> addPostExecutionHandlers(ElasticsearchWork<T> work,\n\t\t\tCompletableFuture<T> workExecutionFuture, CompletableFuture<T> workFutureForCaller,\n\t\t\tSequenceContext sequenceContext) {\nworkExecutionFuture.thenCombine(sequenceContext.refreshFuture,(workResult,refreshResult) -> workResult).whenComplete(Futures.copyHandler(workFutureForCaller));\nreturn workExecutionFuture.exceptionally(Futures.handler(throwable -> {\n  sequenceContext.notifyWorkFailed(work,throwable,workFutureForCaller);\n  throw new PreviousWorkException(throwable);\n}\n));\n}",
        "classSignatureBefore": "class ElasticsearchDefaultWorkSequenceBuilder implements ElasticsearchWorkSequenceBuilder ",
        "methodNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#addNonBulkExecution"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder"
        ],
        "classSignatureBeforeSet": [
            "class ElasticsearchDefaultWorkSequenceBuilder implements ElasticsearchWorkSequenceBuilder "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Changes are within the Extract Method refactoring mechanics\nOverlapped refactoring - can be identical by undoing the overlapped refactoring\n- Remove Parameter-",
                "description": "Remove Parameter refactoring on top the extracted method - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.orchestration.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.function.Supplier;\n\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.backend.elasticsearch.work.impl.BulkableElasticsearchWork;\nimport org.hibernate.search.backend.elasticsearch.work.impl.ElasticsearchWork;\nimport org.hibernate.search.backend.elasticsearch.work.result.impl.BulkResult;\nimport org.hibernate.search.backend.elasticsearch.work.result.impl.BulkResultItemExtractor;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.common.impl.Throwables;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * A simple implementation of {@link ElasticsearchWorkSequenceBuilder}.\n * <p>\n * Works will be executed inside a sequence-scoped context (a {@link ElasticsearchRefreshableWorkExecutionContext}),\n * ultimately leading to a {@link ElasticsearchRefreshableWorkExecutionContext#executePendingRefreshes()}.\n */\nclass ElasticsearchDefaultWorkSequenceBuilder implements ElasticsearchWorkSequenceBuilder {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate final Supplier<ElasticsearchRefreshableWorkExecutionContext> contextSupplier;\n\tprivate final BulkResultExtractionStepImpl bulkResultExtractionStep = new BulkResultExtractionStepImpl();\n\n\tprivate CompletableFuture<?> currentlyBuildingSequenceTail;\n\tprivate SequenceContext currentlyBuildingSequenceContext;\n\n\tElasticsearchDefaultWorkSequenceBuilder(Supplier<ElasticsearchRefreshableWorkExecutionContext> contextSupplier) {\n\t\tthis.contextSupplier = contextSupplier;\n\t}\n\n\t@Override\n\tpublic void init(CompletableFuture<?> previous) {\n\t\t// We only use the previous stage to delay the execution of the sequence, but we ignore its result\n\t\tthis.currentlyBuildingSequenceTail = previous.handle( (ignoredResult, ignoredThrowable) -> null );\n\t\tthis.currentlyBuildingSequenceContext = new SequenceContext(\n\t\t\t\tcontextSupplier.get()\n\t\t);\n\t}\n\n\t/**\n\t * Add a step to execute a new work.\n\t * <p>\n\t * A failure in the previous work will lead to the new work being marked as skipped,\n\t * and a failure during the new work will lead to the new work being marked\n\t * as failed.\n\t *\n\t * @param work The work to be executed\n\t */\n\t@Override\n\tpublic <T> CompletableFuture<T> addNonBulkExecution(ElasticsearchWork<T> work) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\t/*\n\t\t * Use a different future for the caller than the one used in the sequence,\n\t\t * because we manipulate internal exceptions in the sequence\n\t\t * that should not be exposed to the caller.\n \t\t */\n\t\tCompletableFuture<T> workFutureForCaller = new CompletableFuture<>();\n\n\t\t// If the previous work failed, then skip the new work and notify the caller and failure handler as necessary.\n\t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n\t\t\t\t.whenComplete( Futures.handler( (ignoredResult, throwable) -> {\n\t\t\t\t\tif ( throwable != null ) {\n\t\t\t\t\t\tsequenceContext.notifyWorkSkipped( work, throwable, workFutureForCaller );\n\t\t\t\t\t}\n\t\t\t\t} ) )\n\t\t\t\t// If the previous work completed normally, then execute the new work\n\t\t\t\t.thenCompose( Futures.safeComposer(\n\t\t\t\t\t\tignoredPreviousResult -> {\n\t\t\t\t\t\t\tCompletableFuture<T> workExecutionFuture = work.execute( sequenceContext.executionContext );\n\t\t\t\t\t\t\treturn addPostExecutionHandlers( work, workExecutionFuture, workFutureForCaller, sequenceContext );\n\t\t\t\t\t\t}\n\t\t\t\t) );\n\n\t\t/*\n\t\t * Make sure that the sequence will only advance to the next work\n\t\t * after both the work and *all* the handlers are executed,\n\t\t * because otherwise failureHandler.handle() could be called before all failed/skipped works are reported.\n\t\t */\n\t\tcurrentlyBuildingSequenceTail = handledWorkExecutionFuture;\n\n\t\treturn workFutureForCaller;\n\t}\n\n\t/**\n\t * Add a step to execute a bulk work.\n\t * <p>\n\t * The bulk work won't be marked as skipped or failed, regardless of errors.\n\t * Only the bulked works will be marked (as skipped) if a previous work or the bulk work fails.\n\t *\n\t * @param workFuture The work to be executed\n\t */\n\t@Override\n\tpublic CompletableFuture<BulkResult> addBulkExecution(CompletableFuture<? extends ElasticsearchWork<BulkResult>> workFuture) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext currentSequenceAttributes = this.currentlyBuildingSequenceContext;\n\n\t\tCompletableFuture<BulkResult> bulkWorkResultFuture =\n\t\t\t\t// When the previous work completes successfully *and* the bulk work is available...\n\t\t\t\tcurrentlyBuildingSequenceTail.thenCombine( workFuture, (ignored, work) -> work )\n\t\t\t\t// ... execute the bulk work\n\t\t\t\t.thenCompose( work -> work.execute( currentSequenceAttributes.executionContext ) );\n\t\t// Do not propagate the exception as is: we expect the exception to be handled by each bulked work separately.\n\t\t// ... but still propagate *something*, in case a *previous* work failed.\n\t\tcurrentlyBuildingSequenceTail = bulkWorkResultFuture.exceptionally( Futures.handler( throwable -> {\n\t\t\tthrow new PreviousWorkException( throwable );\n\t\t} ) );\n\t\treturn bulkWorkResultFuture;\n\t}\n\n\t@Override\n\tpublic BulkResultExtractionStep addBulkResultExtraction(CompletableFuture<BulkResult> bulkResultFuture) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext currentSequenceAttributes = this.currentlyBuildingSequenceContext;\n\n\t\tCompletableFuture<BulkResultItemExtractor> extractorFuture =\n\t\t\t\tbulkResultFuture.thenApply( bulkResult -> bulkResult.withContext( currentSequenceAttributes.executionContext ) );\n\t\tbulkResultExtractionStep.init( extractorFuture );\n\t\treturn bulkResultExtractionStep;\n\t}\n\n\t@Override\n\tpublic CompletableFuture<Void> build() {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = currentlyBuildingSequenceContext;\n\n\t\treturn Futures.whenCompleteExecute(\n\t\t\t\tcurrentlyBuildingSequenceTail,\n\t\t\t\t() -> sequenceContext.executionContext.executePendingRefreshes()\n\t\t\t\t\t\t.whenComplete( Futures.copyHandler( sequenceContext.refreshFuture ) )\n\t\t)\n\t\t\t\t.exceptionally( Futures.handler( t -> {\n\t\t\t\t\tsequenceContext.notifySequenceFailed( t );\n\t\t\t\t\treturn null;\n\t\t\t\t} ) );\n\t}\n\n\t<T> CompletableFuture<T> addPostExecutionHandlers(ElasticsearchWork<T> work,\n\t\t\tCompletableFuture<T> workExecutionFuture, CompletableFuture<T> workFutureForCaller,\n\t\t\tSequenceContext sequenceContext) {\n\t\t/*\n\t\t * In case of success, wait for the refresh and propagate the result to the client.\n\t\t * We ABSOLUTELY DO NOT WANT the resulting future to be included in the sequence,\n\t\t * because it would create a deadlock:\n\t\t * future A will only complete when the refresh future (B) is executed,\n\t\t * which will only happen when the sequence ends,\n\t\t * which will only happen after A completes...\n\t\t */\n\t\tworkExecutionFuture.thenCombine( sequenceContext.refreshFuture, (workResult, refreshResult) -> workResult )\n\t\t\t\t.whenComplete( Futures.copyHandler( workFutureForCaller ) );\n\t\t/*\n\t\t * In case of error, propagate the exception immediately to both the failure handler and the client.\n\t\t *\n\t\t * Also, make sure to re-throw an exception\n\t\t * so that execution of following works in the sequence will be skipped.\n\t\t *\n\t\t * Make sure to return the resulting stage, and not executedWorkStage,\n\t\t * so that exception handling happens before the end of the sequence,\n\t\t * meaning notifyWorkFailed() is guaranteed to be called before notifySequenceFailed().\n\t\t */\n\t\treturn workExecutionFuture.exceptionally( Futures.handler( throwable -> {\n\t\t\tsequenceContext.notifyWorkFailed( work, throwable, workFutureForCaller );\n\t\t\tthrow new PreviousWorkException( throwable );\n\t\t} ) );\n\t}\n\n\tprivate final class BulkResultExtractionStepImpl implements BulkResultExtractionStep {\n\n\t\tprivate CompletableFuture<BulkResultItemExtractor> extractorFuture;\n\n\t\tvoid init(CompletableFuture<BulkResultItemExtractor> extractorFuture) {\n\t\t\tthis.extractorFuture = extractorFuture;\n\t\t}\n\n\t\t@Override\n\t\tpublic <T> CompletableFuture<T> add(BulkableElasticsearchWork<T> bulkedWork, int index) {\n\t\t\t// Use local variables to make sure the lambdas won't be affected by a reset()\n\t\t\tfinal SequenceContext sequenceContext = ElasticsearchDefaultWorkSequenceBuilder.this.currentlyBuildingSequenceContext;\n\n\t\t\t/*\n\t\t\t * Use a different future for the caller than the one used in the sequence,\n\t\t\t * because we manipulate internal exceptions in the sequence\n\t\t\t * that should not be exposed to the caller.\n\t\t\t */\n\t\t\tCompletableFuture<T> workFutureForCaller = new CompletableFuture<>();\n\n\t\t\t// If the bulk work fails, make sure to notify the caller and failure handler as necessary.\n\t\t\tCompletableFuture<T> handledWorkExecutionFuture = extractorFuture\n\t\t\t\t\t.whenComplete( Futures.handler( (result, throwable) -> {\n\t\t\t\t\t\tif ( throwable == null ) {\n\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse if ( throwable instanceof PreviousWorkException ) {\n\t\t\t\t\t\t\t// The bulk work itself was skipped; mark the bulked work as skipped too\n\t\t\t\t\t\t\tsequenceContext.notifyWorkSkipped( bulkedWork, throwable, workFutureForCaller );\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\t// The bulk work failed; mark the bulked work as failed too\n\t\t\t\t\t\t\tsequenceContext.notifyWorkFailedBecauseBulkFailed( bulkedWork, throwable, workFutureForCaller );\n\t\t\t\t\t\t}\n\t\t\t\t\t} ) )\n\t\t\t\t\t// If the bulk work succeeds, then extract the bulked work result and notify as necessary\n\t\t\t\t\t.thenCompose( extractor -> {\n\t\t\t\t\t\t// Use Futures.create to catch any exception thrown by extractor.extract\n\t\t\t\t\t\tCompletableFuture<T> workExecutionFuture = Futures.create(\n\t\t\t\t\t\t\t\t() -> extractor.extract( bulkedWork, index )\n\t\t\t\t\t\t);\n\t\t\t\t\t\treturn addPostExecutionHandlers( bulkedWork, workExecutionFuture, workFutureForCaller, sequenceContext );\n\t\t\t\t\t} );\n\n\t\t\t/*\n\t\t\t * Make sure that the sequence will only advance to the next work\n\t\t\t * after both the work and *all* the handlers are executed,\n\t\t\t * because otherwise failureHandler.handle(...) could be called before all failed/skipped works are reported.\n\t\t\t */\n\t\t\tcurrentlyBuildingSequenceTail = CompletableFuture.allOf(\n\t\t\t\t\tcurrentlyBuildingSequenceTail,\n\t\t\t\t\thandledWorkExecutionFuture\n\t\t\t);\n\n\t\t\treturn workFutureForCaller;\n\t\t}\n\n\t}\n\n\tprivate static final class PreviousWorkException extends RuntimeException {\n\n\t\tpublic PreviousWorkException(Throwable cause) {\n\t\t\tsuper( cause );\n\t\t}\n\n\t}\n\n\t/**\n\t * Regroups all objects that may be shared among multiple steps in the same sequence.\n\t * <p>\n\t * This was introduced to make references to data from a previous sequence less likely;\n\t * see\n\t * org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilderTest#intertwinedSequenceExecution()\n\t * for an example of what can go wrong if we don't take care to avoid that.\n\t */\n\tprivate static final class SequenceContext {\n\t\tprivate final ElasticsearchRefreshableWorkExecutionContext executionContext;\n\t\tprivate final CompletableFuture<Void> refreshFuture;\n\n\t\tSequenceContext(ElasticsearchRefreshableWorkExecutionContext executionContext) {\n\t\t\tthis.executionContext = executionContext;\n\t\t\tthis.refreshFuture = new CompletableFuture<>();\n\t\t}\n\n\t\t<R> void notifyWorkSkipped(ElasticsearchWork<R> work, Throwable throwable,\n\t\t\t\tCompletableFuture<R> workFutureForCaller) {\n\t\t\tThrowable skippingCause = throwable instanceof PreviousWorkException ? throwable.getCause() : throwable;\n\t\t\tworkFutureForCaller.completeExceptionally(\n\t\t\t\t\tlog.elasticsearchSkippedBecauseOfPreviousWork( skippingCause )\n\t\t\t);\n\t\t}\n\n\t\t<R> void notifyWorkFailedBecauseBulkFailed(BulkableElasticsearchWork<R> work, Throwable throwable,\n\t\t\t\tCompletableFuture<R> workFutureForCaller) {\n\t\t\tnotifyWorkFailed(\n\t\t\t\t\twork,\n\t\t\t\t\tlog.elasticsearchFailedBecauseOfBulkFailure( throwable ),\n\t\t\t\t\tworkFutureForCaller\n\t\t\t);\n\t\t}\n\n\t\t<R> void notifyWorkFailed(ElasticsearchWork<R> work, Throwable throwable,\n\t\t\t\tCompletableFuture<R> workFutureForCaller) {\n\t\t\tworkFutureForCaller.completeExceptionally( throwable );\n\t\t}\n\n\t\tvoid notifySequenceFailed(Throwable throwable) {\n\t\t\tif ( !(throwable instanceof PreviousWorkException) ) {\n\t\t\t\tthrow Throwables.toRuntimeException( throwable );\n\t\t\t}\n\t\t}\n\t}\n}\n",
        "filePathAfter": "backend/elasticsearch/src/main/java/org/hibernate/search/backend/elasticsearch/orchestration/impl/ElasticsearchDefaultWorkSequenceBuilder.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.backend.elasticsearch.orchestration.impl;\n\nimport java.lang.invoke.MethodHandles;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\nimport java.util.function.Supplier;\n\nimport org.hibernate.search.backend.elasticsearch.logging.impl.Log;\nimport org.hibernate.search.backend.elasticsearch.work.impl.BulkableElasticsearchWork;\nimport org.hibernate.search.backend.elasticsearch.work.impl.ElasticsearchWork;\nimport org.hibernate.search.backend.elasticsearch.work.result.impl.BulkResult;\nimport org.hibernate.search.backend.elasticsearch.work.result.impl.BulkResultItemExtractor;\nimport org.hibernate.search.util.common.impl.Futures;\nimport org.hibernate.search.util.common.impl.Throwables;\nimport org.hibernate.search.util.common.logging.impl.LoggerFactory;\n\n/**\n * A simple implementation of {@link ElasticsearchWorkSequenceBuilder}.\n * <p>\n * Works will be executed inside a sequence-scoped context (a {@link ElasticsearchRefreshableWorkExecutionContext}),\n * ultimately leading to a {@link ElasticsearchRefreshableWorkExecutionContext#executePendingRefreshes()}.\n */\nclass ElasticsearchDefaultWorkSequenceBuilder implements ElasticsearchWorkSequenceBuilder {\n\n\tprivate static final Log log = LoggerFactory.make( Log.class, MethodHandles.lookup() );\n\n\tprivate final Supplier<ElasticsearchRefreshableWorkExecutionContext> contextSupplier;\n\tprivate final BulkResultExtractionStepImpl bulkResultExtractionStep = new BulkResultExtractionStepImpl();\n\n\tprivate CompletableFuture<?> currentlyBuildingSequenceTail;\n\tprivate SequenceContext currentlyBuildingSequenceContext;\n\n\tElasticsearchDefaultWorkSequenceBuilder(Supplier<ElasticsearchRefreshableWorkExecutionContext> contextSupplier) {\n\t\tthis.contextSupplier = contextSupplier;\n\t}\n\n\t@Override\n\tpublic void init(CompletableFuture<?> previous) {\n\t\t// We only use the previous stage to delay the execution of the sequence, but we ignore its result\n\t\tthis.currentlyBuildingSequenceTail = previous.handle( (ignoredResult, ignoredThrowable) -> null );\n\t\tthis.currentlyBuildingSequenceContext = new SequenceContext(\n\t\t\t\tcontextSupplier.get()\n\t\t);\n\t}\n\n\t/**\n\t * Add a step to execute a new work.\n\t * <p>\n\t * A failure in the previous work will lead to the new work being marked as skipped,\n\t * and a failure during the new work will lead to the new work being marked\n\t * as failed.\n\t *\n\t * @param work The work to be executed\n\t */\n\t@Override\n\tpublic <T> CompletableFuture<T> addNonBulkExecution(ElasticsearchWork<T> work) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tNonBulkedWorkExecutionState<T> workExecutionState =\n\t\t\t\tnew NonBulkedWorkExecutionState<>( sequenceContext, work );\n\n\t\t// If the previous work failed, then skip the new work and notify the caller and failure handler as necessary.\n\t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n\t\t\t\t.whenComplete( Futures.handler( workExecutionState::onPreviousWorkComplete ) )\n\t\t\t\t// If the previous work completed normally, then execute the new work\n\t\t\t\t.thenCompose( Futures.safeComposer( workExecutionState::onPreviousWorkSuccess ) );\n\n\t\t/*\n\t\t * Make sure that the sequence will only advance to the next work\n\t\t * after both the work and *all* the handlers are executed,\n\t\t * because otherwise failureHandler.handle() could be called before all failed/skipped works are reported.\n\t\t */\n\t\tcurrentlyBuildingSequenceTail = handledWorkExecutionFuture;\n\n\t\treturn workExecutionState.workFutureForCaller;\n\t}\n\n\t/**\n\t * Add a step to execute a bulk work.\n\t * <p>\n\t * The bulk work won't be marked as skipped or failed, regardless of errors.\n\t * Only the bulked works will be marked (as skipped) if a previous work or the bulk work fails.\n\t *\n\t * @param workFuture The work to be executed\n\t */\n\t@Override\n\tpublic CompletableFuture<BulkResult> addBulkExecution(CompletableFuture<? extends ElasticsearchWork<BulkResult>> workFuture) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext currentSequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tCompletableFuture<BulkResult> bulkWorkResultFuture =\n\t\t\t\t// When the previous work completes successfully *and* the bulk work is available...\n\t\t\t\tcurrentlyBuildingSequenceTail.thenCombine( workFuture, (ignored, work) -> work )\n\t\t\t\t// ... execute the bulk work\n\t\t\t\t.thenCompose( currentSequenceContext::execute );\n\t\t// Do not propagate the exception as is: we expect the exception to be handled by each bulked work separately.\n\t\t// ... but still propagate *something*, in case a *previous* work failed.\n\t\tcurrentlyBuildingSequenceTail = bulkWorkResultFuture.exceptionally( Futures.handler( throwable -> {\n\t\t\tthrow new PreviousWorkException( throwable );\n\t\t} ) );\n\t\treturn bulkWorkResultFuture;\n\t}\n\n\t@Override\n\tpublic BulkResultExtractionStep addBulkResultExtraction(CompletableFuture<BulkResult> bulkResultFuture) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext currentSequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tCompletableFuture<BulkResultItemExtractor> extractorFuture =\n\t\t\t\tbulkResultFuture.thenApply( currentSequenceContext::addContext );\n\t\tbulkResultExtractionStep.init( extractorFuture );\n\t\treturn bulkResultExtractionStep;\n\t}\n\n\t@Override\n\tpublic CompletableFuture<Void> build() {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = currentlyBuildingSequenceContext;\n\n\t\treturn Futures.whenCompleteExecute( currentlyBuildingSequenceTail, sequenceContext::onSequenceComplete )\n\t\t\t\t.exceptionally( Futures.handler( sequenceContext::onSequenceFailed ) );\n\t}\n\n\tprivate final class BulkResultExtractionStepImpl implements BulkResultExtractionStep {\n\n\t\tprivate CompletableFuture<BulkResultItemExtractor> extractorFuture;\n\n\t\tvoid init(CompletableFuture<BulkResultItemExtractor> extractorFuture) {\n\t\t\tthis.extractorFuture = extractorFuture;\n\t\t}\n\n\t\t@Override\n\t\tpublic <T> CompletableFuture<T> add(BulkableElasticsearchWork<T> bulkedWork, int index) {\n\t\t\t// Use local variables to make sure the lambdas won't be affected by a reset()\n\t\t\tfinal SequenceContext sequenceContext = ElasticsearchDefaultWorkSequenceBuilder.this.currentlyBuildingSequenceContext;\n\n\t\t\tBulkedWorkExecutionState<T> workExecutionState =\n\t\t\t\t\tnew BulkedWorkExecutionState<>( sequenceContext, bulkedWork, index );\n\n\t\t\t// If the bulk work fails, make sure to notify the caller and failure handler as necessary.\n\t\t\tCompletableFuture<T> handledWorkExecutionFuture = extractorFuture\n\t\t\t\t\t.whenComplete( Futures.handler( workExecutionState::onBulkWorkComplete ) )\n\t\t\t\t\t// If the bulk work succeeds, then extract the bulked work result and notify as necessary\n\t\t\t\t\t.thenCompose( workExecutionState::onBulkWorkSuccess );\n\n\t\t\t/*\n\t\t\t * Make sure that the sequence will only advance to the next work\n\t\t\t * after both the work and *all* the handlers are executed,\n\t\t\t * because otherwise failureHandler.handle(...) could be called before all failed/skipped works are reported.\n\t\t\t */\n\t\t\tcurrentlyBuildingSequenceTail = CompletableFuture.allOf(\n\t\t\t\t\tcurrentlyBuildingSequenceTail,\n\t\t\t\t\thandledWorkExecutionFuture\n\t\t\t);\n\n\t\t\treturn workExecutionState.workFutureForCaller;\n\t\t}\n\n\t}\n\n\tprivate static final class PreviousWorkException extends RuntimeException {\n\n\t\tpublic PreviousWorkException(Throwable cause) {\n\t\t\tsuper( cause );\n\t\t}\n\n\t}\n\n\t/**\n\t * Regroups all objects that may be shared among multiple steps in the same sequence.\n\t * <p>\n\t * This was introduced to make references to data from a previous sequence less likely;\n\t * see\n\t * org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilderTest#intertwinedSequenceExecution()\n\t * for an example of what can go wrong if we don't take care to avoid that.\n\t */\n\tprivate static final class SequenceContext {\n\t\tprivate final ElasticsearchRefreshableWorkExecutionContext executionContext;\n\t\tprivate final CompletableFuture<Void> refreshFuture;\n\n\t\tSequenceContext(ElasticsearchRefreshableWorkExecutionContext executionContext) {\n\t\t\tthis.executionContext = executionContext;\n\t\t\tthis.refreshFuture = new CompletableFuture<>();\n\t\t}\n\n\t\t<T> CompletionStage<T> execute(ElasticsearchWork<T> work) {\n\t\t\treturn work.execute( executionContext );\n\t\t}\n\n\t\tpublic BulkResultItemExtractor addContext(BulkResult bulkResult) {\n\t\t\treturn bulkResult.withContext( executionContext );\n\t\t}\n\n\t\tCompletionStage<Void> onSequenceComplete() {\n\t\t\treturn executionContext.executePendingRefreshes()\n\t\t\t\t\t.whenComplete( Futures.copyHandler( refreshFuture ) );\n\t\t}\n\n\t\t<T> T onSequenceFailed(Throwable throwable) {\n\t\t\tif ( !(throwable instanceof PreviousWorkException) ) {\n\t\t\t\tthrow Throwables.toRuntimeException( throwable );\n\t\t\t}\n\t\t\treturn null;\n\t\t}\n\t}\n\n\tprivate abstract static class AbstractWorkExecutionState<T> {\n\n\t\tprotected final SequenceContext sequenceContext;\n\n\t\tprotected final ElasticsearchWork<T> work;\n\n\t\t/*\n\t\t * Use a different future for the caller than the one used in the sequence,\n\t\t * because we manipulate internal exceptions in the sequence\n\t\t * that should not be exposed to the caller.\n\t\t */\n\t\tfinal CompletableFuture<T> workFutureForCaller = new CompletableFuture<>();\n\n\t\tprivate AbstractWorkExecutionState(SequenceContext sequenceContext, ElasticsearchWork<T> work) {\n\t\t\tthis.sequenceContext = sequenceContext;\n\t\t\tthis.work = work;\n\t\t}\n\n\t\tprotected CompletableFuture<T> addPostExecutionHandlers(CompletableFuture<T> workExecutionFuture) {\n\t\t\t/*\n\t\t\t * In case of success, wait for the refresh and propagate the result to the client.\n\t\t\t * We ABSOLUTELY DO NOT WANT the resulting future to be included in the sequence,\n\t\t\t * because it would create a deadlock:\n\t\t\t * future A will only complete when the refresh future (B) is executed,\n\t\t\t * which will only happen when the sequence ends,\n\t\t\t * which will only happen after A completes...\n\t\t\t */\n\t\t\tworkExecutionFuture.thenCombine( sequenceContext.refreshFuture, (workResult, refreshResult) -> workResult )\n\t\t\t\t\t.whenComplete( Futures.copyHandler( workFutureForCaller ) );\n\t\t\t/*\n\t\t\t * In case of error, propagate the exception immediately to both the failure handler and the client.\n\t\t\t *\n\t\t\t * Also, make sure to re-throw an exception\n\t\t\t * so that execution of following works in the sequence will be skipped.\n\t\t\t *\n\t\t\t * Make sure to return the resulting stage, and not executedWorkStage,\n\t\t\t * so that exception handling happens before the end of the sequence,\n\t\t\t * meaning notifyWorkFailed() is guaranteed to be called before notifySequenceFailed().\n\t\t\t */\n\t\t\treturn workExecutionFuture.exceptionally( Futures.handler( this::fail ) );\n\t\t}\n\n\t\tprotected void skip(Throwable throwable) {\n\t\t\tThrowable skippingCause = throwable instanceof PreviousWorkException ? throwable.getCause() : throwable;\n\t\t\tworkFutureForCaller.completeExceptionally(\n\t\t\t\t\tlog.elasticsearchSkippedBecauseOfPreviousWork( skippingCause )\n\t\t\t);\n\t\t}\n\n\t\tprotected T fail(Throwable throwable) {\n\t\t\tworkFutureForCaller.completeExceptionally( throwable );\n\t\t\tthrow new PreviousWorkException( throwable );\n\t\t}\n\t}\n\n\tprivate static final class NonBulkedWorkExecutionState<R> extends AbstractWorkExecutionState<R> {\n\n\t\tprivate NonBulkedWorkExecutionState(SequenceContext sequenceContext, ElasticsearchWork<R> work) {\n\t\t\tsuper( sequenceContext, work );\n\t\t}\n\n\t\tvoid onPreviousWorkComplete(Object ignored, Throwable throwable) {\n\t\t\tif ( throwable != null ) {\n\t\t\t\tskip( throwable );\n\t\t\t}\n\t\t}\n\n\t\tCompletableFuture<R> onPreviousWorkSuccess(Object ignored) {\n\t\t\tCompletableFuture<R> workExecutionFuture = work.execute( sequenceContext.executionContext );\n\t\t\treturn addPostExecutionHandlers( workExecutionFuture );\n\t\t}\n\t}\n\n\tprivate static final class BulkedWorkExecutionState<R> extends AbstractWorkExecutionState<R> {\n\n\t\tprivate final BulkableElasticsearchWork<R> bulkedWork;\n\n\t\tprivate final int index;\n\n\t\tprivate BulkResultItemExtractor extractor;\n\n\t\tprivate BulkedWorkExecutionState(SequenceContext sequenceContext,\n\t\t\t\tBulkableElasticsearchWork<R> bulkedWork, int index) {\n\t\t\tsuper( sequenceContext, bulkedWork );\n\t\t\tthis.bulkedWork = bulkedWork;\n\t\t\tthis.index = index;\n\t\t}\n\n\t\tvoid onBulkWorkComplete(BulkResultItemExtractor ignored, Throwable throwable) {\n\t\t\tif ( throwable == null ) {\n\t\t\t\t// No failure: nothing to handle.\n\t\t\t\treturn;\n\t\t\t}\n\t\t\telse if ( throwable instanceof PreviousWorkException ) {\n\t\t\t\t// The bulk work itself was skipped; mark the bulked work as skipped too\n\t\t\t\tskip( throwable );\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// The bulk work failed; mark the bulked work as failed too\n\t\t\t\tfailBecauseBulkFailed( throwable );\n\t\t\t}\n\t\t}\n\n\t\tCompletableFuture<R> onBulkWorkSuccess(BulkResultItemExtractor extractor) {\n\t\t\tthis.extractor = extractor;\n\t\t\t// Use Futures.create to catch any exception thrown by extractor.extract\n\t\t\tCompletableFuture<R> workExecutionFuture = Futures.create( this::extract );\n\t\t\treturn addPostExecutionHandlers( workExecutionFuture );\n\t\t}\n\n\t\tprivate CompletableFuture<R> extract() {\n\t\t\treturn extractor.extract( bulkedWork, index );\n\t\t}\n\n\t\tprivate void failBecauseBulkFailed(Throwable throwable) {\n\t\t\tfail( log.elasticsearchFailedBecauseOfBulkFailure( throwable ) );\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [
            "CompletableFuture<R> onPreviousWorkSuccess(Object ignored) {\n\t\t\tCompletableFuture<R> workExecutionFuture = work.execute( sequenceContext.executionContext );\n\t\t\treturn addPostExecutionHandlers( workExecutionFuture );\n\t\t}"
        ],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder.SequenceContext#notifyWorkSkipped\n methodBody: R> void notifyWorkSkipped(ElasticsearchWork<R> work, Throwable throwable,\n\t\t\t\tCompletableFuture<R> workFutureForCaller) {\nThrowable skippingCause=throwable instanceof PreviousWorkException ? throwable.getCause() : throwable;\nworkFutureForCaller.completeExceptionally(log.elasticsearchSkippedBecauseOfPreviousWork(skippingCause));\n}",
            "methodSignature: org.hibernate.search.backend.elasticsearch.orchestration.impl.ElasticsearchDefaultWorkSequenceBuilder#addPostExecutionHandlers\n methodBody: T> CompletableFuture<T> addPostExecutionHandlers(ElasticsearchWork<T> work,\n\t\t\tCompletableFuture<T> workExecutionFuture, CompletableFuture<T> workFutureForCaller,\n\t\t\tSequenceContext sequenceContext) {\nworkExecutionFuture.thenCombine(sequenceContext.refreshFuture,(workResult,refreshResult) -> workResult).whenComplete(Futures.copyHandler(workFutureForCaller));\nreturn workExecutionFuture.exceptionally(Futures.handler(throwable -> {\n  sequenceContext.notifyWorkFailed(work,throwable,workFutureForCaller);\n  throw new PreviousWorkException(throwable);\n}\n));\n}"
        ],
        "sourceCodeAfterRefactoring": "/**\n\t * Add a step to execute a new work.\n\t * <p>\n\t * A failure in the previous work will lead to the new work being marked as skipped,\n\t * and a failure during the new work will lead to the new work being marked\n\t * as failed.\n\t *\n\t * @param work The work to be executed\n\t */\n\t@Override\n\tpublic <T> CompletableFuture<T> addNonBulkExecution(ElasticsearchWork<T> work) {\n\t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n\t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n\n\t\tNonBulkedWorkExecutionState<T> workExecutionState =\n\t\t\t\tnew NonBulkedWorkExecutionState<>( sequenceContext, work );\n\n\t\t// If the previous work failed, then skip the new work and notify the caller and failure handler as necessary.\n\t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n\t\t\t\t.whenComplete( Futures.handler( workExecutionState::onPreviousWorkComplete ) )\n\t\t\t\t// If the previous work completed normally, then execute the new work\n\t\t\t\t.thenCompose( Futures.safeComposer( workExecutionState::onPreviousWorkSuccess ) );\n\n\t\t/*\n\t\t * Make sure that the sequence will only advance to the next work\n\t\t * after both the work and *all* the handlers are executed,\n\t\t * because otherwise failureHandler.handle() could be called before all failed/skipped works are reported.\n\t\t */\n\t\tcurrentlyBuildingSequenceTail = handledWorkExecutionFuture;\n\n\t\treturn workExecutionState.workFutureForCaller;\n\t}\nCompletableFuture<R> onPreviousWorkSuccess(Object ignored) {\n\t\t\tCompletableFuture<R> workExecutionFuture = work.execute( sequenceContext.executionContext );\n\t\t\treturn addPostExecutionHandlers( workExecutionFuture );\n\t\t}",
        "diffSourceCode": "-   51: \t/**\n-   52: \t * Add a step to execute a new work.\n-   53: \t * <p>\n-   54: \t * A failure in the previous work will lead to the new work being marked as skipped,\n-   55: \t * and a failure during the new work will lead to the new work being marked\n-   56: \t * as failed.\n-   57: \t *\n-   58: \t * @param work The work to be executed\n-   59: \t */\n-   60: \t@Override\n-   61: \tpublic <T> CompletableFuture<T> addNonBulkExecution(ElasticsearchWork<T> work) {\n-   62: \t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n-   63: \t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n-   64: \n-   65: \t\t/*\n-   66: \t\t * Use a different future for the caller than the one used in the sequence,\n-   67: \t\t * because we manipulate internal exceptions in the sequence\n-   68: \t\t * that should not be exposed to the caller.\n-   69:  \t\t */\n-   70: \t\tCompletableFuture<T> workFutureForCaller = new CompletableFuture<>();\n-   71: \n-   72: \t\t// If the previous work failed, then skip the new work and notify the caller and failure handler as necessary.\n-   73: \t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n-   74: \t\t\t\t.whenComplete( Futures.handler( (ignoredResult, throwable) -> {\n-   75: \t\t\t\t\tif ( throwable != null ) {\n-   76: \t\t\t\t\t\tsequenceContext.notifyWorkSkipped( work, throwable, workFutureForCaller );\n-   77: \t\t\t\t\t}\n-   78: \t\t\t\t} ) )\n-   79: \t\t\t\t// If the previous work completed normally, then execute the new work\n-   80: \t\t\t\t.thenCompose( Futures.safeComposer(\n-   81: \t\t\t\t\t\tignoredPreviousResult -> {\n-   82: \t\t\t\t\t\t\tCompletableFuture<T> workExecutionFuture = work.execute( sequenceContext.executionContext );\n-   83: \t\t\t\t\t\t\treturn addPostExecutionHandlers( work, workExecutionFuture, workFutureForCaller, sequenceContext );\n-   84: \t\t\t\t\t\t}\n-   85: \t\t\t\t) );\n-   86: \n-   87: \t\t/*\n-   88: \t\t * Make sure that the sequence will only advance to the next work\n-   89: \t\t * after both the work and *all* the handlers are executed,\n-   90: \t\t * because otherwise failureHandler.handle() could be called before all failed/skipped works are reported.\n-   91: \t\t */\n-   92: \t\tcurrentlyBuildingSequenceTail = handledWorkExecutionFuture;\n-   93: \n-   94: \t\treturn workFutureForCaller;\n-   95: \t}\n-  281: \t\t\t\tCompletableFuture<R> workFutureForCaller) {\n-  282: \t\t\tworkFutureForCaller.completeExceptionally( throwable );\n-  283: \t\t}\n-  284: \n+   51: \n+   52: \t/**\n+   53: \t * Add a step to execute a new work.\n+   54: \t * <p>\n+   55: \t * A failure in the previous work will lead to the new work being marked as skipped,\n+   56: \t * and a failure during the new work will lead to the new work being marked\n+   57: \t * as failed.\n+   58: \t *\n+   59: \t * @param work The work to be executed\n+   60: \t */\n+   61: \t@Override\n+   62: \tpublic <T> CompletableFuture<T> addNonBulkExecution(ElasticsearchWork<T> work) {\n+   63: \t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n+   64: \t\tfinal SequenceContext sequenceContext = this.currentlyBuildingSequenceContext;\n+   65: \n+   66: \t\tNonBulkedWorkExecutionState<T> workExecutionState =\n+   67: \t\t\t\tnew NonBulkedWorkExecutionState<>( sequenceContext, work );\n+   68: \n+   69: \t\t// If the previous work failed, then skip the new work and notify the caller and failure handler as necessary.\n+   70: \t\tCompletableFuture<T> handledWorkExecutionFuture = currentlyBuildingSequenceTail\n+   71: \t\t\t\t.whenComplete( Futures.handler( workExecutionState::onPreviousWorkComplete ) )\n+   72: \t\t\t\t// If the previous work completed normally, then execute the new work\n+   73: \t\t\t\t.thenCompose( Futures.safeComposer( workExecutionState::onPreviousWorkSuccess ) );\n+   74: \n+   75: \t\t/*\n+   76: \t\t * Make sure that the sequence will only advance to the next work\n+   77: \t\t * after both the work and *all* the handlers are executed,\n+   78: \t\t * because otherwise failureHandler.handle() could be called before all failed/skipped works are reported.\n+   79: \t\t */\n+   80: \t\tcurrentlyBuildingSequenceTail = handledWorkExecutionFuture;\n+   81: \n+   82: \t\treturn workExecutionState.workFutureForCaller;\n+   83: \t}\n+   84: \n+   85: \t/**\n+   86: \t * Add a step to execute a bulk work.\n+   87: \t * <p>\n+   88: \t * The bulk work won't be marked as skipped or failed, regardless of errors.\n+   89: \t * Only the bulked works will be marked (as skipped) if a previous work or the bulk work fails.\n+   90: \t *\n+   91: \t * @param workFuture The work to be executed\n+   92: \t */\n+   93: \t@Override\n+   94: \tpublic CompletableFuture<BulkResult> addBulkExecution(CompletableFuture<? extends ElasticsearchWork<BulkResult>> workFuture) {\n+   95: \t\t// Use a local variable to make sure lambdas (if any) won't be affected by a reset()\n+  281: \t\tCompletableFuture<R> onPreviousWorkSuccess(Object ignored) {\n+  282: \t\t\tCompletableFuture<R> workExecutionFuture = work.execute( sequenceContext.executionContext );\n+  283: \t\t\treturn addPostExecutionHandlers( workExecutionFuture );\n+  284: \t\t}\n",
        "uniqueId": "3ba4b03373611f27e258496ad8376ea8dc123642_51_95_281_284_52_83",
        "moveFileExist": true,
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21,
        "testResult": true,
        "coverageInfo": {
            "INSTRUCTION": {
                "missed": 0,
                "covered": 33
            },
            "LINE": {
                "missed": 0,
                "covered": 7
            },
            "COMPLEXITY": {
                "missed": 0,
                "covered": 1
            },
            "METHOD": {
                "missed": 0,
                "covered": 1
            }
        }
    },
    {
        "type": "Inline Method",
        "description": "Inline Method\tprivate doMassIndexingWithBook2GetTitleFailure(sessionFactory SessionFactory) : void inlined to public getTitle() : void in class org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT",
        "diffLocations": [
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
                "startLine": 123,
                "endLine": 143,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
                "startLine": 142,
                "endLine": 181,
                "startColumn": 0,
                "endColumn": 0
            },
            {
                "filePath": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
                "startLine": 369,
                "endLine": 390,
                "startColumn": 0,
                "endColumn": 0
            }
        ],
        "sourceCodeBeforeRefactoring": "private void doMassIndexingWithBook2GetTitleFailure(SessionFactory sessionFactory) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\t}",
        "filePathBefore": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
        "isPureRefactoring": true,
        "commitId": "d876cc12f196d470e7db696de0f569d8ee39c49b",
        "packageNameBefore": "org.hibernate.search.integrationtest.mapper.orm.massindexing",
        "classNameBefore": "org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT",
        "methodNameBefore": "org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithBook2GetTitleFailure",
        "invokedMethod": "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithFailure\n methodBody: private void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\nBook.failOnBook2GetId.set(ExecutionExpectation.FAIL.equals(book2GetIdExpectation));\nBook.failOnBook2GetTitle.set(ExecutionExpectation.FAIL.equals(book2GetTitleExpectation));\nAssertionError assertionError=null;\ntryOrmUtils.withinSession(sessionFactory,session -> {\n  SearchSession searchSession=Search.session(session);\n  MassIndexer indexer=indexerProducer.apply(searchSession);\n  MassIndexingFailureHandler massIndexingFailureHandler=getMassIndexingFailureHandler();\n  if (massIndexingFailureHandler != null) {\n    indexer.failureHandler(massIndexingFailureHandler);\n  }\n  for (  Runnable expectationSetter : expectationSetters) {\n    expectationSetter.run();\n  }\n  Runnable runnable=() -> {\n    try {\n      indexer.startAndWait();\n    }\n catch (    InterruptedException e) {\n      fail(\"Unexpected InterruptedException: \" + e.getMessage());\n    }\n  }\n;\n  if (thrownExpectation == null) {\n    runnable.run();\n  }\n else {\n    SubTest.expectException(runnable).assertThrown().satisfies(thrownExpectation);\n  }\n}\n);\nbackendMock.verifyExpectationsMet();\ncatch(AssertionError e)assertionError=e;\nthrow e;\nfinallyBook.failOnBook2GetId.set(false);\nBook.failOnBook2GetTitle.set(false);\nif(assertionError == null){switch(threadExpectation)case CREATED_AND_TERMINATED:Awaitility.await().untilAsserted(() -> assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isNotEmpty().allSatisfy(t -> assertThat(t).extracting(Thread::getState).isEqualTo(Thread.State.TERMINATED)));\nbreak;\ncase NOT_CREATED:assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isEmpty();\nbreak;\n}}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexScopeWork\n methodBody: private Runnable expectIndexScopeWork(StubIndexScopeWork.Type type, ExecutionExpectation executionExpectation) {\nreturn () -> {\nswitch (executionExpectation) {\ncase SUCCEED:    backendMock.expectIndexScopeWorks(Book.NAME).indexScopeWork(type);\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(type.name() + \" failure\"));\nbackendMock.expectIndexScopeWorks(Book.NAME).indexScopeWork(type,failingFuture);\nbreak;\ncase SKIP:break;\n}\n}\n;\n}\nmethodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexingWorks\n methodBody: private Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\nreturn () -> {\nswitch (workTwoExecutionExpectation) {\ncase SUCCEED:    backendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"2\",b -> b.field(\"title\",TITLE_2).field(\"author\",AUTHOR_2)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(\"Indexing failure\"));\nbackendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\nbackendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"2\",b -> b.field(\"title\",TITLE_2).field(\"author\",AUTHOR_2)).processedThenExecuted(failingFuture);\nbreak;\ncase SKIP:backendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\nbreak;\n}\n}\n;\n}",
        "classSignatureBefore": "public abstract class AbstractMassIndexingFailureIT ",
        "methodNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithBook2GetTitleFailure"
        ],
        "classNameBeforeSet": [
            "org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT"
        ],
        "classSignatureBeforeSet": [
            "public abstract class AbstractMassIndexingFailureIT "
        ],
        "purityCheckResultList": [
            {
                "isPure": true,
                "purityComment": "Identical statements",
                "description": "There is no replacement! - all mapped",
                "mappingState": 1
            }
        ],
        "sourceCodeBeforeForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.massindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Fail.fail;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.cfg.EngineSettings;\nimport org.hibernate.search.engine.cfg.spi.EngineSpiSettings;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.automaticindexing.AutomaticIndexingStrategyName;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexer;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexingFailureHandler;\nimport org.hibernate.search.mapper.orm.session.SearchSession;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.ThreadSpy;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubIndexScopeWork;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils;\nimport org.hibernate.search.util.impl.test.SubTest;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.assertj.core.api.InstanceOfAssertFactories;\nimport org.awaitility.Awaitility;\n\npublic abstract class AbstractMassIndexingFailureIT {\n\n\tpublic static final String TITLE_1 = \"Oliver Twist\";\n\tpublic static final String AUTHOR_1 = \"Charles Dickens\";\n\tpublic static final String TITLE_2 = \"Ulysses\";\n\tpublic static final String AUTHOR_2 = \"James Joyce\";\n\tpublic static final String TITLE_3 = \"Frankenstein\";\n\tpublic static final String AUTHOR_3 = \"Mary Shelley\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock );\n\n\t@Rule\n\tpublic ThreadSpy threadSpy = new ThreadSpy();\n\n\t@Test\n\tpublic void indexing() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"Indexing failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getId() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getId failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithBook2GetIdFailure( sessionFactory );\n\n\t\tassertEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getTitle() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getTitle failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithBook2GetTitleFailure( sessionFactory );\n\n\t\tassertEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void purge() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"PURGE failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsBefore() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsAfter() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer().mergeSegmentsOnFinish( true ),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void flush() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"FLUSH failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void indexingAndFlush() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString failingEntityIndexingExceptionMessage = \"Indexing failure\";\n\t\tString failingEntityIndexingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\t\tString failingMassIndexerOperationExceptionMessage = \"FLUSH failure\";\n\t\tString failingMassIndexerOperationAsString = \"MassIndexer operation\";\n\n\t\texpectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( failingMassIndexerOperationExceptionMessage )\n\t\t\t\t\t\t// Indexing failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\t\tfailingEntityIndexingExceptionMessage\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\t}\n\n\tprotected abstract String getBackgroundFailureHandlerReference();\n\n\tprotected abstract MassIndexingFailureHandler getMassIndexingFailureHandler();\n\n\tprotected void assertBeforeSetup() {\n\t}\n\n\tprotected void assertAfterSetup() {\n\t}\n\n\tprotected abstract void expectEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectMassIndexerOperationFailureHandling(\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertMassIndexerOperationFailureHandling(\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprotected abstract void assertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tindexerProducer,\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithBook2GetIdFailure(SessionFactory sessionFactory) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithBook2GetTitleFailure(SessionFactory sessionFactory) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tBook.failOnBook2GetId.set( ExecutionExpectation.FAIL.equals( book2GetIdExpectation ) );\n\t\tBook.failOnBook2GetTitle.set( ExecutionExpectation.FAIL.equals( book2GetTitleExpectation ) );\n\t\tAssertionError assertionError = null;\n\t\ttry {\n\t\t\tOrmUtils.withinSession( sessionFactory, session -> {\n\t\t\t\tSearchSession searchSession = Search.session( session );\n\t\t\t\tMassIndexer indexer = indexerProducer.apply( searchSession );\n\n\t\t\t\tMassIndexingFailureHandler massIndexingFailureHandler = getMassIndexingFailureHandler();\n\t\t\t\tif ( massIndexingFailureHandler != null ) {\n\t\t\t\t\tindexer.failureHandler( massIndexingFailureHandler );\n\t\t\t\t}\n\n\t\t\t\tfor ( Runnable expectationSetter : expectationSetters ) {\n\t\t\t\t\texpectationSetter.run();\n\t\t\t\t}\n\n\t\t\t\t// TODO HSEARCH-3728 simplify this when even indexing exceptions are propagated\n\t\t\t\tRunnable runnable = () -> {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tindexer.startAndWait();\n\t\t\t\t\t}\n\t\t\t\t\tcatch (InterruptedException e) {\n\t\t\t\t\t\tfail( \"Unexpected InterruptedException: \" + e.getMessage() );\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t\tif ( thrownExpectation == null ) {\n\t\t\t\t\trunnable.run();\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tSubTest.expectException( runnable )\n\t\t\t\t\t\t\t.assertThrown()\n\t\t\t\t\t\t\t.satisfies( thrownExpectation );\n\t\t\t\t}\n\t\t\t} );\n\t\t\tbackendMock.verifyExpectationsMet();\n\t\t}\n\t\tcatch (AssertionError e) {\n\t\t\tassertionError = e;\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tBook.failOnBook2GetId.set( false );\n\t\t\tBook.failOnBook2GetTitle.set( false );\n\n\t\t\tif ( assertionError == null ) {\n\t\t\t\tswitch ( threadExpectation ) {\n\t\t\t\t\tcase CREATED_AND_TERMINATED:\n\t\t\t\t\t\tAwaitility.await().untilAsserted(\n\t\t\t\t\t\t\t\t() -> assertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t\t\t.isNotEmpty()\n\t\t\t\t\t\t\t\t\t\t.allSatisfy( t -> assertThat( t )\n\t\t\t\t\t\t\t\t\t\t\t\t.extracting( Thread::getState )\n\t\t\t\t\t\t\t\t\t\t\t\t.isEqualTo( Thread.State.TERMINATED )\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase NOT_CREATED:\n\t\t\t\t\t\tassertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t.isEmpty();\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate Runnable expectIndexScopeWork(StubIndexScopeWork.Type type, ExecutionExpectation executionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( executionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectIndexScopeWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScopeWork( type );\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\t\t\tbackendMock.expectIndexScopeWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScopeWork( type, failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( workTwoExecutionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( \"Indexing failure\" ) );\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted( failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate SessionFactory setup() {\n\t\tassertBeforeSetup();\n\n\t\tbackendMock.expectAnySchema( Book.NAME );\n\n\t\tSessionFactory sessionFactory = ormSetupHelper.start()\n\t\t\t\t.withPropertyRadical( HibernateOrmMapperSettings.Radicals.AUTOMATIC_INDEXING_STRATEGY, AutomaticIndexingStrategyName.NONE )\n\t\t\t\t.withPropertyRadical( EngineSettings.Radicals.BACKGROUND_FAILURE_HANDLER, getBackgroundFailureHandlerReference() )\n\t\t\t\t.withPropertyRadical( EngineSpiSettings.Radicals.THREAD_PROVIDER, threadSpy.getThreadProvider() )\n\t\t\t\t.setup( Book.class );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tOrmUtils.withinTransaction( sessionFactory, session -> {\n\t\t\tsession.persist( new Book( 1, TITLE_1, AUTHOR_1 ) );\n\t\t\tsession.persist( new Book( 2, TITLE_2, AUTHOR_2 ) );\n\t\t\tsession.persist( new Book( 3, TITLE_3, AUTHOR_3 ) );\n\t\t} );\n\n\t\tassertAfterSetup();\n\n\t\treturn sessionFactory;\n\t}\n\n\tprivate enum ExecutionExpectation {\n\t\tSUCCEED,\n\t\tFAIL,\n\t\tSKIP;\n\t}\n\n\tprivate enum ThreadExpectation {\n\t\tCREATED_AND_TERMINATED,\n\t\tNOT_CREATED;\n\t}\n\n\t@Entity(name = Book.NAME)\n\t@Indexed(index = Book.NAME)\n\tpublic static class Book {\n\n\t\tpublic static final String NAME = \"Book\";\n\n\t\tprivate static final AtomicBoolean failOnBook2GetId = new AtomicBoolean( false );\n\t\tprivate static final AtomicBoolean failOnBook2GetTitle = new AtomicBoolean( false );\n\n\t\tprivate Integer id;\n\n\t\tprivate String title;\n\n\t\tprivate String author;\n\n\t\tpublic Book() {\n\t\t}\n\n\t\tpublic Book(Integer id, String title, String author) {\n\t\t\tthis.id = id;\n\t\t\tthis.title = title;\n\t\t\tthis.author = author;\n\t\t}\n\n\t\t@Id // This must be on the getter, so that Hibernate Search uses getters instead of direct field access\n\t\tpublic Integer getId() {\n\t\t\tif ( id == 2 && failOnBook2GetId.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getId failure\" );\n\t\t\t}\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getTitle() {\n\t\t\tif ( id == 2 && failOnBook2GetTitle.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getTitle failure\" );\n\t\t\t}\n\t\t\treturn title;\n\t\t}\n\n\t\tpublic void setTitle(String title) {\n\t\t\tthis.title = title;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getAuthor() {\n\t\t\treturn author;\n\t\t}\n\n\t\tpublic void setAuthor(String author) {\n\t\t\tthis.author = author;\n\t\t}\n\t}\n\n\tprotected static class SimulatedFailure extends RuntimeException {\n\t\tSimulatedFailure(String message) {\n\t\t\tsuper( message );\n\t\t}\n\t}\n}\n",
        "filePathAfter": "integrationtest/mapper/orm/src/test/java/org/hibernate/search/integrationtest/mapper/orm/massindexing/AbstractMassIndexingFailureIT.java",
        "sourceCodeAfterForWhole": "/*\n * Hibernate Search, full-text search for your domain model\n *\n * License: GNU Lesser General Public License (LGPL), version 2.1 or later\n * See the lgpl.txt file in the root directory or <http://www.gnu.org/licenses/lgpl-2.1.html>.\n */\npackage org.hibernate.search.integrationtest.mapper.orm.massindexing;\n\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Fail.fail;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.function.Consumer;\nimport java.util.function.Function;\nimport javax.persistence.Entity;\nimport javax.persistence.Id;\n\nimport org.hibernate.SessionFactory;\nimport org.hibernate.search.engine.backend.work.execution.DocumentCommitStrategy;\nimport org.hibernate.search.engine.backend.work.execution.DocumentRefreshStrategy;\nimport org.hibernate.search.engine.cfg.EngineSettings;\nimport org.hibernate.search.engine.cfg.spi.EngineSpiSettings;\nimport org.hibernate.search.mapper.orm.Search;\nimport org.hibernate.search.mapper.orm.automaticindexing.AutomaticIndexingStrategyName;\nimport org.hibernate.search.mapper.orm.cfg.HibernateOrmMapperSettings;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexer;\nimport org.hibernate.search.mapper.orm.massindexing.MassIndexingFailureHandler;\nimport org.hibernate.search.mapper.orm.session.SearchSession;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.GenericField;\nimport org.hibernate.search.mapper.pojo.mapping.definition.annotation.Indexed;\nimport org.hibernate.search.util.common.SearchException;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.BackendMock;\nimport org.hibernate.search.util.impl.integrationtest.common.rule.ThreadSpy;\nimport org.hibernate.search.util.impl.integrationtest.common.stub.backend.index.StubIndexScopeWork;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmSetupHelper;\nimport org.hibernate.search.util.impl.integrationtest.mapper.orm.OrmUtils;\nimport org.hibernate.search.util.impl.test.SubTest;\n\nimport org.junit.Rule;\nimport org.junit.Test;\n\nimport org.assertj.core.api.InstanceOfAssertFactories;\nimport org.awaitility.Awaitility;\n\npublic abstract class AbstractMassIndexingFailureIT {\n\n\tpublic static final String TITLE_1 = \"Oliver Twist\";\n\tpublic static final String AUTHOR_1 = \"Charles Dickens\";\n\tpublic static final String TITLE_2 = \"Ulysses\";\n\tpublic static final String AUTHOR_2 = \"James Joyce\";\n\tpublic static final String TITLE_3 = \"Frankenstein\";\n\tpublic static final String AUTHOR_3 = \"Mary Shelley\";\n\n\t@Rule\n\tpublic BackendMock backendMock = new BackendMock( \"stubBackend\" );\n\n\t@Rule\n\tpublic OrmSetupHelper ormSetupHelper = OrmSetupHelper.withBackendMock( backendMock );\n\n\t@Rule\n\tpublic ThreadSpy threadSpy = new ThreadSpy();\n\n\t@Test\n\tpublic void indexing() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"Indexing failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\texceptionMessage\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityIndexingFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getId() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getId failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void getTitle() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getTitle failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}\n\n\t@Test\n\tpublic void purge() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"PURGE failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsBefore() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.NOT_CREATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void mergeSegmentsAfter() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer().mergeSegmentsOnFinish( true ),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void flush() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString exceptionMessage = \"FLUSH failure\";\n\t\tString failingOperationAsString = \"MassIndexer operation\";\n\n\t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n\t}\n\n\t@Test\n\tpublic void indexingAndFlush() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString failingEntityIndexingExceptionMessage = \"Indexing failure\";\n\t\tString failingEntityIndexingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\t\tString failingMassIndexerOperationExceptionMessage = \"FLUSH failure\";\n\t\tString failingMassIndexerOperationAsString = \"MassIndexer operation\";\n\n\t\texpectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t.hasMessageContaining( failingMassIndexerOperationExceptionMessage )\n\t\t\t\t\t\t// Indexing failure should also be mentioned as a suppressed exception\n\t\t\t\t\t\t.extracting( Throwable::getSuppressed ).asInstanceOf( InstanceOfAssertFactories.ARRAY )\n\t\t\t\t\t\t.anySatisfy( suppressed -> assertThat( suppressed ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\t\tfailingEntityIndexingExceptionMessage\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t.hasCauseInstanceOf( SimulatedFailure.class )\n\t\t\t\t\t\t),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.FAIL ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.FAIL )\n\t\t);\n\n\t\tassertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\tfailingEntityIndexingExceptionMessage, failingEntityIndexingOperationAsString,\n\t\t\t\tfailingMassIndexerOperationExceptionMessage, failingMassIndexerOperationAsString\n\t\t);\n\t}\n\n\tprotected abstract String getBackgroundFailureHandlerReference();\n\n\tprotected abstract MassIndexingFailureHandler getMassIndexingFailureHandler();\n\n\tprotected void assertBeforeSetup() {\n\t}\n\n\tprotected void assertAfterSetup() {\n\t}\n\n\tprotected abstract void expectEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityIndexingFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertEntityGetterFailureHandling(String entityName, String entityReferenceAsString,\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectMassIndexerOperationFailureHandling(\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void assertMassIndexerOperationFailureHandling(\n\t\t\tString exceptionMessage, String failingOperationAsString);\n\n\tprotected abstract void expectEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprotected abstract void assertEntityIndexingAndMassIndexerOperationFailureHandling(\n\t\t\tString entityName, String entityReferenceAsString,\n\t\t\tString failingEntityIndexingExceptionMessage, String failingEntityIndexingOperationAsString,\n\t\t\tString failingMassIndexerOperationExceptionMessage, String failingMassIndexerOperationAsString);\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tindexerProducer,\n\t\t\t\tthreadExpectation,\n\t\t\t\tthrownExpectation,\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n\t\t\t\texpectationSetters\n\t\t);\n\t}\n\n\tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\n\t\tBook.failOnBook2GetId.set( ExecutionExpectation.FAIL.equals( book2GetIdExpectation ) );\n\t\tBook.failOnBook2GetTitle.set( ExecutionExpectation.FAIL.equals( book2GetTitleExpectation ) );\n\t\tAssertionError assertionError = null;\n\t\ttry {\n\t\t\tOrmUtils.withinSession( sessionFactory, session -> {\n\t\t\t\tSearchSession searchSession = Search.session( session );\n\t\t\t\tMassIndexer indexer = indexerProducer.apply( searchSession );\n\n\t\t\t\tMassIndexingFailureHandler massIndexingFailureHandler = getMassIndexingFailureHandler();\n\t\t\t\tif ( massIndexingFailureHandler != null ) {\n\t\t\t\t\tindexer.failureHandler( massIndexingFailureHandler );\n\t\t\t\t}\n\n\t\t\t\tfor ( Runnable expectationSetter : expectationSetters ) {\n\t\t\t\t\texpectationSetter.run();\n\t\t\t\t}\n\n\t\t\t\t// TODO HSEARCH-3728 simplify this when even indexing exceptions are propagated\n\t\t\t\tRunnable runnable = () -> {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tindexer.startAndWait();\n\t\t\t\t\t}\n\t\t\t\t\tcatch (InterruptedException e) {\n\t\t\t\t\t\tfail( \"Unexpected InterruptedException: \" + e.getMessage() );\n\t\t\t\t\t}\n\t\t\t\t};\n\t\t\t\tif ( thrownExpectation == null ) {\n\t\t\t\t\trunnable.run();\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tSubTest.expectException( runnable )\n\t\t\t\t\t\t\t.assertThrown()\n\t\t\t\t\t\t\t.satisfies( thrownExpectation );\n\t\t\t\t}\n\t\t\t} );\n\t\t\tbackendMock.verifyExpectationsMet();\n\t\t}\n\t\tcatch (AssertionError e) {\n\t\t\tassertionError = e;\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tBook.failOnBook2GetId.set( false );\n\t\t\tBook.failOnBook2GetTitle.set( false );\n\n\t\t\tif ( assertionError == null ) {\n\t\t\t\tswitch ( threadExpectation ) {\n\t\t\t\t\tcase CREATED_AND_TERMINATED:\n\t\t\t\t\t\tAwaitility.await().untilAsserted(\n\t\t\t\t\t\t\t\t() -> assertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t\t\t.isNotEmpty()\n\t\t\t\t\t\t\t\t\t\t.allSatisfy( t -> assertThat( t )\n\t\t\t\t\t\t\t\t\t\t\t\t.extracting( Thread::getState )\n\t\t\t\t\t\t\t\t\t\t\t\t.isEqualTo( Thread.State.TERMINATED )\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase NOT_CREATED:\n\t\t\t\t\t\tassertThat( threadSpy.getCreatedThreads( \"mass index\" ) )\n\t\t\t\t\t\t\t\t.as( \"Mass indexing threads\" )\n\t\t\t\t\t\t\t\t.isEmpty();\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tprivate Runnable expectIndexScopeWork(StubIndexScopeWork.Type type, ExecutionExpectation executionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( executionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectIndexScopeWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScopeWork( type );\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( type.name() + \" failure\" ) );\n\t\t\t\t\tbackendMock.expectIndexScopeWorks( Book.NAME )\n\t\t\t\t\t\t\t.indexScopeWork( type, failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\n\t\treturn () -> {\n\t\t\tswitch ( workTwoExecutionExpectation ) {\n\t\t\t\tcase SUCCEED:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t\tcase FAIL:\n\t\t\t\t\tCompletableFuture<?> failingFuture = new CompletableFuture<>();\n\t\t\t\t\tfailingFuture.completeExceptionally( new SimulatedFailure( \"Indexing failure\" ) );\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"2\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_2 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_2 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted( failingFuture );\n\t\t\t\t\tbreak;\n\t\t\t\tcase SKIP:\n\t\t\t\t\tbackendMock.expectWorksAnyOrder(\n\t\t\t\t\t\t\tBook.NAME, DocumentCommitStrategy.NONE, DocumentRefreshStrategy.NONE\n\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"1\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_1 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_1 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.add( \"3\", b -> b\n\t\t\t\t\t\t\t\t\t.field( \"title\", TITLE_3 )\n\t\t\t\t\t\t\t\t\t.field( \"author\", AUTHOR_3 )\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t.processedThenExecuted();\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t};\n\t}\n\n\tprivate SessionFactory setup() {\n\t\tassertBeforeSetup();\n\n\t\tbackendMock.expectAnySchema( Book.NAME );\n\n\t\tSessionFactory sessionFactory = ormSetupHelper.start()\n\t\t\t\t.withPropertyRadical( HibernateOrmMapperSettings.Radicals.AUTOMATIC_INDEXING_STRATEGY, AutomaticIndexingStrategyName.NONE )\n\t\t\t\t.withPropertyRadical( EngineSettings.Radicals.BACKGROUND_FAILURE_HANDLER, getBackgroundFailureHandlerReference() )\n\t\t\t\t.withPropertyRadical( EngineSpiSettings.Radicals.THREAD_PROVIDER, threadSpy.getThreadProvider() )\n\t\t\t\t.setup( Book.class );\n\n\t\tbackendMock.verifyExpectationsMet();\n\n\t\tOrmUtils.withinTransaction( sessionFactory, session -> {\n\t\t\tsession.persist( new Book( 1, TITLE_1, AUTHOR_1 ) );\n\t\t\tsession.persist( new Book( 2, TITLE_2, AUTHOR_2 ) );\n\t\t\tsession.persist( new Book( 3, TITLE_3, AUTHOR_3 ) );\n\t\t} );\n\n\t\tassertAfterSetup();\n\n\t\treturn sessionFactory;\n\t}\n\n\tprivate enum ExecutionExpectation {\n\t\tSUCCEED,\n\t\tFAIL,\n\t\tSKIP;\n\t}\n\n\tprivate enum ThreadExpectation {\n\t\tCREATED_AND_TERMINATED,\n\t\tNOT_CREATED;\n\t}\n\n\t@Entity(name = Book.NAME)\n\t@Indexed(index = Book.NAME)\n\tpublic static class Book {\n\n\t\tpublic static final String NAME = \"Book\";\n\n\t\tprivate static final AtomicBoolean failOnBook2GetId = new AtomicBoolean( false );\n\t\tprivate static final AtomicBoolean failOnBook2GetTitle = new AtomicBoolean( false );\n\n\t\tprivate Integer id;\n\n\t\tprivate String title;\n\n\t\tprivate String author;\n\n\t\tpublic Book() {\n\t\t}\n\n\t\tpublic Book(Integer id, String title, String author) {\n\t\t\tthis.id = id;\n\t\t\tthis.title = title;\n\t\t\tthis.author = author;\n\t\t}\n\n\t\t@Id // This must be on the getter, so that Hibernate Search uses getters instead of direct field access\n\t\tpublic Integer getId() {\n\t\t\tif ( id == 2 && failOnBook2GetId.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getId failure\" );\n\t\t\t}\n\t\t\treturn id;\n\t\t}\n\n\t\tpublic void setId(Integer id) {\n\t\t\tthis.id = id;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getTitle() {\n\t\t\tif ( id == 2 && failOnBook2GetTitle.get() ) {\n\t\t\t\tthrow new SimulatedFailure( \"getTitle failure\" );\n\t\t\t}\n\t\t\treturn title;\n\t\t}\n\n\t\tpublic void setTitle(String title) {\n\t\t\tthis.title = title;\n\t\t}\n\n\t\t@GenericField\n\t\tpublic String getAuthor() {\n\t\t\treturn author;\n\t\t}\n\n\t\tpublic void setAuthor(String author) {\n\t\t\tthis.author = author;\n\t\t}\n\t}\n\n\tprotected static class SimulatedFailure extends RuntimeException {\n\t\tSimulatedFailure(String message) {\n\t\t\tsuper( message );\n\t\t}\n\t}\n}\n",
        "diffSourceCodeSet": [],
        "invokedMethodSet": [
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#doMassIndexingWithFailure\n methodBody: private void doMassIndexingWithFailure(SessionFactory sessionFactory,\n\t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n\t\t\tThreadExpectation threadExpectation,\n\t\t\tConsumer<Throwable> thrownExpectation,\n\t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n\t\t\tRunnable ... expectationSetters) {\nBook.failOnBook2GetId.set(ExecutionExpectation.FAIL.equals(book2GetIdExpectation));\nBook.failOnBook2GetTitle.set(ExecutionExpectation.FAIL.equals(book2GetTitleExpectation));\nAssertionError assertionError=null;\ntryOrmUtils.withinSession(sessionFactory,session -> {\n  SearchSession searchSession=Search.session(session);\n  MassIndexer indexer=indexerProducer.apply(searchSession);\n  MassIndexingFailureHandler massIndexingFailureHandler=getMassIndexingFailureHandler();\n  if (massIndexingFailureHandler != null) {\n    indexer.failureHandler(massIndexingFailureHandler);\n  }\n  for (  Runnable expectationSetter : expectationSetters) {\n    expectationSetter.run();\n  }\n  Runnable runnable=() -> {\n    try {\n      indexer.startAndWait();\n    }\n catch (    InterruptedException e) {\n      fail(\"Unexpected InterruptedException: \" + e.getMessage());\n    }\n  }\n;\n  if (thrownExpectation == null) {\n    runnable.run();\n  }\n else {\n    SubTest.expectException(runnable).assertThrown().satisfies(thrownExpectation);\n  }\n}\n);\nbackendMock.verifyExpectationsMet();\ncatch(AssertionError e)assertionError=e;\nthrow e;\nfinallyBook.failOnBook2GetId.set(false);\nBook.failOnBook2GetTitle.set(false);\nif(assertionError == null){switch(threadExpectation)case CREATED_AND_TERMINATED:Awaitility.await().untilAsserted(() -> assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isNotEmpty().allSatisfy(t -> assertThat(t).extracting(Thread::getState).isEqualTo(Thread.State.TERMINATED)));\nbreak;\ncase NOT_CREATED:assertThat(threadSpy.getCreatedThreads(\"mass index\")).as(\"Mass indexing threads\").isEmpty();\nbreak;\n}}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexScopeWork\n methodBody: private Runnable expectIndexScopeWork(StubIndexScopeWork.Type type, ExecutionExpectation executionExpectation) {\nreturn () -> {\nswitch (executionExpectation) {\ncase SUCCEED:    backendMock.expectIndexScopeWorks(Book.NAME).indexScopeWork(type);\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(type.name() + \" failure\"));\nbackendMock.expectIndexScopeWorks(Book.NAME).indexScopeWork(type,failingFuture);\nbreak;\ncase SKIP:break;\n}\n}\n;\n}",
            "methodSignature: org.hibernate.search.integrationtest.mapper.orm.massindexing.AbstractMassIndexingFailureIT#expectIndexingWorks\n methodBody: private Runnable expectIndexingWorks(ExecutionExpectation workTwoExecutionExpectation) {\nreturn () -> {\nswitch (workTwoExecutionExpectation) {\ncase SUCCEED:    backendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"2\",b -> b.field(\"title\",TITLE_2).field(\"author\",AUTHOR_2)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\n  break;\ncase FAIL:CompletableFuture<?> failingFuture=new CompletableFuture<>();\nfailingFuture.completeExceptionally(new SimulatedFailure(\"Indexing failure\"));\nbackendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\nbackendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"2\",b -> b.field(\"title\",TITLE_2).field(\"author\",AUTHOR_2)).processedThenExecuted(failingFuture);\nbreak;\ncase SKIP:backendMock.expectWorksAnyOrder(Book.NAME,DocumentCommitStrategy.NONE,DocumentRefreshStrategy.NONE).add(\"1\",b -> b.field(\"title\",TITLE_1).field(\"author\",AUTHOR_1)).add(\"3\",b -> b.field(\"title\",TITLE_3).field(\"author\",AUTHOR_3)).processedThenExecuted();\nbreak;\n}\n}\n;\n}"
        ],
        "sourceCodeAfterRefactoring": "@Test\n\tpublic void getTitle() {\n\t\tSessionFactory sessionFactory = setup();\n\n\t\tString entityName = Book.NAME;\n\t\tString entityReferenceAsString = Book.NAME + \"#2\";\n\t\tString exceptionMessage = \"getTitle failure\";\n\t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n\n\t\texpectEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\n\t\tdoMassIndexingWithFailure(\n\t\t\t\tsessionFactory,\n\t\t\t\tsearchSession -> searchSession.massIndexer(),\n\t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n\t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContainingAll(\n\t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n\t\t\t\t\t\t\t\t\"See the logs for details.\",\n\t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n\t\t\t\t\t\t\t\t\"Exception while invoking\"\n\t\t\t\t\t\t)\n\t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n\t\t\t\t\t\t.isInstanceOf( SearchException.class )\n\t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n\t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n\t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n\t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n\t\t);\n\n\t\tassertEntityGetterFailureHandling(\n\t\t\t\tentityName, entityReferenceAsString,\n\t\t\t\texceptionMessage, failingOperationAsString\n\t\t);\n\t}",
        "diffSourceCode": "-  123: \t@Test\n-  124: \tpublic void getTitle() {\n-  125: \t\tSessionFactory sessionFactory = setup();\n-  126: \n-  127: \t\tString entityName = Book.NAME;\n-  128: \t\tString entityReferenceAsString = Book.NAME + \"#2\";\n-  129: \t\tString exceptionMessage = \"getTitle failure\";\n-  130: \t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n-  131: \n-  132: \t\texpectEntityGetterFailureHandling(\n-  133: \t\t\t\tentityName, entityReferenceAsString,\n-  134: \t\t\t\texceptionMessage, failingOperationAsString\n-  135: \t\t);\n-  136: \n-  137: \t\tdoMassIndexingWithBook2GetTitleFailure( sessionFactory );\n-  138: \n-  139: \t\tassertEntityGetterFailureHandling(\n-  140: \t\t\t\tentityName, entityReferenceAsString,\n-  141: \t\t\t\texceptionMessage, failingOperationAsString\n-  142: \t\t);\n-  143: \t}\n-  144: \n-  145: \t@Test\n-  146: \tpublic void purge() {\n-  147: \t\tSessionFactory sessionFactory = setup();\n-  148: \n-  149: \t\tString exceptionMessage = \"PURGE failure\";\n-  150: \t\tString failingOperationAsString = \"MassIndexer operation\";\n-  151: \n-  152: \t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n-  153: \n-  154: \t\tdoMassIndexingWithFailure(\n-  155: \t\t\t\tsessionFactory,\n-  156: \t\t\t\tThreadExpectation.NOT_CREATED,\n-  157: \t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n-  158: \t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n-  159: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.FAIL )\n-  160: \t\t);\n-  161: \n-  162: \t\tassertMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n-  163: \t}\n-  164: \n-  165: \t@Test\n-  166: \tpublic void mergeSegmentsBefore() {\n-  167: \t\tSessionFactory sessionFactory = setup();\n-  168: \n-  169: \t\tString exceptionMessage = \"MERGE_SEGMENTS failure\";\n-  170: \t\tString failingOperationAsString = \"MassIndexer operation\";\n-  171: \n-  172: \t\texpectMassIndexerOperationFailureHandling( exceptionMessage, failingOperationAsString );\n-  173: \n-  174: \t\tdoMassIndexingWithFailure(\n-  175: \t\t\t\tsessionFactory,\n-  176: \t\t\t\tThreadExpectation.NOT_CREATED,\n-  177: \t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SimulatedFailure.class )\n-  178: \t\t\t\t\t\t.hasMessageContaining( exceptionMessage ),\n-  179: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n-  180: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.FAIL )\n-  181: \t\t);\n-  369: \tprivate void doMassIndexingWithBook2GetTitleFailure(SessionFactory sessionFactory) {\n-  370: \t\tdoMassIndexingWithFailure(\n-  371: \t\t\t\tsessionFactory,\n-  372: \t\t\t\tsearchSession -> searchSession.massIndexer(),\n-  373: \t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n-  374: \t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n-  375: \t\t\t\t\t\t.hasMessageContainingAll(\n-  376: \t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n-  377: \t\t\t\t\t\t\t\t\"See the logs for details.\",\n-  378: \t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n-  379: \t\t\t\t\t\t\t\t\"Exception while invoking\"\n-  380: \t\t\t\t\t\t)\n-  381: \t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n-  382: \t\t\t\t\t\t.isInstanceOf( SearchException.class )\n-  383: \t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n-  384: \t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n-  385: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n-  386: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n-  387: \t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n-  388: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n-  389: \t\t);\n-  390: \t}\n+  123: \t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n+  124: \t\t\t\t\t\t\t\t\"Exception while invoking\"\n+  125: \t\t\t\t\t\t)\n+  126: \t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n+  127: \t\t\t\t\t\t.isInstanceOf( SearchException.class )\n+  128: \t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n+  129: \t\t\t\tExecutionExpectation.FAIL, ExecutionExpectation.SKIP,\n+  130: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n+  131: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n+  132: \t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n+  133: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n+  134: \t\t);\n+  135: \n+  136: \t\tassertEntityGetterFailureHandling(\n+  137: \t\t\t\tentityName, entityReferenceAsString,\n+  138: \t\t\t\texceptionMessage, failingOperationAsString\n+  139: \t\t);\n+  140: \t}\n+  141: \n+  142: \t@Test\n+  143: \tpublic void getTitle() {\n+  144: \t\tSessionFactory sessionFactory = setup();\n+  145: \n+  146: \t\tString entityName = Book.NAME;\n+  147: \t\tString entityReferenceAsString = Book.NAME + \"#2\";\n+  148: \t\tString exceptionMessage = \"getTitle failure\";\n+  149: \t\tString failingOperationAsString = \"Indexing instance of entity '\" + entityName + \"' during mass indexing\";\n+  150: \n+  151: \t\texpectEntityGetterFailureHandling(\n+  152: \t\t\t\tentityName, entityReferenceAsString,\n+  153: \t\t\t\texceptionMessage, failingOperationAsString\n+  154: \t\t);\n+  155: \n+  156: \t\tdoMassIndexingWithFailure(\n+  157: \t\t\t\tsessionFactory,\n+  158: \t\t\t\tsearchSession -> searchSession.massIndexer(),\n+  159: \t\t\t\tThreadExpectation.CREATED_AND_TERMINATED,\n+  160: \t\t\t\tthrowable -> assertThat( throwable ).isInstanceOf( SearchException.class )\n+  161: \t\t\t\t\t\t.hasMessageContainingAll(\n+  162: \t\t\t\t\t\t\t\t\"1 entities could not be indexed\",\n+  163: \t\t\t\t\t\t\t\t\"See the logs for details.\",\n+  164: \t\t\t\t\t\t\t\t\"First failure on entity 'Book#2': \",\n+  165: \t\t\t\t\t\t\t\t\"Exception while invoking\"\n+  166: \t\t\t\t\t\t)\n+  167: \t\t\t\t\t\t.extracting( Throwable::getCause ).asInstanceOf( InstanceOfAssertFactories.THROWABLE )\n+  168: \t\t\t\t\t\t.isInstanceOf( SearchException.class )\n+  169: \t\t\t\t\t\t.hasMessageContaining( \"Exception while invoking\" ),\n+  170: \t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.FAIL,\n+  171: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.PURGE, ExecutionExpectation.SUCCEED ),\n+  172: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.MERGE_SEGMENTS, ExecutionExpectation.SUCCEED ),\n+  173: \t\t\t\texpectIndexingWorks( ExecutionExpectation.SKIP ),\n+  174: \t\t\t\texpectIndexScopeWork( StubIndexScopeWork.Type.FLUSH, ExecutionExpectation.SUCCEED )\n+  175: \t\t);\n+  176: \n+  177: \t\tassertEntityGetterFailureHandling(\n+  178: \t\t\t\tentityName, entityReferenceAsString,\n+  179: \t\t\t\texceptionMessage, failingOperationAsString\n+  180: \t\t);\n+  181: \t}\n+  369: \tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n+  370: \t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n+  371: \t\t\tThreadExpectation threadExpectation,\n+  372: \t\t\tConsumer<Throwable> thrownExpectation,\n+  373: \t\t\tRunnable ... expectationSetters) {\n+  374: \t\tdoMassIndexingWithFailure(\n+  375: \t\t\t\tsessionFactory,\n+  376: \t\t\t\tindexerProducer,\n+  377: \t\t\t\tthreadExpectation,\n+  378: \t\t\t\tthrownExpectation,\n+  379: \t\t\t\tExecutionExpectation.SUCCEED, ExecutionExpectation.SUCCEED,\n+  380: \t\t\t\texpectationSetters\n+  381: \t\t);\n+  382: \t}\n+  383: \n+  384: \tprivate void doMassIndexingWithFailure(SessionFactory sessionFactory,\n+  385: \t\t\tFunction<SearchSession, MassIndexer> indexerProducer,\n+  386: \t\t\tThreadExpectation threadExpectation,\n+  387: \t\t\tConsumer<Throwable> thrownExpectation,\n+  388: \t\t\tExecutionExpectation book2GetIdExpectation, ExecutionExpectation book2GetTitleExpectation,\n+  389: \t\t\tRunnable ... expectationSetters) {\n+  390: \t\tBook.failOnBook2GetId.set( ExecutionExpectation.FAIL.equals( book2GetIdExpectation ) );\n",
        "uniqueId": "d876cc12f196d470e7db696de0f569d8ee39c49b_123_143__142_181_369_390",
        "moveFileExist": true,
        "testResult": true,
        "coverageInfo": {
            "testMethod": {
                "missed": 0,
                "covered": 1
            }
        },
        "compileResultBefore": true,
        "compileResultCurrent": true,
        "compileJDK": 21
    }
]